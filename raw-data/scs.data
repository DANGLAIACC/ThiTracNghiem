5789440
~~~
{"count": 35, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70394746, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to secure the objects in S3 using server-side encryption, subject to the constraint that the key material must be generated and stored in a certified FIPS 140-2 Level 3 machine. In addition, the key material must be available in multiple Regions. The size of objects in S3 ranges from 15 KB to 5 MB.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage an AWS KMS custom key store backed by AWS CloudHSM clusters. Copy backups across Regions</strong></p>\n\n<p>You can use AWS Key Management Service (KMS) to create and control the cryptographic keys that are used to protect your data on AWS. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>By default, AWS KMS creates the key material for a KMS key. However, you can import your own key material into a KMS key, or use a custom key store to create KMS keys that use key material in your AWS CloudHSM cluster or key material in an external key manager that you own and manage outside of AWS.</p>\n\n<p>A key store is a secure location for storing cryptographic keys. The default key store in AWS KMS also supports methods for generating and managing the keys that its stores. By default, the cryptographic key material for the AWS KMS keys that you create in AWS KMS is generated in and protected by hardware security modules (HSMs) that are FIPS 140-2 validated cryptographic modules. A custom key store is a logical key store within AWS KMS that is backed by a key manager outside of AWS KMS that you own and manage.</p>\n\n<p>The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.</p>\n\n<p>AWS KMS supports two types of custom key stores:</p>\n\n<p>An AWS CloudHSM key store is an AWS KMS custom key store backed by an AWS CloudHSM cluster. AWS CloudHSM allows you to copy backups of your CloudHSM Cluster from one region to another for cross-region resilience, global workloads, and disaster recovery purposes. You can use the copied backup to create a clone of the original cluster in the new region. This simplifies the development of globally distributed or cross-region redundant workloads.</p>\n\n<p>An external key store is an AWS KMS custom key store backed by an external key manager outside of AWS that you own and control.</p>\n\n<p>AWS CloudHSM key stores:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html\">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage an AWS KMS customer managed key and store the key material in AWS with key replication enabled across Regions</strong> - Customer managed keys are KMS keys in your AWS account that you create, own, and manage. AWS KMS supports multi-Region keys, which let you encrypt data in one AWS Region and decrypt it in a different AWS Region. However, the cryptographic key material for the AWS KMS keys that you create in AWS KMS is generated in and protected by hardware security modules (HSMs) that are only FIPS 140 - Level 2 compliant. Therefore, this option is incorrect.</p>\n\n<p><strong>Leverage an AWS KMS customer managed key backed by AWS CloudHSM clusters. Store the key material securely in Amazon S3 with cross-Region replication enabled</strong> - Customer managed keys are KMS keys in your AWS account that you create, own, and manage. You cannot store KMS key material in Amazon S3. In addition, AWS KMS customer managed key cannot be backed by an AWS CloudHSM cluster. This option has been added as a distractor.</p>\n\n<p><strong>Leverage AWS CloudHSM to generate the key material. Copy backups across Regions. Use AWS Encryption SDK to encrypt and decrypt the data</strong> - The use case states that the objects in S3 must be secured using server-side encryption. However, AWS Encryption SDK can only be used for client-side encryption, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html\">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-cloudhsm.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html\">https://docs.aws.amazon.com/kms/latest/developerguide/keystore-external.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html\">https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/copy-backup-to-region.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/copy-backup-to-region.html</a></p>\n", "answers": ["<p>Leverage an AWS KMS custom key store backed by AWS CloudHSM clusters. Copy backups across Regions</p>", "<p>Leverage an AWS KMS customer managed key and store the key material in AWS with key replication enabled across Regions</p>", "<p>Leverage an AWS KMS customer managed key backed by AWS CloudHSM clusters. Store the key material securely in Amazon S3 with cross-Region replication enabled</p>", "<p>Leverage AWS CloudHSM to generate the key material. Copy backups across Regions. Use AWS Encryption SDK to encrypt and decrypt the data</p>"]}, "correct_response": ["a"], "section": "Data Protection", "question_plain": "A company wants to secure the objects in S3 using server-side encryption, subject to the constraint that the key material must be generated and stored in a certified FIPS 140-2 Level 3 machine. In addition, the key material must be available in multiple Regions. The size of objects in S3 ranges from 15 KB to 5 MB.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 70394748, "assessment_type": "multiple-choice", "prompt": {"question": "<p>At XYZ Corporation, the IT team had recently discovered a security loophole that could potentially allow unauthorized access to sensitive data. To fix the issue and ensure the protection of their company's information, the team wants to establish the ability to delete an AWS KMS Customer Master Key (CMK) within a 24-hour timeframe. This would prevent the key from being used for encrypt or decrypt operations and keep their data secure.</p>\n\n<p>Which of the following solutions will address the given use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement the KMS import key function to perform an immediate delete operation</strong></p>\n\n<p>An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>You can use the KMS import key function to set an expiration time for the key material in AWS and to manually delete it, but to also make it available again in the future. You can delete the imported key material from a KMS key at any time. Also, when imported key material with an expiration date expires, AWS KMS deletes the key material. In either case, AWS KMS deletes the key material immediately, the key state of the KMS key changes to pending import, and the KMS key can't be used in any cryptographic operations. In contrast, scheduling key deletion requires a waiting period of 7 to 30 days, after which you cannot recover the deleted KMS key. So, this function can be used to delete a key within a 24-hour window, as per the requirement.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the manual key rotation feature within KMS to instantly create a new CMK</strong> - The manual key rotation feature within KMS is used to regularly rotate the key material associated with a CMK, but it does not delete the key immediately. It is a best practice to rotate keys periodically, but it would not address the requirement of being able to delete the CMK within a 24-hour window. So, this option is incorrect.</p>\n\n<p><strong>Utilize the scheduled key deletion feature in KMS to set the minimum wait time for deletion</strong> - The scheduled key deletion feature allows you to schedule the deletion of a CMK with a waiting period of 7 to 30 days, but it does not provide a way to delete the key immediately. So it does not allow the key to be deleted within a 24-hour window. Therefore, this option is incorrect.</p>\n\n<p><strong>Alter the KMS CMK alias to immediately stop any services from utilizing the CMK</strong> - An alias is a friendly name for a AWS KMS key. For example, an alias lets you refer to a KMS key as test-key instead of 1234abcd-12ab-34cd-56ef-1234567890ab. Because an alias is an independent resource, you can change the KMS key associated with an alias. For example, if the test-key alias is associated with one KMS key, you can use the UpdateAlias operation to associate it with a different KMS key. This is one of several ways to manually rotate a KMS key without changing its key material. You cannot update an alias in the AWS KMS console. Also, you cannot use UpdateAlias (or any other operation) to change an alias name. To alter an alias name, delete the current alias and then create a new alias for the KMS key. Even if you alter an alias, the underlying key still remains operational, so it does not address the requirement of preventing the key from being used for encrypt or decrypt operations.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/alias-manage.html#alias-update\">https://docs.aws.amazon.com/kms/latest/developerguide/alias-manage.html#alias-update</a></p>\n", "answers": ["<p>Use the manual key rotation feature within KMS to instantly create a new CMK</p>", "<p>Utilize the scheduled key deletion feature in KMS to set the minimum wait time for deletion</p>", "<p>Alter the KMS CMK alias to immediately stop any services from utilizing the CMK</p>", "<p>Implement the KMS import key function to perform an immediate delete operation</p>"]}, "correct_response": ["d"], "section": "Infrastructure Security", "question_plain": "At XYZ Corporation, the IT team had recently discovered a security loophole that could potentially allow unauthorized access to sensitive data. To fix the issue and ensure the protection of their company's information, the team wants to establish the ability to delete an AWS KMS Customer Master Key (CMK) within a 24-hour timeframe. This would prevent the key from being used for encrypt or decrypt operations and keep their data secure.\n\nWhich of the following solutions will address the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 70394750, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at an e-commerce company wants to ensure that none of the developers from the multiple IT teams can delete the AWS KMS keys. The security team does not want to impact the existing privileges of the developers.</p>\n\n<p>Which of the following represents the most operationally efficient solution?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Organizations to set a service control policy that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</strong></p>\n\n<p>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization\u2019s access control guidelines. SCPs are available only in an organization that has all features enabled.</p>\n\n<p>SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.</p>\n\n<p>You can use SCPs to allow or deny access to AWS services for individual AWS accounts with AWS Organizations member accounts, or for groups of accounts within an organizational unit (OU). The specified actions from an attached SCP affect all IAM identities including the root user of the member account.</p>\n\n<p>AWS services that aren't explicitly allowed by the SCPs associated with an AWS account or its parent OUs are denied access to the AWS accounts or OUs associated with the SCP. SCPs associated with an OU are inherited by all AWS accounts in that OU.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n\n<p>For the given use case, you can create the following sample SCP which is then associated with all AWS Organizations member accounts of the multiple IT teams.</p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Sid\": \"DenyKMSDelete\",\n        \"Effect\": \"Deny\",\n        \"Resource\": \"*\",\n        \"Action\": [\n            \"kms:ScheduleKeyDeletion\",\n            \"kms:Delete*\"\n        ]\n    }\n]\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new IAM user group specifically for developers and remove the \"kms:Delete<em>\" and <code>kms:ScheduleKeyDeletion</code> permissions from the group's policy</em></strong><em> - By creating a new IAM user group specifically for developers and removing the \"kms:Delete</em>\" and <code>kms:ScheduleKeyDeletion</code> permissions from the group's policy, the company can ensure that developers in that group cannot delete KMS keys. This can be done by creating a custom IAM policy that denies the \"kms:Delete*\" and <code>kms:ScheduleKeyDeletion</code> actions, and then attaching the policy to the developer group. However, this option is not operationally efficient, as every time a new developer user is created, then the user has to be associated with the IAM user group.</p>\n\n<p><strong>Use Amazon Inspector to monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions</strong> - Amazon Inspector automatically discovers workloads, such as Amazon EC2 instances, containers, and Lambda functions, and scans them for software vulnerabilities and unintended network exposure. It cannot monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions. This option has been added as a distractor.</p>\n\n<p><strong>Use AWS Config to set a rule that sends an alert when a <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls are made and automatically remediate these actions</strong> - While AWS Config can be used to set rules and send alerts, it cannot prevent specific API actions from being executed. AWS Config can be used to monitor the configuration of resources and alert when changes are made, but it cannot stop those changes from happening. You can apply remediation using AWS Systems Manager Automation documents which can be defined to meet your custom requirements. However, this option does not prevent the initial deletion of the KMS keys. This option provides remediation, not prevention.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/what-is-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/what-is-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/monitor-and-remediate-scheduled-deletion-of-aws-kms-keys.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/monitor-and-remediate-scheduled-deletion-of-aws-kms-keys.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n", "answers": ["<p>Create a new IAM user group specifically for developers and remove the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> permissions from the group's policy</p>", "<p>Use Amazon Inspector to monitor <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls and preempt these actions</p>", "<p>Use AWS Organizations to set a service control policy that denies the <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> actions for all accounts within the organization</p>", "<p>Use AWS Config to set a rule that sends an alert when a <code>kms:Delete*</code> and <code>kms:ScheduleKeyDeletion</code> API calls are made and automatically remediate these actions</p>"]}, "correct_response": ["c"], "section": "Identity and Access Management", "question_plain": "The security team at an e-commerce company wants to ensure that none of the developers from the multiple IT teams can delete the AWS KMS keys. The security team does not want to impact the existing privileges of the developers.\n\nWhich of the following represents the most operationally efficient solution?", "related_lectures": []}, {"_class": "assessment", "id": 70394752, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A rapidly growing e-commerce company stores all of its sensitive customer data in an Amazon S3 bucket. To ensure the safety and security of this data, the company has chosen to encrypt it using an AWS Key Management Service (AWS KMS) customer managed key. The company also uses AWS Lambda functions to perform various tasks within the same account as the S3 bucket. The Lambda functions need to access the data in the S3 bucket but the company must ensure that each Lambda function has its own programmatic access control permissions to use the KMS key.</p>\n\n<p>Which of the following options would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</strong></p>\n\n<p>A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide an execution role when you create a function. When you invoke your function, Lambda automatically provides your function with temporary credentials by assuming this role. You don't have to call sts:AssumeRole in your function code. For Lambda to properly assume your execution role, the role's trust policy must specify the Lambda service principal (lambda.amazonaws.com) as a trusted service.</p>\n\n<p>For the given use case, you need to create a Lambda execution role that provides specific access permissions to use the KMS key for each Lambda function. This is a more efficient solution as it allows for easier management of access permissions for multiple functions. This allows the company to define the permissions for each Lambda function, ensuring that each function only has the necessary access to the KMS key to perform its intended task.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Assign an IAM policy to each Lambda function that grants access to the KMS key</strong> - You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines its permissions. You cannot assign an IAM policy to a Lambda function, so this option is incorrect.</p>\n\n<p><strong>Create a key grant for the Lambda service principal, and adjust the permissions as needed</strong> - A grant is a policy instrument that allows AWS principals to use KMS keys in cryptographic operations. It also can let them view a KMS key (DescribeKey) and create and manage grants. When authorizing access to a KMS key, grants are considered along with key policies and IAM policies. Grants are often used for temporary permissions because you can create one, use its permissions, and delete it without changing your key policies or IAM policies.</p>\n\n<p>Grants are a very flexible and useful access control mechanism. When you create a grant for a KMS key, the grant allows the grantee principal to call the specified grant operations on the KMS key provided that all conditions specified in the grant are met.</p>\n\n<p>The grantee principal can be any AWS principal, including an AWS account (root), an IAM user, an IAM role, a federated role or user, or an assumed role user. The grantee principal can be in the same account as the KMS key or a different account. However, the grantee principal cannot be a service principal, an IAM group, or an AWS organization. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</strong> - This option is a distractor as it refers to an AWS-managed KMS key whereas the use case refers to the customer managed key. In addition, you should note that typically you would assume an IAM role within a Lambda function to access resources in another AWS account.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/grants.html\">https://docs.aws.amazon.com/kms/latest/developerguide/grants.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a></p>\n", "answers": ["<p>Assign an IAM policy to each Lambda function that grants access to the KMS key</p>", "<p>Establish a Lambda execution role that provides access to the KMS key for each Lambda function</p>", "<p>Create a key grant for the Lambda service principal, and adjust the permissions as needed</p>", "<p>Set up each Lambda function to assume an IAM role that grants access to the AWS-managed KMS key for Amazon S3</p>"]}, "correct_response": ["b"], "section": "Identity and Access Management", "question_plain": "A rapidly growing e-commerce company stores all of its sensitive customer data in an Amazon S3 bucket. To ensure the safety and security of this data, the company has chosen to encrypt it using an AWS Key Management Service (AWS KMS) customer managed key. The company also uses AWS Lambda functions to perform various tasks within the same account as the S3 bucket. The Lambda functions need to access the data in the S3 bucket but the company must ensure that each Lambda function has its own programmatic access control permissions to use the KMS key.\n\nWhich of the following options would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 70394754, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company is running an Amazon RDS for MySQL DB instance in a virtual private cloud (VPC) to store sensitive customer data. Due to strict security policies, the company has implemented a VPC that does not allow any network traffic to or from the internet. A security engineer at the company wants to use AWS Secrets Manager to automatically rotate the DB instance credentials for increased security. However, due to the company's security policy, the engineer is not allowed to use the standard AWS Lambda function provided by Secrets Manager to rotate the credentials.</p>\n\n<p>To address this issue, the security engineer deploys a custom Lambda function within the VPC. This function is responsible for rotating the secret in Secrets Manager. The security engineer also edits the DB instance's security group to allow connections from this custom Lambda function. However, when the function is invoked, it is unable to communicate with Secrets Manager and cannot rotate the secret.</p>\n\n<p>Which of the following options will address the given scenario?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function's subnet to use it</strong></p>\n\n<p>A VPC interface endpoint is a VPC component that enables the communication between resources in a VPC and services powered by AWS PrivateLink, without the need for an internet gateway, NAT device, VPN connection or AWS Direct Connect link. It allows for communication between the VPC and the service over an Amazon-provided private IP address, eliminating exposure to the public internet.</p>\n\n<p>AWS PrivateLink enables you to access services over an Amazon-provided IP address from within your VPC, without using public IPs or an internet gateway. With VPC interface endpoint, you can create a private connection between your VPC and supported services powered by AWS PrivateLink, using VPC endpoint services powered by AWS PrivateLink.</p>\n\n<p>A service provider creates an endpoint service to make their service available in a Region. A service consumer creates a VPC endpoint to connect their VPC to an endpoint service. A service consumer must specify the service name of the endpoint service when creating a VPC endpoint.</p>\n\n<p>How AWS PrivateLink works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p>Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p>This is the correct option as it allows the custom Lambda function in the VPC to communicate with Secrets Manager without going through the internet. A VPC endpoint for Secrets Manager is a VPC component that enables the communication between the VPC and Secrets Manager without going through the internet or a VPN connection.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</strong> - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your AWS account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources while the function is running. As mentioned in the explanation above, you can leverage the VPC endpoint to connect to Secrets Manager from within the VPC and thereby avoid the internet.</p>\n\n<p>Internet access from a private subnet requires network address translation (NAT). To give your Lambda function access to the internet, you need to route outbound traffic to a NAT gateway in a public subnet. If you configure the Lambda function to use the NAT Gateway to connect to the Secrets Manager, you will end up using the internet. Therefore, this option serves as a distractor.</p>\n\n<p><strong>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</strong> - AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router. This option is incorrect because Direct Connect is used to establish a dedicated network connection between an on-premises data center and a VPC, not between a VPC and a service like Secrets Manager.</p>\n\n<p><strong>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</strong> - This option is incorrect because VPC Peering is used to connect two VPCs together, not a VPC and a service. Secrets Manager is not a VPC, it's a service, therefore it cannot be connected via VPC Peering.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_vpc-endpoint.html</a></p>\n", "answers": ["<p>Add a VPC Interface Endpoint for Secrets Manager and configure the Lambda function's subnet to use it</p>", "<p>Create a NAT Gateway in the VPC. Configure the Lambda function to use the NAT Gateway for connecting to the Secrets Manager</p>", "<p>Create a VPC Peering connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>", "<p>Create a Direct Connect connection between the VPC and Secrets Manager and configure the Lambda function's subnet to use it</p>"]}, "correct_response": ["a"], "section": "Infrastructure Security", "question_plain": "A financial services company is running an Amazon RDS for MySQL DB instance in a virtual private cloud (VPC) to store sensitive customer data. Due to strict security policies, the company has implemented a VPC that does not allow any network traffic to or from the internet. A security engineer at the company wants to use AWS Secrets Manager to automatically rotate the DB instance credentials for increased security. However, due to the company's security policy, the engineer is not allowed to use the standard AWS Lambda function provided by Secrets Manager to rotate the credentials.\n\nTo address this issue, the security engineer deploys a custom Lambda function within the VPC. This function is responsible for rotating the secret in Secrets Manager. The security engineer also edits the DB instance's security group to allow connections from this custom Lambda function. However, when the function is invoked, it is unable to communicate with Secrets Manager and cannot rotate the secret.\n\nWhich of the following options will address the given scenario?", "related_lectures": []}, {"_class": "assessment", "id": 70394756, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A large company that uses AWS recently received an email from the AWS Abuse team. The email informed them that an IAM user associated with the company's AWS account had their access key and secret access key pair published in public code repositories, although there are no signs yet of any compromise within the company's AWS account. The IAM user in question is designated as a service account and is used in a critical customer-facing production application with hard-coded credentials. To address this situation and minimize application downtime, you have been tasked as an AWS Certified Security Specialist for implementing a solution that protects the AWS account from any unauthorized access.</p>\n\n<p>Which of the following steps would you suggest?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><em>*\n1. Inactivate the publicly exposed IAM access key\n2. Create a new access key and secret access key pair for the IAM user\n3. Update the application to use the new credentials\n4. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n5. Delete AWS Management Console credentials associated with the IAM user\n*</em></p>\n\n<p>This option is correct because it prioritizes securing the AWS account by first inactivating the publicly exposed IAM access key, which would prevent any unauthorized access to the AWS resources. Then, it creates a new access key and secret access key pair for the IAM user and updates the application to use the new credentials to minimize application downtime. Finally, it revokes any temporary AWS STS credentials associated with the IAM user and deletes any AWS Management Console credentials, further reducing the risk of unauthorized access. This sequence of actions would ensure that the AWS account is secure and minimize the risk of any disruption to the customer-facing production application.</p>\n\n<p>Incorrect options:</p>\n\n<p><em>*\n1. Delete AWS Management Console credentials associated with the IAM user\n2. Create a new access key and secret access key pair for the IAM user\n3. Update the application to use the new credentials\n4. Inactivate the publicly exposed IAM access key\n5. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n*</em></p>\n\n<p><em>*\n1. Delete AWS Management Console credentials associated with the IAM user\n2. Create a new access key and secret access key pair for the IAM user\n3. Inactivate the publicly exposed IAM access key\n4. Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n5. Update the application to use the new credentials\n*</em></p>\n\n<p>These two options entail a lower priority for inactivating the publicly exposed IAM access key, which should be the first step to be accomplished for preventing any unauthorized access to the AWS resources. So, both these options are incorrect.</p>\n\n<p><em>*\n1. Revoke temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user\n2. Inactivate the publicly exposed IAM access key\n3. Create a new access key and secret access key pair for the IAM user\n4. Update the application to use the new credentials\n5. Delete AWS Management Console credentials associated with the IAM user\n*</em></p>\n\n<p>Revoking the temporary AWS Security Token Service (AWS STS) credentials upfront would cause immediate application downtime. Therefore, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/\">https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/</a></p>\n", "answers": ["<ol>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n</ol>", "<ol>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n</ol>", "<ol>\n<li>Revoke temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Update the application to use the new credentials</li>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n</ol>", "<ol>\n<li>Delete AWS Management Console credentials associated with the IAM user</li>\n<li>Create a new access key and secret access key pair for the IAM user</li>\n<li>Inactivate the publicly exposed IAM access key</li>\n<li>Revoke any temporary AWS Security Token Service (AWS STS) credentials associated with the IAM user</li>\n<li>Update the application to use the new credentials</li>\n</ol>"]}, "correct_response": ["b"], "section": "Incident Response", "question_plain": "A large company that uses AWS recently received an email from the AWS Abuse team. The email informed them that an IAM user associated with the company's AWS account had their access key and secret access key pair published in public code repositories, although there are no signs yet of any compromise within the company's AWS account. The IAM user in question is designated as a service account and is used in a critical customer-facing production application with hard-coded credentials. To address this situation and minimize application downtime, you have been tasked as an AWS Certified Security Specialist for implementing a solution that protects the AWS account from any unauthorized access.\n\nWhich of the following steps would you suggest?", "related_lectures": []}, {"_class": "assessment", "id": 70394758, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company operates a global data analytics website hosted on AWS. The website relies on Amazon CloudFront to deliver content to its customers. Recently, the company is facing new data regulation policies and is required to block inbound traffic from a specific set of countries. The company needs to find a solution to comply with the new data regulation policies while maintaining the cost-effectiveness of its infrastructure.</p>\n\n<p>What do you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage geographic restrictions in CloudFront to deny traffic from a specific set of countries</strong></p>\n\n<p>You can use geographic restrictions in CloudFront, sometimes known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution.</p>\n\n<p>When a user requests your content, CloudFront typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geographic restrictions feature to do one of the following:</p>\n\n<p>Allow your users to access your content only if they\u2019re in one of the approved countries on your allow list.</p>\n\n<p>Prevent your users from accessing your content if they\u2019re in one of the banned countries on your block list.</p>\n\n<p>For example, if a request comes from a country where you are not authorized to distribute your content, you can use CloudFront geographic restrictions to block the request.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on criteria that you specify, such as the IP addresses that requests originate from or the values of query strings, the service associated with your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p><strong>Leverage an AWS WAF web ACL with an IP match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</strong> - This option is a distractor. If you want to allow or block web requests based on the IP addresses that the requests originate from, you need to create one or more IP match conditions. You cannot use an IP match condition to deny traffic from a specific set of countries.</p>\n\n<p><strong>Leverage an AWS WAF web ACL with a geo match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</strong> - You can use the geo match statement to manage requests from specific countries or regions. Although you can use the geo match condition in a WAF to deny traffic from a specific set of countries, this option is costlier than using the built-in geographic restriction feature of CloudFront. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Leverage geolocation routing policies in CloudFront to deny traffic from a specific set of countries</strong> - This option has been added as a distractor. There is no such thing as geolocation routing policies in CloudFront. You use the geolocation routing policies in Route 53 to localize your content and present some or all of your website in the language of your users. You can also use the geolocation routing policies of Route 53 to restrict the distribution of content to only certain locations.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-geo-restriction/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-geo-restriction/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html</a></p>\n", "answers": ["<p>Leverage geolocation routing policies in CloudFront to deny traffic from a specific set of countries</p>", "<p>Leverage an AWS WAF web ACL with an IP match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</p>", "<p>Leverage an AWS WAF web ACL with a geo match condition to deny traffic from a specific set of countries. Configure the CloudFront distribution to use the web ACL</p>", "<p>Leverage geographic restrictions in CloudFront to deny traffic from a specific set of countries</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "A company operates a global data analytics website hosted on AWS. The website relies on Amazon CloudFront to deliver content to its customers. Recently, the company is facing new data regulation policies and is required to block inbound traffic from a specific set of countries. The company needs to find a solution to comply with the new data regulation policies while maintaining the cost-effectiveness of its infrastructure.\n\nWhat do you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 70394760, "assessment_type": "multi-select", "prompt": {"question": "<p>A Network Load Balancer (NLB) was recently set up in a company's AWS infrastructure, but the target instances are not entering the InService state. The security engineer was called upon to investigate the issue. After conducting a thorough investigation, the engineer determined that the health checks were failing.</p>\n\n<p>Which of the following could cause the health checks to fail? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.</p>\n\n<p>When you create an internet-facing load balancer, you can optionally specify one Elastic IP address per subnet. If you do not choose one of your own Elastic IP addresses, Elastic Load Balancing provides one Elastic IP address per subnet for you. These Elastic IP addresses provide your load balancer with static IP addresses that will not change during the life of the load balancer. You can't change these Elastic IP addresses after you create the load balancer.</p>\n\n<p>When you create an internal load balancer, you can optionally specify one private IP address per subnet. If you do not specify an IP address from the subnet, Elastic Load Balancing chooses one for you. These private IP addresses provide your load balancer with static IP addresses that will not change during the life of the load balancer. You can't change these private IP addresses after you create the load balancer.</p>\n\n<p><strong>The target instance\u2019s subnet network ACL does not allow traffic from the NLB's IP Addresses</strong></p>\n\n<p>A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level. If the network ACL does not allow traffic from the NLB's IP Addresses, then the NLB's health checks will fail.</p>\n\n<p><strong>The target instance\u2019s security group has no rules that allow traffic from the NLB's IP Addresses</strong></p>\n\n<p>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. If the target instance\u2019s security group has no rules to allow traffic from the NLB's IP Addresses, then the NLB's health checks will fail.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p>\n\n<p><strong>The target instance\u2019s security group has rules that are not using the correct IP addresses to allow traffic from the NLB</strong></p>\n\n<p>If the target instance\u2019s security group has rules that are not using the correct IP addresses from the list of the NLB's IP Addresses, then the NLB's health checks will fail, since the traffic from NLB will not be able to reach the instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target instance\u2019s subnet network ACL does not allow traffic from the NLB's security group</strong></p>\n\n<p><strong>The target instance\u2019s security group does not allow traffic from the NLB's security group</strong></p>\n\n<p>An NLB cannot be associated with a security group. So, both these options have been added as distractors.</p>\n\n<p><strong>The target instance\u2019s security group is not using the DNS name of the NLB to allow traffic from the NLB</strong></p>\n\n<p>A security group can only use a single IPv4/IPv6 address, a range of IPv4/IPv6 addresses, a prefix list, or another security group as a source or destination in the rules. Therefore, you cannot use a DNS name as a source or destination, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/network-load-balancers.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p>\n", "answers": ["<p>The target instance\u2019s subnet network ACL does not allow traffic from the NLB's security group</p>", "<p>The target instance\u2019s subnet network ACL does not allow traffic from the NLB's IP Addresses</p>", "<p>The target instance\u2019s security group has no rules that allow traffic from the NLB's IP Addresses</p>", "<p>The target instance\u2019s security group does not allow traffic from the NLB's security group</p>", "<p>The target instance\u2019s security group is not using the DNS name of the NLB to allow traffic from the NLB</p>", "<p>The target instance\u2019s security group has rules that are not using the correct IP addresses to allow traffic from the NLB</p>"]}, "correct_response": ["b", "c", "f"], "section": "Infrastructure Security", "question_plain": "A Network Load Balancer (NLB) was recently set up in a company's AWS infrastructure, but the target instances are not entering the InService state. The security engineer was called upon to investigate the issue. After conducting a thorough investigation, the engineer determined that the health checks were failing.\n\nWhich of the following could cause the health checks to fail? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394762, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A mid-sized company stores sensitive data on an Amazon Elastic Block Store (EBS) volume attached to an Amazon Elastic Compute Cloud (EC2) instance. To ensure data durability, the company also replicates this sensitive data to an Amazon Simple Storage Service (S3) bucket. Both the EBS volume and S3 bucket are encrypted using the same AWS Key Management Service (KMS) Customer Master Key (CMK). The security team at the company has noticed that the CMK has been deleted as a former employee had set the key for deletion before leaving the company.</p>\n\n<p>As a Security Specialist, what do you suggest to access the data?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</strong></p>\n\n<p>You can use AWS Key Management Service (KMS) to create and control the cryptographic keys that are used to protect your data on AWS. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you perform cryptographic operations with the KMS key.</p>\n\n<p>By default, AWS KMS creates the key material for a KMS key. However, you can import your own key material into a KMS key, or use a custom key store to create KMS keys that use key material in your AWS CloudHSM cluster, or key material in an external key manager that you own and manage outside of AWS.</p>\n\n<p>Deleting a KMS key deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable. Because it is destructive and potentially dangerous to delete a KMS key, AWS KMS requires you to set a waiting period of 7 \u2013 30 days. The default waiting period is 30 days.</p>\n\n<p>For the given use case, the EBS volume has been encrypted with a KMS key. When you attach the EBS volume to an EC2 instance, Amazon EC2 uses your KMS key to decrypt the EBS volume's encrypted data key. Amazon EC2 stores the plaintext data key in hypervisor memory and uses it to encrypt disk I/O to the EBS volume. The data key persists in memory as long as the EBS volume is attached to the EC2 instance. Even if someone has scheduled the key for deletion and the key is past the waiting period for deletion, this has no immediate effect on the EC2 instance or the EBS volume. Amazon EC2 is using the plaintext data key\u2014not the KMS key\u2014to encrypt all disk I/O while the volume is attached to the instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</strong> - You cannot use the AWS account root user privileges to restore the deleted key and then use it to recover the data from the EBS volume. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</strong> - AWS Support cannot restore a deleted KMS key. Once deleted, a KMS key can never be accessed again.</p>\n\n<p><strong>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</strong> - You cannot take a snapshot of an EBS encrypted volume for which the KMS key has been deleted as the snapshot itself needs to be encrypted using the same KMS key.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q9-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p>\n", "answers": ["<p>Login as the AWS account root user to restore the deleted key and then use it to recover the data from the EBS volume</p>", "<p>Copy the data directly from the EBS encrypted volume before the volume is detached from the EC2 instance</p>", "<p>Take a snapshot of the EBS encrypted volume before the volume is detached from the EC2 instance</p>", "<p>Raise a ticket with AWS Support to restore the deleted key and then use it to recover the data from the EBS volume</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "A mid-sized company stores sensitive data on an Amazon Elastic Block Store (EBS) volume attached to an Amazon Elastic Compute Cloud (EC2) instance. To ensure data durability, the company also replicates this sensitive data to an Amazon Simple Storage Service (S3) bucket. Both the EBS volume and S3 bucket are encrypted using the same AWS Key Management Service (KMS) Customer Master Key (CMK). The security team at the company has noticed that the CMK has been deleted as a former employee had set the key for deletion before leaving the company.\n\nAs a Security Specialist, what do you suggest to access the data?", "related_lectures": []}, {"_class": "assessment", "id": 70394764, "assessment_type": "multiple-choice", "prompt": {"question": "<p>During regular maintenance tasks, an application support team noticed an abnormal activity on an Amazon EC2 instance that is configured with an EBS volume. The team immediately informed a Security Engineer of the anomaly. The instance is part of an Auto Scaling Group fronted by an Elastic Load Balancer.</p>\n\n<p>What immediate steps should the Security Engineer take for preventing any further attacks to secure the connecting systems and understand the root cause?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</strong></p>\n\n<p>AWS recommends the following actions when a potential security anomaly is detected on your Amazon EC2 instance:</p>\n\n<ol>\n<li><p>Capture the metadata from the Amazon EC2 instance, before you make any changes to your environment.</p></li>\n<li><p>Protect the Amazon EC2 instance from accidental termination by enabling termination protection for the instance.</p></li>\n<li><p>Isolate the Amazon EC2 instance by switching the VPC Security Group. However, be aware of VPC connection tracking and other containment techniques.</p></li>\n<li><p>Detach the Amazon EC2 instance from any AWS Auto Scaling groups.</p></li>\n<li><p>Deregister the Amazon EC2 instance from any related Elastic Load Balancing service.</p></li>\n<li><p>Snapshot the Amazon EBS data volumes that are attached to the EC2 instance for preservation and follow-up investigations.</p></li>\n<li><p>Tag the Amazon EC2 instance as quarantined for investigation, and add any pertinent metadata, such as the trouble ticket associated with the investigation.</p></li>\n</ol>\n\n<p>You can perform all of the preceding steps using the AWS APIs, AWS SDKs, AWS CLI, and AWS Management Console. To interact with AWS using these methods, the IAM service helps you securely control access to AWS resources. You use IAM to control who is authenticated and authorized to use resources at the Account Level. The IAM service provides the authentication and authorization for you to perform these actions and interact with the service domain.</p>\n\n<p>A snapshot of an Amazon EBS volume is a point-in-time, block-level copy of an EBS data volume, which occurs asynchronously and might take time to complete, but it is a delta of that data going forward. You can create new EBS volumes from these copies and mount them to the forensic EC2 instance for deep analysis offline by forensic investigators.</p>\n\n<p>EC2 Instance Isolation and Snapshots:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf\">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</strong> - You cannot launch a new EC2 instance with a forensic toolkit and then connect it to the suspicious instance for investigation as this goes against the best practice of quarantining and isolating the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Therefore, this option is incorrect.</p>\n\n<p><strong>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</strong> - You must quarantine and isolate the suspicious instance immediately. You must not inspect live traffic coming from the suspicious instance. You must also note that placing the suspicious instance within an isolation security group would not allow any new connections from the forensic EC2 instance. Hence this option is incorrect.</p>\n\n<p><strong>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</strong> - Connecting the suspicious EBS volume directly to another EC2 instance does not help contain the malicious activity. Also, the EBS volume connected to the EC2 instance is of great importance in investigating the root cause of the attack. A snapshot of EBS volume is necessary for data preservation and follow-up investigations. Hence, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf\">https://d1.awsstatic.com/WWPS/pdf/aws_security_incident_response.pdf</a></p>\n", "answers": ["<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group. Launch a new EC2 instance with a forensic toolkit, and allow the forensic toolkit image to connect to the suspicious instance to perform the investigation</p>", "<p>Remove the instance from the Auto Scaling group and deregister the instance from the Elastic Load Balancer. Place the instance within an isolation security group and snapshot the Amazon EBS data volumes that are attached to the EC2 instance. Launch an EC2 instance with a forensic toolkit and attach an EBS volume created from the snapshot of the suspicious EBS volume</p>", "<p>Remove the instance from the Auto Scaling group. Place the instance within an isolation security group. Launch an EC2 instance with a forensic toolkit, and use the forensic toolkit image to deploy another ENI to inspect all traffic coming from the suspicious instance</p>", "<p>Detach the instance from the Auto Scaling group and place it within an isolation security group. Detach the suspicious EBS volume. Launch an EC2 instance with a forensic toolkit and attach the detached EBS volume to investigate</p>"]}, "correct_response": ["b"], "section": "Incident Response", "question_plain": "During regular maintenance tasks, an application support team noticed an abnormal activity on an Amazon EC2 instance that is configured with an EBS volume. The team immediately informed a Security Engineer of the anomaly. The instance is part of an Auto Scaling Group fronted by an Elastic Load Balancer.\n\nWhat immediate steps should the Security Engineer take for preventing any further attacks to secure the connecting systems and understand the root cause?", "related_lectures": []}, {"_class": "assessment", "id": 70394766, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer must ensure that all certificates imported into AWS Certificate Manager (ACM) in all AWS Regions, must be notified of expiry, 30 days before their actual expiry via a single notification to the security administrator. The notification along with the certificate information should be sent to the security administrator and the Security Hub for centralized management.</p>\n\n<p>Which steps must be taken to perform these tasks optimally?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the <code>DaysToExpiry</code> CloudWatch metric to schedule a batch search of expiring ACM certificates and trigger an AWS Lambda function to send the certificates-to-be-expired notification to an SNS topic. This Lambda function can also be configured to log all the expiring certificates as findings in Security Hub</strong></p>\n\n<p>ACM provides managed renewals that automatically renew certificates in most cases, there are exceptions, such as imported certs, where an automatic renewal isn\u2019t possible.</p>\n\n<p>This option provides a scheduled solution to examine all expiring certificates in ACM, log all the findings in Security Hub, and generate a single notification through SNS for all certificates that are found. The option workflow is as follows:\n1. CloudWatch runs the rule on a timer and invokes a Lambda function.\n2. The function finds all certificates that have a DaysToExpiry metric in CloudWatch.\n3. The function logs all the expiring certificates as findings in Security Hub.\n4. The function publishes a notification to an SNS topic with the expiration details.\n5. SNS creates a notification (most commonly, through email) to any subscribers of the topic.</p>\n\n<p>Monitor expirations of imported certificates in AWS ACM using the scheduled solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/\">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Security Hub has a built-in feature to monitor certificate expirations of ACM certificates. Configure the security Hub to trigger SNS notifications 30 days before the actual expiry date of the certificate</strong> - Although Security Hub can be used to monitor certificate expirations without the solution described above, Security Hub is a Regional service, therefore the monitoring of certificate expirations across Regions can be time-consuming for the initial configuration and difficult to maintain. So this option is not the best fit. The correct solution described above consolidates all certificate notifications from all Regions in which the solution is deployed into the findings of a single Region.</p>\n\n<p><strong>ACM built-in Certificate Expiration event raised through Amazon EventBridge, can be used to invoke a Lambda function. This event-based function raised from a specific certificate, can be configured to publish the result as a finding to Security Hub, and further to an SNS topic used for email subscriptions. An IT service management system can be configured to automatically open a case or incident through SNS and remediate the issue</strong> - This option uses the ACM built-in Certificate Expiration event, which is raised through Amazon EventBridge, to invoke a Lambda function. In this option, the function is configured to publish the result as a finding in Security Hub, and also as an SNS topic used for email subscriptions. As a result, an administrator can be notified of a specific expiring certificate, or an IT service management (ITSM) system can automatically open a case or incident through email or SNS.</p>\n\n<p>This solution provides a Lambda function that makes use of CloudWatch rules to report back those certificates that are due to expire within a pre-defined amount of time. Since the event is based on an event that is raised from a specific certificate, the function examines the single certificate and then generates a separate notification for each certificate that is marked for expiry. This option is not optimal for the given use case since the ask is to receive alerts via a single notification for all certificates marked for expiry.</p>\n\n<p>Monitor expirations of imported certificates in AWS ACM using an event-based solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/\">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p><strong>Leverage the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule provided by AWS Config to automatically renew the certificates imported into ACM. Forward the rule invocation to trigger SNS notifications to the security administrator</strong> - You can use the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule to check if AWS Certificate Manager certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. You cannot use this managed rule to automatically renew the certificates imported into ACM, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/\">https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/cloudwatch-metrics.html\">https://docs.aws.amazon.com/acm/latest/userguide/cloudwatch-metrics.html</a></p>\n", "answers": ["<p>Leverage the ACM_CERTIFICATE_EXPIRATION_CHECK managed rule provided by AWS Config to automatically renew the certificates imported into ACM. Forward the rule invocation to trigger SNS notifications to the security administrator</p>", "<p>Security Hub has a built-in feature to monitor certificate expirations of ACM certificates. Configure the security Hub to trigger SNS notifications 30 days before the actual expiry date of the certificate</p>", "<p>Configure the <code>DaysToExpiry</code> CloudWatch metric to schedule a batch search of expiring ACM certificates and trigger an AWS Lambda function to send the certificates-to-be-expired notification to an SNS topic. This Lambda function can also be configured to log all the expiring certificates as findings in Security Hub</p>", "<p>ACM built-in Certificate Expiration event raised through Amazon EventBridge, can be used to invoke a Lambda function. This event-based function raised from a specific certificate, can be configured to publish the result as a finding to Security Hub, and further to an SNS topic used for email subscriptions. An IT service management system can be configured to automatically open a case or incident through SNS and remediate the issue</p>"]}, "correct_response": ["c"], "section": "Data Protection", "question_plain": "A security engineer must ensure that all certificates imported into AWS Certificate Manager (ACM) in all AWS Regions, must be notified of expiry, 30 days before their actual expiry via a single notification to the security administrator. The notification along with the certificate information should be sent to the security administrator and the Security Hub for centralized management.\n\nWhich steps must be taken to perform these tasks optimally?", "related_lectures": []}, {"_class": "assessment", "id": 70394768, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer has been tasked with the job of configuring access control and authentication for the AWS KMS keys of a particular AWS account.</p>\n\n<p>Which of the following would you identify as valid points of consideration for configuring the requirement correctly? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>The IAM identity that creates a KMS key is not considered to be the key owner. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant</strong></p>\n\n<p>KMS keys belong to the AWS account in which they were created. However, no identity or principal, including the AWS account root user, has permission to use or manage a KMS key unless that permission is explicitly provided in a key policy, IAM policy, or grant. The IAM identity that creates a KMS key is not considered to be the key owner and they don't automatically have permission to use or manage the KMS key that they created. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant.</p>\n\n<p><strong>AWS identities that have the <code>kms:CreateKey</code> permission can set the initial key policy and give themselves permission to use or manage the key</strong></p>\n\n<p>The AWS identities that have the <code>kms:CreateKey</code> permission can set the initial key policy and give themselves permission to use or manage the key.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>KMS keys belong to the AWS account in which they were created. The AWS account root user alone has full permission on the keys until other identities are given permission through a key policy, IAM policy, or grant</strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>Authorization to use KMS keys is given through federated identity or user sign-in access. Resource-based policies and Access control lists (ACLs) are other forms of authorization that KMS accepts</strong> - While resource-based policies and Access control lists (ACLs) are forms of authorization that KMS accepts, federated identity or user sign-in access are modes of authentication and not authorization.</p>\n\n<p>Authentication is the process of verifying your identity. To send a request to AWS KMS, you must or sign into AWS using your AWS credentials. Authorization provides permission to send requests to create, manage, or use AWS KMS resources. For example, you must be authorized to use a KMS key in a cryptographic operation.</p>\n\n<p><strong>Permissions to resources are given through Resource-based policies that are JSON policy documents that contain details about the resource, the principal, and the conditions under which the policy can be used</strong> - Resource-based policies are JSON policy documents that you attach to a resource, such as a KMS key, to control access to the specific resource. The resource-based policy defines the actions that a specified principal can perform on that resource and under what conditions. You don't specify the resource in a resource-based policy, but you must specify a principal, such as accounts, users, roles, federated users, or AWS services. Resource-based policies are inline policies that are located in the service that manages the resource.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html\">https://docs.aws.amazon.com/kms/latest/developerguide/control-access.html</a></p>\n", "answers": ["<p>The IAM identity that creates a KMS key is not considered to be the key owner. Like any other identity, the key creator needs to get permission through a key policy, IAM policy, or grant</p>", "<p>KMS keys belong to the AWS account in which they were created. The AWS account root user alone has full permission on the keys until other identities are given permission through a key policy, IAM policy, or grant</p>", "<p>Authorization to use KMS keys is given through federated identity or user sign-in access. Resource-based policies and Access control lists (ACLs) are other forms of authorization that KMS accepts</p>", "<p>Permissions to resources are given through Resource-based policies that are JSON policy documents which contain details about the resource, the principal, and the conditions under which the policy can be used</p>", "<p>AWS identities that have the kms:CreateKey permission can set the initial key policy and give themselves permission to use or manage the key</p>"]}, "correct_response": ["a", "e"], "section": "Identity and Access Management", "question_plain": "A Security Engineer has been tasked with the job of configuring access control and authentication for the AWS KMS keys of a particular AWS account.\n\nWhich of the following would you identify as valid points of consideration for configuring the requirement correctly? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394770, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer has been tasked to evaluate the outcome of different policies, including but not limited to identity-based policies, resource-based policies, IAM permissions boundaries, session policies, and AWS Organizations service control policies (SCPs) of an AWS account.</p>\n\n<p>Which of the following are valid statements regarding the aforementioned policy evaluations? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</strong></p>\n\n<p>Resource-based policy logic differs from other policy types if the specified principal is an IAM user, an IAM role, or a session principal. If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary or a session policy does not impact the final decision.</p>\n\n<p><strong>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</strong></p>\n\n<p>This statement is correct. Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy.</p>\n\n<p>Impact of resource-based policies for different principal types:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p><strong>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</strong></p>\n\n<p>SCPs apply to principals of the account where the SCPs are attached. If the enforcement code does not find any applicable <code>Allow</code> statements in the SCPs, the request is explicitly denied, even if the denial is implicit. The enforcement code returns a final decision of Deny. If there is no SCP, or if the SCP allows the requested action, the enforcement code evaluation continues.</p>\n\n<p>Determining whether a request is allowed or denied within an account:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</strong> - As discussed above, this statement is incorrect.</p>\n\n<p><strong>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</strong> - This statement is incorrect. If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are limited by an implicit deny in a permission boundary or session policy.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n", "answers": ["<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision</p>", "<p>If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy results in a final <code>Deny</code></p>", "<p>If a resource-based policy grants permission to the ARN of the federated IAM user, then requests made by the federated user during the session are not limited by an implicit deny in a permission boundary or session policy</p>", "<p>Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy</p>", "<p>If there is no explicit <code>Allow</code> in the SCP, the request is implicitly given an <code>Allow</code> after the SCP is evaluated</p>", "<p>If no applicable <code>Allow</code> statement is found in the SCPs, the request is explicitly denied, even if the denial is implicit</p>"]}, "correct_response": ["a", "d", "f"], "section": "Identity and Access Management", "question_plain": "A Security Engineer has been tasked to evaluate the outcome of different policies, including but not limited to identity-based policies, resource-based policies, IAM permissions boundaries, session policies, and AWS Organizations service control policies (SCPs) of an AWS account.\n\nWhich of the following are valid statements regarding the aforementioned policy evaluations? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394772, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer has been asked to configure an interface VPC endpoint to access an Amazon API Gateway private REST API that is in another AWS account.</p>\n\n<p>What are the key points of consideration while creating an interface endpoint in the Amazon VPC account for the given requirement? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</strong> - When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC.</p>\n\n<p>When a private DNS is enabled on a VPC endpoint, the API's invoke URL is covered by the private DNS name <code>*.execute-api.us-east-1.amazonaws.com</code> where * is a placeholder for the API ID. When a DNS query is resolved for a public API from inside a VPC, the resolved DNS points to the private IP of the associated VPC endpoint instead of the public IP of the public API. The API call is then routed to the public API through the VPC endpoint instead of routing it through the internet. Because VPC endpoints can route traffic only to private APIs, the result is an HTTP 403 error.</p>\n\n<p><strong>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</strong> - The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from either of the following: An IP address range in your Amazon VPC or another security group in your Amazon VPC.</p>\n\n<p>Key points to remember when creating an interface endpoint:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</strong> - This statement is incorrect. As best practice AWS suggests configuring subnets across multiple Availability Zones to make your interface endpoint resilient to possible Availability Zone failures. However, it is not mandatory. Another best practice is to use a VPC endpoint policy to restrict endpoint access by API ID. It's also a best practice to use the API Gateway resource policy to restrict endpoint access by the principal.</p>\n\n<p><strong>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</strong> - This statement is incorrect. It is not possible to connect to public APIs using a VPC endpoint.</p>\n\n<p><strong>You cannot access your private API endpoint from an on-premises network using public DNS names</strong> - This is incorrect. You can use AWS Direct Connect to establish a dedicated private connection from an on-premises network to Amazon VPC and access your private API endpoint over that connection by using public DNS names.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-api-test-invoke-url.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/\">https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-vpc-connections/</a></p>\n", "answers": ["<p>For better resilience, it is mandatory to select multiple subnets across multiple Availability Zones when creating an interface endpoint</p>", "<p>To connect to public APIs using a VPC endpoint, enable private DNS on your VPC</p>", "<p>When you activate private DNS for an interface VPC endpoint, you can no longer access API Gateway public APIs from your Amazon VPC</p>", "<p>The security groups that you choose must have a rule that allows TCP Port 443 inbound HTTPS traffic from an IP address range in your Amazon VPC</p>", "<p>You cannot access your private API endpoint from an on-premises network using public DNS names</p>"]}, "correct_response": ["c", "d"], "section": "Data Protection", "question_plain": "A Security Engineer has been asked to configure an interface VPC endpoint to access an Amazon API Gateway private REST API that is in another AWS account.\n\nWhat are the key points of consideration while creating an interface endpoint in the Amazon VPC account for the given requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394774, "assessment_type": "multi-select", "prompt": {"question": "<p>The security team at a company is working to create VPC endpoints so that the AWS Systems Manager can be used to manage private EC2 instances without internet access.</p>\n\n<p>As an AWS Certified Security Specialist, which options will you combine to build a solution to meet the given requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Verify that SSM Agent is installed on the instance</strong></p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to your private EC2 instance</strong></p>\n\n<p><strong>Create three virtual private cloud (VPC) endpoints for the Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></strong></p>\n\n<p>You need to create a virtual private cloud (VPC) endpoint for Systems Manager for three services: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code> and <code>com.amazonaws.[region].ssmmessages</code>. After the three endpoints are created, your instance appears in Managed Instances and can be managed using Systems Manager.</p>\n\n<p>Complete steps to be followed for above expected configuration:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q15-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) instance profile for the Systems Manager. Attach the IAM role to the VPC endpoint policy</strong></p>\n\n<p><strong>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></strong></p>\n\n<p>These two statements are incorrect and given only as distractors.</p>\n\n<p><strong>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code></strong> - The question mentions that the instance is in a private subnet with no internet access, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p>\n", "answers": ["<p>Verify that SSM Agent is installed on the instance</p>", "<p>Allow outbound internet access on your managed instances. The managed instances must be configured to also allow HTTPS (port 443) outbound traffic to the following endpoints: <code>ssm.[region].amazonaws.com</code>, <code>ssmmessages.[region].amazonaws.com</code>, <code>ec2messages.[region].amazonaws.com</code></p>", "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to your private EC2 instance</p>", "<p>Create three virtual private cloud (VPC) endpoints for Systems Manager with service names: <code>com.amazonaws.[region].ssm</code>, <code>com.amazonaws.[region].ec2messages</code>, <code>com.amazonaws.[region].ssmmessages</code></p>", "<p>Create an AWS Identity and Access Management (IAM) instance profile for Systems Manager. Attach the IAM role to the VPC endpoint policy</p>", "<p>Create a virtual private cloud (VPC) endpoint for Systems Manager with service name <code>com.amazonaws.[region].ssm</code></p>"]}, "correct_response": ["a", "c", "d"], "section": "Data Protection", "question_plain": "The security team at a company is working to create VPC endpoints so that the AWS Systems Manager can be used to manage private EC2 instances without internet access.\n\nAs an AWS Certified Security Specialist, which options will you combine to build a solution to meet the given requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394776, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A user is trying to upload a large file to an Amazon S3 bucket present in a given AWS account. In the upload request, the user is passing the encryption information using an AWS Key Management Service (AWS KMS) key, also present in the same account. However, the user is getting an Access Denied error. Meanwhile, when the user uploads a smaller file with encryption information, the upload succeeds.</p>\n\n<p>As a Security Engineer, how will you fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Verify that <code>kms:Decrypt</code> permissions are specified in the key policy, otherwise they need to be added to the policy</strong></p>\n\n<p>The AWS CLI (aws s3 commands), AWS SDKs, and many third-party programs automatically perform a multipart upload when the file is large. To perform a multipart upload with encryption using an AWS KMS key, the requester must have <code>kms:GenerateDataKey</code> and <code>kms:Decrypt</code> permissions. The <code>kms:GenerateDataKey</code> permissions allow the requester to initiate the upload. With <code>kms:Decrypt</code> permissions, newly uploaded parts can be encrypted with the same key used for previous parts of the same object.</p>\n\n<p>After all the parts are uploaded successfully, the uploaded parts must be assembled to complete the multipart upload operation. Because the uploaded parts are server-side encrypted using a KMS key, object parts must be decrypted before they can be assembled. For this reason, the requester must have <code>kms:Decrypt</code> permissions for multipart upload requests using server-side encryption with KMS CMKs (SSE-KMS).</p>\n\n<p>Since the user can successfully upload smaller files, it is clear that the user already has <code>kms:GenerateDataKey</code> permissions. Hence, only the <code>kms:Decrypt</code> permission needs to be added to the policy.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Verify that the requester has <code>kms:GenerateDataKey</code> permissions. This permission is needed for multipart upload to work successfully</strong> - <code>kms:GenerateDataKey</code> is needed to upload encrypted objects to the S3 bucket. Since the user can upload smaller files, the user already has this permission.</p>\n\n<p><strong>Verify that <code>kms:Decrypt</code> permissions are specified in both the key policy as well as the IAM policy of the user</strong> - If your AWS Identity and Access Management (IAM) role and key are in the same account, then <code>kms:Decrypt</code> permissions must be specified in the key policy. If your IAM role belongs to a different account than the key, <code>kms:Decrypt</code> permissions must be specified in both the key and IAM policy. Since the question clearly states that the role and key are from the same account, this option stands incorrect.</p>\n\n<p><strong>Verify that <code>kms:Encrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</strong> - As mentioned in the explanation above, the requester must have <code>kms:GenerateDataKey</code> and <code>kms:Decrypt</code> permissions. <code>kms:Encrypt</code> can only be used to encrypt plaintext of up to 4,096 bytes using a KMS key. So, this option acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/s3-large-file-encryption-kms-key\">https://repost.aws/knowledge-center/s3-large-file-encryption-kms-key</a></p>\n", "answers": ["<p>Verify that the requester has <code>kms:GenerateDataKey</code> permissions. This permission is needed for multipart upload to work successfully</p>", "<p>Verify that <code>kms:Encrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</p>", "<p>Verify that <code>kms:Decrypt</code> permissions are specified in the key policy, otherwise, they need to be added to the policy</p>", "<p>Verify that <code>kms:Decrypt</code> permissions are specified in both the key policy as well as the IAM policy of the user</p>"]}, "correct_response": ["c"], "section": "Data Protection", "question_plain": "A user is trying to upload a large file to an Amazon S3 bucket present in a given AWS account. In the upload request, the user is passing the encryption information using an AWS Key Management Service (AWS KMS) key, also present in the same account. However, the user is getting an Access Denied error. Meanwhile, when the user uploads a smaller file with encryption information, the upload succeeds.\n\nAs a Security Engineer, how will you fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394778, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company stores its critical business data on Amazon S3 buckets. A customer does not use TLS versions 1.2 or higher and hence is unable to access content stored in Amazon Simple Storage Service (Amazon S3) buckets.</p>\n\n<p>As a Security Engineer, how will you set up a solution to allow the customers to access content in the Amazon S3 buckets using TLS 1.0 or 1.1 while keeping the communication channel secure?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Make your S3 bucket private and configure access through Amazon CloudFront only by using signed requests to access the S3 bucket</strong></p>\n\n<p>AWS is enforcing the use of TLS 1.2 or higher on all AWS API endpoints. To continue to connect to AWS services, you must update all software that uses TLS 1.0 or 1.1.</p>\n\n<p>Amazon CloudFront allows the use of older TLS versions by abstracting customers from the TLS protocol that's used between your CloudFront distribution and Amazon S3.</p>\n\n<p>CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). Create a CloudFront distribution with Origin Access Control (OAC). OAC is a feature that enables CloudFront customers to easily secure their S3 origins by permitting only designated CloudFront distributions to access their S3 buckets. Customers can enable AWS Signature Version 4 (SigV4) on CloudFront requests to S3 buckets with the ability to set when and if CloudFront should sign requests.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a CloudFront distribution with an Amazon S3 bucket as the custom origin. CloudFront supports anonymous and public requests to S3 buckets for undisrupted user access</strong> - If you set up an S3 bucket as a custom origin then the Amazon S3 bucket should be configured as a website endpoint. This means you cannot use origin access control (OAC) or origin access identity (OAI) to secure your bucket access. Hence, this option is incorrect for the current use case.</p>\n\n<p><strong>The wildcard domain name feature of AWS Certificate Manager (ACM) can be used to work around the TLS version limitations</strong> - In an ACM certificate, a wildcard domain name matches any first-level subdomain or hostname in a domain. A first-level subdomain is a single domain name label that does not contain a period (dot). This is irrelevant to the given use case, so it is an invalid option.</p>\n\n<p><strong>Create a bucket policy that allows secure data access via TLS 1.0 or 1.1</strong> - This option has been added as a distractor. You cannot allow secure data access via TLS 1.0 or 1.1 using a bucket policy.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-cloudfront-origin-access-control/\">https://aws.amazon.com/about-aws/whats-new/2022/08/amazon-cloudfront-origin-access-control/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href=\"https://repost.aws/knowledge-center/s3-access-old-tls\">https://repost.aws/knowledge-center/s3-access-old-tls</a></p>\n", "answers": ["<p>Configure a CloudFront distribution with an Amazon S3 bucket as the custom origin. CloudFront supports anonymous and public requests to S3 buckets for undisrupted user access</p>", "<p>Create a CloudFront distribution with Origin Access Control (OAC). Make your S3 bucket private and configure access through Amazon CloudFront only by using signed requests to access the S3 bucket</p>", "<p>The wildcard domain name feature of AWS Certificate Manager (ACM) can be used to work around the TLS version limitations</p>", "<p>Create a bucket policy that allows secure data access via TLS 1.0 or 1.1</p>"]}, "correct_response": ["b"], "section": "Data Protection", "question_plain": "A company stores its critical business data on Amazon S3 buckets. A customer does not use TLS versions 1.2 or higher and hence is unable to access content stored in Amazon Simple Storage Service (Amazon S3) buckets.\n\nAs a Security Engineer, how will you set up a solution to allow the customers to access content in the Amazon S3 buckets using TLS 1.0 or 1.1 while keeping the communication channel secure?", "related_lectures": []}, {"_class": "assessment", "id": 70394780, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at a company has been assigned the responsibility of configuring outgoing email using Simple Email Service (SES) that leverages the Amazon SES API with mandatory TLS for the secure transfer of data.</p>\n\n<p>Which configuration should the engineer choose to make TLS mandatory for SES API?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the behavior of SES by using configuration sets. Set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code></strong></p>\n\n<p>By default, Amazon SES uses opportunistic TLS.</p>\n\n<p>To address the given requirement, you can use the <code>PutConfigurationSetDeliveryOptions</code> API operation to set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code>. You can use the AWS CLI to make this change.</p>\n\n<p>Configuration for mandatory TLS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html\">https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure STARTTLS mechanism on SES to establish a TLS-encrypted connection with the client</strong></p>\n\n<p><strong>Configure TLS Wrapper mechanism on SES to establish a secure TLS-encrypted connection with the client</strong></p>\n\n<p>If you are accessing Amazon SES through the SMTP interface, you're required to encrypt your connection using Transport Layer Security (TLS). Amazon SES supports two mechanisms for establishing a TLS-encrypted connection: STARTTLS and TLS Wrapper. Since the use case mentions using Amazon SES API, these two options are irrelevant to the given use case.</p>\n\n<p><strong>By default, Amazon SES configuration mandates TLS. Custom configurations are not needed to achieve secure communication</strong> - This statement is incorrect. By default, Amazon SES uses opportunistic TLS. This means that Amazon SES always attempts to make a secure connection to the receiving mail server. If Amazon SES can't establish a secure connection, it sends the message unencrypted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html\">https://docs.aws.amazon.com/ses/latest/dg/security-protocols.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/ses/latest/dg/data-protection.html\">https://docs.aws.amazon.com/ses/latest/dg/data-protection.html</a></p>\n", "answers": ["<p>By default, Amazon SES configuration mandates TLS. Custom configurations are not needed to achieve secure communication</p>", "<p>Configure STARTTLS mechanism on SES to establish a TLS-encrypted connection with the client</p>", "<p>Configure TLS Wrapper mechanism on SES to establish a secure TLS-encrypted connection with the client</p>", "<p>Change the behavior of SES by using configuration sets. Set the <code>TlsPolicy</code> property for a configuration set to <code>Require</code></p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "The security team at a company has been assigned the responsibility of configuring outgoing email using Simple Email Service (SES) that leverages the Amazon SES API with mandatory TLS for the secure transfer of data.\n\nWhich configuration should the engineer choose to make TLS mandatory for SES API?", "related_lectures": []}, {"_class": "assessment", "id": 70394782, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An open banking system enables secure open API integrations for financial institutions. The banking system needs mutual TLS (mTLS) authentication as part of its security standards. The application will be hosted on Amazon EC2 instances.</p>\n\n<p>As a Security Engineer, how will you configure this requirement to support mTLS if a load balancing service is needed for the instances?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a TCP listener using a Network Load Balancer and implement mTLS on the target</strong></p>\n\n<p>By default, the TLS protocol only requires a server to authenticate itself to the client. The authentication of the client to the server is managed by the application layer. The TLS protocol also offers the ability for the server to request that the client send an X.509 certificate to prove its identity. This is called mutual TLS (mTLS) as both parties are authenticated via certificates with TLS.</p>\n\n<p>For mTLS support, you need to create a TCP listener using a Network Load Balancer or a Classic Load Balancer and implement mTLS on the target. The load balancer passes the request through as is, so you can implement mTLS on the target.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an HTTPS listener on an Application Load Balancer and enable mutual TLS authentication for better security of the application</strong> - This statement is incorrect. Application Load Balancers do not support mutual TLS authentication (mTLS).</p>\n\n<p><strong>Network Load Balancers support TLS renegotiation and mutual TLS authentication (mTLS). Configure a TLS listener for a Network Load Balancer to use mutual TLS authentication</strong> - Network Load Balancers do not support TLS renegotiation or mutual TLS authentication (mTLS). For mTLS support, you need to create a TCP listener instead of a TLS listener.</p>\n\n<p><strong>Configure a TCP listener on an Application Load Balancer and enable mutual TLS authentication on it</strong> - Application Load Balancer only supports HTTP and HTTPS protocols. Network Load Balancer is used for TCP traffic.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n", "answers": ["<p>Configure an HTTPS listener on an Application Load Balancer and enable mutual TLS authentication for better security of the application</p>", "<p>Network Load Balancers support TLS renegotiation and mutual TLS authentication (mTLS). Configure a TLS listener for a Network Load Balancer to use mutual TLS authentication</p>", "<p>Create a TCP listener using a Network Load Balancer and implement mTLS on the target</p>", "<p>Configure a TCP listener on an Application Load Balancer and enable mutual TLS authentication on it</p>"]}, "correct_response": ["c"], "section": "Infrastructure Security", "question_plain": "An open banking system enables secure open API integrations for financial institutions. The banking system needs mutual TLS (mTLS) authentication as part of its security standards. The application will be hosted on Amazon EC2 instances.\n\nAs a Security Engineer, how will you configure this requirement to support mTLS if a load balancing service is needed for the instances?", "related_lectures": []}, {"_class": "assessment", "id": 70394784, "assessment_type": "multi-select", "prompt": {"question": "<p>An AWS root user has logged in to the AWS account and realized that there is no access to an Amazon S3 bucket under the given AWS account.</p>\n\n<p>What is the reason for this behavior and how will you fix the issue? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>If there is a bucket policy on the Amazon S3 bucket that doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket</strong></p>\n\n<p><strong>Modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI</strong></p>\n\n<p>In some cases, you might have an IAM user with full access to IAM and Amazon S3. If the IAM user assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket. However, as the root user, you can still access the bucket. To do that, modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI. Use the following principal and replace 123456789012 with the ID of the AWS account.</p>\n\n<p><code>\"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" }</code></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Only An IAM user with full access to IAM and the S3 bucket will be able to add the root user as principal to the bucket policy</strong> - As discussed above, the root user can make the changes to the bucket policy to grant the necessary permissions.</p>\n\n<p><strong>The access key of the root user account could be expired and hence needs to be recreated before accessing the S3 bucket</strong> - You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. This is irrelevant to the given use case.</p>\n\n<p><strong>A root user has full access permissions on all the AWS resources in his user account. Contact the AWS support team to sort the access issue</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html</a></p>\n", "answers": ["<p>Modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI</p>", "<p>Only An IAM user with full access to IAM and the S3 bucket will be able to add the root user as principal to the bucket policy</p>", "<p>The access key of the root user account could be expired and hence needs to be recreated before accessing the S3 bucket</p>", "<p>If there is a bucket policy on the Amazon S3 bucket that doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket</p>", "<p>A root user has full access permissions on all the AWS resources in his user account. Contact the AWS support team to sort out the access issue</p>"]}, "correct_response": ["a", "d"], "section": "Identity and Access Management", "question_plain": "An AWS root user has logged in to the AWS account and realized that there is no access to an Amazon S3 bucket under the given AWS account.\n\nWhat is the reason for this behavior and how will you fix the issue? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394786, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has decided to revamp the security for its IT infrastructure and tighten rules for access to AWS resources across the organization. In this context, a Security Engineer has been tasked with creating optimal access credentials/permissions for the company's applications to access the required resources. Some of these applications will run on EC2 instances and need cross-account access privileges for resources present in another AWS account. The company also maintains a few mobile applications that need to access AWS resources.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend as the best practices to configure access credentials/permissions for these applications? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</strong></p>\n\n<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't have to create individual IAM users in each account. In addition, users don't have to sign out of one account and sign into another to access resources in different AWS accounts.</p>\n\n<p><strong>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</strong></p>\n\n<p>Don't embed access keys with the app, even in encrypted storage. Instead, use Amazon Cognito to manage user identities in your app. This service lets you authenticate users using Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)\u2013compatible identity provider. You can then use the Amazon Cognito credentials provider to manage the credentials that your app uses to make requests to access AWS resources.</p>\n\n<p><strong>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</strong></p>\n\n<p>Don't use access keys directly in your application. Don't pass access keys to the application, embed them in the application, or let the application read access keys from any source. Instead, define an IAM role that has appropriate permissions for your application and launch the Amazon Elastic Compute Cloud (Amazon EC2) instance with this role associated with the instance. This practice also enables the application to get temporary security credentials that it can, in turn, use to make programmatic calls to AWS. The AWS SDKs and the AWS Command Line Interface (AWS CLI) can get temporary credentials from the role automatically.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create long-term access keys associated with the AWS account IAM user and use them to provide access to an application running on an EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</strong></p>\n\n<p><strong>Use access keys to provide long-term credentials to AWS for an application running on an Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</strong></p>\n\n<p>These two options are incorrect from a security standpoint. Long-term access keys, such as those associated with IAM users and AWS account root users, remain valid until you manually revoke them. However, temporary security credentials obtained through IAM roles and other features of the AWS Security Token Service expire after a short period of time. AWS suggests using temporary security credentials to help reduce the risk in case credentials are accidentally exposed.</p>\n\n<p><strong>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</strong> - Don't embed access keys with the app, even in encrypted storage. This is considered a security bad practice.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html\">https://docs.aws.amazon.com/accounts/latest/reference/credentials-access-keys-best-practices.html</a></p>\n", "answers": ["<p>Use access keys to provide long-term credentials to AWS for an application running on Amazon EC2 instance. Encrypt the keys to avoid exposure to the internet. Rotate access keys periodically</p>", "<p>Use an IAM role to establish trust between accounts, and then grant users in one account limited permissions to access the trusted account</p>", "<p>Embed access keys with the mobile application and store them in encrypted storage to avoid exposure. As an added layer of security, you can add envelope encryption to the encrypted access keys</p>", "<p>Use Amazon Cognito to manage user identities in your mobile application. You can then use the Amazon Cognito credentials provider to manage credentials that your application uses to make requests to access AWS resources</p>", "<p>Create long-term access keys associated with AWS account IAM user and use them to provide access to an application running on EC2 instance. Since the instances run on safe private subnets on AWS Cloud, the long-term credentials are a perfect fit for this scenario with no overhead of creating and maintaining short-term credentials. It is to be noted that long-term access keys should not be associated with the AWS root user account</p>", "<p>Define an IAM role that has appropriate permissions for the application and launch the Amazon EC2 instance with this role associated with the instance</p>"]}, "correct_response": ["b", "d", "f"], "section": "Identity and Access Management", "question_plain": "A company has decided to revamp the security for its IT infrastructure and tighten rules for access to AWS resources across the organization. In this context, a Security Engineer has been tasked with creating optimal access credentials/permissions for the company's applications to access the required resources. Some of these applications will run on EC2 instances and need cross-account access privileges for resources present in another AWS account. The company also maintains a few mobile applications that need to access AWS resources.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend as the best practices to configure access credentials/permissions for these applications? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394788, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Security Engineer has been asked to create an identity-based policy that allows access to add objects to an Amazon S3 bucket. But, the access should be given from April 1, 2023, through April 30, 2023 (UTC) inclusive.</p>\n\n<p>How will you define this identity-based policy?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This identity-based policy allows access to actions based on date and time. This policy restricts access to actions that occur between April 1, 2023, and April 30, 2023 (UTC), inclusive. This policy grants the permissions necessary to complete this action programmatically from the AWS API or AWS CLI.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use a policy variable with the Date condition operator. In addition, using the <code>{aws:logintime}</code> policy variable is incorrect for the given use case. You should also note that the Deny Effect is logically opposite of what the use case requires.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Principal\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy definition is incorrect. You cannot use the Principal element in an identity-based policy. Identity-based policies are permissions policies that you attach to IAM identities (users, groups, or roles). In those cases, the principal is implicitly the identity where the policy is attached.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html</a></p>\n", "answers": ["<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"{aws:logintime}\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"{aws:logintime}\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Principal\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2023-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2023-04-30T23:59:59Z\"}\n            }\n        }\n    ]\n}\n</code></pre>"]}, "correct_response": ["d"], "section": "Identity and Access Management", "question_plain": "A Security Engineer has been asked to create an identity-based policy that allows access to add objects to an Amazon S3 bucket. But, the access should be given from April 1, 2023, through April 30, 2023 (UTC) inclusive.\n\nHow will you define this identity-based policy?", "related_lectures": []}, {"_class": "assessment", "id": 70394790, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is planning to launch a mobile application for its business critical functions. Mobile users should have access to AWS resources without having to define an AWS identity for each of them. Guest user access is a necessity for the application.</p>\n\n<p>As a Security Engineer, which of the following would you suggest as the most optimal way of configuring the security credentials for mobile users?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Cognito with AWS SDKs for mobile development to create unique identities for the users</strong></p>\n\n<p>For mobile applications, AWS recommends using Amazon Cognito. You can use this service with AWS SDKs for mobile development to create unique identities for users and authenticate them for secure access to your AWS resources. Amazon Cognito supports the same identity providers as AWS STS and also supports unauthenticated (guest) access and lets you migrate user data when a user signs in. Amazon Cognito also provides API operations for synchronizing user data so that it is preserved as users move between devices.</p>\n\n<p>With Amazon Cognito, you can add user sign-up and sign-in features and control access to your web and mobile applications. Amazon Cognito provides an identity store that scales to millions of users, supports social and enterprise identity federation, and offers advanced security features to protect your consumers and business. Built on open identity standards, Amazon Cognito supports various compliance regulations and integrates with frontend and backend development resources.</p>\n\n<p>Amazon Cognito:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create access keys for any IAM user belonging to the AWS account that holds the resources needed for the mobile application. Use these access keys to sign programmatic requests with AWS SDKs for mobile development</strong> - Access keys are long-term credentials for an IAM user or the AWS account root user. As a security best practice, AWS recommends the use of temporary security credentials instead of creating long-term credentials like access keys. Temporary credentials are possible for mobile application scenarios and hence long-term credentials that pose security and managing risks are not preferred.</p>\n\n<p><strong>Use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources</strong> - Amazon Cognito supports the same identity providers as AWS STS and also supports unauthenticated (guest) access and lets you migrate user data when a user signs in. Amazon Cognito has a better feature set for mobile application scenarios and hence is the right choice here.</p>\n\n<p><strong>Use Access Control Lists (ACLs) with AWS SDKs for mobile development to create unique identities for the users</strong> - This option has been added as a distractor. You use Access Control Lists (ACLs) in Amazon S3 to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html#sts-introduction\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html#sts-introduction</a></p>\n", "answers": ["<p>Create access keys for any IAM user belonging to the AWS account that holds the resources needed for the mobile application. Use these access keys to sign programmatic requests with AWS SDKs for mobile development</p>", "<p>Use Access Control Lists (ACLs) with AWS SDKs for mobile development to create unique identities for the users</p>", "<p>Use Amazon Cognito with AWS SDKs for mobile development to create unique identities for the users</p>", "<p>Use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources</p>"]}, "correct_response": ["c"], "section": "Identity and Access Management", "question_plain": "A company is planning to launch a mobile application for its business critical functions. Mobile users should have access to AWS resources without having to define an AWS identity for each of them. Guest user access is a necessity for the application.\n\nAs a Security Engineer, which of the following would you suggest as the most optimal way of configuring the security credentials for mobile users?", "related_lectures": []}, {"_class": "assessment", "id": 70394792, "assessment_type": "multiple-choice", "prompt": {"question": "<p>To improve the security of private APIs, a Security Engineer has been tasked to configure API Gateway to use an interface VPC endpoint. The VPC endpoint policy should only allow full access to two specific private APIs through the endpoint.</p>\n\n<p>Which policy should be attached to the VPC endpoint to meet the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<pre><code>{\n    \"Statement\": [\n        {\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"execute-api:Invoke\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>A VPC endpoint policy is an IAM resource policy that you can attach to an interface VPC endpoint to control access to the endpoint. You can use an endpoint policy to restrict the traffic going from your internal network to access your private APIs. You can choose to allow or disallow access to specific private APIs that can be accessed through the VPC endpoint. You can create policies for Amazon Virtual Private Cloud endpoints for Amazon API Gateway in which you can specify:\n1. The principal that can perform actions.\n2. The actions that can be performed.\n3. The resources that can have actions performed on them.</p>\n\n<p>Incorrect options:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\":[\n        {\n            \"Effect\": \"Allow\",\n            \"Action\":\"ec2:*VpcEndpoint*\",\n            \"Resource\":[\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\":[\n        {\n            \"Effect\": \"Allow\",\n            \"Action\":\"ec2:*VpcEndpoint*\",\n            \"Resource\":[\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>By default, users do not have permission to work with endpoints. An identity-based policy can grant users permission to create, modify, describe, and delete endpoints via the action <code>ec2:*VpcEndpoint*</code>. However, this action is not relevant to the given use case, so both of these options are incorrect.</p>\n\n<pre><code>{\n    \"Statement\": [\n        {\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"execute-api:Invoke\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>This policy grants users access only to the GET methods for the specific API via the VPC endpoint that the policy is attached to. So, this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-vpc-endpoint-policies.html</a></p>\n", "answers": ["<pre><code>{\n    \"Statement\": [\n        {\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"execute-api:Invoke\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\"\n            ]\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Statement\": [\n        {\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"execute-api:Invoke\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*\"\n            ]\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\":[\n        {\n            \"Effect\": \"Allow\",\n            \"Action\":\"ec2:*VpcEndpoint*\",\n            \"Resource\":[\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:aaaaa11111/*\"\n            ]\n        }\n    ]\n}\n</code></pre>", "<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\":[\n        {\n            \"Effect\": \"Allow\",\n            \"Action\":\"ec2:*VpcEndpoint*\",\n            \"Resource\":[\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\",\n                \"arn:aws:execute-api:us-east-1:123412341234:a1b2c3d4e5/stageName/GET/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"]}, "correct_response": ["b"], "section": "Identity and Access Management", "question_plain": "To improve the security of private APIs, a Security Engineer has been tasked to configure API Gateway to use an interface VPC endpoint. The VPC endpoint policy should only allow full access to two specific private APIs through the endpoint.\n\nWhich policy should be attached to the VPC endpoint to meet the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70394794, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media company uses S3 to store artifacts that may only be accessible to EC2 instances running in a private VPC. The security team at the company is apprehensive about an attack vector wherein any team member with access to this instance could also set up an EC2 instance in another VPC to access these artifacts.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following solutions will you recommend to prevent such unauthorized access to the artifacts in S3?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure an S3 VPC endpoint and create an S3 bucket policy to allow access only from this VPC endpoint</strong></p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. One of the ways of letting EC2 instances running in private subnets of a VPC access S3-based resources is by setting up NAT instances in a public subnet and then access those S3-based resources. However, there is a more efficient and secure way. The EC2 instances running in private subnets of a VPC can control access to S3 buckets, objects, and API functions that are in the same Region as the VPC by using the S3 gateway endpoints.</p>\n\n<p>Here are the steps to set up a gateway endpoint:</p>\n\n<p><img src=\"https://media.amazonwebservices.com/blog/2015/vpc_config_endpoint_5.png\"></p>\n\n<p><img src=\"https://media.amazonwebservices.com/blog/2015/vpc_config_endpoint_routes_2.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/\">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p>\n\n<p>Important Characteristics for S3 Gateway Endpoints:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p>You can further use an S3 bucket policy to indicate which VPCs and VPC Endpoints have access to your S3 buckets.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM role that allows access to the artifacts in S3 and then create an S3 bucket policy to allow access only from this role attached to the instance profile</strong> - This allows the possibility to attach the given role to multiple EC2 instance profiles and therefore opens up doors for unauthorized access from different EC2 instances. Hence this option is incorrect.</p>\n\n<p><strong>Attach an Elastic IP to the EC2 instance and create an S3 bucket policy to allow access only from this Elastic IP</strong> - As described in the explanation above, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC endpoint. This applies to IAM policies for users and roles, and any bucket policies. Hence this option is incorrect.</p>\n\n<p><strong>Set up a highly restricted Security Group for the EC2 instance and create an S3 bucket policy to allow access only from this Security Group</strong> - This option has been added as a distractor as a Security Group is not a valid Principal to be used in an S3 bucket policy. Security Group also cannot be used in a valid Condition statement in the bucket policy.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/\">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p>\n", "answers": ["<p>Set up an IAM role that allows access to the artifacts in S3 and then create an S3 bucket policy to allow access only from this role attached to the instance profile</p>", "<p>Attach an Elastic IP to the EC2 instance and create an S3 bucket policy to allow access only from this Elastic IP</p>", "<p>Configure an S3 VPC endpoint and create an S3 bucket policy to allow access only from this VPC endpoint</p>", "<p>Set up a highly restricted Security Group for the EC2 instance and create an S3 bucket policy to allow access only from this Security Group</p>"]}, "correct_response": ["c"], "section": "Identity and Access Management", "question_plain": "A media company uses S3 to store artifacts that may only be accessible to EC2 instances running in a private VPC. The security team at the company is apprehensive about an attack vector wherein any team member with access to this instance could also set up an EC2 instance in another VPC to access these artifacts.\n\nAs an AWS Certified Security Specialist, which of the following solutions will you recommend to prevent such unauthorized access to the artifacts in S3?", "related_lectures": []}, {"_class": "assessment", "id": 70394796, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company recently saw a huge spike in its monthly AWS spend. Upon further investigation, it was found that some developers had accidentally launched Amazon RDS instances in unexpected Regions. The company has hired you as an AWS Certified Security Specialist to establish best practices around the least privileges for developers and control access to on-premises as well as AWS Cloud resources using Active Directory. The company has mandated that you institute a mechanism to control costs by restricting the level of access that developers have to the AWS Management Console without impacting their productivity. The company would also like to allow developers to launch RDS instances only in <code>us-east-1</code> Region without limiting access to other services in any Region.</p>\n\n<p>How can you help the company achieve the new security mandate while minimizing the operational burden on the systems administration team?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></strong></p>\n\n<p>Security Assertion Markup Language 2.0 (SAML) is an open federation standard that allows an identity provider (IdP) to authenticate users and pass identity and security information about them to a service provider which is an AWS application or service for the current use case. With SAML, you can enable a single sign-on experience for your users across many SAML-enabled applications and services. Users authenticate with the IdP once using a single set of credentials and then get access to multiple applications and services without additional sign-ins.</p>\n\n<p>For the given scenario, the company wants to control access to on-premises as well as AWS Cloud resources (specifically via the AWS Management Console) using Active Directory, so it should use SAML 2.0 federated users to access the AWS Management Console. You also create an IAM role with a trust policy that sets the SAML provider as the principal, which establishes a trust relationship between your organization and AWS. The role's permission policy establishes what users from your organization are allowed to do in AWS. In this case, the role will have a PowerUserAccess managed policy attached. As the PowerUserAccess managed policy will allow the developers to create RDS instances in any Region, therefore, you also need to attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code>.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p>\n\n<p>At a high level, it is useful to think of these access privileges in the form of this equation:</p>\n\n<p>PowerUserAccess = AdministrativeAccess - IAM</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has the AdministrativeAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></strong> - Using an IAM role with an AdministrativeAccess managed policy attache to it would violate the key requirement of providing the least privileges for developers. PowerUserAccess provides full access to AWS services and resources but does not allow management of users and groups.</p>\n\n<p>At a high level, it is useful to think of these access privileges in the form of this equation:</p>\n\n<p>PowerUserAccess = AdministrativeAccess - IAM</p>\n\n<p>So, PowerUserAccess provides just the right access privileges required for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q26-i2.jpg\"></p>\n\n<p><strong>Set up an IAM user for each developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that allows the developers access to RDS only in <code>us-east-1</code> Region</strong> - Setting up an IAM user for each developer and adding them to the developer IAM group goes against the requirement of minimizing the operational burden on the DevOps team because this solution does not take advantage of the existing Active Directory that supports SAML-based authentication.</p>\n\n<p><strong>Configure SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer-managed policy that denies all the developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only RDS service in <code>us-east-1</code> region</strong> - This option is a distractor as it's too restrictive. As the customer-managed policy denies the developers access to any AWS services except AWS Service Catalog, therefore it would limit access to all other services in any Region, hence this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p>\n", "answers": ["<p>Configure SAML-based authentication tied to an IAM role that has the AdministrativeAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></p>", "<p>Set up an IAM user for each developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that allows the developers access to RDS only in <code>us-east-1</code> Region</p>", "<p>Configure SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer-managed policy that denies access to RDS in any AWS Region except <code>us-east-1</code></p>", "<p>Configure SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer-managed policy that denies all the developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only RDS service in <code>us-east-1</code> region</p>"]}, "correct_response": ["c"], "section": "Identity and Access Management", "question_plain": "An e-commerce company recently saw a huge spike in its monthly AWS spend. Upon further investigation, it was found that some developers had accidentally launched Amazon RDS instances in unexpected Regions. The company has hired you as an AWS Certified Security Specialist to establish best practices around the least privileges for developers and control access to on-premises as well as AWS Cloud resources using Active Directory. The company has mandated that you institute a mechanism to control costs by restricting the level of access that developers have to the AWS Management Console without impacting their productivity. The company would also like to allow developers to launch RDS instances only in us-east-1 Region without limiting access to other services in any Region.\n\nHow can you help the company achieve the new security mandate while minimizing the operational burden on the systems administration team?", "related_lectures": []}, {"_class": "assessment", "id": 70394798, "assessment_type": "multi-select", "prompt": {"question": "<p>A financial services company manages its IT infrastructure on AWS. The security team at the company has been tasked to monitor and report all the root user activities of the AWS account.</p>\n\n<p>Which options should be combined as a solution so that the security team can meet these requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up a CloudWatch Events rule that is triggered on any API call from the root user</strong></p>\n\n<p><strong>Using Amazon SNS as a target of the trigger that further notifies the security team</strong></p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/07/06/flow_diagram-1.jpg\">\nvia - <a href=\"https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/\">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p>For the given use case, you can set up a CloudWatch rule that catches a console login event and all other API events by a root user, and triggers the Lambda function (set as a target) when such events are detected. This is accomplished by leveraging the CloudTrail API that captures the AWS API call for the root user sign-in.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/07/06/CWE_snapshot.png\">\nvia - <a href=\"https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/\">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p>The function collects the necessary information about the root API event and publishes it to the SNS topic. The function parses the name of the event and the AWS account alias where this root API event occurred and puts them in the subject field for the message that it publishes to the SNS topic.</p>\n\n<p>Finally, the SNS topic sends the email notification published by the Lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Trusted Advisor to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</strong> - AWS Trusted Advisor is a service that continuously analyzes your AWS accounts and provides recommendations to help you to follow AWS best practices and AWS Well-Architected guidelines. Trusted Advisor cannot monitor root user API calls on the company's AWS account and trigger a downstream event to SNS.</p>\n\n<p><strong>Set up an AWS Config rule that triggers a downstream event to SNS on all API calls from the root user</strong> - AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. AWS Config has a managed rule - <code>iam-root-access-key-check</code> that checks if the root user access key is available. The rule is COMPLIANT if the user access key does not exist. Otherwise, NON_COMPLIANT. AWS Config cannot monitor all root user API calls on the company's AWS account and trigger a downstream event to SNS.</p>\n\n<p><strong>Set up AWS Inspector to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. Inspector cannot monitor root user API calls on the company's AWS account and trigger a downstream event to SNS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/\">https://aws.amazon.com/es/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/</a></p>\n", "answers": ["<p>Set up AWS Trusted Advisor to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</p>", "<p>Set up an AWS Config rule that triggers a downstream event to SNS on all API calls from the root user</p>", "<p>Set up AWS Inspector to monitor root user API calls on the company's AWS account and trigger a downstream event to SNS</p>", "<p>Set up a CloudWatch Events rule that is triggered on any API call from the root user</p>", "<p>Using Amazon SNS as a target of the trigger that further notifies the security team</p>"]}, "correct_response": ["d", "e"], "section": "Logging and Monitoring", "question_plain": "A financial services company manages its IT infrastructure on AWS. The security team at the company has been tasked to monitor and report all the root user activities of the AWS account.\n\nWhich options should be combined as a solution so that the security team can meet these requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394800, "assessment_type": "multi-select", "prompt": {"question": "<p>A financial services company recently faced a security event resulting in an S3 bucket with sensitive data containing Personally Identifiable Information (PII) for customers being made public. The company policy mandates never to have public S3 objects so the Governance and Compliance team must be notified immediately as soon as any public objects are identified. The company has hired you as an AWS Certified Security Specialist to help build a solution that detects the presence of a public S3 object, which in turn sets off an alarm to trigger notifications and then automatically remediate the said object.</p>\n\n<p>Which of the following solutions would you combine to address the requirements of the given use case? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure a Lambda function as one of the SNS topic subscribers, which is invoked to secure the objects in the S3 bucket</strong></p>\n\n<p><strong>Enable object-level logging for S3. Set up an EventBridge event pattern when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs and set the target as an SNS topic for downstream notifications</strong></p>\n\n<p>You can enable object-level logging for an S3 bucket to send logs to CloudTrail for object-level API operations such as GetObject, DeleteObject, and PutObject. These events are called data events. By default, CloudTrail trails don't log data events, but you can configure trails to log data events for S3 buckets that you specify, or to log data events for all the Amazon S3 buckets in your AWS account.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html</a></p>\n\n<p>You need to further configure an EventBridge event-pattern based rule to analyze the CloudTrail logs for S3 PutObject API call with public-read permissions. The target for this rule can be set as an SNS topic. The SNS would send the notification via an email or SMS as soon as a public object is detected. Moreover, the SNS topic is also subscribed by a Lambda function which runs custom code to secure the objects in the S3 bucket.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable object-level logging for S3. When a PutObject API call is made with public-read permission, use S3 event notifications to trigger a Lambda that sends a notification via SNS</strong> - S3 event notification allows you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. S3 can publish notifications for the new create object events.</p>\n\n<p>You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*), however, you cannot check if the API call was made with public-read permission. So, this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><strong>Leverage AWS Trusted Advisor to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</strong> - Trusted Advisor is an application that inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. The Trusted Advisor notification feature helps you stay up-to-date with your AWS resource deployment. However, you will only be notified by weekly email when you opt-in for this service, so this does not meet the key requirement for the use case wherein the notification should be sent as soon as a public object is uploaded. Also, Trusted Advisor just checks buckets in Amazon Simple Storage Service (Amazon S3) that have open access permissions. It cannot be used for near real-time detection of a new public object uploaded on S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i4.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/faqs/\">https://aws.amazon.com/premiumsupport/faqs/</a></p>\n\n<p><strong>Leverage AWS Access Analyzer to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</strong> - You can use AWS Access Analyzer to receive findings into the source and level of public or shared access for each public or shared bucket. For example, Access Analyzer for S3 might show that a bucket has read or write access provided through a bucket access control list (ACL), a bucket policy, or an access point policy. It cannot be used for near real-time detection of a new public object uploaded on S3. Additionally, you cannot invoke a Lambda function from Access Analyzer. The findings for Access Analyzer are available within the AWS Console or they can be downloaded in a CSV report.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q28-i5.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/welcome.html\">https://docs.aws.amazon.com/sns/latest/dg/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/faqs/\">https://aws.amazon.com/premiumsupport/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html</a></p>\n", "answers": ["<p>Enable object-level logging for S3. When a PutObject API call is made with public-read permission, use S3 event notifications to trigger a Lambda that sends a notification via SNS</p>", "<p>Leverage AWS Trusted Advisor to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</p>", "<p>Leverage AWS Access Analyzer to check for S3 bucket public-read permissions and invoke a Lambda function to send a notification via SNS as soon as a public object is uploaded</p>", "<p>Configure a Lambda function as one of the SNS topic subscribers, which is invoked to secure the objects in the S3 bucket</p>", "<p>Enable object-level logging for S3. Set up an EventBridge event pattern when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs and set the target as an SNS topic for downstream notifications</p>"]}, "correct_response": ["d", "e"], "section": "Infrastructure Security", "question_plain": "A financial services company recently faced a security event resulting in an S3 bucket with sensitive data containing Personally Identifiable Information (PII) for customers being made public. The company policy mandates never to have public S3 objects so the Governance and Compliance team must be notified immediately as soon as any public objects are identified. The company has hired you as an AWS Certified Security Specialist to help build a solution that detects the presence of a public S3 object, which in turn sets off an alarm to trigger notifications and then automatically remediate the said object.\n\nWhich of the following solutions would you combine to address the requirements of the given use case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394802, "assessment_type": "multi-select", "prompt": {"question": "<p>A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The Security team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources. You have been hired as an AWS Certified Security Specialist to spearhead this strategic initiative.</p>\n\n<p>Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing, and monitoring the configurations of AWS resources? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Leverage Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \u201cWhat did my AWS resource look like at xyz point in time?\u201d.</p>\n\n<p>How AWS Config Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant.</p>\n\n<p>There are two types of evaluation trigger types for Config rules:</p>\n\n<p>Configuration changes \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p>\n\n<p>Periodic \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p>\n\n<p><strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong></p>\n\n<p>CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.</p>\n\n<p>CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren't viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events.</p>\n\n<p>CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution.</p>\n\n<p><strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p><strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n", "answers": ["<p>Leverage Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</p>", "<p>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</p>", "<p>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</p>", "<p>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</p>", "<p>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</p>"]}, "correct_response": ["a", "b"], "section": "Infrastructure Security", "question_plain": "A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The Security team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources. You have been hired as an AWS Certified Security Specialist to spearhead this strategic initiative.\n\nWhich of the following strategies would you adopt to address these business requirements for continuously assessing, auditing, and monitoring the configurations of AWS resources? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394804, "assessment_type": "multi-select", "prompt": {"question": "<p>A retail company has a three-tier web application with separate subnets for Web, Application, and Database tiers. The CTO at the company wants to monitor any malicious activity targeting the web application running on EC2 instances. You have been tasked with developing a solution to notify the security team in case the network exposure of EC2 instances on specific ports violates the security policies of the company.</p>\n\n<p>Which AWS Services would you use to build an automated notification system to meet these requirements with the least development effort? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon SNS</strong></p>\n\n<p><strong>Amazon Inspector</strong></p>\n\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n\n<p>You can perform network security assessments via your own custom solutions, however, that entails significant time and effort. You might need to run network port-scanning tools to test routing and firewall configurations, then validate what processes are listening on your instance network ports, before finally mapping the IPs identified in the port scan back to the host\u2019s owner.</p>\n\n<p>To make this process simpler for its customers, AWS offers the Network Reachability rules package in Amazon Inspector, which is an automated security assessment service that enables you to understand and improve the security and compliance of applications deployed on AWS. The existing Amazon Inspector host assessment rules packages check the software and configurations on your Amazon Elastic Compute Cloud (Amazon EC2) instances for vulnerabilities and deviations from best practices.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>You can use these rules packages to analyze the accessibility of critical ports, as well as all other network ports. For critical ports, Amazon Inspector will show the exposure of each and will offer findings per port. When critical, well-known ports (based on Amazon\u2019s standard guidance) are reachable, findings will be created with higher severities.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q30-i2.jpg\">\nvia <a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n\n<p>The findings also have recommendations that include information about exactly which Security Group you can edit to remove the access. And like all Amazon Inspector findings, these can be published to an SNS topic for additional processing or you could use a Lambda to automatically remove ingress rules in the Security Group to address a network reachability finding. For the given use case, the network engineer can use the SNS topic to send notifications to the team.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Shield</strong> - AWS Shield is a managed service that protects against Distributed Denial of Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is automatically enabled for all AWS customers at no additional cost. AWS Shield Advanced is an optional paid service. AWS Shield Advanced provides additional protections against more sophisticated and larger attacks for your applications running on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53. AWS Shield cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch cannot be used to assess network exposure of EC2 instances on specific ports.</p>\n\n<p><strong>VPC Flow Logs</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. You can use VPC Flow Logs to assess network exposure of EC2 instances on specific ports but the solution would entail significant development effort to parse through the logs and identify the exposed ports. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\">https://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/</a></p>\n", "answers": ["<p>Amazon SNS</p>", "<p>Amazon Inspector</p>", "<p>AWS Shield</p>", "<p>Amazon CloudWatch</p>", "<p>VPC Flow Logs</p>"]}, "correct_response": ["a", "b"], "section": "Logging and Monitoring", "question_plain": "A retail company has a three-tier web application with separate subnets for Web, Application, and Database tiers. The CTO at the company wants to monitor any malicious activity targeting the web application running on EC2 instances. You have been tasked with developing a solution to notify the security team in case the network exposure of EC2 instances on specific ports violates the security policies of the company.\n\nWhich AWS Services would you use to build an automated notification system to meet these requirements with the least development effort? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394806, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A mid-sized company recently deployed Amazon GuardDuty to monitor their AWS environment for potential security threats. The security team noticed a high number of RDP brute force attacks originating from an Amazon EC2 instance and decided to take action to prevent any issues. The company's security engineer was tasked with implementing an automated solution that could block the suspicious instance until the issue could be investigated and remediated.</p>\n\n<p>Which of the following solutions should the security engineer implement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure a Lambda function to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</strong></p>\n\n<p>AWS Security Hub provides you with a comprehensive view of your security state in AWS and helps you check your environment against security industry standards and best practices.</p>\n\n<p>Security Hub collects security data from across AWS accounts, services (such as GuardDuty), and supported third-party partner products and helps you analyze your security trends and identify the highest priority security issues.</p>\n\n<p>How Security Hub works:\n<img src=\"https://d1.awsstatic.com/Digital%20Marketing/House/Hero/products/Security%20Hub/Product-Page-Diagram_AWS-Security-Hub%402x.7e7c0483e9ce1507af2e9214247a1825a27d6bde.png\">\nvia - <a href=\"https://aws.amazon.com/security-hub/\">https://aws.amazon.com/security-hub/</a></p>\n\n<p>Leveraging Amazon EventBridge's integration with Security Hub, you can automate your AWS services to respond automatically to system events such as application availability issues or resource changes. Events from AWS services are delivered to EventBridge in near-real time and on a guaranteed basis. You can write simple rules to indicate which events you are interested in and what automated actions to take when an event matches a rule. The actions that can be automatically triggered include the following:</p>\n\n<p>Invoking an AWS Lambda function</p>\n\n<p>Invoking the Amazon EC2 run command</p>\n\n<p>Relaying the event to Amazon Kinesis Data Streams</p>\n\n<p>Activating an AWS Step Functions state machine</p>\n\n<p>Notifying an Amazon SNS topic or an Amazon SQS queue</p>\n\n<p>Sending a finding to a third-party ticketing, chat, SIEM, or incident response and management tool</p>\n\n<p>For the given use case, you can process the Security Hub events in Kinesis Data Streams by using a Lambda function that monitors any <code>UnauthorizedAccess:EC2/RDPBruteForce\n</code> finding from GuardDuty that is relayed via Security Hub. This finding informs you that an EC2 instance in your AWS environment was involved in a brute force attack aimed at obtaining passwords to RDP services on Windows-based systems. This can indicate unauthorized access to your AWS resources. When the Lambda function sees a matching finding, it can block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the WAF web ACL</strong> - WAF web ACL can only be applied to the following resource types: CloudFront distribution, Amazon API Gateway REST API, Application Load Balancer, AWS AppSync GraphQL API and Amazon Cognito user pool. You can use AWS WAF to control how your protected resources respond to HTTP(S) web requests. The given use case is about RDP brute force attacks originating from an EC2 instance, so using WAF web ACL is not relevant, as it cannot monitor traffic originating from an EC2 instance.</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the network ACL rules</strong> - Using Network ACL rules would impact all instances in a subnet. It will not isolate the traffic only for the suspicious instance. Hence this option is incorrect.</p>\n\n<p><strong>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure Kinesis Data Analytics to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</strong> - Amazon Kinesis Data Analytics can be used to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open-source framework and engine for processing data streams. Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services. This option has been added as a distractor as Kinesis Data Analytics cannot be used to update the security groups for an instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/security-hub/\">https://aws.amazon.com/security-hub/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#unauthorizedaccess-ec2-rdpbruteforce\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#unauthorizedaccess-ec2-rdpbruteforce</a></p>\n", "answers": ["<p>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure Kinesis Data Analytics to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</p>", "<p>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the WAF web ACL</p>", "<p>Have Security Hub ingest GuardDuty findings and send events to EventBridge that triggers a Lambda function to block traffic to/from the suspicious instance by updating the network ACL rules</p>", "<p>Have Security Hub ingest GuardDuty findings and send events to Kinesis Data Streams via EventBridge. Configure a Lambda function to process the data stream and block traffic to/from the suspicious instance by updating the security group so that it has no inbound and outbound rules</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "A mid-sized company recently deployed Amazon GuardDuty to monitor their AWS environment for potential security threats. The security team noticed a high number of RDP brute force attacks originating from an Amazon EC2 instance and decided to take action to prevent any issues. The company's security engineer was tasked with implementing an automated solution that could block the suspicious instance until the issue could be investigated and remediated.\n\nWhich of the following solutions should the security engineer implement?", "related_lectures": []}, {"_class": "assessment", "id": 70394808, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A cybersecurity company is using AWS Systems Manager Session Manager to manage Amazon EC2 instances in the us-east-1 AWS Region. A user is unable to connect to a new EC2 instance that runs Amazon Linux 2 in a private subnet in a newly created VPC. The systems administrator has confirmed that the new EC2 instance has the correct IAM instance profile attached.</p>\n\n<p>As an AWS Certified Security Specialist, what would you attribute as the root cause behind this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The EC2 instance is in a private subnet and it does not have the <code>com.amazonaws.us-east-1.ssmmessages</code> VPC endpoint for Session Manager</strong></p>\n\n<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, on-premises servers, and virtual machines (VMs). You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI).</p>\n\n<p>If you want to use Systems Manager to manage private EC2 instances without internet access, you need to create VPC endpoint for Session Manager that uses <code>com.amazonaws.us-east-1.ssmmessages</code> as the service name. Systems Manager uses the ssmmessages endpoint for API operations from SSM Agent to Session Manager, a capability of AWS Systems Manager, in the cloud. This endpoint is required to create and delete session channels with the Session Manager service in the cloud.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance security group has no rule to allow inbound SSH traffic on port 22</strong></p>\n\n<p><strong>The EC2 key pair associated with the EC2 instance is invalid for the given user</strong></p>\n\n<p><strong>There is no bastion host to facilitate connection from the AWS Systems Manager Session Manager</strong></p>\n\n<p>Session Manager provides secure and auditable node management without the need to open inbound SSH ports, maintain bastion hosts, or manage SSH keys. So these three options are incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt1-q32-i2.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\">https://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-messageAPIs.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-setting-up-messageAPIs.html</a></p>\n", "answers": ["<p>The EC2 instance security group has no rule to allow inbound SSH traffic on port 22</p>", "<p>There is no bastion host to facilitate connection from the AWS Systems Manager Session Manager</p>", "<p>The EC2 key pair associated with the EC2 instance is invalid for the given user</p>", "<p>The EC2 instance is in a private subnet and it does not have the <code>com.amazonaws.us-east-1.ssmmessages</code> VPC endpoint for Session Manager</p>"]}, "correct_response": ["d"], "section": "Infrastructure Security", "question_plain": "A cybersecurity company is using AWS Systems Manager Session Manager to manage Amazon EC2 instances in the us-east-1 AWS Region. A user is unable to connect to a new EC2 instance that runs Amazon Linux 2 in a private subnet in a newly created VPC. The systems administrator has confirmed that the new EC2 instance has the correct IAM instance profile attached.\n\nAs an AWS Certified Security Specialist, what would you attribute as the root cause behind this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394810, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has moved its business-critical data to an Amazon EFS file system which will be accessed by multiple EC2 instances.</p>\n\n<p>Which of the following would you recommend to exercise access control such that only the permitted EC2 instances can read from the EFS file system? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use VPC security groups to control the network traffic to and from your file system</strong></p>\n\n<p><strong>Use an IAM policy to control access for clients who can mount your file system with the required permissions</strong></p>\n\n<p>You control which EC2 instances can access your EFS file system by using VPC security group rules and AWS Identity and Access Management (IAM) policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and you may use EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.</p>\n\n<p>Files and directories in an Amazon EFS file system support standard Unix-style read, write, and execute permissions based on the user ID and group IDs. When an NFS client mounts an EFS file system without using an access point, the user ID and group ID provided by the client is trusted. You can also use EFS access points to override user ID and group IDs used by the NFS client. When users attempt to access files and directories, Amazon EFS checks their user IDs and group IDs to verify that each user has permission to access the objects</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</strong> - Network ACLs operate at the subnet level and not at the instance level.</p>\n\n<p><strong>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</strong> - There is no such thing as an IAM policy root credentials and this statement has been added as a distractor.</p>\n\n<p><strong>Use Amazon GuardDuty to curb unwanted access to the EFS file system</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. It cannot be used for access control to the EFS file system.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html\">https://docs.aws.amazon.com/efs/latest/ug/accessing-fs-nfs-permissions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html\">https://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html</a></p>\n", "answers": ["<p>Use VPC security groups to control the network traffic to and from your file system</p>", "<p>Use an IAM policy to control access for clients who can mount your file system with the required permissions</p>", "<p>Use Network ACLs to control the network traffic to and from your Amazon EC2 instance</p>", "<p>Use Amazon GuardDuty to curb unwanted access to the EFS file system</p>", "<p>Set up the IAM policy root credentials to control and configure the clients accessing the EFS file system</p>"]}, "correct_response": ["a", "b"], "section": "Infrastructure Security", "question_plain": "A company has moved its business-critical data to an Amazon EFS file system which will be accessed by multiple EC2 instances.\n\nWhich of the following would you recommend to exercise access control such that only the permitted EC2 instances can read from the EFS file system? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394812, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A business maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the business has moved from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all data to the Amazon S3 bucket, the business is looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.</p>\n\n<p>How will you implement this requirement without adding the overhead of splitting the data into logical groups?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong></p>\n\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. When you use server-side encryption with Amazon S3 managed keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates.</p>\n\n<p>Note: Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</strong> - Server-side encryption with Amazon S3 managed keys (SSE-S3) is the easiest way to implement the given requirement, as there is no additional overhead of splitting data. Multiple S3 buckets are redundant for this requirement.</p>\n\n<p><strong>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</strong> - Server-side encryption is the encryption of data at its destination by the application or service that receives it. The requirement is about server-side encryption and not about client-side encryption, hence this choice is incorrect.</p>\n\n<p><strong>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</strong> - An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>\n", "answers": ["<p>Store the logically divided data into different Amazon S3 buckets. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>", "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data</p>", "<p>Use Multi-Region keys for client-side encryption in the AWS S3 Encryption Client to generate unique keys for each file of data</p>", "<p>Configure a single Amazon S3 bucket to hold all data. Use server-side encryption with AWS KMS (SSE-KMS) and use encryption context to generate a different key for each file/object that you store in the S3 bucket</p>"]}, "correct_response": ["b"], "section": "Data Protection", "question_plain": "A business maintains its business-critical customer data on an on-premises system in an encrypted format. Over the years, the business has moved from using a single encryption key to multiple encryption keys by dividing the data into logical chunks. With the decision to move all data to the Amazon S3 bucket, the business is looking for a technique to encrypt each file with a different encryption key to provide maximum security to the migrated on-premises data.\n\nHow will you implement this requirement without adding the overhead of splitting the data into logical groups?", "related_lectures": []}, {"_class": "assessment", "id": 70394814, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As per the latest security guidelines of a company, root user login access should be intimated to the security team every time it is used.</p>\n\n<p>How will you create a solution for this requirement in the most efficient way?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon EventBridge event rule to monitor <code>userIdentity</code> root logins from the AWS Management Console and trigger notifications to the SNS topic when root user login activity is detected</strong></p>\n\n<p>This option is the most efficient way of configuring the given requirement. You can even launch an AWS CloudFormation stack to create an Amazon Simple Notification Service (Amazon SNS) topic. Then, create an Amazon EventBridge event rule to monitor userIdentity root logins from the AWS Management Console.</p>\n\n<p>Before you begin, confirm that the AWS CloudTrail Management read/write events are set to <code>All</code> or <code>Write-only</code> for EventBridge events to trigger the log-in event notification.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon CloudWatch Events rule that detects any AWS account root user API events. This rule triggers an AWS Lambda function which publishes the message to the created SNS topic</strong> - This option will be the right choice if all root user activities have to be notified. Since the use case only asks for root user login access notifications, this option is not the most efficient way of configuring the requirement.</p>\n\n<p>AWS suggests using Amazon EventBridge to manage your events. CloudWatch Events and EventBridge are the same underlying service and API, but EventBridge provides more features.</p>\n\n<p><strong>Save the AWS CloudTrail logs to an Amazon S3 bucket in the AWS account used by the security team. Analyze the logs using AWS Athena. Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Configure a Lambda function to run an Athena query and trigger notifications to this SNS topic when root user login is detected</strong> - This option is an unnecessarily complicated solution since Amazon CloudWatch Events or Amazon EventBridge events offer better features to easily implement the asked requirement.</p>\n\n<p><strong>Send the data of VPC Flow logs to Amazon Simple Queue Service (SQS). Use the AWS Lambda function to process these messages and send notifications to the SNS topic in case root user login activity is detected</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. It does not deal with user access information and hence is an incorrect choice for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/root-user-account-eventbridge-rule/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/\">https://aws.amazon.com/blogs/mt/monitor-and-notify-on-aws-account-root-user-activity/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n", "answers": ["<p>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon CloudWatch Events rule that detects any AWS account root user API events. This rule triggers an AWS Lambda function which publishes the message to the created SNS topic</p>", "<p>Save the AWS CloudTrail logs to an Amazon S3 bucket in the AWS account used by the security team. Analyze the logs using AWS Athena. Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Configure a Lambda function to run an Athena query and trigger notifications to this SNS topic when root user login is detected</p>", "<p>Send the data of VPC Flow logs to Amazon Simple Queue Service (SQS). Use AWS Lambda function to process these messages and send notifications to SNS topic in case root user login activity is detected</p>", "<p>Create an Amazon Simple Notification Service (Amazon SNS) topic and configure the users of the security team as subscribers to the topic. Create an Amazon EventBridge event rule to monitor <code>userIdentity</code> root logins from the AWS Management Console and trigger notifications to the SNS topic when root user login activity is detected</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "As per the latest security guidelines of a company, root user login access should be intimated to the security team every time it is used.\n\nHow will you create a solution for this requirement in the most efficient way?", "related_lectures": []}]}
5789442
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70394880, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a security engineer for an IT company, you have received a notice from AWS that the resources for your company's AWS account were reported for abusive activity.</p>\n\n<p>What should be your course of action after receiving the notice?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Review the abuse notice and reply explaining how you will prevent the abusive activity from recurring in the future</strong> - The AWS Trust &amp; Safety Team sends abuse reports to the security contact on your account. If there is no security contact listed, the AWS Trust &amp; Safety Team contacts you using the email address listed on your account.</p>\n\n<p>If you receive an abuse notice from AWS, do the following:</p>\n\n<ol>\n<li><p>Review the abuse notice to see what content or activity was reported. Logs that implicate abuse are included along with the abuse report, as provided by the reporter.</p></li>\n<li><p>Reply directly to the abuse report and explain how you're preventing the abusive activity from recurring in the future.</p></li>\n</ol>\n\n<p>If you don't respond to an abuse notice within 24 hours, AWS might block your resources or suspend your AWS account. If more information is required, reply directly to the email from the AWS Trust &amp; Safety Team. The team can request additional information from the reporter.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS Trust &amp; Safety Team provides technical support for issues related to abusive activity. Contact the team and resolve the issue with their assistance</strong></p>\n\n<p><strong>The technical support provided by AWS Trust &amp; Safety Team is available for only Enterprise and Business accounts. Upgrade your account and then contact the AWS Trust &amp; Safety Team for technical support. Otherwise, you need to contact the AWS Support team</strong></p>\n\n<p>These two options are incorrect since the AWS Trust &amp; Safety Team doesn't provide technical support.</p>\n\n<p>You should also note that the Developer, Business, Enterprise On-Ramp, or Enterprise Support plans provide one-on-one fast-response support from experienced technical support engineers. With these Support plans, you get pay-by-the-month pricing and unlimited support cases. If you have operational issues or technical questions, you can contact a team of support engineers and receive predictable response times and personalized support.</p>\n\n<p><strong>Make sure that your instances and all applications are properly secured as per the shared responsibility model</strong> - Security and Compliance is a shared responsibility between AWS and the customer. This shared responsibility model can help relieve the customer\u2019s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall. The shared responsibility model provides guidelines and best practices that can help keep all your AWS resources safe. However, it is not the correct solution for the given issue.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/whitepapers/latest/aws-risk-and-compliance/images/image2.png\">\nvia - <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aws-abuse-report/\">https://aws.amazon.com/premiumsupport/knowledge-center/aws-abuse-report/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/get-aws-technical-support/\">https://aws.amazon.com/premiumsupport/knowledge-center/get-aws-technical-support/</a></p>\n\n<p><a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n", "answers": ["<p>The AWS Trust &amp; Safety Team provides technical support for issues related to abusive activity. Contact the team and resolve the issue with their assistance</p>", "<p>The technical support provided by AWS Trust &amp; Safety Team is available for only Enterprise and Business accounts. Upgrade your account and then contact the AWS Trust &amp; Safety Team for technical support. Otherwise, you need to contact the AWS Support team</p>", "<p>Make sure that your instances and all applications are properly secured as per the shared responsibility model</p>", "<p>Review the abuse notice and reply explaining how you will prevent the abusive activity from recurring in the future</p>"]}, "correct_response": ["d"], "section": "Incident Response", "question_plain": "As a security engineer for an IT company, you have received a notice from AWS that the resources for your company's AWS account were reported for abusive activity.\n\nWhat should be your course of action after receiving the notice?", "related_lectures": []}, {"_class": "assessment", "id": 70394882, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An AWS Firewall Manager policy scope has been defined for all resources of an AWS Organization. Due to a recent organization-wide resource optimization effort, a Security Engineer is reviewing the status of several out-of-scope resources that were earlier covered under the policy.</p>\n\n<p>Which of the following correctly outlines the default behavior of AWS Firewall Manager for the given context?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:\n<strong>The associated AWS Config-managed rules are deleted. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</strong></p>\n\n<p>The policy scope defines where the policy applies. You can either apply centrally controlled policies to all of your accounts and resources within your organization in AWS Organizations or to a subset of your accounts and resources.\nTo determine which resources should be removed from protection when a customer resource leaves the policy scope, Firewall Manager follows these guidelines:</p>\n\n<p>Default behavior:\n1. The associated AWS Config-managed rules are deleted. This behavior is independent of the check box.\n2. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. This behavior is independent of the check box.\n3. Any protected resource that goes out of scope remains associated and protected. For example, an Application Load Balancer or API from API Gateway that's associated with a web ACL remains associated with the web ACL, and the protection remains in place.</p>\n\n<p>Firewall Manager behavior:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html\">https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>An Application Load Balancer that's associated with a web ACL is deleted from the web ACL while the protection remains in place. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</strong></p>\n\n<p><strong>Any protected resource that goes out of scope is automatically disassociated and removed from protection when it leaves the policy scope</strong></p>\n\n<p><strong>Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. Similarly, an Amazon EC2 instance is automatically disassociated from the replicated security group when it leaves the policy scope</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html\">https://docs.aws.amazon.com/waf/latest/developerguide/policy-scope.html</a></p>\n", "answers": ["<p>The associated AWS Config managed rules are deleted. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</p>", "<p>An Application Load Balancer that's associated with a web ACL is deleted from the web ACL while the protection remains in place. Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted</p>", "<p>Any protected resource that goes out of scope is automatically disassociated and removed from protection when it leaves the policy scope</p>", "<p>Any associated AWS WAF web access control lists (web ACLs) that don't contain any resources are deleted. Similarly, an Amazon EC2 instance is automatically disassociated from the replicated security group when it leaves the policy scope</p>"]}, "correct_response": ["a"], "section": "Infrastructure Security", "question_plain": "An AWS Firewall Manager policy scope has been defined for all resources of an AWS Organization. Due to a recent organization-wide resource optimization effort, a Security Engineer is reviewing the status of several out-of-scope resources that were earlier covered under the policy.\n\nWhich of the following correctly outlines the default behavior of AWS Firewall Manager for the given context?", "related_lectures": []}, {"_class": "assessment", "id": 70394884, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a Security Specialist, you have been asked to create an AWS Identity and Access Management (IAM) policy that explicitly grants permissions to an IAM role for creating and managing Amazon Elastic Compute Cloud (Amazon EC2) instances in a specified VPC. The policy must limit permissions so that the IAM role can only create EC2 instances with specific tags and then manage those EC2 instances in a VPC by using those tags.</p>\n\n<p>Which of the following solutions will meet this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use policy condition <code>ec2:ResourceTags</code> to limit control to instances</strong></p>\n\n<p>Amazon EC2 provides limited supported resource-level permissions, but there are several actions, resources, and conditions to consider. Certain Amazon EC2 API actions, such as launching an EC2 instance, can be controlled through the VPC ARN using tags to control the instances.</p>\n\n<p>For the given use case, you can apply a custom IAM policy to restrict the permissions of an IAM user, group, or role for creating EC2 instances in a specified VPC with tags. Use policy condition \"ec2:ResourceTags\" to limit control to instances. This policy grants permission to launch EC2 instances in a designated VPC with a unique tag. You can then manage those EC2 instances using restrictive tags.</p>\n\n<p>Check out the relevant snippet of the policy definition here:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}</strong> - If you're assigning this policy to only IAM users or groups, then you can replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}. Since the IAM role is being used in this example, this configuration is incorrect.</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>aws:sourceVPC</code> so that it also limits the instances within the specified VPC</strong> - This option has been added as a distractor. You can create an S3 bucket policy that restricts access for the VPC endpoint to a specific VPC by using the aws:SourceVpc condition. This is useful if you have multiple VPC endpoints configured in the same VPC, and you want to manage access to your Amazon S3 buckets for all of your endpoints.</p>\n\n<p><strong>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use the policy condition <code>ec2:CreateTags</code> to limit control to instances</strong> - This option has been added as a distractor. You should note that <code>ec2:CreateTags</code> is an action and not a policy condition. In addition, the <code>ec2:CreateTags</code> action is used for creating tags for ec2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-permission-ec2-tags-vpc/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html</a></p>\n", "answers": ["<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Replace the TAG-KEY or TAG-VALUE parameters with the IAM policy variable ${aws:username}</p>", "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>ec2:ResourceTags</code> to limit control to instances</p>", "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags using the policy condition <code>aws:sourceVPC</code> so that it also limits the instances within the specified VPC</p>", "<p>Apply a custom IAM policy to restrict the permissions of the IAM role for creating EC2 instances in a specified VPC with tags. Use policy condition <code>ec2:CreateTags</code> to limit control to instances</p>"]}, "correct_response": ["b"], "section": "Identity and Access Management", "question_plain": "As a Security Specialist, you have been asked to create an AWS Identity and Access Management (IAM) policy that explicitly grants permissions to an IAM role for creating and managing Amazon Elastic Compute Cloud (Amazon EC2) instances in a specified VPC. The policy must limit permissions so that the IAM role can only create EC2 instances with specific tags and then manage those EC2 instances in a VPC by using those tags.\n\nWhich of the following solutions will meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394886, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A member of the security team accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. As an AWS Certified Security Specialist, you have been tasked by the company to provide a solution for this issue.</p>\n\n<p>Which of the following steps would you recommend to solve this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</strong></p>\n\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.</p>\n\n<p>Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a CMK in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the CMK status and key state is Pending deletion. To recover the CMK, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the CMK.</p>\n\n<p>How Deleting Customer Master Keys Works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Contact AWS support to retrieve the CMK from their backup</strong></p>\n\n<p><strong>The CMK can be recovered by the AWS root account user</strong></p>\n\n<p>The AWS root account user cannot recover CMK and the AWS support does not have access to CMK via any backups. Both these options have been added as distractors.</p>\n\n<p><strong>The company should issue a notification on its web application informing the users about the loss of their data</strong> - This option is not required as the data can be recovered via the cancel key deletion feature.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-scheduling-key-deletion.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-scheduling-key-deletion.html</a></p>\n", "answers": ["<p>Contact AWS support to retrieve the CMK from their backup</p>", "<p>The company should issue a notification on its web application informing the users about the loss of their data</p>", "<p>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</p>", "<p>The CMK can be recovered by the AWS root account user</p>"]}, "correct_response": ["c"], "section": "Data Protection", "question_plain": "A media company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A member of the security team accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. As an AWS Certified Security Specialist, you have been tasked by the company to provide a solution for this issue.\n\nWhich of the following steps would you recommend to solve this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394888, "assessment_type": "multi-select", "prompt": {"question": "<p>The security team at a financial services company has received a notification that the resources in the company's AWS account might be compromised.</p>\n\n<p>What actions would you recommend to handle this issue? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys</strong></p>\n\n<p>If your application currently uses access keys, you need to replace the existing keys with new ones. To start, create a new access key. Then, modify your application to use the new access key. You can then deactivate the original access keys that your application no longer uses. Verify that there aren't any issues with your application. If everything works, then you can delete the original access keys.</p>\n\n<p><strong>Check your AWS account bill to know the charged resources</strong></p>\n\n<p>The Bills page of your AWS Management Console lists all charges for all resources on your account. Check your bill for the following:</p>\n\n<ol>\n<li>AWS services that you don't normally use</li>\n<li>Resources in AWS Regions that you don't normally use</li>\n<li>A significant change in the size of your bill</li>\n</ol>\n\n<p>You can use this information to help you to delete or terminate any resources you don't want to keep.</p>\n\n<p><strong>Use AWS Git projects to scan for evidence of unauthorized use</strong></p>\n\n<p>AWS offers Git projects that you can install to help you protect your account:</p>\n\n<ol>\n<li><p>Git Secrets can scan merges, commits, and commit messages for secret information (that is, access keys). If Git Secrets detects prohibited regular expressions, it can reject those commits from being posted to public repositories.</p></li>\n<li><p>Use AWS Step Functions and AWS Lambda to generate Amazon CloudWatch Events from AWS Health or by AWS Trusted Advisor. If there is evidence that your access keys are exposed, the projects can help you to automatically detect, log, and mitigate the event.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the health check report in AWS Systems Manager (formerly known as SSM) to find out the details about the compromised AWS resources</strong> - AWS Systems Manager (formerly known as SSM) is an AWS service that you can use to view and control your infrastructure on AWS. Systems Manager cannot be used to know if a resource is compromised.</p>\n\n<p><strong>Use AWS Trusted Advisor security check report to find out the details about the compromised AWS resources</strong> - AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor can help improve the security of your AWS environment by suggesting foundational security best practices curated by security experts. Trusted Advisor provides suggestions based on best practices and cannot be used to know if a resource is compromised.</p>\n\n<p><strong>Use Amazon Inspector to detect the compromised resources of your account</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single click. Inspector cannot be used to detect the compromised resources of your account.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n", "answers": ["<p>Use the health check report in AWS Systems Manager (formerly known as SSM) to find out the details about the compromised AWS resources</p>", "<p>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys</p>", "<p>Use AWS Git projects to scan for evidence of unauthorized use</p>", "<p>Use AWS Trusted Advisor security check report to find out the details about the compromised AWS resources</p>", "<p>Check your AWS account bill to know the charged resources</p>", "<p>Use Amazon Inspector to detect the compromised resources of your account</p>"]}, "correct_response": ["b", "c", "e"], "section": "Infrastructure Security", "question_plain": "The security team at a financial services company has received a notification that the resources in the company's AWS account might be compromised.\n\nWhat actions would you recommend to handle this issue? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394890, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The IT Security team at a financial services firm has informed that a user's AWS access key has been found on the internet. As a security engineer, you must ensure that the access key is immediately disabled and the user's activities must be assessed for a potential breach.</p>\n\n<p>Which steps must be taken to meet the above needs?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete or rotate the user\u2019s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</strong></p>\n\n<p>Deleting or rotating the user\u2019s access key ensures that it is not further used for any unauthorized activities. AWS CloudTrail logs will log the user access key usage which will help in tracking the AWS resources for which the key has been used. This information is valuable in narrowing down on any unauthorized resources created by using the access key or any existing resources modified by using the access key.</p>\n\n<p>Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best practice, use temporary security credentials (IAM roles) instead of access keys and disable any AWS account root user access keys.</p>\n\n<p>If you still need to use long-term access keys, you can create, modify, view, or rotate your access keys (access key IDs and secret access keys). You can have a maximum of two access keys. This allows you to rotate the active keys according to best practices.</p>\n\n<p>Rotating IAM user access keys from the console:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey</a></p>\n\n<p>As AWS CloudTrail logs API activity for supported services, it provides an audit trail of your AWS account that you can use to track the history of an adversary. The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights, and does not require any tools to be installed. Refer to the complete list of steps below:</p>\n\n<p>Steps to investigate AWS CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q6-i2.jpg\">\nvia - <a href=\"https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/\">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</strong> - Deleting the IAM user will also remove all the bonafide resources that are created by the user account (resources, policies, tags, S3 buckets, etc). Hence, this option is not a workable solution.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Rotate the user's key and re-deploy all the resources with the new credentials</strong> - This option does not mention the steps needed to access the historic usage of the access key, which is crucial to identify any potential breach.</p>\n\n<p><strong>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</strong> - This option does not address any of the identified issues mentioned in the use case, such as, disabling the current key and assessing any potential breach.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/\">https://www.wellarchitectedlabs.com/security/300_labs/300_incident_response_with_aws_console_and_cli/2_iam/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/security-logging-and-monitoring.html</a></p>\n", "answers": ["<p>Delete or rotate the user\u2019s key. Review the AWS CloudTrail logs in all AWS regions and delete any unauthorized resources created or updated</p>", "<p>Delete the IAM user and all the resources created by the user. Create fresh user credentials and relaunch the resources from this user</p>", "<p>Call on the user to remove the access credentials from the internet. Rotate the user's key and re-deploy all the resources with the new credentials</p>", "<p>Call on the user to remove the access credentials from the internet. Report abuse to AWS Trust &amp; Safety team</p>"]}, "correct_response": ["a"], "section": "Identity and Access Management", "question_plain": "The IT Security team at a financial services firm has informed that a user's AWS access key has been found on the internet. As a security engineer, you must ensure that the access key is immediately disabled and the user's activities must be assessed for a potential breach.\n\nWhich steps must be taken to meet the above needs?", "related_lectures": []}, {"_class": "assessment", "id": 70394892, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The latest guidelines issued by the security team at a company mandate an application to block HTTP requests that don't have a User-Agent header or have a specific User-Agent in the request.</p>\n\n<p>How will you block these requests using AWS WAF?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using custom Rules. Block requests that don\u2019t contain a User-Agent header using either AWS Managed Rules or custom rules</strong></p>\n\n<p>By default, AWS WAF filters don't check whether the HTTP request parameters are present or not. To check whether the HTTP request parameters are present or not, do the following:</p>\n\n<p>Block requests that don't contain a User-Agent header using AWS Managed Rules.</p>\n\n<p>-or-</p>\n\n<p>Block requests that don't contain a User-Agent header or block traffic if the requests contain a specific User-Agent using custom rules.</p>\n\n<p>The following rules inspect requests missing the HTTP User-Agent header and User-Agent strings that don't seem to be from a web browser:</p>\n\n<p>NoUserAgent_HEADER\nThis rule is from the Core rule set (CRS) managed rule group. This rule inspects for requests that are missing the HTTP User-Agent header.</p>\n\n<p>SignalNonBrowserUserAgent\nThis rule is from the AWS WAF Bot Control rule group. This rule inspects for User-Agent strings that don't seem to be from a web browser including requests with no User-Agent.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q7-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don\u2019t contain a User-Agent header using security group rules</strong></p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using custom rules. Block requests that don\u2019t contain a User-Agent header using security group rules</strong></p>\n\n<p>These two options have been added as distractors. Security group rules cannot be associated with a WAF. You should also note that security group rules can only allow requests.</p>\n\n<p><strong>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don\u2019t contain a User-Agent header using either AWS Managed Rules or custom rules</strong> - AWS Managed Rules can only be used to block requests that don\u2019t contain a User-Agent header. You cannot use the AWS Managed Rules to block requests that contain a specific User-Agent.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-regex-pattern-set-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-regex-pattern-set-match.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-http-requests-no-user-agent/</a></p>\n", "answers": ["<p>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don\u2019t contain a User-Agent header using security group rules</p>", "<p>Block requests that contain a specific User-Agent in the request using AWS Managed Rules. Block requests that don\u2019t contain a User-Agent header using either AWS Managed Rules or custom rules</p>", "<p>Block requests that contain a specific User-Agent in the request using custom rules. Block requests that don\u2019t contain a User-Agent header using security group rules</p>", "<p>Block requests that contain a specific User-Agent in the request using custom Rules. Block requests that don\u2019t contain a User-Agent header using either AWS Managed Rules or custom rules</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "The latest guidelines issued by the security team at a company mandate an application to block HTTP requests that don't have a User-Agent header or have a specific User-Agent in the request.\n\nHow will you block these requests using AWS WAF?", "related_lectures": []}, {"_class": "assessment", "id": 70394894, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare company only operates in the <code>us-east-1</code> region and stores encrypted data in S3 using SSE-KMS. Since the company wants to improve the backup and recovery architecture, it wants the encrypted data in S3 to be replicated into the <code>us-west-1</code> AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.</p>\n\n<p>Which of the following represents the best solution to address these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</strong></p>\n\n<p>AWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably \u2013 as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS.</p>\n\n<p>You can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key.</p>\n\n<p>Multi-region AWS KMS keys:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p>For the given use case, you must create a new bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. This would ensure that the data is available in another region for backup and recovery purposes. You should also enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key so that the data can be encrypted and decrypted using the same key in both AWS regions. Since the existing data in the current bucket was encrypted using the AWS KMS key restricted to the <code>us-east-1</code> region, so data must be copied to the new bucket in <code>us-east-1</code> region for replication as well as multi-region KMS key-based encryption to kick-in.</p>\n\n<p>To require server-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy. For example, the following bucket policy denies the upload object (s3:PutObject) permission to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS.</p>\n\n<pre><code>{\n   \"Version\":\"2012-10-17\",\n   \"Id\":\"PutObjectPolicy\",\n   \"Statement\":[{\n         \"Sid\":\"DenyUnEncryptedObjectUploads\",\n         \"Effect\":\"Deny\",\n         \"Principal\":\"*\",\n         \"Action\":\"s3:PutObject\",\n         \"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\",\n         \"Condition\":{\n            \"StringNotEquals\":{\n               \"s3:x-amz-server-side-encryption\":\"aws:kms\"\n            }\n         }\n      }\n   ]\n}\n</code></pre>\n\n<p>The following example IAM policies show statements for using AWS KMS server-side encryption with replication.</p>\n\n<p>In this example, the encryption context is the object ARN. If you use SSE-KMS with an S3 Bucket Key enabled, you must use the bucket ARN as the encryption context.</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n            \"Action\": [\"kms:Decrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"List of AWS KMS key ARNs used to encrypt source objects.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.source-bucket-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::source-bucket-name/key-prefix1/*\"\n                }\n            }\n        },\n\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-1-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-name-1/key-prefix1/*\"\n                }\n            }\n        },\n        {\n            \"Action\": [\"kms:Encrypt\"],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"kms:ViaService\": \"s3.destination-bucket-2-region.amazonaws.com\",\n                    \"kms:EncryptionContext:aws:s3:arn\": \"arn:aws:s3:::destination-bucket-2-name/key-prefix1*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</strong> - S3 batch replication can certainly be used to replicate the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region.</p>\n\n<p>However, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect.</p>\n\n<p><strong>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</strong> - You cannot share an AWS KMS key to another region, so this option is incorrect.</p>\n\n<p><strong>Set up a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</strong> - This option is a distractor as the daily frequency of data replication would result in significant data loss in case of a disaster. In addition, this option involves significant development effort to create the functionality to reliably replicate the data from source to destination buckets. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html</a></p>\n", "answers": ["<p>Set up a CloudWatch scheduled rule to invoke a Lambda function to copy the daily data from the source bucket in <code>us-east-1</code> region to the destination bucket in <code>us-west-1</code> region. Provide AWS KMS key access to the Lambda function for encryption and decryption operations on the data in the source and destination S3 buckets</p>", "<p>Set up a new S3 bucket in the <code>us-east-1</code> region with replication enabled from this new bucket into another bucket in <code>us-west-1</code> region. Enable SSE-KMS encryption on the new bucket in <code>us-east-1</code> region by using an AWS KMS multi-region key. Copy the existing data from the current S3 bucket in <code>us-east-1</code> region into this new S3 bucket in <code>us-east-1</code> region</p>", "<p>Change the AWS KMS single region key used for the current S3 bucket into an AWS KMS multi-region key. Enable S3 batch replication for the existing data in the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region</p>", "<p>Enable replication for the current bucket in <code>us-east-1</code> region into another bucket in <code>us-west-1</code> region. Share the existing AWS KMS key from <code>us-east-1</code> region to <code>us-west-1</code> region</p>"]}, "correct_response": ["b"], "section": "Data Protection", "question_plain": "A healthcare company only operates in the us-east-1 region and stores encrypted data in S3 using SSE-KMS. Since the company wants to improve the backup and recovery architecture, it wants the encrypted data in S3 to be replicated into the us-west-1 AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.\n\nWhich of the following represents the best solution to address these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70394896, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security team configured an Amazon CloudWatch alarm to notify one of the team members when a metric breaches a defined threshold for multiple periods in a row. But, the CloudWatch alarm is notifying the team after just one breach of the threshold.</p>\n\n<p>What is the issue and how will you fix the CloudWatch alarm to behave as expected?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as NOT BREACHING</strong></p>\n\n<p>Your CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point.</p>\n\n<p>You might have a series of non-breaching data points, followed by a single breaching data point, and then missing data points. Your alarm counts any missing data points following a breaching data point as additional breaches. This causes your alarm to notify you after only one data point breaches the defined threshold.</p>\n\n<p>When all data points are missing except one breaching data point, your alarm goes into an ALARM state. This happens when the oldest available breaching data point in the alarm's evaluation range is at least as old as the value of Datapoints to Alarm, and all other more recent data points are breaching or missing. This causes an ALARM state even if the total number of breaching data points is lower than the Datapoints to Alarm setting, and even when missing data is being treated as missing.</p>\n\n<p>Edit your alarm to do one or more of the following:\n1. If you don't want to be notified after a single breach when data points may be intermittent, change how missing data points are evaluated to NOT BREACHING.</p>\n\n<ol>\n<li><p>Use a longer period for a data point.</p></li>\n<li><p>Increase the number of evaluation periods before the alarm is triggered.</p></li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>CloudWatch alarm might not be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as BREACHING</strong> - This statement is incorrect, as it contradicts the explanation provided above.</p>\n\n<p><strong>If the CloudWatch alarm is unable to access the metric to be monitored, the alarm is raised as a default behavior</strong> - This option is invalid and given only as a distractor.</p>\n\n<p><strong>The metric must be reporting data only intermittently by design. For such metrics, AWS Lambda function is used to send continuous data as per business logic</strong> - Sometimes, not every expected data point for a metric gets reported to CloudWatch. For example, this can happen when a connection is lost, a server goes down, or when a metric reports data only intermittently by design. AWS Lambda function cannot be used for generating data points. Amazon CloudWatch can be configured to react a certain way when data is reported only intermittently.</p>\n\n<p>For each alarm, you can specify CloudWatch to treat missing data points as any of the following:</p>\n\n<ol>\n<li><p>notBreaching \u2013 Missing data points are treated as \"good\" and within the threshold</p></li>\n<li><p>breaching \u2013 Missing data points are treated as \"bad\" and breaching the threshold</p></li>\n<li><p>ignore \u2013 The current alarm state is maintained</p></li>\n<li><p>missing \u2013 If all data points in the alarm evaluation range are missing, the alarm transitions to INSUFFICIENT_DATA.</p></li>\n</ol>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarms-and-missing-data</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-alarm-single-data-point/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-alarm-single-data-point/</a></p>\n", "answers": ["<p>If the CloudWatch alarm is unable to access the metric to be monitored, the alarm is raised as a default behavior</p>", "<p>The metric must be reporting data only intermittently by design. For such metrics, the AWS Lambda function is used to send continuous data as per business logic</p>", "<p>CloudWatch alarm might not be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as BREACHING</p>", "<p>CloudWatch alarm might be configured to treat a missing data point the same way as a breaching data point. Configure the alarm to evaluate missing data points as NOT BREACHING</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "A security team configured an Amazon CloudWatch alarm to notify one of the team members when a metric breaches a defined threshold for multiple periods in a row. But, the CloudWatch alarm is notifying the team after just one breach of the threshold.\n\nWhat is the issue and how will you fix the CloudWatch alarm to behave as expected?", "related_lectures": []}, {"_class": "assessment", "id": 70394898, "assessment_type": "multi-select", "prompt": {"question": "<p>After a recent DDoS assault, the IT security team of a media company has asked the Security Engineer to revamp the security of the application to prevent future attacks. The website is hosted on an Amazon EC2 instance and data is maintained on Amazon RDS. A large part of the application data is static and this data is in the form of images.</p>\n\n<p>Which of the following steps can be combined to constitute the revamped security model? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use Amazon Route 53 to distribute traffic</strong></p>\n\n<p><strong>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync \u2013 services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end users. Blocked requests are stopped before they reach your web servers.</p>\n\n<p>Route 53 DNS requests and subsequent application traffic routed through CloudFront are inspected inline. Always-on monitoring, anomaly detection, and mitigation against common infrastructure DDoS attacks such as SYN/ACK floods, UDP floods, and reflection attacks are built into both Route 53 and CloudFront.</p>\n\n<p>Route 53 is also designed to withstand DNS query floods, which are real DNS requests that can continue for hours and attempt to exhaust DNS server resources. Route 53 uses shuffle sharding and anycast striping to spread DNS traffic across edge locations and help protect the availability of the service.</p>\n\n<p>When used with Amazon CloudFront distribution, AWS Shield adds security against DDoS attacks.</p>\n\n<p>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.</p>\n\n<p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</strong> - AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync \u2013 services that AWS customers commonly use to deliver content for their websites and applications. WAF cannot be directly configured in front of an ASG, so this option is incorrect.</p>\n\n<p><strong>Use Global Accelerator to distribute traffic</strong> - Global Accelerator is effective in traffic distribution across AWS Regions. However, the given use case needs services that can help mitigate DDoS attacks.</p>\n\n<p><strong>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. It cannot be used to mitigate DDoS attacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/\">https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/</a></p>\n\n<p><a href=\"https://aws.amazon.com/waf/faqs/b\">https://aws.amazon.com/waf/faqs/</a></p>\n", "answers": ["<p>Use Amazon Route 53 to distribute traffic</p>", "<p>Configure the Amazon EC2 instance with an Auto Scaling Group (ASG) to scale in case of a DDoS assault. Front the ASG with AWS Web Application Firewall (AWS WAF) for another layer of security</p>", "<p>Use Global Accelerator to distribute traffic</p>", "<p>Move the static content to Amazon S3, and front this with an Amazon CloudFront distribution. Configure another layer of protection by adding AWS Web Application Firewall (AWS WAF) to the CloudFront distribution</p>", "<p>Configure Amazon Inspector with AWS Security Hub to mitigate DDoS attacks by continual scanning that delivers near real-time vulnerability findings</p>"]}, "correct_response": ["a", "d"], "section": "Infrastructure Security", "question_plain": "After a recent DDoS assault, the IT security team of a media company has asked the Security Engineer to revamp the security of the application to prevent future attacks. The website is hosted on an Amazon EC2 instance and data is maintained on Amazon RDS. A large part of the application data is static and this data is in the form of images.\n\nWhich of the following steps can be combined to constitute the revamped security model? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394900, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses an Amazon S3 bucket to store its business-critical data. Recently, all the members of the development team, that access the given S3 bucket, have been given MFA devices. A security engineer must configure permissions such that access to the given S3 bucket is allowed only after MFA authentication.</p>\n\n<p>How will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"BoolIfExists\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></strong></p>\n\n<p>AWS recommends the use of <code>BoolIfExists</code> operator to check whether a request is authenticated using MFA. The aws:MultiFactorAuthPresent key is not present when an API or CLI command is called with long-term credentials, such as user access key pairs. Therefore AWS recommends that when you check for this key that you use the <code>IfExists</code> versions of the condition operators, like so:</p>\n\n<pre><code>    \"Effect\" : \"Deny\",\n    \"Condition\" : { \"BoolIfExists\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }\n</code></pre>\n\n<p>This combination of Deny, BoolIfExists, and false denies requests that are not authenticated using MFA. Specifically, it denies requests from temporary credentials that do not include MFA. It also denies requests that are made using long-term credentials, such as AWS CLI or AWS API operations made using access keys. The *IfExists operator checks for the presence of the aws:MultiFactorAuthPresent key and whether or not it could be present, as indicated by its existence. Use this when you want to deny any request that is not authenticated using MFA. This is more secure but can break any code or scripts that use access keys to access the AWS CLI or AWS API.</p>\n\n<p>Recommended Combination to mandate MFA through the policy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"Bool\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></strong> - This combination of the Deny effect, Bool element, and false value denies requests that can be authenticated using MFA, but were not. This applies only to temporary credentials that support using MFA. This statement does not deny access to requests that are made using long-term credentials, or to requests that are authenticated using MFA. Use this example with caution because its logic is complicated and it does not test whether MFA-authentication was actually used. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\": {\"BoolIfExists\": {\"aws:MultiFactorAuthPresent\": \"true\"}}</code></strong> - This condition matches either if the key exists and is present or if the key does not exist. This combination of Allow, BoolIfExists, and true allows requests that are authenticated using MFA, or requests that cannot be authenticated using MFA. This means that AWS CLI, AWS API, and AWS SDK operations are allowed when the requester uses their long-term access keys. This combination does not allow requests from temporary credentials that could, but do not include MFA. Therefore, this option is incorrect for the given use case.</p>\n\n<p><strong>Create an IAM group having the development team users. Add a customer-managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"BoolIfExists\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></strong> - This allows any request that is not authenticated using MFA. This is the opposite of what the use case demands.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-multifactorauthpresent</a></p>\n", "answers": ["<p>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"Bool\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></p>", "<p>Create an IAM group having the development team users. Add a customer-managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"BoolIfExists\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></p>", "<p>Create an IAM group having the development team users. Add a customer-managed policy with Deny Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\" : { \"BoolIfExists\" : { \"aws:MultiFactorAuthPresent\" : \"false\" } }</code></p>", "<p>Create an IAM group having the development team users. Add a customer managed policy with Allow Effect to the group for all s3:*actions with the condition defined as <code>\"Condition\": {\"BoolIfExists\": {\"aws:MultiFactorAuthPresent\": \"true\"}}</code></p>"]}, "correct_response": ["c"], "section": "Data Protection", "question_plain": "A company uses an Amazon S3 bucket to store its business-critical data. Recently, all the members of the development team, that access the given S3 bucket, have been given MFA devices. A security engineer must configure permissions such that access to the given S3 bucket is allowed only after MFA authentication.\n\nHow will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394902, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A web application is hosted on Amazon EC2 instances that are fronted by Application Load Balancer (ALB) configured with an Auto Scaling group (ASG). Enhanced security is provided to the ALB by AWS WAF web ACLs. As per the company's security policy, AWS CloudTrail is activated and logs are configured to be stored on Amazon S3 and CloudWatch Logs.</p>\n\n<p>A discount sales offer was run on the application for a week. The support team has noticed that a few of the instances have rebooted taking down the log files and all temporary data with them. Initial analysis has confirmed that the incident took place during off-peak hours. Even though the incident did not cause any sales or revenue loss, the CTO has asked the security team to fix the security error that has allowed the incident to go unnoticed and eventually untraceable.</p>\n\n<p>As Security Engineer, which series of steps will you implement to permanently record all traffic coming into the application?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</strong></p>\n\n<p>The logging destinations that you can choose from for your AWS WAF logs are:</p>\n\n<ol>\n<li>Amazon CloudWatch Logs</li>\n<li>Amazon Simple Storage Service</li>\n<li>Amazon Kinesis Data Firehose</li>\n</ol>\n\n<p>To send logs to Amazon Kinesis Data Firehose, you send logs from your web ACL to an Amazon Kinesis Data Firehose with a configured storage destination. After you enable logging, AWS WAF delivers logs to your storage destination through the HTTPS endpoint of Kinesis Data Firehose.</p>\n\n<p>One AWS WAF log is equivalent to one Kinesis Data Firehose record. If you typically receive 10,000 requests per second and you enable full logs, you should have 10,000 records per second setting in Kinesis Data Firehose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</strong> - As discussed above, the logging destinations that you can choose from for your AWS WAF logs are Amazon CloudWatch Logs, Amazon Simple Storage Service, and Amazon Kinesis Data Firehose. Amazon CloudTrail is not a valid destination for WAF ACL logs.</p>\n\n<p><strong>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</strong> - VPC Flow You should also note that VPC Flow Logs cannot capture all traffic coming into the application as these can only capture information about the IP traffic going to and from network interfaces in your VPC. In addition, VPC Flow Logs can be directly published only to the following destinations: Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose. So this option is incorrect.</p>\n\n<p><strong>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</strong> - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Elastic Load Balancing access logs are stored in Amazon S3 buckets and it is not possible to directly write the logs to Kinesis Data Firehose.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/\">https://aws.amazon.com/about-aws/whats-new/2021/12/awf-waf-cloudwatch-log-s3-bucket/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n", "answers": ["<p>Configure the WAF web ACL to deliver logs to Amazon CloudTrail and create a trail that applies to all Regions. This delivers log files from all Regions to an S3 bucket. Use Athena to query the logs for errors and tracking</p>", "<p>Configure the WAF web ACL to deliver logs to Amazon Kinesis Data Firehose, which should be configured to eventually store the logs in an Amazon S3 bucket. Use Athena to query the logs for errors and tracking</p>", "<p>To capture information about the IP traffic going to and from network interfaces, configure VPC Flow Logs to be directly streamed to Kinesis Data Streams and create alarms for automatic monitoring</p>", "<p>Configure Elastic Load Balancing to write access logs to Amazon Kinesis Data Firehose. The logs can be further directed from Firehose into an Amazon S3 bucket for further analysis and reporting</p>"]}, "correct_response": ["b"], "section": "Logging and Monitoring", "question_plain": "A web application is hosted on Amazon EC2 instances that are fronted by Application Load Balancer (ALB) configured with an Auto Scaling group (ASG). Enhanced security is provided to the ALB by AWS WAF web ACLs. As per the company's security policy, AWS CloudTrail is activated and logs are configured to be stored on Amazon S3 and CloudWatch Logs.\n\nA discount sales offer was run on the application for a week. The support team has noticed that a few of the instances have rebooted taking down the log files and all temporary data with them. Initial analysis has confirmed that the incident took place during off-peak hours. Even though the incident did not cause any sales or revenue loss, the CTO has asked the security team to fix the security error that has allowed the incident to go unnoticed and eventually untraceable.\n\nAs Security Engineer, which series of steps will you implement to permanently record all traffic coming into the application?", "related_lectures": []}, {"_class": "assessment", "id": 70394904, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A standard three-tier application is hosted on Amazon EC2 instances that are fronted by an Application Load Balancer. The application maintenance team has reported several small-scale malicious attacks on the application. The project manager has decided to ramp up the security of the application.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend as part of the best practices to scan and mitigate the known vulnerabilities?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Inspector to periodically scan the EC2 instances for vulnerabilities</strong></p>\n\n<p>When you have Amazon EC2 instances behind an Application Load Balancer, the instances themselves might not need to be publicly accessible. Instead, you could provide users with access to the Application Load Balancer on certain TCP ports and allow only the Application Load Balancer to communicate with the instances. All internet traffic to a security group is implicitly denied unless you create an allow rule to permit the traffic.</p>\n\n<p>For example, if you have a web application that uses Elastic Load Balancing and multiple Amazon EC2 instances, you might decide to create one security group for the Elastic Load Balancing (Elastic Load Balancing security group) and one for the instances (web application server security group). You can then create an allow rule to permit internet traffic to the ELB security group, and another rule to permit traffic from the ELB security group to the web application server security group. This ensures that internet traffic can\u2019t directly communicate with your Amazon EC2 instances, which makes it more difficult for an attacker to learn about and impact your application.</p>\n\n<p>To scan for known vulnerabilities, use Amazon Inspector. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p>You can enable Amazon Inspector for your entire organization or an individual account with a few clicks in the AWS Management Console. Once enabled, Amazon Inspector automatically discovers running Amazon EC2 instances and Amazon ECR repositories and immediately starts continually scanning workloads for software vulnerabilities and unintended network exposure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Systems Manager to periodically scan the EC2 instances for vulnerabilities</strong> -  The Amazon Inspector uses the Systems Manager (SSM) agent to collect the software application inventory of the Amazon EC2 instances. Then, Inspector scans this data and identifies software vulnerabilities, a crucial step in vulnerability management. The Systems Manager itself cannot scan the EC2 instances for vulnerabilities.</p>\n\n<p><strong>Use AWS Key Management Service (KMS) to encrypt the traffic between the client and application servers. Configure the application security groups to ensure that only the necessary ports are open</strong> - This option is a distractor as KMS cannot be used to encrypt the traffic between the client and application servers. AWS Key Management Service (AWS KMS) lets you create, manage, and control cryptographic keys across your applications. For the given scenario, you can use HTTPS to enable website encryption by running HTTP over the Transport Layer Security (TLS) protocol.</p>\n\n<p><strong>Install AWS Certificate Manager (ACM) SSL/TLS certificate on the EC2 instances to secure traffic moving to and from the application servers</strong> - ACM certificates cannot be installed on Amazon EC2 instances. An exception to this is a public ACM certificate that can be installed on Amazon EC2 instances that are connected to a Nitro Enclave. ACM certificates are supported by the following services: Elastic Load Balancing, Amazon CloudFront, Amazon Cognito, AWS Elastic Beanstalk, AWS App Runner, Amazon API Gateway, AWS CloudFormation, AWS Amplify, Amazon OpenSearch Service, and AWS Nitro Enclaves.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/automate-vulnerability-management-and-remediation-in-aws-using-amazon-inspector-and-aws-systems-manager-part-1/\">https://aws.amazon.com/blogs/mt/automate-vulnerability-management-and-remediation-in-aws-using-amazon-inspector-and-aws-systems-manager-part-1/</a></p>\n", "answers": ["<p>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Systems Manager to periodically scan the EC2 instances for vulnerabilities</p>", "<p>Configure the application security groups to ensure that only the necessary ports are open. Use Amazon Inspector to periodically scan the EC2 instances for vulnerabilities</p>", "<p>Use AWS Key Management Services to encrypt all the traffic between the client and application servers. Configure the application security groups to ensure that only the necessary ports are open</p>", "<p>Install AWS Certificate Manager (ACM) SSL/TLS certificate on the EC2 instances to secure traffic moving to and from the application servers</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "A standard three-tier application is hosted on Amazon EC2 instances that are fronted by an Application Load Balancer. The application maintenance team has reported several small-scale malicious attacks on the application. The project manager has decided to ramp up the security of the application.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend as part of the best practices to scan and mitigate the known vulnerabilities?", "related_lectures": []}, {"_class": "assessment", "id": 70394906, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer has followed the best practices to set up a trusted IP address list for Amazon GuardDuty. However, GuardDuty is generating alert findings for the configured trusted IP addresses.</p>\n\n<p>Which of the following checks will you perform to ensure GuardDuty works as expected? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</strong></p>\n\n<p>Trusted IP lists and threat lists apply only to traffic destined for publicly routable IP addresses. The effects of a list apply to all VPC Flow Log and CloudTrail findings, but do not apply to DNS findings.</p>\n\n<p><strong>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</strong></p>\n\n<p>Trusted IP lists and threat lists are account and Region-specific. At any given time, you can have only one uploaded trusted IP list per AWS account per Region. Whereas, you can have up to six uploaded threat lists per AWS account per Region.</p>\n\n<p>AWS suggested best practices to verify the trusted IP list settings:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/\">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ensure that multiple trusted IP lists per AWS account per Region have been configured</strong> - This statement is incorrect. At any given time, you can have only one uploaded trusted IP list per AWS account per Region.</p>\n\n<p><strong>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator's trusted IP lists</strong> - This statement is incorrect. In multi-account environments, GuardDuty generates findings for member accounts based on activity that involves known malicious IP addresses from the administrator's threat lists. It does not generate findings based on activity that involves IP addresses from the administrator's trusted IP lists.</p>\n\n<p><strong>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</strong> - If you include the same IP on both a trusted IP list and threat list it will be processed by the trusted IP list first, and will not generate a finding.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/\">https://aws.amazon.com/premiumsupport/knowledge-center/guardduty-trusted-ip-list-alert/</a></p>\n", "answers": ["<p>Ensure that multiple trusted IP lists per AWS account per Region have been configured</p>", "<p>Ensure that in multi-account environments, GuardDuty generates findings for member accounts based on activity that involves IP addresses from the administrator's trusted IP lists</p>", "<p>Ensure that IP addresses added in the trusted IP list are publicly routable IPv4 addresses</p>", "<p>Ensure that the trusted IP lists are uploaded in the same AWS Region as your GuardDuty findings</p>", "<p>Ensure that the same IP is not enlisted on both a trusted IP list as well as a threat list, as it will be processed by the threat list on priority, thereby resulting in a finding</p>"]}, "correct_response": ["c", "d"], "section": "Infrastructure Security", "question_plain": "A Security Engineer has followed the best practices to set up a trusted IP address list for Amazon GuardDuty. However, GuardDuty is generating alert findings for the configured trusted IP addresses.\n\nWhich of the following checks will you perform to ensure GuardDuty works as expected? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394908, "assessment_type": "multi-select", "prompt": {"question": "<p>As a Security Engineer, you have been tasked with the job of automating the detection and remediation of threats against your AWS environments using Amazon GuardDuty findings.</p>\n\n<p>Which steps will you follow to implement this solution most efficiently? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure CloudWatch Event to filter GuardDuty findings when a malicious activity is suspected. Configure the CloudWatch Event to invoke a Lambda function to parse the GuardDuty finding and store it in the Amazon DynamoDB table, if required</strong></p>\n\n<p><strong>After checking the existing entries in the Amazon DynamoDB table, AWS Lambda function creates a Rule inside AWS WAF and in a VPC NAC, and a notification email is sent via Amazon Simple Notification Service (SNS)</strong></p>\n\n<p>Amazon GuardDuty can be configured to automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings.</p>\n\n<p>How the solution works:</p>\n\n<ol>\n<li><p>A GuardDuty finding is raised with suspected malicious activity.</p></li>\n<li><p>A CloudWatch Event is configured to filter for GuardDuty Finding type.</p></li>\n<li><p>A Lambda function is invoked by the CloudWatch Event and parses the GuardDuty finding.</p></li>\n<li><p>State data for blocked hosts is stored in the Amazon DynamoDB table. The Lambda function checks the state table for existing host entry.</p></li>\n<li><p>The Lambda function creates a Rule inside AWS WAF and in a VPC NACL.</p></li>\n<li><p>A notification email is sent via Amazon Simple Notification Service (SNS).</p></li>\n</ol>\n\n<p>Diagrammatic representation of the proposed solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q15-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/\">https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure GuardDuty to export its findings to an Amazon S3 bucket. Configure a Lambda function to be triggered every time an object is added to the Amazon S3 bucket</strong> - GuardDuty supports exporting active findings to CloudWatch Events and, optionally, to an Amazon S3 bucket. To configure settings for exporting Active findings to an Amazon S3 bucket you will need a KMS key that GuardDuty can use to encrypt findings and an S3 bucket with permissions that allows GuardDuty to upload objects. This implies that using S3 involves setting up more resources than required, as the solution can be built by leveraging the CloudWatch Events directly. So this option is not the best fit.</p>\n\n<p><strong>Configure GuardDuty to trigger an AWS Lambda function every time a finding is generated. Configure an Amazon DynamoDB table to store the data received by Lambda from GuarDuty integration</strong> - GuardDuty supports exporting active findings to CloudWatch Events and, optionally, to an Amazon S3 bucket. GuardDuty cannot directly invoke the Lambda function to process its findings.</p>\n\n<p><strong>Configure AWS Lambda function to create a Rule inside AWS WAF and in a VPC NAC for every GuardDuty finding and trigger an email notification via Amazon Simple Notification Service (SNS)</strong> - This step is correct if the state data of blocked hosts is stored in some database solution. In the absence of permanent storage, Lambda will continue to add duplicate entries for the blocked hosts in AWS WAF and VPC NACL.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/\">https://aws.amazon.com/blogs/security/how-to-use-amazon-guardduty-and-aws-web-application-firewall-to-automatically-block-suspicious-hosts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_exportfindings.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_exportfindings.html</a></p>\n", "answers": ["<p>Configure CloudWatch Event to filter GuardDuty findings when a malicious activity is suspected. Configure the CloudWatch Event to invoke a Lambda function to parse the GuardDuty finding and store it in the Amazon DynamoDB table, if required</p>", "<p>Configure GuardDuty to export its findings to an Amazon S3 bucket. Configure a Lambda function to be triggered every time an object is added to the Amazon S3 bucket</p>", "<p>After checking the existing entries in the Amazon DynamoDB table, AWS Lambda function creates a Rule inside AWS WAF and in a VPC NAC, and a notification email is sent via Amazon Simple Notification Service (SNS)</p>", "<p>Configure GuardDuty to trigger an AWS Lambda function every time a finding is generated. Configure an Amazon DynamoDB table to store the data received by Lambda from GuardDuty integration</p>", "<p>Configure AWS Lambda function to create a Rule inside AWS WAF and in a VPC NAC for every GuardDuty finding and trigger an email notification via Amazon Simple Notification Service (SNS)</p>"]}, "correct_response": ["a", "c"], "section": "Incident Response", "question_plain": "As a Security Engineer, you have been tasked with the job of automating the detection and remediation of threats against your AWS environments using Amazon GuardDuty findings.\n\nWhich steps will you follow to implement this solution most efficiently? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394818, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company recently faced a cyber attack and lost all its data stored in the EBS volumes for the EC2 instances. However, the EBS snapshots were not manipulated. The company could restore the data from the EBS snapshots. However, the incident highlighted the security gaps in the current security plan. An immediate need is to protect the EBS snapshots from any manipulation or deletion.</p>\n\n<p>As a Security Engineer, what measures will you take to protect these AWS KMS Customer Master Keys (CMKs) encrypted snapshots?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new AWS account with limited privileges. Allow the newly created account to access the AWS KMS CMK key used to encrypt the EBS snapshots. Copy the encrypted snapshots to the new account on a regular basis</strong></p>\n\n<p>Automating cross-account snapshot copies enables you to copy your Amazon EBS snapshots to specific Regions in an isolated account and encrypt those snapshots with an encryption key. This enables you to protect yourself against data loss in the event of your account being compromised. When you share an encrypted snapshot, you must also share the customer-managed key used to encrypt the snapshot.</p>\n\n<p>By giving the new account access to the AWS KMS CMK key, the account will have the necessary permissions to copy the EBS snapshots. The new account will safeguard the EBS snapshots in case the account created by them is compromised while also restricting access through limited privileges.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new AWS account with limited privileges. Configure the snapshot to use encryption with the default AWS-managed key while copying the encrypted snapshots to the new account. Since default encryption is used, you don't need to share access to the AWS KMS CMK key</strong> - You can only share snapshots that are unencrypted or that are encrypted using a customer-managed key. You can't share snapshots that are encrypted with the default EBS encryption KMS key. Also, you cannot change the encryption key of a snapshot once it is created.</p>\n\n<p><strong>Create a new Amazon S3 bucket. Use AWS Systems Manager to move EBS snapshots to the new S3 bucket. Use S3 lifecycle policies to move the snapshots to Amazon S3 Glacier and subsequently apply Glacier Vault policies to prevent deletion</strong> - AWS Systems Manager allows you to safely automate common and repetitive IT operations and management tasks. With Systems Manager Automation, you use predefined playbooks, or you can build, run, and share wiki-style automated playbooks to enable AWS resource management across multiple accounts and AWS Regions. It does not manage EBS snapshots or its backups.</p>\n\n<p><strong>Use S3 Lifecycle transitions to regularly copy EBS snapshots to Amazon S3 through automation</strong> - Amazon S3 supports lifecycle transitions between storage classes using an S3 Lifecycle configuration. This option has been added as a distractor. You cannot use S3 lifecycle transitions to copy EBS snapshots to Amazon S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/event-policy.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/event-policy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html</a></p>\n", "answers": ["<p>Create a new Amazon S3 bucket. Use AWS Systems Manager to move EBS snapshots to the new S3 bucket. Use S3 lifecycle policies to move the snapshots to Amazon S3 Glacier and subsequently apply Glacier Vault policies to prevent deletion</p>", "<p>Use S3 Lifecycle transitions to regularly copy EBS snapshots to Amazon S3 through automation</p>", "<p>Create a new AWS account with limited privileges. Configure the snapshot to use encryption with the default AWS-managed key while copying the encrypted snapshots to the new account. Since default encryption is used, you don't need to share access to the AWS KMS CMK key</p>", "<p>Create a new AWS account with limited privileges. Allow the newly created account to access the AWS KMS CMK key used to encrypt the EBS snapshots. Copy the encrypted snapshots to the new account on a regular basis</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "A retail company recently faced a cyber attack and lost all its data stored in the EBS volumes for the EC2 instances. However, the EBS snapshots were not manipulated. The company could restore the data from the EBS snapshots. However, the incident highlighted the security gaps in the current security plan. An immediate need is to protect the EBS snapshots from any manipulation or deletion.\n\nAs a Security Engineer, what measures will you take to protect these AWS KMS Customer Master Keys (CMKs) encrypted snapshots?", "related_lectures": []}, {"_class": "assessment", "id": 70394820, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Security Engineer received a GuardDuty security alert pertaining to one of the Amazon EC2 instances that is attempting to communicate with the IP address of a remote host known to hold credentials and stolen data captured by malware. The Security Engineer immediately tried to isolate the instance by activating the isolation security group on the instance. However, within a few minutes, the engineer received a similar alert again.</p>\n\n<p>Which of the following represents the underlying reason for this behavior and what is the solution to remediate the issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</strong></p>\n\n<p>When you change a security group rule, its tracked connections are not immediately interrupted. The security group continues to allow packets until existing connections time out. To ensure that traffic is immediately interrupted, the tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the instance completely.</p>\n\n<p>An untracked flow of traffic is immediately interrupted if the rule that enables the flow is removed or modified. For example, if you have an open (0.0.0.0/0) outbound rule, and you remove a rule that allows all (0.0.0.0/0) inbound SSH (TCP port 22) traffic to the instance (or modify it such that the connection would no longer be permitted), your existing SSH connections to the instance are immediately dropped. The connection was not previously being tracked, so the change will break the connection. On the other hand, if you have a narrower inbound rule that initially allows an SSH connection (meaning that the connection was tracked), but change that rule to no longer allow new connections from the address of the current SSH client, the existing SSH connection is not interrupted because it is tracked.</p>\n\n<p>There are multiple ways to change tracked connections to being untracked. You can implement the isolation with an existing security group using the following steps:</p>\n\n<ol>\n<li>Identify the security group of the instance</li>\n<li>Delete all existing rules</li>\n<li>Create a single rule of 0.0.0.0/0 (0-65535) for all traffic in both inbound and outbound rules. This converts all existing and new traffic to being untracked.</li>\n<li>Remove the 0.0.0.0/0 (0-65535) inbound and outbound rules to terminate all connections and isolate the instance.</li>\n</ol>\n\n<p>Security Group level containment:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://www.youtube.com/watch?v=pPCuCYrhIyI\">https://www.youtube.com/watch?v=pPCuCYrhIyI</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</strong> - Security group rules can only allow traffic; you can't create rules that deny access. Hence, this option is incorrect.</p>\n\n<p><strong>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</strong> - Shutting down an instance is the last resort when trying to isolate the compromised instance. The reason is when an instance is shut down all the cache data is lost which is crucial in understanding the security breach that has taken place on the instance and the extent to which AWS resources have been compromised.</p>\n\n<p><strong>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway enables resources (like EC2 instances) in your public subnets to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Internet Gateway operates at the VPC level and not at the instance level. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-connection-tracking.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/\">https://aws.amazon.com/blogs/security/automate-amazon-ec2-instance-isolation-by-using-tags/</a></p>\n", "answers": ["<p>When you associate multiple security groups with an instance, rules with deny access need to be mutually exclusive. Delete all the security groups and create only the isolation security group to isolate the compromised instance</p>", "<p>When you change a security group rule, its tracked connections are not immediately interrupted. The tracked connections need to be configured to change to untracked connections and then apply the isolation security group to isolate the compromised instance</p>", "<p>When the isolation security group is unable to isolate an instance, the immediate fix is to shut down the compromised instance to cut off further damage to your AWS resources</p>", "<p>If you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Hence, to isolate the instance, cut off Internet Gateway from the instance</p>"]}, "correct_response": ["b"], "section": "Incident Response", "question_plain": "A Security Engineer received a GuardDuty security alert pertaining to one of the Amazon EC2 instances that is attempting to communicate with the IP address of a remote host known to hold credentials and stolen data captured by malware. The Security Engineer immediately tried to isolate the instance by activating the isolation security group on the instance. However, within a few minutes, the engineer received a similar alert again.\n\nWhich of the following represents the underlying reason for this behavior and what is the solution to remediate the issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394822, "assessment_type": "multi-select", "prompt": {"question": "<p>A web application is deployed on EC2 instances running under an Auto Scaling Group. The application needs to be accessible from an Application Load Balancer that provides HTTPS termination, and accesses a PostgreSQL database managed by RDS.</p>\n\n<p>As an AWS Certified Security Specialist, how would you configure the security groups? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</strong></p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</strong></p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 443</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.\nThe following are the characteristics of security group rules:\nBy default, security groups allow all outbound traffic.\nSecurity group rules are always permissive; you can't create rules that deny access.\nSecurity groups are stateful</p>\n\n<p>PostgreSQL port = 5432\nHTTP port = 80\nHTTPS port = 443</p>\n\n<p>The traffic follows this route :\nThe client sends an HTTPS request to ALB on port 443. This is handled by the rule - <strong>The security group of the ALB should have an inbound rule from anywhere on port 443.</strong>\nThe ALB then forwards the request to one of the EC2 instances. This is handled by the rule - <strong>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80.</strong>\nThe EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432. This is handled by the rule - <strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432.</strong></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group of the ALB should have an inbound rule from anywhere on port 80</strong> - The client sends an HTTPS request to ALB on port 443 and not on port 80, so this is incorrect.</p>\n\n<p><strong>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</strong> - The security group of the EC2 instances should have an inbound rule from the security group of the ALB and not from the security group of the RDS database, so this option is incorrect.</p>\n\n<p><strong>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</strong> - The EC2 instance further accesses the PostgreSQL database managed by RDS on port 5432 and not on port 80, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n", "answers": ["<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 5432</p>", "<p>The security group of the EC2 instances should have an inbound rule from the security group of the ALB on port 80</p>", "<p>The security group of the ALB should have an inbound rule from anywhere on port 443</p>", "<p>The security group of the ALB should have an inbound rule from anywhere on port 80</p>", "<p>The security group of the EC2 instances should have an inbound rule from the security group of the RDS database on port 5432</p>", "<p>The security group of RDS should have an inbound rule from the security group of the EC2 instances in the ASG on port 80</p>"]}, "correct_response": ["a", "b", "c"], "section": "Infrastructure Security", "question_plain": "A web application is deployed on EC2 instances running under an Auto Scaling Group. The application needs to be accessible from an Application Load Balancer that provides HTTPS termination, and accesses a PostgreSQL database managed by RDS.\n\nAs an AWS Certified Security Specialist, how would you configure the security groups? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394824, "assessment_type": "multi-select", "prompt": {"question": "<p>A company's security policy mandates enforcing VPC Flow Logs for all the VPCs defined on AWS. A Security Engineer has been tasked to automate this compliance check and subsequently inform the governance teams if any VPC is found to be non-compliant.</p>\n\n<p>Which steps will you combine for automating the process to meet the compliance guidelines? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</strong></p>\n\n<p><strong>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</strong></p>\n\n<p>To create a custom rule that audits AWS resources for security compliance by enabling VPC Flow Logs for an Amazon Virtual Private Cloud (VPC), the following steps have to be followed:</p>\n\n<ol>\n<li>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant.</li>\n<li>Create a custom Config rule that uses the Lambda function created in Step 1 as the source.</li>\n<li>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS.</li>\n</ol>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/\">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</strong> - AWS Lambda has predefined states - such as active, inactive, pending, failed - that can be used while implementing business logic. Lambda does not propagate a <code>non-compliant</code> state. To understand the full list of states that AWS Lambda supports, please check the link in the references.</p>\n\n<p><strong>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</strong> - This option has been added as a distractor. Just querying the VPC Flow Logs using Athena is not sufficient as this option does not provide any information about the total numbers of VPCs set up in the given AWS account and how many of those VPCs have Flow Logs enabled. So it cannot detect non-compliant status for the given requirement.</p>\n\n<p><strong>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</strong> - This option has been added as a distractor. This option relies on Athena queries for detecting any non-compliant resources. As mentioned earlier, any Athena query running only on the VPC Flow Logs data would have insufficient information to detect non-compliant status for the given requirement.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/\">https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/\">https://aws.amazon.com/blogs/compute/coming-soon-expansion-of-aws-lambda-states-to-all-functions/</a></p>\n", "answers": ["<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create a custom Config rule that uses this Lambda function as its source</p>", "<p>Create a Lambda function containing the logic to determine if a resource is compliant or non-compliant. Create an Amazon CloudWatch Event rule that triggers when the state of the earlier declared Lambda function changes to non-compliant</p>", "<p>Create a Lambda function that polls Config to detect non-compliant resources daily and send notifications via Amazon SNS</p>", "<p>Publish VPC Flow Logs to Amazon S3 bucket and query the data with AWS Athena for determining the non-compliant resources</p>", "<p>Create a Lambda function that checks the AWS Athena query status on a daily basis for detecting any non-compliant resources daily and sending notifications via Amazon SNS</p>"]}, "correct_response": ["a", "c"], "section": "Logging and Monitoring", "question_plain": "A company's security policy mandates enforcing VPC Flow Logs for all the VPCs defined on AWS. A Security Engineer has been tasked to automate this compliance check and subsequently inform the governance teams if any VPC is found to be non-compliant.\n\nWhich steps will you combine for automating the process to meet the compliance guidelines? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394826, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at a company needs to analyze the AWS Web Application Firewall (AWS WAF) logs quickly and it wants to build multiple dashboards using a serverless architecture. The logging process should be automated so that the log data for dashboards is available on a real-time or near-real-time basis.</p>\n\n<p>Which of the following represents the most optimal solution for this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Firehose. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</strong></p>\n\n<p>This solution will show you how to analyze AWS Web Application Firewall (AWS WAF) logs and quickly build multiple dashboards, without booting up any servers.</p>\n\n<p>With the new AWS WAF full logs feature, you can log all traffic inspected by AWS WAF into Amazon Simple Storage Service (Amazon S3) buckets by leveraging Amazon Kinesis Data Firehose. First, you need to enable AWS WAF logging for the given web ACL(s). You can then create an Amazon Kinesis Data Firehose delivery stream into which the AWS WAF full logs are delivered. Then you need to set up an AWS Glue crawler job and an Amazon Athena table. Finally, you\u2019ll set up Amazon QuickSight dashboards to read the logs data by querying the Athena tables and thereby help you visualize the web traffic traversing through the AWS WAF layer.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q20-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/enabling-serverless-security-analytics-using-aws-waf-full-logs/\">https://aws.amazon.com/blogs/security/enabling-serverless-security-analytics-using-aws-waf-full-logs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to send logs to CloudWatch Logs. Use CloudWatch Logs Insights to interactively search and analyze your log data. Publish logs data to Amazon QuickSight dashboards through direct CloudWatch integration with QuickSight</strong> - It is possible to send web ACL traffic logs to a CloudWatch Logs log group. However, CloudWatch and Amazon QuickSight cannot directly connect and will need Amazon Athena as part of the solution. Hence, this option is incorrect.</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Streams. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</strong> - AWS WAF logs only support the following logging destinations - Amazon CloudWatch Logs, Amazon Simple Storage Service, and Amazon Kinesis Data Firehose. So this option is incorrect.</p>\n\n<p><strong>Configure AWS Web Application Firewall (AWS WAF) to send your web ACL traffic logs to the Amazon S3 bucket. Use Amazon Redshift Spectrum to query data directly from files on Amazon S3. Using the existing integration of RedShift Spectrum with Amazon QuickSight, generate visualizations to be used in manager dashboards</strong> - To use Redshift Spectrum, you need an Amazon Redshift cluster and a SQL client that's connected to your cluster, so that you can run SQL commands. Since the required solution should be serverless, this option is not the right fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-destinations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\">https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html</a></p>\n", "answers": ["<p>Configure AWS Web Application Firewall (AWS WAF) to send logs to CloudWatch Logs. Use CloudWatch Logs Insights to interactively search and analyze your log data. Publish logs data to Amazon QuickSight dashboards through direct CloudWatch integration with QuickSight</p>", "<p>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Streams. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</p>", "<p>Configure AWS Web Application Firewall (AWS WAF) to feed logs to Amazon S3 bucket via Amazon Kinesis Data Firehose. Set up an AWS Glue crawler job and an Amazon Athena table to query for required data and create visualizations using Amazon QuickSight dashboards</p>", "<p>Configure AWS Web Application Firewall (AWS WAF) to send your web ACL traffic logs to Amazon S3 bucket. Use Amazon Redshift Spectrum to query data directly from files on Amazon S3. Using the existing integration of RedShift Spectrum with Amazon QuickSight, generate visualizations to be used in manager dashboards</p>"]}, "correct_response": ["c"], "section": "Logging and Monitoring", "question_plain": "The security team at a company needs to analyze the AWS Web Application Firewall (AWS WAF) logs quickly and it wants to build multiple dashboards using a serverless architecture. The logging process should be automated so that the log data for dashboards is available on a real-time or near-real-time basis.\n\nWhich of the following represents the most optimal solution for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394828, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company is revamping its technology solutions on AWS to meet the company's new security guidelines that mandate the use of the company's own imported key material to create Customer Master keys (CMKs) to be used with AWS services. All encryption keys must also be rotated annually.</p>\n\n<p>How will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new CMK and import the new key material into it. Point the key alias of the older CMK to the new CMK created</strong></p>\n\n<p>When you import key material into a KMS key, the KMS key is permanently associated with that key material. You can reimport the same key material, but you cannot import a different key material into that KMS key. Also, you cannot enable automatic key rotation for a KMS key with imported key material. However, you can manually rotate a KMS key with imported key material.</p>\n\n<p>How to enable and disable automatic key rotation:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n\n<p>Because the new KMS key is a different resource from the current KMS key, it has a different key ID and ARN. When you change KMS keys, you need to update references to the KMS key ID or ARN in your applications. Aliases, which associate a friendly name with a KMS key, make this process easier. Use an alias to refer to a KMS key in your applications. Then, when you want to change the KMS key that the application uses, change the target KMS key of the alias.</p>\n\n<p>Rotating keys manually:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n\n<p>A key alias allows you to abstract key users away from the underlying Region-specific key ID and key ARN. Authorized individuals can create a key alias that allows their applications to use a specific CMK independent of the Region or rotation schedule. Thus, multi-Region applications can use the same key alias to refer to KMS keys in multiple Regions without worrying about the key ID or the key ARN. You can also trigger the manual rotation of a CMK by pointing a given key alias to a different CMK. Similar to how Domain Name Services (DNS) allows the abstraction of IP addresses, a key alias does the same for the key ID. When you are creating a key alias, we recommend that you determine a naming scheme that can be applied across your accounts such as alias/&lt;Environment&gt;-&lt;Function&gt;-&lt;Service Team&gt;.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable automatic key rotation for the KMS key with imported key material. Use this method to rotate the keys annually</strong> - You cannot automatically rotate asymmetric KMS keys, HMAC KMS keys, KMS keys with imported key material, or KMS keys in custom key stores. However, you can rotate them manually. So, this option is incorrect.</p>\n\n<p><strong>Associate the existing CMK with the new key material and run the List operation to update the association</strong> - When you import key material into a KMS key, the KMS key is permanently associated with that key material. You can reimport the same key material, but you cannot import a different key material into an existing KMS key.</p>\n\n<p><strong>Delete the old KMS key first and create a new key with the same name immediately. Import new key material into this newly created KMS key</strong> - Because it is destructive and potentially dangerous to delete a KMS key, AWS KMS requires you to set a waiting period of 7 \u2013 30 days. The default waiting period is 30 days. Hence, it is not possible to delete a KMS key and immediately create a new one with the same name.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/key-aliases.html\">https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/key-aliases.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-keys-manually</a></p>\n", "answers": ["<p>Enable automatic key rotation for the KMS key with imported key material. Use this method to rotate the keys annually</p>", "<p>Associate the existing CMK with the new key material and run the List operation to update the association</p>", "<p>Create a new CMK and import the new key material into it. Point the key alias of the older CMK to the new CMK created</p>", "<p>Delete the old KMS key first and create a new key with the same name immediately. Import new key material into this newly created KMS key</p>"]}, "correct_response": ["c"], "section": "Data Protection", "question_plain": "A financial services company is revamping its technology solutions on AWS to meet the company's new security guidelines that mandate the use of the company's own imported key material to create Customer Master keys (CMKs) to be used with AWS services. All encryption keys must also be rotated annually.\n\nHow will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394830, "assessment_type": "multi-select", "prompt": {"question": "<p>A company manages separate AWS accounts for each of its business units. An enhanced monitoring solution has been proposed by the security team that mandates tracking all the API calls using CloudTrail for all the AWS accounts. The centralized monitoring logs will be available in a new AWS account created for security and audit purposes. Logs of one business unit should be distinguishable from others via its own top-level prefix. Also, any updates to the log files should be traceable.</p>\n\n<p>As a Security Engineer, which of the following options will you combine to implement this requirement? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in AWS accounts of all business units. Use unique log file prefixes for trails in each AWS account</strong></p>\n\n<p><strong>Apply a bucket policy to the new centralized S3 bucket that permits the CloudTrail service to use the \"s3 PutObject\" action and the \"s3 GetBucketACL\" action, and specify the appropriate resource ARNs for the CloudTrail trails</strong> -</p>\n\n<p>A new S3 bucket should be created in the centralized account as per the given requirements. Log file validation on CloudTrail logs is necessary to determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection.</p>\n\n<p>A complete log file object name with Top-level prefix:\n<code>bucket_name/prefix_name/AWSLogs/Account ID/CloudTrail/region/YYYY/MM/DD/file_name.json.gz</code></p>\n\n<p>It helps in identifying the logs of a business unit at the top level without having to filter for it at the log file level.</p>\n\n<p>For a bucket to receive log files from multiple accounts, its bucket policy must grant CloudTrail permission to write log files from all the accounts you specify. This means that you must modify the bucket policy on your destination bucket to grant CloudTrail permission to write log files from each specified account.</p>\n\n<p>S3 bucket policy so that files can be received from multiple accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-set-bucket-policy-for-multiple-accounts.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-set-bucket-policy-for-multiple-accounts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in all AWS accounts including the centralized account. Use unique log file prefixes for trails in each AWS account</strong> - Enabling Log file validation in the centralized account is not required for the given use case.</p>\n\n<p><strong>Create a new Amazon S3 bucket in each of the AWS accounts. Use S3 bucket replication to copy the CloudTrail logs to the S3 bucket in the centralized account. Enable log file validation on all Trails in all of the AWS accounts used. Use unique log file prefixes for trails in each AWS account</strong> - AWS provides a straightforward solution for logging Trails from different AWS accounts. The S3 bucket that receives the log files from multiple accounts should have a bucket policy that must grant CloudTrail permission to write log files from all the accounts you specify. There is no reason to use S3 bucket replication and complicate the process.</p>\n\n<p><strong>Apply a bucket policy to all S3 buckets to permit the CloudTrail service to use the \"s3 PutObject\" action, \"s3 GetObjectAcl\" action, and the \"s3 GetObject\" action. Specify the appropriate resource ARNs for the CloudTrail trails</strong> - As discussed above, the S3 bucket policy should permit the CloudTrail service to use the \"s3 PutObject\" action and the \"s3 GetBucketACL\" action only.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html</a></p>\n", "answers": ["<p>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in all AWS accounts including the centralized account. Use unique log file prefixes for trails in each AWS account</p>", "<p>Create a new Amazon S3 bucket in the centralized account to store all the CloudTrail log files. Enable log file validation on all Trails in AWS accounts of all business units. Use unique log file prefixes for trails in each AWS account</p>", "<p>Apply a bucket policy to the new centralized S3 bucket that permits the CloudTrail service to use the <code>s3 PutObject</code> action and the <code>s3 GetBucketACL</code> action, and specify the appropriate resource ARNs for the CloudTrail trails</p>", "<p>Create a new Amazon S3 bucket in each of the AWS accounts. Use S3 bucket replication to copy the CloudTrail logs to the S3 bucket in the centralized account. Enable log file validation on all Trails in all of the AWS accounts used. Use unique log file prefixes for trails in each AWS account</p>", "<p>Apply a bucket policy to all S3 buckets to permit the CloudTrail service to use the <code>s3 PutObject</code> action, <code>s3 GetObjectAcl</code> action, and the <code>s3 GetObject</code> action. Specify the appropriate resource ARNs for the CloudTrail trails</p>"]}, "correct_response": ["b", "c"], "section": "Logging and Monitoring", "question_plain": "A company manages separate AWS accounts for each of its business units. An enhanced monitoring solution has been proposed by the security team that mandates tracking all the API calls using CloudTrail for all the AWS accounts. The centralized monitoring logs will be available in a new AWS account created for security and audit purposes. Logs of one business unit should be distinguishable from others via its own top-level prefix. Also, any updates to the log files should be traceable.\n\nAs a Security Engineer, which of the following options will you combine to implement this requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394832, "assessment_type": "multiple-choice", "prompt": {"question": "<p>During an internal IT Audit, the security team realized that AWS CloudTrail was disabled for a few AWS Regions leading to security and audit lapses. Now, the management wants to tighten the security measures across the company. As an AWS Certified Security Specialist, you have been tasked to build a solution for automatic re-enabling of AWS CloudTrail in any AWS Region if it happens to be turned off.</p>\n\n<p>What is the most optimal way of addressing this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can customize the behavior of a managed rule to suit your needs.</p>\n\n<p>To ensure that CloudTrail remains enabled in your account, AWS Config provides the 'cloudtrail-enabled' managed rule. If CloudTrail is turned off, the 'cloudtrail-enabled' rule automatically re-enables it by using automatic remediation.</p>\n\n<p>However, you must make sure that you follow security best practices for CloudTrail if you use automatic remediation. These best practices include enabling CloudTrail in all AWS Regions, logging read and write workloads, enabling insights, and encrypting log files with server-side encryption using AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).</p>\n\n<p>This pattern helps you follow these security best practices by providing a custom remediation action to automatically re-enable CloudTrail in your account.</p>\n\n<p>Automatically re-enable AWS CloudTrail by using a custom remediation rule in AWS Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Security Hub with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</strong> - AWS Security Hub provides you with a comprehensive view of your security state in AWS and helps you check your environment against security industry standards and best practices. Security Hub collects security data from across AWS accounts, services, and supported third-party partner products and helps you analyze your security trends and identify the highest priority security issues. AWS Security Hub does not have any built-in managed rules to handle remediation for CloudTrail non-compliant status. In fact, AWS Config itself uses Amazon EventBridge to send AWS Config rule evaluations to Security Hub. So, this option is not the right fit.</p>\n\n<p><strong>Create an event in Amazon EventBridge with Cloudtrail as the event source and a StopLogging event name to trigger an AWS Lambda function to call the StartLogging API</strong> - You would need to develop and maintain the code in the Lambda function to handle this requirement. Instead, it is optimal to use the off-the-shelf capability offered by AWS Config to address the given use case.</p>\n\n<p><strong>Use AWS Trusted Advisor security check on AWS CloudTrail Logging to trigger a Lambda function in case logging is disabled. The Lambda function implements the functionality to enable CloudTrail logging if it is disabled</strong> - AWS Trusted Advisor cannot be used to trigger an AWS Lambda function for non-compliant security checks. Hence, this option is invalid.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n", "answers": ["<p>Use AWS Security Hub with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</p>", "<p>Use AWS Config with a managed rule <code>cloudtrail-enabled</code> to trigger a remediation action to fix the non-compliant status using AWS Systems Manager Automation documents</p>", "<p>Create an Amazon CloudWatch alarm with a cloudtrail.amazonaws.com event source and a StartLogging event name to trigger an AWS Lambda function to call the StartLogging API</p>", "<p>Use AWS Trusted Advisor security check on AWS CloudTrail Logging to trigger a Lambda function in case logging is disabled. The Lambda function implements the functionality to enable CloudTrail logging if it is disabled</p>"]}, "correct_response": ["b"], "section": "Logging and Monitoring", "question_plain": "During an internal IT Audit, the security team realized that AWS CloudTrail was disabled for a few AWS Regions leading to security and audit lapses. Now, the management wants to tighten the security measures across the company. As an AWS Certified Security Specialist, you have been tasked to build a solution for automatic re-enabling of AWS CloudTrail in any AWS Region if it happens to be turned off.\n\nWhat is the most optimal way of addressing this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394834, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at a company has recently decided that CloudTrail logs of each department will be prefixed with the department code. Currently, CloudTrail logs are created with similar names across the company with no immediate way of identifying the departments sending those logs. When the security team tried to add the prefix to the log files in the CloudTrail console, the following error popped up: 'There is a problem with the bucket policy'.</p>\n\n<p>How will you fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the Amazon S3 console to update the prefix in the current bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail</strong></p>\n\n<p>If you try to add, modify, or remove a log file prefix for an S3 bucket that receives logs from a trail, you might see the error: \"There is a problem with the bucket policy.\" A bucket policy with an incorrect prefix can prevent your trail from delivering logs to the bucket. To resolve this issue, use the Amazon S3 console to update the prefix in the bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail.</p>\n\n<p>Changing a prefix for an existing bucket:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the service-specific context keys used in the Condition element of the Amazon S3 bucket policy statements</strong> - CloudTrail does not have service-specific context keys that can be used in the Condition element of policy statements.</p>\n\n<p>Policy best practices for CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html</a></p>\n\n<p><strong>Update the permissions of all users in the security team to <code>AWSCloudTrail_FullAccess</code> policy to impart all the necessary permissions to the users on CloudTrail logs</strong> - The <code>AWSCloudTrail_FullAccess</code> policy is not intended to be shared broadly across your AWS account. Users with this role can disable or reconfigure the most sensitive and important auditing functions in their AWS accounts. For this reason, this policy should be applied only to account administrators, and the use of this policy should be closely controlled and monitored.</p>\n\n<p><strong>Manually edit your Amazon S3 bucket policy to add an <code>aws:SourceArn</code> condition key to the policy statement attached for CloudTrail</strong> - This change does not affect the given error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/security_iam_id-based-policy-examples.html</a></p>\n", "answers": ["<p>Update the service-specific context keys used in the Condition element of the Amazon S3 bucket policy statements</p>", "<p>Update the permissions of all users in the security team to <code>AWSCloudTrail_FullAccess</code> policy to impart all the necessary permissions to the users on CloudTrail logs</p>", "<p>Manually edit your Amazon S3 bucket policy to add an <code>aws:SourceArn</code> condition key to the policy statement attached for CloudTrail</p>", "<p>Use the Amazon S3 console to update the prefix in the current bucket policy, and then use the CloudTrail console to specify the same prefix for the bucket in the trail</p>"]}, "correct_response": ["d"], "section": "Identity and Access Management", "question_plain": "The security team at a company has recently decided that CloudTrail logs of each department will be prefixed with the department code. Currently, CloudTrail logs are created with similar names across the company with no immediate way of identifying the departments sending those logs. When the security team tried to add the prefix to the log files in the CloudTrail console, the following error popped up: 'There is a problem with the bucket policy'.\n\nHow will you fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394836, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.</p>\n\n<p>As a Security Engineer, which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.</p>\n\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral ports</p>\n\n<p>When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n\n<p>The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.</p>\n\n<p>Network ACL basics:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement.</p>\n\n<p><strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic.</p>\n\n<p><em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a></p>\n", "answers": ["<p>The configuration is complete on the EC2 instance for accepting and responding to requests</p>", "<p>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</p>", "<p>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</p>", "<p>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.\n\nAs a Security Engineer, which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?", "related_lectures": []}, {"_class": "assessment", "id": 70394838, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company wants to share sensitive accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has another AWS account and must own a copy of the database.</p>\n\n<p>Which of the following would you recommend to securely share the database with the auditor?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</strong></p>\n\n<p>You can share the AWS Key Management Service (AWS KMS) customer master key (CMK) that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS CMKs with another AWS account by adding the other account to the AWS KMS key policy.</p>\n\n<p>Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</strong> - RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the RDS instance is located. RDS stores these on your behalf and you do not have direct access to these snapshots in S3, so it's not possible to grant access to the snapshot objects in S3.</p>\n\n<p><strong>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</strong> - This solution is feasible though not optimal. It requires a lot of unnecessary work and is difficult to audit when such bulk data is exported into text files.</p>\n\n<p><strong>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</strong> - Read Replicas make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Creating Read Replicas for audit purposes is overkill. Also, the question mentions that the auditor needs to own a copy of the database, which is not possible with replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html</a></p>\n", "answers": ["<p>Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key</p>", "<p>Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket</p>", "<p>Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket</p>", "<p>Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access</p>"]}, "correct_response": ["a"], "section": "Data Protection", "question_plain": "A financial services company wants to share sensitive accounting data that is stored in an Amazon RDS DB instance with an external auditor. The auditor has another AWS account and must own a copy of the database.\n\nWhich of the following would you recommend to securely share the database with the auditor?", "related_lectures": []}, {"_class": "assessment", "id": 70394840, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare company has recently completed a security review that has highlighted several gaps in the security standards mandated by the company while using the AWS Key Management Service (AWS KMS) keys. As an initial step to address the gap, the security team has decided that access to AWS KMS keys should be restricted to only the principals belonging to their AWS Organizations.</p>\n\n<p>How will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. You need to specify the Organization ID in the <code>Condition</code> element</strong></p>\n\n<p>The aws:PrincipalOrgID global condition key can be used with the Principal element in a resource-based policy with AWS KMS. Instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p>Create an AWS KMS key policy to allow all accounts in an AWS Organization to perform AWS KMS actions using the AWS global condition context key <code>aws:PrincipalOrgID</code>. It is a best practice to grant the least privilege permissions with AWS Identity and Access Management (IAM) policies. Specify your AWS Organization ID in the condition element of the statement to make sure that only the principals from the accounts in your Organization can access the AWS KMS key.</p>\n\n<p><code>aws:PrincipalOrgID</code> - Use this key to compare the identifier of the organization in AWS Organizations to which the requesting principal belongs with the identifier specified in the policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization. You can use this condition key to simplify specifying the Principal element in a resource-based policy. You can specify the organization ID in the condition element. When you add and remove accounts, policies that include the aws:PrincipalOrgID key automatically include the correct accounts and don't require manual updating.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition context key can be used to restrict access to an AWS service principal</strong> - AWS Organizations help you centrally manage and govern your environment as you grow and scale your AWS resources.  The <code>aws:PrincipalOrgID</code> global condition context key can't be used to restrict access to an AWS service principal. AWS services that invoke an API call are made from an internal AWS account that is not part of the AWS Organizations.</p>\n\n<p><strong>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</strong> - It is possible to configure the requirement with all the account IDs. But, such a solution is not an elegant solution. Instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p><strong>The <code>aws:PrincipalIsAWSService</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</strong> - You can use this key to check whether the call to your resource is being made directly by an AWS service principal. For example, AWS CloudTrail uses the service principal cloudtrail.amazonaws.com to write logs to your Amazon S3 bucket. Also, instead of listing all the AWS account IDs in an Organization, you can specify the Organization ID in the Condition element.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalisawsservice\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-principalisawsservice</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kms-key-organization-account/\">https://aws.amazon.com/premiumsupport/knowledge-center/kms-key-organization-account/</a></p>\n", "answers": ["<p>The <code>aws:PrincipalOrgID</code> global condition context key can be used to restrict access to an AWS service principal</p>", "<p>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</p>", "<p>The <code>aws:PrincipalIsAWSService</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. List all the AWS account IDs in the <code>Condition</code> element</p>", "<p>The <code>aws:PrincipalOrgID</code> global condition key can be used with the Principal element in a resource-based policy with AWS KMS. You need to specify the Organization ID in the <code>Condition</code> element</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "A healthcare company has recently completed a security review that has highlighted several gaps in the security standards mandated by the company while using the AWS Key Management Service (AWS KMS) keys. As an initial step to address the gap, the security team has decided that access to AWS KMS keys should be restricted to only the principals belonging to their AWS Organizations.\n\nHow will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394842, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application hosted on an Amazon EC2 instance writes its request logs, availability logs, and threat logs to a text file. This file is read by a custom program to track and process any security issues inferred from the logs. An increase in log data has resulted in the malfunctioning of the custom program. The company is looking at a scalable solution to collect and analyze log files.</p>\n\n<p>Which design will ensure that the aforementioned criteria are met with the LEAST amount of effort?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Install and configure the unified CloudWatch agent on the application's EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</strong></p>\n\n<p>The unified CloudWatch agent enables you to do the following:</p>\n\n<ol>\n<li><p>Collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances. The additional metrics that can be collected are listed in the Metrics collected by the CloudWatch agent.</p></li>\n<li><p>Collect system-level metrics from on-premises servers.</p></li>\n<li><p>Retrieve custom metrics from your applications or services using the <code>StatsD</code> and <code>collected</code> protocols.</p></li>\n<li><p>Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p></li>\n</ol>\n\n<p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs, just like logs collected by the older CloudWatch Logs agent.</p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</strong> - This option is invalid since the logs for the given use case cannot be imported or written into CloudTrail.</p>\n\n<p><strong>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector is a very useful tool, but it cannot process log files as described in the given use case.</p>\n\n<p><strong>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</strong> - Although this solution is certainly feasible, however, it is not an optimal solution when compared to directly using the unified CloudWatch agent.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n", "answers": ["<p>Create a scheduled process to copy the application log files to AWS CloudTrail. Configure a Lambda function that processes CloudTrail logs and sends an SNS notification whenever a log file is created</p>", "<p>Configure Amazon Inspector to collect all log files from the EC2 instance. Use Amazon EventBridge integration with Amazon Inspector to trigger Lambda function that can read the logs and raise notifications for events on security</p>", "<p>Install and configure the unified CloudWatch agent on the application's EC2 instance. Create a CloudWatch metric filter to monitor the application logs. Configure CloudWatch alerts based on these metrics</p>", "<p>Create a cron job on the Amazon EC2 instance to copy the logs into the Amazon S3 bucket. Use S3 events to trigger a Lambda function that refreshes Amazon CloudWatch metrics with the log data. Set up CloudWatch alerts based on the metrics</p>"]}, "correct_response": ["c"], "section": "Logging and Monitoring", "question_plain": "An application hosted on an Amazon EC2 instance writes its request logs, availability logs, and threat logs to a text file. This file is read by a custom program to track and process any security issues inferred from the logs. An increase in log data has resulted in the malfunctioning of the custom program. The company is looking at a scalable solution to collect and analyze log files.\n\nWhich design will ensure that the aforementioned criteria are met with the LEAST amount of effort?", "related_lectures": []}, {"_class": "assessment", "id": 70394844, "assessment_type": "multiple-choice", "prompt": {"question": "<p>While consolidating logs for the weekly reporting, a development team at a retail company realized that an unusually large number of illegal AWS API queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.</p>\n\n<p>Which of the following represents the best solution for the given scenario?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Amazon CloudWatch metric filter that processes CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an SNS notification to the required team</strong></p>\n\n<p>AWS CloudTrail log data can be ingested into Amazon CloudWatch to monitor and identify your AWS account activity against security threats, and create a governance framework for security best practices. You can analyze log trail event data in CloudWatch using features such as Logs Insight, Contributor Insights, Metric filters, and CloudWatch Alarms.</p>\n\n<p>CloudTrail integrates with the CloudWatch service to publish the API calls being made to resources or services in the AWS account. The published event has invaluable information that can be used for compliance, auditing, and governance of your AWS accounts. Below we introduce several features available in CloudWatch to monitor API activity, analyze the logs at scale, and take action when malicious activity is discovered, without provisioning your infrastructure.</p>\n\n<p>For the Cloudtrail logs available in CloudWatch Logs, you can begin searching and filtering the log data by creating one or more metric filters. Use these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set a CloudWatch Alarm on.</p>\n\n<p>Using CloudWatch Metric Filters to analyze log data from CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q29-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/\">https://aws.amazon.com/blogs/mt/analyzing-cloudtrail-in-cloudwatch/</a></p>\n\n<p>Note: AWS CloudTrail Insights helps AWS users identify and respond to unusual activity associated with <code>write</code> API calls by continuously analyzing CloudTrail management events.</p>\n\n<p>Insights events are logged when CloudTrail detects unusual <code>write</code> management API activity in your account. If you have CloudTrail Insights enabled and CloudTrail detects unusual activity, Insights events are delivered to the destination S3 bucket for your trail. You can also see the type of insight and the incident time when you view Insights events on the CloudTrail console. Unlike other types of events captured in a CloudTrail trail, Insights events are logged only when CloudTrail detects changes in your account's API usage that differ significantly from the account's typical usage patterns.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Kinesis stream-level metrics in the CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</strong> -  AWS CloudTrail cannot stream data to Amazon Kinesis. Amazon S3 buckets and CloudWatch logs are the only destinations possible.</p>\n\n<p><strong>Run Amazon Athena SQL queries against CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</strong> - Generating reports and visualizations help in understanding and analyzing patterns but is not useful as a near-real-time automatic solution for the given problem.</p>\n\n<p><strong>Trusted Advisor publishes metrics about check results to CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify you when the service quota is reached or exceeded</strong> - When AWS Trusted Advisor refreshes your checks, Trusted Advisor publishes metrics about your check results to CloudWatch. You can view the metrics in CloudWatch. You can also create alarms to detect status changes to Trusted Advisor checks and status changes for resources, and service quota usage (formerly referred to as limits). The alarm will then notify you when you reach or exceed a service quota for your AWS account. However, the alarm is triggered only when the service limit is reached. We need a solution that raises an alarm when the number of API calls randomly increases or an abnormal pattern is detected. Hence, this option is not the right fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-authorization-failures</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-insights-events-with-cloudtrail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-metrics-ta.html</a></p>\n", "answers": ["<p>Configure AWS CloudTrail to stream event data to Amazon Kinesis. Use Kinesis stream-level metrics in the CloudWatch to trigger an AWS Lambda function that will trigger an error workflow</p>", "<p>Run Amazon Athena SQL queries against CloudTrail log files stored in Amazon S3 buckets. Use Amazon QuickSight to generate reports for managerial dashboards</p>", "<p>Trusted Advisor publishes metrics about check results to CloudWatch. Create an alarm to track status changes for checks in the Service Limits category for the APIs. The alarm will then notify when the service quota is reached or exceeded</p>", "<p>Create an Amazon CloudWatch metric filter that processes CloudTrail logs having API call details and looks at any errors by factoring in all the error codes that need to be tracked. Create an alarm based on this metric's rate to send an SNS notification to the required team</p>"]}, "correct_response": ["d"], "section": "Logging and Monitoring", "question_plain": "While consolidating logs for the weekly reporting, a development team at a retail company realized that an unusually large number of illegal AWS API queries were made sometime during the week. Due to the off-season, there was no visible impact on the systems. However, this event led the management team to seek an automated solution that can trigger near-real-time warnings in case such an event recurs.\n\nWhich of the following represents the best solution for the given scenario?", "related_lectures": []}, {"_class": "assessment", "id": 70394846, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of the organization-wide security best practices, a company has mandated that all software installed on the EC2 instances should be upgraded to its most recent authorized version every 30 days. For this requirement, the Security Administrator has to provide a weekly report that lists all the instances that do not have the latest software updates deployed.</p>\n\n<p>What is the most optimal way to implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Patch Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software updates on a schedule, and scan targets on demand</strong></p>\n\n<p>Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed nodes with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can use Patch Manager to install Service Packs on Windows nodes and perform minor version upgrades on Linux nodes.</p>\n\n<p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, in addition to a list of approved and rejected patches. You can install patches regularly by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to a large group of managed nodes by using tags.</p>\n\n<p>Patch Manager provides options to scan your managed nodes and report compliance on a schedule, install available patches on a schedule, and patch or scan targets on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single managed node, reports include details of all patches for the node. For a report on all managed nodes, only a summary of how many patches are missing is provided.</p>\n\n<p>Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage.</p>\n\n<p>How AWS Systems Manager works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q30-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Inspector to determine the systems that do not have the latest patches applied after a time of 30 days and configure Inspector to redeploy these instances with the latest AMI version</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2) and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single click. Amazon Inspector, however, does not cater to patch management requirements.</p>\n\n<p><strong>Use Version Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software versions on a schedule, and scan targets on demand</strong> - There is no such feature as Version Manager within AWS Systems Manager. This is a made-up option, meant to serve as a distractor.</p>\n\n<p><strong>Use Change Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule and install available software updates on a schedule through automated runbooks</strong> - Change Manager, a capability of AWS Systems Manager, is an enterprise change management framework for requesting, approving, implementing, and reporting on operational changes to your application configuration and infrastructure. From a single delegated administrator account, if you use AWS Organizations, you can manage changes across multiple AWS accounts and AWS Regions. Change Manager is not meant for managing software updates via patching.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/11/aws-announces-aws-systems-manager/\">https://aws.amazon.com/about-aws/whats-new/2017/11/aws-announces-aws-systems-manager/</a></p>\n", "answers": ["<p>Use Amazon Inspector to determine the systems that do not have the latest patches applied after a time of 30 days and configure Inspector to redeploy these instances with the latest AMI version</p>", "<p>Use Version Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software versions on a schedule, and scan targets on demand</p>", "<p>Use Patch Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule, install available software updates on a schedule, and scan targets on demand</p>", "<p>Use Change Manager, a capability of AWS Systems Manager to automatically scan your instances and report compliance on a schedule and install available software updates on a schedule through automated runbooks</p>"]}, "correct_response": ["c"], "section": "Infrastructure Security", "question_plain": "As part of the organization-wide security best practices, a company has mandated that all software installed on the EC2 instances should be upgraded to its most recent authorized version every 30 days. For this requirement, the Security Administrator has to provide a weekly report that lists all the instances that do not have the latest software updates deployed.\n\nWhat is the most optimal way to implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394848, "assessment_type": "multi-select", "prompt": {"question": "<p>A company maintains separate AWS accounts for its various lines of business. All the accounts are configured with Amazon GuardDuty to detect threats and malicious activities. A partner security firm generates a common threat list quarterly and shares it with all the business lines.</p>\n\n<p>As a Security Engineer, how will you configure the threat list across all AWS accounts with minimum effort? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and share the access with the administrator account</strong></p>\n\n<p><strong>Specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. Add the threat list to the administrator account by referencing the S3 object that contains the threat list</strong></p>\n\n<p>If the accounts you want to associate with are not part of your AWS Organizations organization, you can specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. When the invited account accepts the invitation, that account becomes a GuardDuty member account associated with the administrator account.</p>\n\n<p>When you use GuardDuty in a multiple-account environment, the administrator account can manage certain aspects of GuardDuty on behalf of the member accounts. The primary functions the administrator account can perform are the following:</p>\n\n<ol>\n<li><p>Add and remove associated member accounts. The process by which this is done differs based on whether the accounts are associated through organizations or by invitation.</p></li>\n<li><p>Manage the status of GuardDuty within associated member accounts, including enabling and suspending GuardDuty.</p></li>\n<li><p>Customize findings within the GuardDuty network through the creation and management of suppression rules, trusted IP lists, and threat lists. Member accounts lose access to these features in a multiple-account environment.</p></li>\n</ol>\n\n<p>Relationship between GuardDuty administrator and member accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html</a></p>\n\n<p>For the given use case, you can upload the threat list to an Amazon S3 bucket and share the access with the administrator account for referencing the S3 object that contains the threat list, which is finally propagated through the GuardDuty network across all accounts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and share the access with the organization's delegated administrator for GuardDuty</strong> - This statement is correct only if AWS Organizations is used by the company. Since it is not specified for the given use case, so this option is not the right fit.</p>\n\n<p><strong>Configure all AWS accounts to be part of AWS Organizations and add the threat list to all members of the organization using AWS Resource Access Manager (RAM)</strong> - AWS Organizations is not mentioned in the given use case, so this option is not the right fit. Also, the organization's delegated administrator for GuardDuty will upload a threat list to all member accounts, it is not done through AWS Resource Access Manager.</p>\n\n<p><strong>Upload the threat list to an Amazon S3 bucket and trigger an Amazon EventBridge event every time a new threat list is added to the bucket. Define Amazon GuardDuty as a target to EventBridge to automatically configure the threat list to the administrator account in GuardDuty</strong> - Amazon GuardDuty is not a supported target for Amazon EventBridge and hence this option is invalid.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_accounts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html</a></p>\n", "answers": ["<p>Upload the threat list to an Amazon S3 bucket and share the access with the organization's delegated administrator for GuardDuty</p>", "<p>Configure all AWS accounts to be part of AWS Organizations and add the threat list to all members of the organization using AWS Resource Access Manager (RAM)</p>", "<p>Upload the threat list to an Amazon S3 bucket and trigger an Amazon EventBridge event every time a new threat list is added to the bucket. Define Amazon GuardDuty as a target to EventBridge to automatically configure the threat list to the administrator account in GuardDuty</p>", "<p>Upload the threat list to an Amazon S3 bucket and share the access with the administrator account</p>", "<p>Specify an administrator account in GuardDuty and then use the administrator account to invite other AWS accounts to become member accounts. Add the threat list to the administrator account by referencing the S3 object that contains the threat list</p>"]}, "correct_response": ["d", "e"], "section": "Infrastructure Security", "question_plain": "A company maintains separate AWS accounts for its various lines of business. All the accounts are configured with Amazon GuardDuty to detect threats and malicious activities. A partner security firm generates a common threat list quarterly and shares it with all the business lines.\n\nAs a Security Engineer, how will you configure the threat list across all AWS accounts with minimum effort? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394850, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer has configured trusted IP lists and threat lists on Amazon GuardDuty to monitor the security of the AWS environment. Consider the following scenarios:</p>\n\n<p>a) While configuring the lists the engineer mistakenly added the same IP to both lists. What is the outcome of this configuration?</p>\n\n<p>b) To grant the identities full access (such as renaming, deactivating, uploading, activating, deleting) for working with trusted IP lists and threat lists, which managed policy needs to be added? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>The IP will be processed by the trusted IP list first, and will not generate a finding</strong></p>\n\n<p>Trusted IP lists consist of IP addresses that you have trusted for secure communication with your AWS infrastructure and applications. GuardDuty does not generate VPC flow logs or CloudTrail findings for IP addresses on trusted IP lists.</p>\n\n<p>Threat lists consist of known malicious IP addresses. This list can be supplied by third-party threat intelligence or created specifically for your organization. In addition to generating findings because of potentially suspicious activity, GuardDuty also generates findings based on these threat lists.</p>\n\n<p>If you include the same IP on both a trusted IP list and a threat list it will be processed by the trusted IP list first, and will not generate a finding.</p>\n\n<p><strong>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists. You also need to add the following privileges</strong></p>\n\n<pre><code>{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"iam:PutRolePolicy\",\n        \"iam:DeleteRolePolicy\"\n    ],\n    \"Resource\": \"arn:aws:iam::123456789123:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty\"\n}\n</code></pre>\n\n<p>Various IAM identities require special permissions to work with trusted IP lists and threat lists in GuardDuty. Identity with the attached <code>AmazonGuardDutyFullAccess</code> managed policy can only rename and deactivate uploaded trusted IP lists and threat lists.</p>\n\n<p>Permissions required to upload trusted IP lists and threat lists:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IP will be processed by the threat IP list first, and will generate findings</strong> - As explained above, this statement is incorrect.</p>\n\n<p><strong>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists</strong> - An identity with the attached <code>AmazonGuardDutyFullAccess</code> managed policy can only rename and deactivate uploaded trusted IP lists and threat lists.</p>\n\n<p><strong>Attach <code>AWSServiceRoleForAmazonGuardDuty</code> policy to your IAM entities to provide full access privileges to an identify to work with trusted IP lists and threat lists</strong> - This statement is incorrect. You can't attach AWSServiceRoleForAmazonGuardDuty to your IAM entities. This AWS-managed policy is attached to a service-linked role that allows GuardDuty to perform actions on your behalf.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_upload-lists.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonGuardDutyFullAccess\">https://docs.aws.amazon.com/guardduty/latest/ug/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonGuardDutyFullAccess</a></p>\n", "answers": ["<p>The IP will be processed by the trusted IP list first, and will not generate a finding</p>", "<p>The IP will be processed by the threat IP list first, and will generate findings</p>", "<p>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists. You also need to add the following privileges\n<code>{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"iam:PutRolePolicy\",\n        \"iam:DeleteRolePolicy\"\n    ],\n    \"Resource\": \"arn:aws:iam::123456789123:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty\"\n}\n</code></p>", "<p>Attach <code>AmazonGuardDutyFullAccess</code> managed policy to provide full access privileges to an identity to work with trusted IP lists and threat lists</p>", "<p>Attach <code>AWSServiceRoleForAmazonGuardDuty</code> policy to your IAM entities to provide full access privileges to an identity to work with trusted IP lists and threat lists</p>"]}, "correct_response": ["a", "c"], "section": "Infrastructure Security", "question_plain": "A security engineer has configured trusted IP lists and threat lists on Amazon GuardDuty to monitor the security of the AWS environment. Consider the following scenarios:\n\na) While configuring the lists the engineer mistakenly added the same IP to both lists. What is the outcome of this configuration?\n\nb) To grant the identities full access (such as renaming, deactivating, uploading, activating, deleting) for working with trusted IP lists and threat lists, which managed policy needs to be added? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394852, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer has configured a unified CloudWatch agent to push Amazon EC2 logs to Amazon CloudWatch Logs. However, the security team can't see any logs in the CloudWatch Logs console.</p>\n\n<p>Why isn't the unified CloudWatch agent pushing log events? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Creating an Amazon Machine Image (AMI) after the CloudWatch agent is installed can lead to errors in the CloudWatch agent</strong></p>\n\n<p>It's a best practice to install the CloudWatch agent at launch using AWS CloudFormation, AWS Systems Manager Agent (SSM Agent), user data scripts, or the AWS CLI. It is also a best practice to create an AMI before installing the CloudWatch agent. AMIs typically capture unique information from the original instance. Metadata becomes out of sync, and this state can lead to the CloudWatch agent not working as intended. Out-of-sync metadata is the reason that many Windows instances require Sysprep when working with AMI.</p>\n\n<p><strong>IAM user or IAM role policy should include the following IAM permissions:</strong></p>\n\n<pre><code>\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:DescribeLogStreams\"\n</code></pre>\n\n<p>The CloudWatch agent uses credentials from either the IAM user or IAM role policy to push log events to the CloudWatch service. Before a log event can be published, you must create a log group and log stream. If there's no log group or log stream, the CloudWatch agent creates them.</p>\n\n<p>You must also confirm that your policy includes the following IAM permissions:</p>\n\n<pre><code>\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:DescribeLogStreams\"\n</code></pre>\n\n<p>Add any missing IAM permissions to the user policy or the role policy.</p>\n\n<p>Complete list of scenarios to be checked:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Installing the CloudWatch agent after creating the Amazon Machine Image (AMI) can lead to errors in CloudWatch agent</strong> - As explained above, this option is incorrect.</p>\n\n<p><strong>If the CloudWatch Logs endpoint is configured to be a public endpoint using an internet gateway the connectivity fails. VPC endpoints have to be used to keep the log files in the AWS network</strong> - This statement is incorrect. Publicly routable endpoints accessed through an internet gateway or a network address translation (NAT) gateway is an acceptable configuration for a unified CloudWatch agent.</p>\n\n<p><strong>CloudWatch agent runs into errors if <code>run_as_user</code> parameter is any user other than the root user</strong> - CloudWatch agent can be run as a non-root user too. If you're using the run_as_user parameter, the user should have permission to the log location path. Without the necessary permissions, the CloudWatch agent can't write logs to the location. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-push-logs-with-unified-agent/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/rande.html#cwl_region\">https://docs.aws.amazon.com/general/latest/gr/rande.html#cwl_region</a></p>\n", "answers": ["<p>If the CloudWatch Logs endpoint is configured to be a public endpoint using an internet gateway the connectivity fails. VPC endpoints have to be used to keep the log files in AWS network</p>", "<p>Installing the CloudWatch agent after creating the Amazon Machine Image (AMI) can lead to errors in the CloudWatch agent</p>", "<p>Creating an Amazon Machine Image (AMI) after the CloudWatch agent is installed can lead to errors in the CloudWatch agent</p>", "<p>IAM user or IAM role policy should include the following IAM permissions:</p>\n\n<pre><code>\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:DescribeLogStreams\"\n</code></pre>", "<p>CloudWatch agent runs into errors if <code>run_as_user</code> parameter is any user other than the root user</p>"]}, "correct_response": ["c", "d"], "section": "Logging and Monitoring", "question_plain": "A security engineer has configured a unified CloudWatch agent to push Amazon EC2 logs to Amazon CloudWatch Logs. However, the security team can't see any logs in the CloudWatch Logs console.\n\nWhy isn't the unified CloudWatch agent pushing log events? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394854, "assessment_type": "multi-select", "prompt": {"question": "<p>A project manager has connected with you for the resolution of an issue. Although an AWS Identity and Access Management (IAM) entity has admin permissions, it has received an access denied error.</p>\n\n<p>As an AWS Certified Security Specialist, how will you troubleshoot and resolve this issue? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</strong></p>\n\n<p>A VPC endpoint policy is a resource-based policy that you can attach to a VPC endpoint. It can restrict access to IAM entities. If you route your requests through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy.</p>\n\n<p><strong>A session policy is in place and is causing an authorization issue</strong></p>\n\n<p>Session policies can be passed programmatically when you create a temporary session for your IAM role for a federated user. The permissions for a session are at the intersection of the identity-based policies assigned to the IAM entity that the session is created for and the session policy itself. Check if a session policy is passed for your IAM role session using the AWS CloudTrail logs for <code>AssumeRole/AssumeRoleWithSAML/AssumeRoleWithWebIdentity</code> API calls. To check for session policies passed for a federated user session, check CloudTrail logs for GetFederationToken API calls.</p>\n\n<p>Resolving authorization issues for IAM entities with admin permissions\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</strong> - A permission boundary defines the maximum permissions that an identity-based policy can grant to an entity, not a resource-based policy. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the definition of the resource-based policy is incorrect in this statement.</p>\n\n<p><strong>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</strong> - An Organization has a Service Control Policy (SCP) for restricting access to a service. A Network Access Control List (NACL) is an optional layer of security for a VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p>\n\n<p><strong>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</strong> - A permissions boundary limits the actions your entity can perform. If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the permissions boundary. So the reference to the resource-based policy is incorrect in this statement.</p>\n\n<p>Evaluating effective permissions with boundaries:\n<img src=\"https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/EffectivePermissions-rbp-boundary-id.png\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-access-denied-root-user/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n", "answers": ["<p>A resource-based policy defines the maximum permissions that an identity-based policy can grant to an entity. Check for any restrictive resource-based policies</p>", "<p>An Organization NACL can restrict access to member account IAM entities. Check for restrictions coming from an NACL using the management account of the Organization</p>", "<p>If you use a permissions boundary, then the entity can only perform actions that are allowed in both the identity-based policy and the concerned resource-based policy. Check for any restrictive permissions boundary</p>", "<p>If the requests are routed through a VPC endpoint, then check for any restrictions coming from the associated VPC endpoint policy</p>", "<p>A session policy is in place and is causing an authorization issue</p>"]}, "correct_response": ["d", "e"], "section": "Identity and Access Management", "question_plain": "A project manager has connected with you for the resolution of an issue. Although an AWS Identity and Access Management (IAM) entity has admin permissions, it has received an access denied error.\n\nAs an AWS Certified Security Specialist, how will you troubleshoot and resolve this issue? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394856, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has migrated most of its business to AWS Cloud using Amazon EC2 instances for Windows to host its applications. The domain services used by these applications are built on Active Directory servers which have been retained as on-premises servers. The company has issued guidelines to enable GuardDuty for all its applications.</p>\n\n<p>While analyzing GuardDuty reports, the security team realized that DNS logs are not being tracked/reported by GuardDuty. How will you fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>If you use a custom DNS resolver, then GuardDuty cannot access and process data from this data source</strong></p>\n\n<p>If you use AWS DNS resolvers for your Amazon EC2 instances (the default setting), then GuardDuty can access and process your request and response DNS logs through the internal AWS DNS resolvers. If you use another DNS resolver, such as OpenDNS or GoogleDNS, or if you set up your own DNS resolvers, then GuardDuty cannot access and process data from this data source.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>GuardDuty reports only on VPC Flow Logs, CloudTrail global events, and Kubernetes audit logs. GuardDuty does not analyze DNS logs</strong> - This statement is incorrect. GuardDuty can analyze DNS logs for AWS DNS resolvers.</p>\n\n<p><strong>GuardDuty analyzes your DNS logs from the stream of data provided through the Route 53 Resolver query logging feature. Check the path specified in this configuration</strong> - This statement is incorrect. When you enable GuardDuty, it immediately starts analyzing your DNS logs from an independent stream of data. This data stream is separate from the data provided through the Route 53 Resolver query logging feature. Configuration of this feature does not affect GuardDuty analysis.</p>\n\n<p><strong>Check the permissions attached on the IAM role used by GuardDuty for accessing DNS logs</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_data-sources.html\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_data-sources.html</a></p>\n", "answers": ["<p>GuardDuty reports only on VPC Flow Logs, CloudTrail global events, and Kubernetes audit logs. GuardDuty does not analyze DNS logs</p>", "<p>GuardDuty analyzes your DNS logs from the stream of data provided through the Route 53 Resolver query logging feature. Check the path specified in this configuration</p>", "<p>If you use a custom DNS resolver, then GuardDuty cannot access and process data from this data source</p>", "<p>Check the permissions attached on the IAM role used by GuardDuty for accessing DNS logs</p>"]}, "correct_response": ["c"], "section": "Infrastructure Security", "question_plain": "A company has migrated most of its business to AWS Cloud using Amazon EC2 instances for Windows to host its applications. The domain services used by these applications are built on Active Directory servers which have been retained as on-premises servers. The company has issued guidelines to enable GuardDuty for all its applications.\n\nWhile analyzing GuardDuty reports, the security team realized that DNS logs are not being tracked/reported by GuardDuty. How will you fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394858, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:</p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"key-policy-1\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetPut\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::ExampleBucket/*\"\n    },\n    {\n      \"Sid\": \"KMS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:Encrypt\"\n      ],\n      \"Resource\": \"arn:aws:kms:us-west-1:111122223333:key/keyid-12345\"\n    }\n  ]\n}\n</code></pre>\n\n<p>The team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.</p>\n\n<p>Which IAM policy action should be added to the IAM policy to resolve the error?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>kms:GenerateDataKey</strong></p>\n\n<p>GenerateDataKey returns a unique symmetric data key for use outside of AWS KMS. This operation returns a plaintext copy of the data key and a copy that is encrypted under a symmetric encryption KMS key that you specify. The bytes in the plaintext key are random; they are not related to the caller or the KMS key. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>kms:GetPublicKey</strong> - This option returns the public key of an asymmetric KMS key. Unlike the private key of an asymmetric KMS key, which never leaves AWS KMS unencrypted, callers with kms:GetPublicKey permission can download the public key of an asymmetric KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetKeyPolicy</strong> - This option gets a key policy attached to the specified KMS key. It cannot be used for a client-side encryption mechanism.</p>\n\n<p><strong>kms:GetDataKey</strong> - This is a made-up option that serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetKeyPolicy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_GetPublicKey.html</a></p>\n", "answers": ["<p>kms:GetPublicKey</p>", "<p>kms:GetKeyPolicy</p>", "<p>kms:GetDataKey</p>", "<p>kms:GenerateDataKey</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "The security team at a company needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The team created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose. The team created the following IAM policy and attached it to an IAM role:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"key-policy-1\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetPut\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::ExampleBucket/*\"\n    },\n    {\n      \"Sid\": \"KMS\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"kms:Decrypt\",\n        \"kms:Encrypt\"\n      ],\n      \"Resource\": \"arn:aws:kms:us-west-1:111122223333:key/keyid-12345\"\n    }\n  ]\n}\n\n\nThe team was able to successfully get existing objects from the S3 bucket while testing. But any attempts to upload a new object resulted in an error. The error message stated that the action was forbidden.\n\nWhich IAM policy action should be added to the IAM policy to resolve the error?", "related_lectures": []}, {"_class": "assessment", "id": 70394860, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer has attached an AWS Identity and Access Management (IAM) role to an Amazon Elastic Compute Cloud (Amazon EC2) instance. Upon testing, the engineer realized that the Amazon EC2 instance makes API calls with an IAM user instead of the attached IAM role.</p>\n\n<p>What is the issue and how will you fix it?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Check if the IAM user credentials are stored in the .aws/credentials file. Because these credentials have higher precedence over role credentials, IAM user credentials will be used to make the API calls. Delete this credentials file</strong></p>\n\n<p>The AWS CLI uses credentials and configuration settings located in multiple places, such as the system or user environment variables, local AWS configuration files, or explicitly declared on the command line as a parameter. Certain locations take precedence over others. The AWS CLI credentials and configuration settings take precedence in a particular order.</p>\n\n<p>The credentials and config file are updated when you run the command aws configure. This file can contain the credential details for the default profile and any named profiles. If this file is present, it takes precedence over the IAM role defined in the instance.</p>\n\n<p>Configuration settings and precedence:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence\">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The IAM role attached does not have enough permissions to make the API calls. Hence, the default user credentials of the instance are being used for the API calls. Add the required permissions to the role</strong> - As explained above, this is an incorrect statement, given only as a distractor.</p>\n\n<p><strong>You cannot associate an IAM role with your Amazon Elastic Container Service (Amazon ECS) task definitions. While this association does not result in an error, the IAM role credentials are not used. Use service-based roles for container applications</strong> - You can associate an IAM role with each of your Amazon Elastic Container Service (Amazon ECS) task definitions. Temporary credentials for that role are then available to that task's containers. So this option is incorrect.</p>\n\n<p><strong>The EC2 instance needs to be refreshed after attaching the necessary IAM role. Refresh the instance and the API calls with be done using the newly attached IAM role</strong> - This option has been added as a distractor. EC2 instance refresh is a feature in EC2 Auto Scaling that enables automatic deployments of instances in Auto Scaling Groups (ASGs), in order to release new application versions or make infrastructure updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence\">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence</a></p>\n", "answers": ["<p>The IAM role attached does not have enough permissions to make the API calls. Hence, the default user credentials of the instance are being used for the API calls. Add the required permissions to the role</p>", "<p>You cannot associate an IAM role with your Amazon Elastic Container Service (Amazon ECS) task definitions. While this association does not result in an error, the IAM role credentials are not used. Use service-based roles for container applications</p>", "<p>Check if the IAM user credentials are stored in the .aws/credentials file. Because these credentials have higher precedence over role credentials, IAM user credentials will be used to make the API calls. Delete the credentials file</p>", "<p>The EC2 instance needs to be refreshed after attaching the necessary IAM role. Refresh the instance and the API calls with be done using the newly attached IAM role</p>"]}, "correct_response": ["c"], "section": "Identity and Access Management", "question_plain": "A security engineer has attached an AWS Identity and Access Management (IAM) role to an Amazon Elastic Compute Cloud (Amazon EC2) instance. Upon testing, the engineer realized that the Amazon EC2 instance makes API calls with an IAM user instead of the attached IAM role.\n\nWhat is the issue and how will you fix it?", "related_lectures": []}, {"_class": "assessment", "id": 70394862, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A project manager has connected with you for a security requirement from the client. The client wants to ensure that the authenticated encryption with associated data encryption is used when calling AWS Key Management Service (AWS KMS) Encrypt, Decrypt, and ReEncrypt APIs.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend to address this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use encryption context that you can use to verify the authenticity of AWS KMS API calls, and the integrity of the ciphertext returned by the AWS Decrypt API</strong></p>\n\n<p>All AWS KMS cryptographic operations with symmetric encryption KMS keys accept an encryption context, an optional set of key\u2013value pairs that can contain additional contextual information about the data. AWS KMS uses the encryption context as additional authenticated data (AAD) to support authenticated encryption.</p>\n\n<p>When you include an encryption context in an encryption request, it is cryptographically bound to the ciphertext such that the same encryption context is required to decrypt (or decrypt and re-encrypt) the data. If the encryption context provided in the decryption request is not an exact, case-sensitive match, the decrypt request fails. Only the order of the key-value pairs in the encryption context can vary.</p>\n\n<p>The encryption context is not secret and not encrypted. It appears in plaintext in AWS CloudTrail Logs so you can use it to identify and categorize your cryptographic operations. Your encryption context should not include sensitive information.</p>\n\n<p>An example explaining the usage of encryption context in KMS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-protect-the-integrity-of-your-encrypted-data-by-using-aws-key-management-service-and-encryptioncontext/\">https://aws.amazon.com/blogs/security/how-to-protect-the-integrity-of-your-encrypted-data-by-using-aws-key-management-service-and-encryptioncontext/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the envelope encryption strategy of AWS KMS to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - AWS KMS supports sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency. So, using envelope encryption for validating the KMS API request for the given use case does not make sense.</p>\n\n<p><strong>Use multi-factor authentication (MFA) to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - When you enable multi-factor authentication (MFA), users must sign in to the AWS access portal with their user name and password. This is the first factor, something they know. Users must also sign in with either a code or security key. This is the second factor, something they have or something they are. The second factor could be either an authentication code generated from their mobile device or alternatively by tapping on a security key connected to their computer. Taken together, these multiple factors provide increased security by preventing unauthorized access to your AWS resources unless a valid MFA challenge has been successfully completed.</p>\n\n<p>MFA does not apply to the given use case where we want the authenticated encryption with associated data encryption to be used with KMS API calls.</p>\n\n<p><strong>Use AWS CloudHSM to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</strong> - The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.</p>\n\n<p>Using CloudHSM is not relevant to the given use case where we want the authenticated encryption with associated data encryption to be used with KMS API calls.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/use-envelope-encryption-with-customer-master-keys.html\">https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/use-envelope-encryption-with-customer-master-keys.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-sdk.html\">https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-sdk.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/enable-mfa.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/enable-mfa.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudhsm/\">https://aws.amazon.com/cloudhsm/</a></p>\n", "answers": ["<p>Use encryption context that you can use to verify the authenticity of AWS KMS API calls and the integrity of the ciphertext returned by the AWS Decrypt API</p>", "<p>Use envelope encryption strategy of AWS KMS to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>", "<p>Use AWS CloudHSM to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>", "<p>Use multi-factor authentication (MFA) to verify the authenticity of AWS KMS API calls and safeguard the integrity of ciphertext</p>"]}, "correct_response": ["a"], "section": "Data Protection", "question_plain": "A project manager has connected with you for a security requirement from the client. The client wants to ensure that the authenticated encryption with associated data encryption is used when calling AWS Key Management Service (AWS KMS) Encrypt, Decrypt, and ReEncrypt APIs.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend to address this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394864, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Amazon EC2 instance connects to an Amazon S3 bucket using an IAM role with necessary permissions. While analyzing the logs, a security engineer raised the possibility of the instance being compromised. The instance hosts a critical application and cannot be immediately terminated.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following will you suggest as the fastest way to block further access to sensitive data from the compromised instance?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Revoke all active sessions for the IAM role. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile</strong></p>\n\n<p>Revoke all active sessions for the IAM role - This step ensures that all users with current sessions created by assuming the role are denied access to all AWS actions and resources. This can result in users losing unsaved work but the compromised sessions trying to access the critical S3 data will also be dropped.</p>\n\n<p>Update the S3 bucket policy to deny access to the IAM role - This step will force S3 to deny access to the IAM role.</p>\n\n<p>Remove the IAM role from the EC2 instance profile - Finally, remove the IAM role from the compromised instance to stop further access to S3 bucket.</p>\n\n<p>Revoking active sessions and session permissions:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Remove the IAM role from the EC2 instance profile. Block all public access to the S3 bucket. To allow access to your S3 objects to trusted entities outside your account, create a pre-signed URL through S3</strong> - This option has been added as a distractor. This will not immediately stop the active sessions from accessing the S3 data from the instance.</p>\n\n<p><strong>Update the S3 bucket policy to deny access to the IAM role. Create an Amazon EBS snapshot of the instance and terminate the instance</strong> - This will not immediately stop the active sessions from accessing the S3 data from the instance. Also, it is mentioned in the question that the instance cannot be terminated. Hence, this option is incorrect.</p>\n\n<p><strong>Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile. Use S3 Access Points to create permission sets that restrict access to only those within your private network</strong> - This option as well does not consider the active sessions that need to be dropped immediately to curb further access to the critical S3 data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_remediate.html#compromised-ec2\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_remediate.html#compromised-ec2</a></p>\n", "answers": ["<p>Remove the IAM role from the EC2 instance profile. Block all public access to the S3 bucket. To allow access to your S3 objects to trusted entities outside your account, create a pre-signed URL through S3</p>", "<p>Revoke all active sessions for the IAM role. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile</p>", "<p>Update the S3 bucket policy to deny access to the IAM role. Create an Amazon EBS snapshot of the instance and terminate the instance</p>", "<p>Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile. Use S3 Access Points to create permission sets that restrict access to only those within your private network</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "An Amazon EC2 instance connects to an Amazon S3 bucket using an IAM role with necessary permissions. While analyzing the logs, a security engineer raised the possibility of the instance being compromised. The instance hosts a critical application and cannot be immediately terminated.\n\nAs an AWS Certified Security Specialist, which of the following will you suggest as the fastest way to block further access to sensitive data from the compromised instance?", "related_lectures": []}, {"_class": "assessment", "id": 70394866, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A pharmaceutical company is showcasing its new business lines and is promoting them to its partner organizations. These flagship applications are hosted on Amazon EC2 instances. The technology teams at the partner organizations are expected to access these instances for a first-hand understanding of these applications. The EC2 instances will be shared, and non-root SSH access is needed for the teams.</p>\n\n<p>As a Security Engineer, how will you block the EC2 instance metadata service for the given use case to avoid an assault on other AWS account resources?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Implement local firewall rules using iptables based restrictions on the instances</strong></p>\n\n<p>iptables is a command line interface used to set up and maintain tables for the Netfilter firewall for IPv4, included in the Linux kernel. The firewall matches packets with rules defined in these tables and then takes the specified action on a possible match.</p>\n\n<p>You can consider using local firewall rules to disable access from some or all processes to the instance metadata service. The following example uses Linux iptables and its owner module to prevent the Apache webserver (based on its default installation user ID of apache) from accessing 169.254.169.254. It uses a deny rule to reject all instance metadata requests (whether IMDSv1 or IMDSv2) from any process running as that user.</p>\n\n<p><code>sudo iptables --append OUTPUT --proto tcp --destination 169.254.169.254 --match owner --uid-owner apache --jump REJECT</code></p>\n\n<p>The following example prevents access to the instance metadata service by all processes, except for processes running in the user account trustworthy-user.</p>\n\n<p><code>sudo iptables --append OUTPUT --proto tcp --destination 169.254.169.254 --match owner ! --uid-owner trustworthy-user --jump REJECT</code></p>\n\n<p>Using iptables to limit access:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the instance metadata service on each instance so that users must use Instance Metadata Service Version 2 (IMDSv2). The session-oriented methods will not respond to usual request/response queries</strong> - You can access instance metadata from a running instance using one of the following methods:\n1. Instance Metadata Service Version 1 (IMDSv1) \u2013 a request/response method\n2. Instance Metadata Service Version 2 (IMDSv2) \u2013 a session-oriented method</p>\n\n<p>By default, you can use either IMDSv1 or IMDSv2, or both. The instance metadata service distinguishes between IMDSv1 and IMDSv2 requests based on whether, for any given request, either the PUT or GET headers, which are unique to IMDSv2, are present in that request.</p>\n\n<p>This solution will not help block access to instance metadata as needed.</p>\n\n<p><strong>Disable the instance metadata service on all the instances</strong> - An application on the instance retrieves the security credentials provided by the role from the instance metadata item <code>iam/security-credentials/role-name</code>. The application is granted the permissions for the actions and resources that you've defined for the role through the security credentials associated with the role. Therefore, disabling metadata service can cause applications that use the roles to crash. Hence, this option is incorrect.</p>\n\n<p><strong>Install intrusion prevention software (IPS) on each instance to disable access to instance metadata</strong> - EC2 Instance IDS/IPS solutions offer key features to help protect your EC2 instances. This includes alerting administrators of malicious activity and policy violations, as well as identifying and taking action against attacks. This solution is not useful for blocking access to instance metadata.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/configuring-instance-metadata-service.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/configuring-instance-metadata-service.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/mp/scenarios/security/ids/\">https://aws.amazon.com/mp/scenarios/security/ids/</a></p>\n", "answers": ["<p>Configure the instance metadata service on each instance so that users must use Instance Metadata Service Version 2 (IMDSv2). The session-oriented methods will not respond to usual request/response queries</p>", "<p>Implement local firewall rules using iptables based restrictions on the instances</p>", "<p>Disable the instance metadata service on all the instances</p>", "<p>Install intrusion prevention software (IPS) on each instance to disable access to instance metadata</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "A pharmaceutical company is showcasing its new business lines and is promoting them to its partner organizations. These flagship applications are hosted on Amazon EC2 instances. The technology teams at the partner organizations are expected to access these instances for a first-hand understanding of these applications. The EC2 instances will be shared, and non-root SSH access is needed for the teams.\n\nAs a Security Engineer, how will you block the EC2 instance metadata service for the given use case to avoid an assault on other AWS account resources?", "related_lectures": []}, {"_class": "assessment", "id": 70394868, "assessment_type": "multi-select", "prompt": {"question": "<p>As a Security Engineer, you received a notification from AWS about suspicious activity in your account. What are the security checks/actions that you will need to perform before responding to the AWS Support Center? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create new access keys and modify the application to use new ones. Deactivate the exposed account access keys immediately. Subsequently, delete the exposed keys only when you have verified the proper functioning of the application</strong></p>\n\n<ol>\n<li>Create a new AWS access key.</li>\n<li>Modify your application to use the new access key.</li>\n<li>Deactivate the original access key. Don't delete the original access key yet. Deactivate the original access key only.</li>\n<li>Verify that there aren't any issues with your application. If there are issues, reactivate the original access key temporarily to remediate the problem.</li>\n<li>If your application is fully functional after deactivating the original access key, then delete the original access key.</li>\n<li>Delete the AWS account root user access keys that you no longer need or didn't create.</li>\n</ol>\n\n<p><strong>If you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance</strong></p>\n\n<p>Delete any unrecognized or unauthorized resources. If you must keep any resources for investigation, consider backing up those resources. For example, if you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance.</p>\n\n<p>Steps to follow to delete any unrecognized or unauthorized resources:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q41-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n\n<p><strong>In the IAM console, under the Permissions tab, look for a policy named <code>AWSExposedCredentialPolicy_DO_NOT_REMOVE</code>. If the user has this policy attached, then rotate the access keys for the user</strong> - Rotate any potentially unauthorized IAM user credentials.</p>\n\n<p>Rotate any potentially unauthorized IAM user credentials:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com//premiumsupport/knowledge-center/potential-account-compromise/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To contain access for an IAM principal where an IAM access key has been compromised, the access key can be deactivated or deleted. It is important to note that an IAM principal can have up to five access keys at any given time</strong> - While it is correct that it is necessary to contain access for an IAM principal where an IAM access key has been compromised, an IAM principal can only have up to two access keys at any given time and not five as given in the statement.</p>\n\n<p><strong>If the exposed EC2 instance cannot be shut down, move it to Isolation VPC to contain the exposure of other resources while having the ability to keep the instance working</strong> - Isolation VPCs can be used to provide effective containment of resources while providing access to legitimate traffic. Isolation VPCs can be preconfigured in advance of a security event to permit valid IP addresses and ports, and targeted resources can immediately be moved into this isolation VPC during an active security event to contain the resource while allowing legitimate traffic to be sent and received by the targeted resource during subsequent phases of incident response.</p>\n\n<p>An important aspect of using an isolation VPC is that resources, such as EC2 instances, need to be shut down and relaunched in the new isolation VPC before use.</p>\n\n<p><strong>There is no need to revoke any temporary IAM security credentials</strong> - Temporary credentials are typically used by IAM roles and do not have to be rotated or explicitly revoked because they have a limited lifetime. In cases where a security event occurs involving a temporary security credential before the temporary security credential expiration, you might need to alter the effective permissions of the existing temporary security credentials. When you permit users to access the AWS Management Console with a long session duration time (such as 12 hours), their temporary credentials do not expire as quickly. If users inadvertently expose their credentials to an unauthorized third-party, that party has access for the duration of the session. However, you can immediately revoke all permissions to the role's credentials issued before a certain point in time if you need to. All temporary credentials for that role issued before the specified time become invalid. This forces all users to re-authenticate and request new credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/technique-access-containment.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/technique-access-containment.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/destination-containment.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-security-incident-response-guide/destination-containment.html</a></p>\n", "answers": ["<p>Create new access keys and modify the application to use new ones. Deactivate the exposed account access keys immediately. Subsequently, delete the exposed keys only when you have verified the proper functioning of the application</p>", "<p>To contain access for an IAM principal where an IAM access key has been compromised, the access key can be deactivated or deleted. It is important to note that an IAM principal can have up to five access keys at any given time</p>", "<p>If the exposed EC2 instance cannot be shut down, move it to Isolation VPC to contain the expose of other resources while having the ability to keep the instance working</p>", "<p>There is no need to revoke any temporary IAM security credentials</p>", "<p>If you must retain an EC2 instance for regulatory, compliance, or legal reasons, then create an Amazon EBS snapshot before terminating the instance</p>", "<p>In the IAM console, under the Permissions tab, look for a policy named <code>AWSExposedCredentialPolicy_DO_NOT_REMOVE</code>. If the user has this policy attached, then rotate the access keys for the user</p>"]}, "correct_response": ["a", "e", "f"], "section": "Incident Response", "question_plain": "As a Security Engineer, you received a notification from AWS about suspicious activity in your account. What are the security checks/actions that you will need to perform before responding to the AWS Support Center? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394870, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer has been asked to enable AWS CloudTrail trail to log data events on an S3 bucket with an empty object prefix. The S3 bucket is owned by the Owner user. Another user Bob has a separate account that has been granted access to the S3 bucket. Bob also wants to log data events for all objects in the same S3 bucket, so Bob configures a trail and specifies the same S3 bucket with an empty object prefix.</p>\n\n<p>Consider the following events:</p>\n\n<ol>\n<li><p>Bob uploads an object to the S3 bucket with the <code>PutObject</code> API operation.</p></li>\n<li><p>Owner uploads an object to the S3 bucket.</p></li>\n</ol>\n\n<p>What will be the outcome of the two events defined above? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings also match the event, so the event is logged in Owner's trail too</strong></p>\n\n<p><strong>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account</strong></p>\n\n<p>When you configure your trail to log data events, you can also specify S3 objects that belong to other AWS accounts. When an event occurs on a specified object, CloudTrail evaluates whether the event matches any trails in each account. If the event matches the settings for a trail, the trail processes and logs the event for that account. Generally, both API callers and resource owners can receive events.</p>\n\n<p>If you own an S3 object and you specify it in your trail, your trail logs events that occur on the object in your account. Because you own the object, your trail also logs events when other accounts call the object.</p>\n\n<p>If you specify an S3 object in your trail, and another account owns the object, your trail only logs events that occur on that object in your account. Your trail doesn't log events that occur in other accounts.</p>\n\n<p>Logging data events for an Amazon S3 object for two AWS accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q42-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings do not match the event, so the event is not logged in Owner's trail</strong></p>\n\n<p><strong>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account. The event also matches the trail settings of Bob, so Bob's trail too logs the event in Bob's account</strong></p>\n\n<p><strong>In both the upload events, the trail is logged only in the uploaded user's account ie Bob's upload event is only logged in Bob's account and Owner's upload event is logged only in Owner's account</strong></p>\n\n<p>These three options contradict the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html</a></p>\n", "answers": ["<p>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings also match the event, so the event is logged in Owner's trail too</p>", "<p>When Bob uploaded the object, the upload event occurred in Bob's account and it matches the settings for Bob's trail. Bob's trail processes and logs the event. The Owner's trail settings do not match the event, so the event is not logged in Owner's trail</p>", "<p>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account. The event also matches trail settings of Bob, so Bob's trail logs the event in Bob's account</p>", "<p>When the Owner uploaded the object, the upload event occurs in Owner's account and it matches the settings for Owner's trail. The trail processes and logs the event in Owner's account</p>", "<p>In both the upload events, the trail is logged only in the uploaded user's account ie Bob's upload event is only logged in Bob's account and Owner's upload event is logged only in Owner's account</p>"]}, "correct_response": ["a", "d"], "section": "Logging and Monitoring", "question_plain": "A security engineer has been asked to enable AWS CloudTrail trail to log data events on an S3 bucket with an empty object prefix. The S3 bucket is owned by the Owner user. Another user Bob has a separate account that has been granted access to the S3 bucket. Bob also wants to log data events for all objects in the same S3 bucket, so Bob configures a trail and specifies the same S3 bucket with an empty object prefix.\n\nConsider the following events:\n\n\nBob uploads an object to the S3 bucket with the PutObject API operation.\nOwner uploads an object to the S3 bucket.\n\n\nWhat will be the outcome of the two events defined above? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394872, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Security Engineer is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the engineer wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.</p>\n\n<p>Which of the following actions meets the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</strong> - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization\u2019s access control guidelines.</p>\n\n<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with <em>/</em> permissions to the user.</p>\n\n<p>SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</strong> - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users.</p>\n\n<p><strong>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</strong> - The root user can modify this IAM policy itself, so this option is not correct.</p>\n\n<p><strong>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role.</p>\n\n<p>The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n", "answers": ["<p>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</p>", "<p>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</p>", "<p>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</p>", "<p>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</p>"]}, "correct_response": ["b"], "section": "Identity and Access Management", "question_plain": "A Security Engineer is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the engineer wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.\n\nWhich of the following actions meets the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70394874, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An organization has added virtual machine images, software, and a few databases to its AWS Service Catalog. These will be used by multiple development teams to build their business workloads. The organization does not want the end users to launch and manage products using their own IAM credentials.</p>\n\n<p>How will you address this requirement and implement it in the least possible time?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add launch constraint(s) to each product in the service catalog portfolio</strong></p>\n\n<p>A launch constraint specifies the AWS Identity and Access Management (IAM) role that Service Catalog assumes when an end user launches, updates, or terminates a product. An IAM role is a collection of permissions that an IAM user or AWS service can assume temporarily to use AWS services.</p>\n\n<p>Launch constraints apply to products in the portfolio (product portfolio association). Launch constraints do not apply at the portfolio level or to a product across all portfolios. To associate a launch constraint with all products in a portfolio, you must apply the launch constraint to each product individually.</p>\n\n<p>Without a launch constraint, end users must launch and manage products using their own IAM credentials. To do so, they must have permissions for AWS CloudFormation, AWS services that the products use, and Service Catalog. By using a launch role, you can instead limit the end users' permissions to the minimum they require for that product.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new portfolio and add the new products to it. Attach a launch constraint to this portfolio</strong> - Launch constraints are at the product level and not at the portfolio level. Hence, this option is incorrect.</p>\n\n<p><strong>Use the service actions feature of the service catalog to define rules and constraints for the products in the portfolio. A CloudFormation template can also be used for easier implementation</strong> - Service Catalog enables you to reduce administrative maintenance and end-user training while adhering to compliance and security measures. With service actions, as the administrator, you can enable end users to perform operational tasks, troubleshoot issues, run approved commands, or request permissions in Service Catalog. Service actions cannot be used for limiting user permissions on the service catalog products.</p>\n\n<p><strong>Add the newly added products under a single tag. Add tag constraints to control the end user behavior and permissions on the products</strong> - This is a made-up option. There are no tag constraints, only tag update constraints. With tag update constraints, Service Catalog administrators can allow or disallow end users to update tags on resources associated with a Service Catalog provisioned product.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-launch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/using-service-actions.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/using-service-actions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-resourceupdate.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-resourceupdate.html</a></p>\n", "answers": ["<p>Create a new portfolio and add the new products to it. Attach a launch constraint to this portfolio</p>", "<p>Use the service actions feature of the service catalog to define rules and constraints for the products in the portfolio. A CloudFormation template can also be used for easier implementation</p>", "<p>Add the newly added products under a single tag. Add tag constraints to control the end user behavior and permissions on the products</p>", "<p>Add launch constraint(s) to each product in the service catalog portfolio</p>"]}, "correct_response": ["d"], "section": "Identity and Access Management", "question_plain": "An organization has added virtual machine images, software, and a few databases to its AWS Service Catalog. These will be used by multiple development teams to build their business workloads. The organization does not want the end users to launch and manage products using their own IAM credentials.\n\nHow will you address this requirement and implement it in the least possible time?", "related_lectures": []}, {"_class": "assessment", "id": 70394876, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Security Engineer has configured an AWS Web Application Firewall (WAF) for all the Application Load Balancers (ALBs) after getting a possible threat alert from the company's IT security department.</p>\n\n<p>How can the Engineer validate if the AWS WAF rules are working?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable WAF comprehensive logs that are delivered through Amazon Kinesis Firehose to a destination of your choice</strong></p>\n\n<p>AWS WAF supports full logging of all web requests inspected by the service. Customers can store these logs in Amazon S3 for compliance and auditing needs as well as use them for debugging and additional forensics. The logs will help customers understand why certain rules are triggered and why certain web requests are blocked. Customers can also integrate the logs with their SIEM and log analysis tools.</p>\n\n<p>For each web request, AWS WAF logs now provide raw HTTP/S headers along with information on which AWS WAF rules are triggered. This is useful for troubleshooting custom WAF rules and Managed Rules for AWS WAF. These logs will be made available via Amazon Kinesis Data Firehose in JSON format.</p>\n\n<p>Enabling AWS WAF full logs is done in two steps. First, on the Amazon Kinesis console, create an instance of the Amazon Kinesis Data Firehose in the relevant account(s). As part of this configuration, customers can choose a destination for the data from Amazon S3, Amazon ElasticSearch, or Amazon RedShift. Customers can also leverage third-party tool(s) from Splunk or Sumo Logic to enable advanced SIEM solutions, giving them a platform for advanced monitoring. Second, on the AWS WAF console, enable the logs and select the Firehose instance. When configuring, customers also have the option of redacting fields from web requests that they do not want to be logged.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS WAF reports metrics once a minute to CloudTrail. You can use statistics in Amazon CloudTrail to gather insights about the WAF responses</strong> - This statement is incorrect. If CloudTrail is substituted with CloudWatch then this statement stands correct i.e. AWS WAF reports metrics once a minute to CloudWatch. You can use statistics in Amazon CloudWatch to gather insights about the WAF responses.</p>\n\n<p><strong>Use iPerf Testing tool to emulate DDoS attack on the resources and check for WAF responses through Amazon CloudWatch logs</strong> - iPerf is a tool for network performance measurement and tuning. It is a cross-platform tool that can produce standardized performance measurements for any network. This is not relevant to the given use case.</p>\n\n<p><strong>Request penetration testing for login request flooding or API request flooding, whichever is applicable for your configuration</strong> - This statement is incorrect. Firstly, WAF service falls under \u201cPermitted Services\u201d which can be used to carry out penetration testing without prior approval from AWS. Secondly, Request flooding (login request flooding, API request flooding) fall under prohibited activities and hence should not be carried out on AWS resources.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/08/aws-waf-launches-new-comprehensive-logging-functionality/\">https://aws.amazon.com/about-aws/whats-new/2018/08/aws-waf-launches-new-comprehensive-logging-functionality/</a></p>\n\n<p><a href=\"https://aws.amazon.com/security/penetration-testing/\">https://aws.amazon.com/security/penetration-testing/</a></p>\n\n<p><a href=\"https://aws.amazon.com/shield/faqs/\">https://aws.amazon.com/shield/faqs/</a></p>\n", "answers": ["<p>AWS WAF reports metrics once a minute to CloudTrail. You can use statistics in Amazon CloudTrail to gather insights about the WAF responses</p>", "<p>Enable WAF comprehensive logs that are delivered through Amazon Kinesis Firehose to a destination of your choice</p>", "<p>Use iPerf Testing tool to emulate DDoS attack on the resources and check for WAF responses through Amazon CloudWatch logs</p>", "<p>Request penetration testing for login request flooding or API request flooding, whichever is applicable for your configuration</p>"]}, "correct_response": ["b"], "section": "Infrastructure Security", "question_plain": "A Security Engineer has configured an AWS Web Application Firewall (WAF) for all the Application Load Balancers (ALBs) after getting a possible threat alert from the company's IT security department.\n\nHow can the Engineer validate if the AWS WAF rules are working?", "related_lectures": []}, {"_class": "assessment", "id": 70394878, "assessment_type": "multi-select", "prompt": {"question": "<p>A Systems Administrator is no longer able to access the Windows Amazon EC2 instance because the Windows administrator password is lost. As a Security Engineer, you have been tasked with the job of resetting the password of the instance.</p>\n\n<p>Which of the following steps would you suggest to reset the password using EC2Launch v2? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</strong></p>\n\n<p><strong>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></strong></p>\n\n<p><strong>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</strong></p>\n\n<p>If you have lost your Windows administrator password and are using a supported Windows AMI that includes the EC2Launch v2 agent, you can use EC2Launch v2 to generate a new password.</p>\n\n<p>The steps to be followed are as follows:\n1. Verify that the EC2Launch v2 service is running</p>\n\n<ol>\n<li><p>Detach the root volume from the instance - You can't use EC2Launch v2 to reset an administrator password if the volume on which the password is stored is attached to an instance as the root volume. You must detach the volume from the original instance before you can attach it to a temporary instance as a secondary volume.</p></li>\n<li><p>Attach the volume to a temporary instance - Launch a temporary instance and attach the volume to it as a secondary volume. This is the instance you use to modify the configuration file. The temporary instance must be in the same Availability Zone as the original instance.</p></li>\n<li><p>Delete the <code>.run-once</code> file - After you have attached the volume to the temporary instance as a secondary volume, delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code>. This directs EC2Launch v2 to run all tasks with a frequency of once, which includes setting the administrator password.</p></li>\n<li><p>Restart the original instance - After you have deleted the .run-once file, reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></strong></p>\n\n<p><strong>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</strong></p>\n\n<p>These two steps are part of resetting the Windows administrator password using EC2Launch when EC2Launch v2 is not installed on the instance.</p>\n\n<p><strong>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</strong> - To avoid disk signature collisions, you must select an AMI for a different version of Windows. For example, if the original instance runs Windows Server 2019, launch the temporary instance using the base AMI for Windows Server 2016. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ResettingAdminPassword_EC2Launchv2.html</a></p>\n", "answers": ["<p>To reset the administrator password, Download the EC2Rescue for Windows Server zip file, extract the contents, and run <code>EC2Rescue.exe</code></p>", "<p>Verify that the EC2Launch v2 service is running. Detach the EBS root volume from the instance</p>", "<p>Select Offline Instance Option -&gt; Diagnose and Rescue -&gt; Reset Administrator Password. Reattach the volume to the original instance, then restart the instance</p>", "<p>Reattach the volume to the original instance as the root volume and connect to the instance using its key pair to retrieve the administrator password. Connect to the instance using its current public DNS name</p>", "<p>Launch a temporary instance and attach the volume to it as a secondary volume. Delete the <code>.run-once</code> file from the instance, located at <code>%ProgramData%/Amazon/EC2Launch/state/.run-once</code></p>", "<p>When you launch the temporary instance, to avoid disk signature collisions, you must select an AMI for the same version of Windows</p>"]}, "correct_response": ["b", "d", "e"], "section": "Incident Response", "question_plain": "A Systems Administrator is no longer able to access the Windows Amazon EC2 instance because the Windows administrator password is lost. As a Security Engineer, you have been tasked with the job of resetting the password of the instance.\n\nWhich of the following steps would you suggest to reset the password using EC2Launch v2? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394910, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A fault management application at a company connects to several other systems to monitor the status of the systems hosting the suite of flagship applications for the company. As per the security policy of the company, Cloudtrail and VPC flow logs have been enabled for all AWS resources. A recent internal error from the support team led to several minutes of outage on the fault management application and a few hours of analysis to understand the root cause of the error.</p>\n\n<p>The company is now looking for a solution that can analyze data from various logs as well as security findings to quickly triage the root-cause linked to the security issues. What is the best-fit solution for the company's requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Detective in conjunction with Amazon GuardDuty to monitor malicious activity and unauthorized behavior on the AWS resources and quickly identify the root cause of potential security issues through linked datasets</strong></p>\n\n<p>Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. Amazon Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to build a linked set of data that enables you to easily conduct faster and more efficient security investigations.</p>\n\n<p>Amazon Detective simplifies the investigative process and helps security teams conduct faster and more effective investigations. Amazon Detective\u2019s prebuilt data aggregations, summaries, and context help you to quickly analyze and determine the nature and extent of possible security issues. Amazon Detective maintains up to a year of aggregated data and makes it easily available through a set of visualizations that shows changes in the type and volume of activity over a selected time window, and links those changes to security findings.</p>\n\n<p>Amazon Detective provides a variety of visualizations that present context and insights about AWS resources such as AWS accounts, EC2 instances, users, roles, IP addresses, and Amazon GuardDuty findings. Each visualization is designed to answer specific questions that may come up as you analyze findings and the related activity. Each visualization provides textual guidance that clearly explains how to interpret the panel and use its information to answer your investigative questions.</p>\n\n<p>Amazon Detective requires that you have Amazon GuardDuty enabled on your accounts for at least 48 hours before you enable Detective on those accounts. However, you can use Amazon Detective to investigate more than just your Amazon GuardDuty findings. Amazon Detective provides detailed summaries, analyses, and visualizations of the behaviors and interactions amongst your AWS accounts, EC2 instances, AWS users, roles, and IP addresses. This information can be very useful in understanding security issues or operational account activity.</p>\n\n<p>How Amazon Detective works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q47-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/detective/\">https://aws.amazon.com/detective/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon GuardDuty that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Integrate it into your workflow system and initiate AWS Lambda to automatically remediate the issue</strong> - As discussed above, Amazon Detective requires Amazon GuardDuty as a pre-requisite. Amazon Detective automatically groups related GuardDuty findings and the affected AWS resources like Amazon EC2 instances, AWS accounts, or Amazon S3 buckets to help you investigate a single security event rather than individual GuardDuty findings.</p>\n\n<p>Amazon Detective conforms to the AWS shared responsibility model, which includes regulations and guidelines for data protection. Once enabled, Amazon Detective will process data from AWS CloudTrail logs, Amazon VPC Flow Logs, Amazon EKS audit logs, and Amazon GuardDuty findings for any accounts where it has been turned on.</p>\n\n<p><strong>Use AWS Security Hub, a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services to help analyze the security data under one service for easy root cause analyses</strong> - With AWS Security Hub, you have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Amazon Detective simplifies the process of investigating security findings and identifying the root cause. Amazon Detective analyzes trillions of events from multiple data sources such as Amazon VPC Flow Logs, AWS CloudTrail logs, Amazon EKS audit logs, and Amazon GuardDuty findings and automatically creates a graph model that provides you with a unified, interactive view of your resources, users, and the interactions between them over time.</p>\n\n<p><strong>Use Amazon Inspector to automatically scan and manage the known vulnerabilities and integration with AWS Security Hub and Amazon EventBridge to automate workflows for root cause analysis</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. Amazon Inspector removes the operational overhead associated with deploying and configuring a vulnerability management solution by allowing you to deploy Amazon Inspector across all accounts with a single step.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/detective/faqs/\">https://aws.amazon.com/detective/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n", "answers": ["<p>Configure Amazon GuardDuty that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. Integrate it into your workflow system and initiate AWS Lambda to automatically remediate the issue</p>", "<p>Use AWS Security Hub, a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services to help analyze the security data under one service for easy root cause analyses</p>", "<p>Use Amazon Inspector to automatically scan and manage the known vulnerabilities and integration with AWS Security Hub and Amazon EventBridge to automate workflows for root cause analysis</p>", "<p>Use Amazon Detective in conjunction with Amazon GuardDuty to monitor malicious activity and unauthorized behavior on the AWS resources and quickly identify the root cause of potential security issues through linked datasets</p>"]}, "correct_response": ["d"], "section": "Infrastructure Security", "question_plain": "A fault management application at a company connects to several other systems to monitor the status of the systems hosting the suite of flagship applications for the company. As per the security policy of the company, Cloudtrail and VPC flow logs have been enabled for all AWS resources. A recent internal error from the support team led to several minutes of outage on the fault management application and a few hours of analysis to understand the root cause of the error.\n\nThe company is now looking for a solution that can analyze data from various logs as well as security findings to quickly triage the root-cause linked to the security issues. What is the best-fit solution for the company's requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70394912, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An AWS organization manages its security and compliance units through two different AWS accounts. Both the accounts need AWS Config configuration and compliance data from multiple AWS accounts and Regions to get a centralized view of the resource inventory. Currently, the teams use shared access to the management account to fetch the required data.</p>\n\n<p>To enforce enhanced security measures, the company is looking at eliminating the need to share management account credentials with the team. As a Security Engineer, how will you implement this requirement with the least time and effort?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</strong> - An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from multiple AWS accounts and Regions into a single account and Region to get a centralized view of your resource inventory and compliance.</p>\n\n<p>You can also use an aggregator to collect configuration and compliance data from an organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled. Previously, organization-wide data aggregation was available only from the organization management account, but AWS Config recently now supports organization-wide resource data aggregation in a delegated administrator account.</p>\n\n<p>A delegated administrator account is an account in an AWS organization that is granted additional administrative permissions for a specified AWS service. This means that in addition to the management account, you can also use a delegated admin account to aggregate data from all the member accounts in AWS Organizations without any additional authorization. With this capability, different teams in an organization (auditing, security, or compliance) can use separate accounts and aggregate organization-wide data in their respective administration accounts for centralized governance. This capability also eliminates the need for those teams to gain access to the management account to fetch the aggregated data.</p>\n\n<p>How AWS Config Aggregator works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</strong> - OpsCenter is a Systems Manager capability that provides a central location where operations engineers, IT professionals, and others can view, investigate, and resolve operational issues related to their environment. You can use a central account to view operational issues in another account (e.g. impaired instances, degraded storage volumes, or non-compliant resources), view pertinent diagnostic information for each issue, and use pre-defined automation runbooks to remediate the issues.</p>\n\n<p>OpsCenter is designed to reduce mean time to resolution (MTTR) for impacted AWS and hybrid cloud resources. For the AWS resource, OpsCenter aggregates information from AWS Config, AWS CloudTrail logs, and Amazon CloudWatch Events, so you don't have to navigate across multiple console pages during your investigation.</p>\n\n<p><strong>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</strong> - AWS Systems Manager Explorer is a customizable operations dashboard that reports information about your AWS resources. Explorer displays an aggregated view of operations data (OpsData) for your AWS accounts and across AWS Regions. Explorer displays information from supporting AWS services like AWS Config, AWS Trusted Advisor, AWS Compute Optimizer, and AWS Support (support cases).</p>\n\n<p>For Multiple-account/multiple-Region support, Explorer aggregates all account data into a management account. This is exactly what the use case wants to avoid, hence this is not the right option.</p>\n\n<p><strong>Use the Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</strong> - AWS Config provides you with a configuration snapshot, which is a point-in-time capture of all your resources and their configurations. Configuration snapshots are generated on demand by using the AWS CLI or API and delivered to the Amazon S3 bucket that you specify. But saving to S3 buckets and analyzing from buckets is not an optimal way of implementing that asked requirement when the aggregator feature of AWS Config is tailor-made for such requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\">https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/\">https://aws.amazon.com/about-aws/whats-new/2022/11/aws-systems-manager-opscenter-supports-managing-opsitems-across-accounts/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/Explorer.html</a></p>\n", "answers": ["<p>Use OpsCenter capability of AWS Systems Manager with AWS Config to detect a resource that is out of compliance and automate remediation</p>", "<p>Configure AWS Systems Manager Explorer with a customizable operations dashboard that displays information from AWS Config</p>", "<p>Use the aggregator feature of AWS Config to provide access to AWS Config data to both accounts without the need to share the management account details</p>", "<p>Use Configuration snapshots feature of AWS Config to create point-in-time capture of all your resources and their configurations. Save these snapshots in an Amazon S3 bucket and analyze the data using AWS Athena for trends and security aberrations</p>"]}, "correct_response": ["c"], "section": "Logging and Monitoring", "question_plain": "An AWS organization manages its security and compliance units through two different AWS accounts. Both the accounts need AWS Config configuration and compliance data from multiple AWS accounts and Regions to get a centralized view of the resource inventory. Currently, the teams use shared access to the management account to fetch the required data.\n\nTo enforce enhanced security measures, the company is looking at eliminating the need to share management account credentials with the team. As a Security Engineer, how will you implement this requirement with the least time and effort?", "related_lectures": []}, {"_class": "assessment", "id": 70394914, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer noticed that an application layer (layer 7) DDoS attack is underway on one of the critical systems.</p>\n\n<p>What should the immediate response of the engineer be to control the damage? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create your own AWS WAF rules in your web ACL to mitigate the attack</strong></p>\n\n<p><strong>You can contact the AWS Support Center to get help with mitigations if you're a Shield Advanced customer</strong> - AWS automatically mitigates network and transport layer (layer 3 and layer 4) Distributed Denial of Service (DDoS) attacks.</p>\n\n<p>For application layer (layer 7) DDoS attacks, AWS attempts to detect and notify AWS Shield Advanced customers through CloudWatch alarms. By default, it doesn't automatically apply mitigations, to avoid inadvertently blocking valid user traffic.</p>\n\n<p>For application layer (layer 7) resources, you have the following options available for responding to an attack.\n1. Provide your own mitigations \u2013 You can investigate and mitigate the attack on your own. To manually mitigate a potential application layer DDoS attack you can create your own AWS WAF rules in your web ACL to mitigate the attack. This is the only option available if you aren't a Shield Advanced customer.</p>\n\n<ol>\n<li>Contact support \u2013 If you're a Shield Advanced customer, you can contact the AWS Support Center to get help with mitigations. Critical and urgent cases are routed directly to DDoS experts.</li>\n</ol>\n\n<p>Manually mitigating an application layer DDoS attack:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q49-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual</a></p>\n\n<p>How AWS Shield Advanced works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q49-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access</strong></p>\n\n<p><strong>Monitor the CloudWatch metrics: The maximum size of the Auto Scaling group, Amazon EC2 instance's CPUUtilization, and NetworkIn parameters to detect a DDoS attack and send an SNS notification to the security team</strong></p>\n\n<p>These two options can be used to proactively tighten the security of AWS infrastructure. But, these are not meant to be used as immediate responses when a DDos attack is underway.</p>\n\n<p><strong>Define an AWS Systems Manager document (SSM document) to block all vulnerable ports, lock public access to Amazon S3 buckets and stop internet traffic to affected EC2 instances. Run the SSM using AWS Systems Manager CLI</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html#ddos-responding-manual</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/operational-techniques.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/operational-techniques.html</a></p>\n", "answers": ["<p>Create your own AWS WAF rules in your web ACL to mitigate the attack</p>", "<p>Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access</p>", "<p>Define an AWS Systems Manager document (SSM document) to block all vulnerable ports, lock public access to Amazon S3 buckets, and stop internet traffic to affected EC2 instances. Run the SSM using AWS Systems Manager CLI</p>", "<p>You can contact the AWS Support Center to get help with mitigations if you're a Shield Advanced customer</p>", "<p>Monitor the CloudWatch metrics: The maximum size of the Auto Scaling group, Amazon EC2 instance's CPUUtilization and NetworkIn parameters to detect a DDoS attack and send an SNS notification to the security team</p>"]}, "correct_response": ["a", "d"], "section": "Incident Response", "question_plain": "A Security Engineer noticed that an application layer (layer 7) DDoS attack is underway on one of the critical systems.\n\nWhat should the immediate response of the engineer be to control the damage? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394916, "assessment_type": "multi-select", "prompt": {"question": "<p>A Security Engineer is planning for a DDoS-resilient architecture for a three-tier web application. What are the best practices to consider for DDoS mitigation? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>When using Amazon CloudFront and AWS WAF with Amazon API Gateway, configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint</strong></p>\n\n<p>Configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint. By doing this, CloudFront will treat the content as dynamic and skip caching the content.</p>\n\n<p><strong>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources</strong></p>\n\n<p>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources. DDoS attacks against Elastic IP addresses that have been registered as Protected Resources are detected more quickly, which can result in a faster time to mitigate. When an attack is detected, the DDoS mitigation systems read the network ACL that corresponds to the targeted Elastic IP and enforce it at the AWS network border. This significantly reduces your risk of impact from infrastructure layer DDoS attacks.</p>\n\n<p><strong>The security groups assigned to Application Load Balancers should be configured to not use connection tracking</strong></p>\n\n<p>Security groups are stateful, depending on the configuration. To allow the response traffic, the security group uses connection tracking to track information about traffic on the resource where the security group is applied. There is a maximum number of connections that can be tracked per resource. After the maximum is reached, both new inbound and outbound connections cannot be established. This condition exhausts the number of connections tracked by the security group that can be met during a DDoS attack.</p>\n\n<p>To improve the DDoS resilience of resources in your VPC, AWS recommends ensuring that the security groups assigned to managed resources such as Classic and Application Load Balancers are configured to not use connection tracking (untracked connections).</p>\n\n<p>DDoS-resilient reference architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q50-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/mitigation-techniques.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/mitigation-techniques.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Network Load Balancer to route traffic to targets based on content and accept only well-formed web requests. Network Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks</strong> - This statement is incorrect. Network Load Balancers cannot route traffic based on content since they are not at layer 7, unlike Application Load Balancers.</p>\n\n<p><strong>Use AWS WAF to configure web access control lists (Web ACLs) on your Amazon S3 buckets with critical data to filter and block requests based on request signatures</strong> - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync. Amazon S3 bucket is not a supported target.</p>\n\n<p><strong>Configure Amazon API Gateway with edge-optimized API endpoints whenever possible and associate it with your Amazon CloudFront distribution</strong> - When you use Amazon API Gateway, you can choose from two types of API endpoints. The first is the default option: edge-optimized API endpoints that are accessed through an Amazon CloudFront distribution. The distribution is created and managed by API Gateway, however, so you don\u2019t have control over it. The second option is to use a regional API endpoint that is accessed from the same AWS region in which your REST API is deployed. AWS recommends that you use the second type of endpoint and associate it with your own Amazon CloudFront distribution. This gives you control over the Amazon CloudFront distribution and the ability to use AWS WAF for application layer protection. This mode provides you with access to scaled DDoS mitigation capacity across the AWS global edge network.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/best-practices-for-ddos-mitigation.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/best-practices-for-ddos-mitigation.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/use-aws-edge-locations-for-scale-bp1-bp3.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/use-aws-edge-locations-for-scale-bp1-bp3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/application-layer-defense-bp1-bp2.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/application-layer-defense-bp1-bp2.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/attack-surface-reduction.html</a></p>\n", "answers": ["<p>When using Amazon CloudFront and AWS WAF with Amazon API Gateway, configure the cache behavior for your distributions to forward all headers to the API Gateway regional endpoint</p>", "<p>If you are subscribed to AWS Shield Advanced, you can register Elastic IP addresses as Protected Resources</p>", "<p>Use Network Load Balancer to route traffic to targets based on content and accept only well-formed web requests. Network Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks</p>", "<p>The security groups assigned to Application Load Balancers should be configured to not use connection tracking</p>", "<p>Use AWS WAF to configure web access control lists (Web ACLs) on your Amazon S3 buckets with critical data to filter and block requests based on request signatures</p>", "<p>Configure Amazon API Gateway with edge-optimized API endpoints whenever possible and associate it with your Amazon CloudFront distribution</p>"]}, "correct_response": ["a", "b", "d"], "section": "Infrastructure Security", "question_plain": "A Security Engineer is planning for a DDoS-resilient architecture for a three-tier web application. What are the best practices to consider for DDoS mitigation? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394918, "assessment_type": "multi-select", "prompt": {"question": "<p>For auditing purposes, a company needs to showcase a report of changes made to the security group(s) for an Amazon Virtual Private Cloud (Amazon VPC).</p>\n\n<p>What are the different ways to review security group changes in an AWS account? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use AWS CloudTrail Event history to review security group changes in your AWS account</strong></p>\n\n<p>You can troubleshoot operational and security incidents in the CloudTrail console by viewing Event history. The Event history provides a read-only view of the last 90 days of recorded API activity (management events) in an AWS Region.</p>\n\n<p>You can look up events related to the creation, modification, or deletion of resources (such as IAM users or Amazon EC2 instances) in your AWS account on a per-region basis. Events can be viewed and downloaded by using the AWS CloudTrail console. You can customize the view of event history in the console by selecting which columns are displayed and which are hidden. You can programmatically look up events by using the AWS SDKs or AWS Command Line Interface. You can also compare the details of events in Event history side-by-side.</p>\n\n<p><strong>Create an AWS CloudTrail trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. Use Athena to query CloudTrail Logs over the last 30-45 days</strong></p>\n\n<p>To use Athena to query CloudTrail Logs, you must have a trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. You can use Athena to query CloudTrail Logs over the last 90 days.</p>\n\n<p>You can create a non-partitioned Athena table for querying CloudTrail logs directly from the CloudTrail console. Creating an Athena table from the CloudTrail console requires that you be logged in with a role that has sufficient permissions to create tables in Athena.</p>\n\n<p><strong>Use AWS Config to view configuration history for security groups. You must have the AWS Config configuration recorder turned on</strong></p>\n\n<p>AWS Config records details of changes to your AWS resources to provide you with a configuration history. You can use the AWS Management Console, API, or CLI to obtain details of what a resource\u2019s configuration looked like at any point in the past. AWS Config will also automatically deliver a configuration history file to the Amazon Simple Storage Service (S3) bucket you specify.</p>\n\n<p>AWS Config discovers, maps, and tracks AWS resource relationships in your account. For example, if a new EC2 security group is associated with an EC2 instance, AWS Config records the updated configurations of both the EC2 security group and the EC2 instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the AWS AppConfig capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations and track changes in them</strong> - AWS AppConfig is a capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations. A configuration is a collection of settings that influence the behavior of your application. You can use AWS AppConfig with applications hosted on Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS Lambda, containers, mobile applications, or IoT devices. You cannot monitor and track changes to security groups using AppConfig.</p>\n\n<p><strong>Use CloudTrail Lake to create trails that aggregate information from multiple AWS accounts across regions</strong> - If you want to perform SQL queries on CloudTrail event information across accounts, regions, and dates, consider using CloudTrail Lake. CloudTrail Lake is an AWS alternative to creating trails that aggregate information from an enterprise into a single, searchable event data store. Instead of using Amazon S3 bucket storage, it stores events in a data lake, which allows richer, faster queries. This option has been added as a distractor.</p>\n\n<p><strong>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when security group changes occur. CloudTrail supports sending only data events to CloudWatch Logs. Configure Amazon EventBridge to monitor management events</strong> - This statement is incorrect. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-understanding\">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-understanding</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html\">https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html</a></p>\n", "answers": ["<p>Use AWS CloudTrail Event history to review security group changes in your AWS account</p>", "<p>Use AWS AppConfig capability of AWS Systems Manager, to create, manage, and quickly deploy application configurations and track changes in them</p>", "<p>Use CloudTrail Lake to create trails that aggregate information from multiple AWS accounts across regions</p>", "<p>Create an AWS CloudTrail trail configured to log to an Amazon Simple Storage Service (Amazon S3) bucket. Use Athena to query CloudTrail Logs over the last 30-45 days</p>", "<p>Use AWS Config to view configuration history for security groups. You must have the AWS Config configuration recorder turned on</p>", "<p>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when security group changes occur. CloudTrail supports sending only data events to CloudWatch Logs. Configure Amazon EventBridge to monitor management events</p>"]}, "correct_response": ["a", "d", "e"], "section": "Logging and Monitoring", "question_plain": "For auditing purposes, a company needs to showcase a report of changes made to the security group(s) for an Amazon Virtual Private Cloud (Amazon VPC).\n\nWhat are the different ways to review security group changes in an AWS account? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394920, "assessment_type": "multi-select", "prompt": {"question": "<p>A data analytics company processes the sensitive data of several financial institutions across the country. The company needs an automated and efficient way to identify sensitive information and operationalize security for its customers while keeping costs low. The solution should also have a security dashboard that aggregates alerts and facilitates automated remediation of security issues while having a complete view of the security architecture of the systems. A high-performing interactive query service is also needed for business purposes.</p>\n\n<p>As a Security Engineer, which options will you combine to implement a cost-optimal and high-performance solution for the given requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Store the data cost-effectively on Amazon S3 buckets and use Amazon Macie to automatically discover, classify and protect the highly sensitive data</strong></p>\n\n<p>Macie uses machine learning and pattern matching to discover sensitive data at scale in a cost-efficient way. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of your data stored in Amazon Simple Storage Service (Amazon S3). Macie gives visibility the company wouldn\u2019t otherwise have because it wouldn\u2019t have the means or the resources to do this type of discovery without a tool like Macie.</p>\n\n<p><strong>Configure AWS Security Hub to have a central dashboard for higher visibility of the environment and remediate issues quickly</strong></p>\n\n<p>Alongside Macie, use AWS Security Hub, a cloud security posture management service that performs security best-practice checks, aggregates alerts, and facilitates automated remediation. The security scores provided by Security Hub help to build frameworks for how to secure the environment in the cloud. Having a central dashboard where you can improve visibility into the configurations in the environment is very helpful because you can just go in there and remediate the issues.</p>\n\n<p><strong>Use Amazon Athena to analyze data in Amazon Simple Storage Service (Amazon S3) to retrieve any amount of data from anywhere\u2014using standard SQL</strong></p>\n\n<p>Amazon Athena, an interactive query service makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3)\u2014object storage built to retrieve any amount of data from anywhere\u2014using standard SQL. Using Macie alongside Athena provides the team with comprehensive visibility in its data security posture, including the presence of sensitive information.</p>\n\n<p>Sample reference architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q52-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/solutions/case-studies/kasasa-case-study/\">https://aws.amazon.com/solutions/case-studies/kasasa-case-study/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Detective to analyze, investigate, and quickly identify the root cause of potential security issues along with Amazon GuardDuty. Use Amazon GuardDuty to continuously monitor for malicious activity and unauthorized behavior to protect your AWS accounts, and Amazon Elastic Compute Cloud (EC2) workloads</strong> - While Amazon Detective and GaurdDuty are useful services for the use-case, AWS Security Hub gives a comprehensive view of high priority security alerts and compliance status across your AWS accounts. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions.</p>\n\n<p><strong>Use Amazon QuickSight to quickly embed interactive dashboards and visualizations into your applications without needing to build your own analytics capabilities</strong> - Amazon QuickSight has a serverless architecture that automatically scales to hundreds of thousands of users without the need to set up, configure, or manage your own servers. It also ensures that your users don\u2019t have to deal with slow dashboards during peak hours when multiple business intelligence (BI) users are accessing the same dashboards or datasets. Amazon QuickSight offers amazing visualizations, unlike traditional Business Intelligence (BI) solutions. However, this option is not the best fit for the given use case.</p>\n\n<p><strong>Use Amazon S3 buckets to store data and include S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns</strong> - Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. While S3 Intelligent Tiering is extremely useful for several use cases, it is not an alternative to Amazon Macie, which can automatically discover, classify and protect highly sensitive data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-amazon-detective/\">https://aws.amazon.com/about-aws/whats-new/2019/12/aws-security-hub-integrates-with-amazon-detective/</a></p>\n", "answers": ["<p>Configure Amazon Detective to analyze, investigate, and quickly identify the root cause of potential security issues along with Amazon GuardDuty. Use Amazon GuardDuty to continuously monitor for malicious activity and unauthorized behavior to protect your AWS accounts, and Amazon Elastic Compute Cloud (EC2) workloads</p>", "<p>Store the data cost-effectively on Amazon S3 buckets and use Amazon Macie to automatically discover, classify and protect the highly sensitive data</p>", "<p>Configure AWS Security Hub to have a central dashboard for higher visibility of the environment and remediate issues quickly</p>", "<p>Use Amazon Athena to analyze data in Amazon Simple Storage Service (Amazon S3) to retrieve any amount of data from anywhere\u2014using standard SQL</p>", "<p>Use Amazon QuickSight to quickly embed interactive dashboards and visualizations into your applications without needing to build your own analytics capabilities</p>", "<p>Use Amazon S3 buckets to store data and include S3 Intelligent-Tiering for automatic cost savings for data with unknown or changing access patterns</p>"]}, "correct_response": ["b", "c", "d"], "section": "Data Protection", "question_plain": "A data analytics company processes the sensitive data of several financial institutions across the country. The company needs an automated and efficient way to identify sensitive information and operationalize security for its customers while keeping costs low. The solution should also have a security dashboard that aggregates alerts and facilitates automated remediation of security issues while having a complete view of the security architecture of the systems. A high-performing interactive query service is also needed for business purposes.\n\nAs a Security Engineer, which options will you combine to implement a cost-optimal and high-performance solution for the given requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394922, "assessment_type": "multi-select", "prompt": {"question": "<p>A media company stores all of its business data on Amazon S3 buckets. Since a massive growth in the number of customers has resulted in complicated bucket policies, the company has now hired you as an AWS Certified Security Specialist for simplifying the company's S3 buckets configuration to facilitate access for the company's customers as well as other connected applications.</p>\n\n<p>What are the important configuration characteristics to consider while defining access points for the S3 buckets? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>You can only use access points to perform operations on objects. You can't use access points to perform Amazon S3 operations, such as modifying or deleting buckets</strong> - Access points support access only over HTTPS.</p>\n\n<p><strong>The cross-account access points don\u2019t grant access to data until you are granted permissions from the bucket owner</strong> - The cross-account access points don\u2019t grant access to data until you are granted permissions from the bucket owner. The bucket owner always retains ultimate control of the data and must update the bucket policy to authorize requests from the cross-account access point.</p>\n\n<p><strong>You can't configure Cross-Region Replication to operate through an access point</strong> - You can't use an access point as a destination for S3 Replication.</p>\n\n<p>Access points restrictions and limitations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-restrictions-limitations.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-restrictions-limitations.html</a></p>\n\n<p>Access point alias limitations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q53-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Access points support access over HTTP and HTTPS alone. It does not offer support over TCP and UDP protocols</strong> - This statement is incorrect. Access points support access only over HTTPS.</p>\n\n<p><strong>Aliases for S3 Access Points are interchangeable with S3 bucket names. Aliases can be used as a logging destination for AWS CloudTrail logs and S3 server access logs. However, an alias cannot be used in AWS Identity and Access Management (IAM) policies</strong> - Aliases for S3 Access Points are automatically generated and are interchangeable with S3 bucket names anywhere you use a bucket name for data access. Every time you create an access point for a bucket, S3 automatically generates a new Access Point Alias.</p>\n\n<p>Aliases cannot be used as a logging destination for S3 server access logs. Aliases cannot be used as a logging destination for AWS CloudTrail logs.</p>\n\n<p><strong>After you create an access point, you can't change its virtual private cloud (VPC) configuration from the console anymore. An AWS CLI has to be used for modifying the configuration</strong> - After you create an access point, you can't change its virtual private cloud (VPC) configuration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/features/access-points/\">https://aws.amazon.com/s3/features/access-points/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-alias.html</a></p>\n", "answers": ["<p>Access points support access over HTTP and HTTPS alone. It does not offer support over TCP and UDP protocols</p>", "<p>You can only use access points to perform operations on objects. You can't use access points to perform Amazon S3 operations, such as modifying or deleting buckets</p>", "<p>The cross-account access points don\u2019t grant access to data until you are granted permissions from the bucket owner</p>", "<p>You can't configure Cross-Region Replication to operate through an access point</p>", "<p>Aliases for S3 Access Points are interchangeable with S3 bucket names. Aliases can be used as a logging destination for AWS CloudTrail logs and S3 server access logs. However, an alias cannot be used in AWS Identity and Access Management (IAM) policies</p>", "<p>After you create an access point, you can't change its virtual private cloud (VPC) configuration from the console anymore. An AWS CLI has to be used for modifying the configuration</p>"]}, "correct_response": ["b", "c", "d"], "section": "Data Protection", "question_plain": "A media company stores all of its business data on Amazon S3 buckets. Since a massive growth in the number of customers has resulted in complicated bucket policies, the company has now hired you as an AWS Certified Security Specialist for simplifying the company's S3 buckets configuration to facilitate access for the company's customers as well as other connected applications.\n\nWhat are the important configuration characteristics to consider while defining access points for the S3 buckets? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394924, "assessment_type": "multi-select", "prompt": {"question": "<p>The development team at an e-commerce company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Security Specialist to advise on CloudFront capabilities on routing and security.</p>\n\n<p>Which of the following would you identify as correct regarding CloudFront? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>CloudFront can route to multiple origins based on the content type</strong></p>\n\n<p>You can configure a single CloudFront web distribution to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p>\n\n<p><strong>Use an origin group with primary and secondary origins to configure CloudFront for high availability and failover</strong></p>\n\n<p>You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.</p>\n\n<p>To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Finally, you create or update a cache behavior to use the origin group.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/origingroups-overview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><strong>Use field-level encryption in CloudFront to protect sensitive data for specific content</strong></p>\n\n<p>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data\u2014and have the credentials to decrypt it\u2014can do so.</p>\n\n<p>To use field-level encryption, when you configure your CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. (You can\u2019t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt.)</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/fleoverview.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use KMS encryption in CloudFront to protect sensitive data for specific content</strong> - This option has been added as a distractor. You can use field level encryption in CloudFront to protect sensitive data for specific content.</p>\n\n<p><strong>Use geo-restriction to configure CloudFront for high-availability and failover</strong> - You can use geo-restriction, also known as geo-blocking, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. Geo restriction is not used to configure CloudFront for high availability and failover.</p>\n\n<p><strong>CloudFront can route to multiple origins based on the price class</strong> - CloudFront edge locations are grouped into geographic regions, and AWS has grouped regions into price classes. The default price class includes all regions. Another price class includes most regions (the United States; Canada; Europe; Hong Kong, Philippines, South Korea, Taiwan, and Singapore; Japan; India; South Africa; and Middle East regions) but excludes the most expensive regions. A third price class includes only the least expensive regions (the United States, Canada, and Europe regions). CloudFront can only route to multiple origins based on content type and not based on the price class.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PriceClass.html</a></p>\n", "answers": ["<p>Use KMS encryption in CloudFront to protect sensitive data for specific content</p>", "<p>Use geo-restriction to configure CloudFront for high-availability and failover</p>", "<p>CloudFront can route to multiple origins based on the price class</p>", "<p>CloudFront can route to multiple origins based on the content type</p>", "<p>Use an origin group with primary and secondary origins to configure CloudFront for high-availability and failover</p>", "<p>Use field-level encryption in CloudFront to protect sensitive data for specific content</p>"]}, "correct_response": ["d", "e", "f"], "section": "Infrastructure Security", "question_plain": "The development team at an e-commerce company has recently migrated to AWS Cloud from its on-premises data center. The team is evaluating CloudFront to be used as a CDN for its flagship application. The team has hired you as an AWS Certified Security Specialist to advise on CloudFront capabilities on routing and security.\n\nWhich of the following would you identify as correct regarding CloudFront? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 70394926, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Security Engineer has created a web ACL using an AWS Firewall Manager AWS WAF policy. Still, the web ACL isn't correctly associated with its in-scope resources.</p>\n\n<p>What could be the underlying reason for this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>If <code>auto remediate any non-compliant resources</code> isn't turned on, then the Firewall Manager created web ACL won't be associated with in-scope resources</strong></p>\n\n<p>The web ACL association behavior for the Firewall Manager AWS WAF policy depends on the following:</p>\n\n<ol>\n<li><p>How auto-remediation is configured</p></li>\n<li><p>If your in-scope resource already has a web ACL associated</p></li>\n</ol>\n\n<p>If <code>auto remediate any non-compliant resources</code> isn't turned on, then the Firewall Manager created web ACL won't be associated with in-scope resources.</p>\n\n<p>If only <code>auto remediate any non-compliant resources</code> is turned on, then the following happens:</p>\n\n<ol>\n<li><p>For non-compliant AWS accounts that are within the policy scope, Firewall Manager creates a web ACL whose name starts with FMManagedWebACLV2 . This web ACL contains the rule groups that are defined in the policy.</p></li>\n<li><p>Firewall Manager associates the web ACL with all non-compliant resources in the accounts. However, if an in-scope resource already has a web ACL associated with it, then it won't replace the existing web ACL with the Firewall Manager policy web ACL.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> are turned on for a Firewall Manager AWS WAF policy, and if an in-scope resource has a Web ACL created by Firewall Manager AWS WAF policy, then it gets replaced by Firewall Manager AWS WAF policy web ACL</strong></p>\n\n<p><strong>If only <code>auto remediate any non-compliant resources</code> is turned on, Firewall Manager associates the web ACL with all non-compliant resources in the accounts irrespective of whether it has a web ACL associated with it</strong></p>\n\n<p><strong>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> is turned on for a Firewall Manager AWS WAF policy and if an in-scope resource has a Web ACL created by AWS Shield Advanced policy, then it cannot be replaced by Firewall Manager AWS WAF policy web ACL</strong></p>\n\n<p>All three options are incorrect. The web ACL association behavior for the Firewall Manager AWS WAF policy for the above use cases is described below.</p>\n\n<p>The web ACL association behavior:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q55-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/\">https://aws.amazon.com/premiumsupport/knowledge-center/waf-web-acl-association-behavior/</a></p>\n", "answers": ["<p>If <code>auto remediate any non-compliant resources</code> isn't turned on, then the Firewall Manager created web ACL won't be associated with in-scope resources</p>", "<p>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> are turned on for a Firewall Manager AWS WAF policy, then if an in-scope resource has a Web ACL created by Firewall Manager AWS WAF policy, then it gets replaced by Firewall Manager AWS WAF policy web ACL</p>", "<p>If only <code>auto remediate any non-compliant resources</code> is turned on, Firewall Manager associates the web ACL with all non-compliant resources in the accounts irrespective of whether it has a web ACL associated with it</p>", "<p>If <code>auto remediate any non-compliant resources and Replace web ACLs that are currently associated with in-scope resources with the web ACLs created by this policy</code> is turned on for a Firewall Manager AWS WAF policy and if an in-scope resource has a Web ACL created by AWS Shield Advanced policy, then it cannot be replaced by Firewall Manager AWS WAF policy web ACL</p>"]}, "correct_response": ["a"], "section": "Data Protection", "question_plain": "A Security Engineer has created a web ACL using an AWS Firewall Manager AWS WAF policy. Still, the web ACL isn't correctly associated with its in-scope resources.\n\nWhat could be the underlying reason for this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394928, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company is evaluating storage options on Amazon S3 standard storage to meet regulatory guidelines. The data should be stored in such a way on S3 that it cannot be deleted until the regulatory period has expired.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you recommend for the given requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Object Lock</strong></p>\n\n<p>Amazon S3 Object Lock is an Amazon S3 feature that allows you to store objects using a write once, read many (WORM) model. You can use WORM protection for scenarios where it is imperative that data is not changed or deleted after it has been written. Whether your business has a requirement to satisfy compliance regulations in the financial or healthcare sector, or you simply want to capture a golden copy of business records for later auditing and reconciliation, S3 Object Lock is the right tool for you. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier Vault Lock</strong></p>\n\n<p>A vault is a container for storing archives on Glacier. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. Since Vault Lock is only for Glacier and not for S3 standards storage, it cannot be used for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q56-i3.jpg\"></p>\n\n<p>\"Use S3 cross-Region Replication\" - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. The object may be replicated to a single destination bucket or multiple destination buckets. Both source and destination buckets must have versioning enabled. By default, when Amazon S3 Replication is enabled and an object is deleted in the source bucket, Amazon S3 adds a delete marker in the source bucket only. This action protects data from malicious deletions. If you have delete marker replication enabled, these markers are copied to the destination buckets, and Amazon S3 behaves as if the object was deleted in both source and destination buckets. However, someone with administrative access to S3 can disable cross-Region replication and then delete all versions from both source as well as the destination, so this option will not be able to safeguard your data compared to S3 Object Lock.</p>\n\n<p><strong>Activate MFA delete on the S3 bucket</strong> - When working with S3 Versioning in Amazon S3 buckets, you can optionally add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) delete. When you do this, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket. Only the root account can enable MFA delete. MFA delete cannot be used for the given use case because it just represents an additional security layer and can be disabled by anyone having access to the root account credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/\">https://aws.amazon.com/blogs/storage/protecting-data-with-amazon-s3-object-lock/</a></p>\n", "answers": ["<p>Use S3 cross-Region Replication</p>", "<p>Use S3 Glacier Vault Lock</p>", "<p>Activate MFA delete on the S3 bucket</p>", "<p>Use S3 Object Lock</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "A financial services company is evaluating storage options on Amazon S3 standard storage to meet regulatory guidelines. The data should be stored in such a way on S3 that it cannot be deleted until the regulatory period has expired.\n\nAs an AWS Certified Security Specialist, which of the following would you recommend for the given requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70394930, "assessment_type": "multi-select", "prompt": {"question": "<p>The development team at a company is moving the static content from the company's e-commerce website hosted on EC2 instances to an S3 bucket. The team wants to use a CloudFront distribution to deliver the static content. The security group used by the EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post the migration to CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.</p>\n\n<p>Which options would you combine to build a solution to meet these requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</strong></p>\n\n<p>When you use CloudFront with an Amazon S3 bucket as the origin, you can configure CloudFront and Amazon S3 in a way that provides the following benefits:</p>\n\n<p>Restricts access to the Amazon S3 bucket so that it's not publicly accessible</p>\n\n<p>Makes sure that viewers (users) can access the content in the bucket only through the specified CloudFront distribution\u2014that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution</p>\n\n<p>To do this, configure CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from CloudFront. CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI).</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note that AWS recommends using OAC because it supports:</p>\n\n<p>All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022</p>\n\n<p>Amazon S3 server-side encryption with AWS KMS (SSE-KMS)</p>\n\n<p>Dynamic requests (POST, PUT, etc.) to Amazon S3</p>\n\n<p>OAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios. However, you will continue to see answers enlisting OAI as the preferred option in the actual exam as it takes about 6 months/1 year for a new feature to appear in the exam.</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to your protected web application resources. You can protect the following resource types:</p>\n\n<p>Amazon CloudFront distribution</p>\n\n<p>Amazon API Gateway REST API</p>\n\n<p>Application Load Balancer</p>\n\n<p>AWS AppSync GraphQL API</p>\n\n<p>Amazon Cognito user pool</p>\n\n<p>AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, your protected resource responds to requests either with the requested content, with an HTTP 403 status code (Forbidden), or with a custom response.</p>\n\n<p>If you want to allow or block web requests based on the IP addresses that the requests originate from, create one or more IP match conditions via your AWS WAF. An IP match condition lists up to 10,000 IP addresses or IP address ranges that your requests originate from.</p>\n\n<p>For the given use case, you should add those IP addresses that are allowed in the EC2 security group into the IP match condition.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</strong> - You cannot associate a WAF ACL with an S3 bucket policy.</p>\n\n<p><strong>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</strong> - NACL is associated with a subnet within a VPC. CloudFront delivers your content through a worldwide network of data centers called edge locations. So an NACL cannot be associated with a CloudFront distribution.</p>\n\n<p><strong>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</strong> - A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance. CloudFront delivers your content through a worldwide network of data centers called edge locations. So a security group cannot be associated with a CloudFront distribution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p>\n", "answers": ["<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the S3 bucket policy</p>", "<p>Create a new NACL that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new NACL with the CloudFront distribution</p>", "<p>Create a new security group that allows traffic from the same IPs as specified in the current EC2 security group. Associate this new security group with the CloudFront distribution</p>", "<p>Configure an origin access identity (OAI) and associate it with the CloudFront distribution. Set up the permissions in the S3 bucket policy so that only the OAI can read the objects</p>", "<p>Create an AWS WAF ACL and use an IP match condition to allow traffic only from those IPs that are allowed in the EC2 security group. Associate this new WAF ACL with the CloudFront distribution</p>"]}, "correct_response": ["d", "e"], "section": "Identity and Access Management", "question_plain": "The development team at a company is moving the static content from the company's e-commerce website hosted on EC2 instances to an S3 bucket. The team wants to use a CloudFront distribution to deliver the static content. The security group used by the EC2 instances allows the website to be accessed by a limited set of IP ranges from the company's suppliers. Post the migration to CloudFront, access to the static content should only be allowed from the aforementioned IP addresses.\n\nWhich options would you combine to build a solution to meet these requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394932, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following would you identify as an INVALID option for setting up such a configuration?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>You can use an Internet Gateway ID as the custom source for the inbound rule</strong></p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>Please see this list of allowed source or destination for security group rules:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can use a security group as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</strong></p>\n\n<p><strong>You can use an IP address as the custom source for the inbound rule</strong></p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</p>\n", "answers": ["<p>You can use a security group as the custom source for the inbound rule</p>", "<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>", "<p>You can use an IP address as the custom source for the inbound rule</p>", "<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "The security team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group.\n\nAs an AWS Certified Security Specialist, which of the following would you identify as an INVALID option for setting up such a configuration?", "related_lectures": []}, {"_class": "assessment", "id": 70394934, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company is using Amazon Macie, AWS Shield Advanced, Amazon Inspector and AWS Firewall Manager in its AWS account. The company wants to receive alerts in case a DDoS attack occurs against the account.</p>\n\n<p>As an AWS Certified Security Specialist, what would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Shield Advanced metrics for an active DDoS event</strong></p>\n\n<p>AWS Shield Advanced is a managed service that helps you protect your application against external threats, like DDoS attacks, volumetric bots, and vulnerability exploitation attempts. For higher levels of protection against attacks, you can subscribe to AWS Shield Advanced. When you subscribe to Shield Advanced and add protection to your resources, Shield Advanced provides expanded DDoS attack protection for those resources. The protections that you receive from Shield Advanced can vary depending on your architecture and configuration choices. Use the information in this guide to build and protect resilient applications using Shield Advanced, and to escalate when you need expert help.</p>\n\n<p>AWS Shield Advanced publishes Amazon CloudWatch event metrics for all resources that it protects. These metrics improve your ability to monitor your resources by making it possible to create and configure CloudWatch dashboards and alarms for them.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Amazon Macie findings for an active DDoS event</strong> - Amazon Macie is a data security service that uses machine learning (ML) and pattern matching to discover and help protect your sensitive data in Amazon S3. You cannot use it to monitor active DDoS events.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors Amazon Inspector findings for an active DDoS event</strong> - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. You cannot use it to monitor active DDoS events.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that monitors AWS Firewall Manager metrics for an active DDoS event</strong> - AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization. You cannot use it to monitor active DDoS events.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-cloudwatch-metrics.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-advanced-summary.html</a></p>\n", "answers": ["<p>Set up an Amazon CloudWatch alarm that monitors Amazon Macie findings for an active DDoS event</p>", "<p>Set up an Amazon CloudWatch alarm that monitors Amazon Inspector findings for an active DDoS event</p>", "<p>Set up an Amazon CloudWatch alarm that monitors Shield Advanced metrics for an active DDoS event</p>", "<p>Set up an Amazon CloudWatch alarm that monitors AWS Firewall Manager metrics for an active DDoS event</p>"]}, "correct_response": ["c"], "section": "Infrastructure Security", "question_plain": "An e-commerce company is using Amazon Macie, AWS Shield Advanced, Amazon Inspector and AWS Firewall Manager in its AWS account. The company wants to receive alerts in case a DDoS attack occurs against the account.\n\nAs an AWS Certified Security Specialist, what would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 70394936, "assessment_type": "multi-select", "prompt": {"question": "<p>A social media company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF.</p>\n\n<p>As an AWS Certified Security Specialist, which of the following solutions will you combine to address the given use case? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use WAF geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use WAF IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p>\n\n<p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p>\n\n<p>AWS WAF - How it Works\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\">\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p>To block specific countries, you can create a WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a deny rule for the blocked countries in the NACL associated to each of the EC2 instances</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACL cannot block traffic based on geographic match conditions.</p>\n\n<p><strong>Use ALB geo match statement listing the countries that you want to block</strong></p>\n\n<p><strong>Use ALB IP set statement that specifies the IP addresses that you want to allow through</strong></p>\n\n<p>An Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets \u2013 EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing for modern application architectures, including microservices and container-based applications.</p>\n\n<p>An ALB cannot block or allow traffic based on geographic match conditions or IP-based conditions. Both these options have been added as distractors.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p>\n", "answers": ["<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances</p>", "<p>Use ALB geo match statement listing the countries that you want to block</p>", "<p>Use ALB IP set statement that specifies the IP addresses that you want to allow through</p>", "<p>Use WAF geo match statement listing the countries that you want to block</p>", "<p>Use WAF IP set statement that specifies the IP addresses that you want to allow through</p>"]}, "correct_response": ["d", "e"], "section": "Data Protection", "question_plain": "A social media company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF.\n\nAs an AWS Certified Security Specialist, which of the following solutions will you combine to address the given use case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394938, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company has its flagship application hosted on Amazon EC2 instances that are configured in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). The application should only be accessible to users from a specific country. The company also needs the ability to monitor any prohibited requests for further analysis by the security team.</p>\n\n<p>What will you suggest as the most optimal and low-maintenance solution for the given use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong></p>\n\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, Amazon CloudFront, Amazon API Gateway, Application Load Balancer, or AWS AppSync responds to requests either with the requested content or with an HTTP 403 status code (Forbidden).</p>\n\n<p>A web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, and AWS AppSync resources. You can use criteria like the following to allow or block requests:\n1. IP address origin of the request\n2. Country of origin of the request\n3. String match or regular expression (regex) match in a part of the request\n4. Size of a particular part of the request\n5. Detection of malicious SQL code or scripting</p>\n\n<p>More on web request inspection and handling criteria:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p>\n\n<p>Geographic match rule statement - To allow or block web requests based on country of origin, create one or more geographical, or geo, match statements. You can use this to block access to your site from specific countries or to only allow access from specific countries. If you want to allow some web requests and block others based on country of origin, add a geo match statement for the countries that you want to allow and add a second one for the countries that you want to block.</p>\n\n<p>More on Geographic match rule:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q61-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</strong> - The IP set match statement inspects the IP address of a web request against a set of IP addresses and address ranges. You can use this to allow or block web requests based on the IP addresses that the requests originate from. However, this is not an optimal solution. While defining an IP set, you need to enter one IP address or IP address range per line. Every time an IP address changes, it has to be manually added to this IP set. Hence, this option is not correct for the given use case.</p>\n\n<p><strong>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield Standard is automatically enabled when you use AWS services like Elastic Load Balancing (ELB), Application Load Balancer, Amazon CloudFront, and Amazon Route 53. You cannot use AWS Shield to block traffic from a specified country.</p>\n\n<p><strong>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</strong> - AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, and Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. WAF cannot be configured with AWS Global Accelerator.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p>\n", "answers": ["<p>Set up an AWS Web Application Firewall (WAF) web ACL. Create a rule to deny any requests that do not originate from the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>", "<p>Set up an AWS WAF web ACL. Create a rule to block the requests that do not originate from the IP range defined in an IP set containing a list of IP ranges that belong to the specified country. Attach the rule with the web ACL. Attach the web ACL with the ALB</p>", "<p>Set up AWS Shield to block any request that does not originate from the specified country. Attach AWS Shield with the ALB</p>", "<p>Create a Global Accelerator and attach the WAF to it. Create a rule to block any requests that do not originate from the specified country. Create the Global Accelerator to front the existing ALB</p>"]}, "correct_response": ["a"], "section": "Infrastructure Security", "question_plain": "A retail company has its flagship application hosted on Amazon EC2 instances that are configured in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). The application should only be accessible to users from a specific country. The company also needs the ability to monitor any prohibited requests for further analysis by the security team.\n\nWhat will you suggest as the most optimal and low-maintenance solution for the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 70394940, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security specialist with administrator permissions is using the AWS management console to access the CloudWatch logs for a Lambda function named \"myFunc\". However, upon choosing the option to view the logs in the AWS Lambda console, the specialist encountered an error message reading \"error loading Log Streams\". The specialist was unable to retrieve the logs as desired and must now find a solution to this issue.</p>\n\n<p>Following is an example IAM policy for the Lambda function's execution role:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:log-group:/aws/lambda/myFunc:*\"\n            ]\n        }\n    ]\n}\n</code></pre>\n\n<p>Which of the following solutions would you suggest to the specialist for addressing the issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the logs:CreateLogStream action to the second Allow statement</strong></p>\n\n<p>A log stream is a sequence of log events that share the same source. Each separate source of logs in CloudWatch Logs makes up a separate log stream.</p>\n\n<p>A log group is a group of log streams that share the same retention, monitoring, and access control settings. You can define log groups and specify which streams to put into each group. There is no limit on the number of log streams that can belong to one log group.</p>\n\n<p>CreateLogStream creates a log stream for the specified log group.</p>\n\n<p>For the given use case, you must ensure that the write actions CreateLogGroup and CreateLogStream are allowed.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>Since the security specialist already has administrator privileges as an IAM user, so there is no lack of permissions that's causing the error while the specialist is trying to \"view\" the logs.</p>\n\n<p>The root cause of the issue is that the Lambda function itself needs the CreateLogStream permission to be able to create the log stream and thereby successfully write the logs into CloudWatch Logs.</p>\n\n<p><strong>Add the logs:GetLogEvents action to the second Allow statement</strong></p>\n\n<p><strong>Add the logs:DescribeLogStreams action to the second Allow statement</strong></p>\n\n<p>The GetLogEvents and DescribeLogStreams are both \"read\" type of permissions which are not needed for the Lambda to successfully write the logs. Hence, both these options are incorrect.</p>\n\n<p><strong>Move the logs:CreateLogGroup action to the second Allow statement</strong> - This option is a distractor. The CreateLogGroup action needs to be in the first Allow statement only.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Working-with-log-groups-and-streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/permissions-reference-cwl.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n", "answers": ["<p>Add the logs:GetLogEvents action to the second Allow statement</p>", "<p>Add the logs:CreateLogStream action to the second Allow statement</p>", "<p>Add the logs:DescribeLogStreams action to the second Allow statement</p>", "<p>Move the logs:CreateLogGroup action to the second Allow statement</p>"]}, "correct_response": ["b"], "section": "Logging and Monitoring", "question_plain": "A security specialist with administrator permissions is using the AWS management console to access the CloudWatch logs for a Lambda function named \"myFunc\". However, upon choosing the option to view the logs in the AWS Lambda console, the specialist encountered an error message reading \"error loading Log Streams\". The specialist was unable to retrieve the logs as desired and must now find a solution to this issue.\n\nFollowing is an example IAM policy for the Lambda function's execution role:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"logs:CreateLogGroup\",\n            \"Resource\": \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:&lt;region&gt;:&lt;accountId&gt;:log-group:/aws/lambda/myFunc:*\"\n            ]\n        }\n    ]\n}\n\n\nWhich of the following solutions would you suggest to the specialist for addressing the issue?", "related_lectures": []}, {"_class": "assessment", "id": 70394942, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at an e-commerce company has noticed that several Amazon Elastic Block Store (Amazon EBS) volumes are not encrypted. These unencrypted EBS volumes are attached to Amazon EC2 instances that are provisioned with an Auto Scaling group and a launch template. You have been hired as an AWS Certified Security Specialist to implement a solution that ensures all EBS volumes are encrypted both now and in the future.</p>\n\n<p>What would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</strong></p>\n\n<p>You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. For example, Amazon EBS encrypts the EBS volumes created when you launch an instance and the snapshots that you copy from an unencrypted snapshot. You should note that encryption by default does not affect existing EBS volumes or snapshots.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q63-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-cloudwatch-log-streams-error/</a></p>\n\n<p>For the given use case, you can use the instance refresh feature of an Auto Scaling group to update the instances in your Auto Scaling group instead of manually replacing instances a few at a time. This can be useful when a configuration change requires you to replace instances, and you have a large number of instances in your Auto Scaling group. Amazon EC2 Auto Scaling starts performing a rolling replacement of the instances. It takes a set of instances out of service, terminates them, and launches a set of instances with the new desired configuration. Then, it waits until the instances pass your health checks and complete warmup before it moves on to replacing other instances.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the existing instances that have unencrypted EBS volumes</strong> - Since the given use case requires all EBS volumes to be encrypted both now and in the future, so you cannot let the Auto Scaling group to replace all the existing instances (with unencrypted EBS volumes) in due course of time. Hence this option is incorrect.</p>\n\n<p><strong>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</strong> - You cannot modify a launch template, rather you need to create a new version of the launch template and then leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances. This option has been added as a distractor.</p>\n\n<p><strong>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</strong> - This option acts as a distractor, as there is no such setting in the Auto Scaling group that propagates the automatic encryption of new EBS volumes by default.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/\">https://aws.amazon.com/premiumsupport/knowledge-center/ebs-automatic-encryption/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-instance-refresh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/manage-launch-template-versions.html</a></p>\n", "answers": ["<p>Configure a new launch template from the existing launch template, such that the encrypted flag for all EBS volumes is set to true in the new launch template. Update the Auto Scaling group to use the new launch template. In due course of time, let the Auto Scaling group replace all the old instances that have unencrypted EBS volumes</p>", "<p>Modify the launch template by setting the encrypted flag for all EBS volumes to true. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>", "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Propagate this setting to the Auto Scaling group so it will automatically replace existing instances with new instances</p>", "<p>Leverage the Amazon EC2 console to enable encryption of new EBS volumes by default. Leverage the Auto Scaling group's instance refresh feature to replace existing instances with new instances</p>"]}, "correct_response": ["d"], "section": "Data Protection", "question_plain": "The security team at an e-commerce company has noticed that several Amazon Elastic Block Store (Amazon EBS) volumes are not encrypted. These unencrypted EBS volumes are attached to Amazon EC2 instances that are provisioned with an Auto Scaling group and a launch template. You have been hired as an AWS Certified Security Specialist to implement a solution that ensures all EBS volumes are encrypted both now and in the future.\n\nWhat would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 70394944, "assessment_type": "multi-select", "prompt": {"question": "<p>The security team at a company has set up an IAM user with full permissions for the EC2 service, yet the user is unable to start an Amazon EC2 instance after it was stopped for maintenance purposes. The instance would change its state to \"Pending\" but would eventually switch back to \"Stopped\" with the error \"client error on launch\". Upon investigating the issue, it was discovered that the EC2 instance had attached Amazon EBS volumes that were encrypted using a Customer Master Key (CMK). Detaching the encrypted volumes from the EC2 instance resolved the issue and allowed the user to start the instance successfully.</p>\n\n<p>Following is a snippet of the existing IAM user policy:</p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                    &lt;action&gt;\n            ],\n            \"Resource\": \"arn:aws:kms:&lt;region&gt;:&lt;accountId&gt;:key/kms-encryption-key-for-ebs\",\n            \"Condition\": &lt;condition&gt;\n        }\n    ]\n}\n</code></pre>\n\n<p>You have been tasked to build a solution to fix this issue. What do you recommend? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</strong></p>\n\n<p>This issue occurs with EC2 instances with encrypted volumes attached if:</p>\n\n<p>The AWS Key Management Service (AWS KMS) or an AWS Identity and Access Management (IAM) user launching the instances doesn't have the required permissions.\nThe KMS key usage is restricted by the SourceIp condition key.\nThe IAM user must have permission from AWS KMS to decrypt the AWS KMS key.</p>\n\n<p>You should note that the default KMS key policy gives the AWS account that owns the KMS key permission to use IAM policies to allow access to all AWS KMS operations on the KMS key. To allow access to decrypt a KMS key, you must use the key policy with IAM policies or grants. IAM policies alone aren't sufficient to allow access to a KMS key, but you can use them in combination with a KMS key's policy (which is the default key policy in this case).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n\n<p><strong>Add the condition as <code>{ \"Bool\": { \"kms:GrantIsForAWSResource\": true }</code> to the IAM user policy</strong></p>\n\n<p>The condition <code>kms:GrantIsForAWSResource</code> allows or denies permission for the CreateGrant, ListGrants, or RevokeGrant operations only when an AWS service integrated with AWS KMS calls the operation on the user's behalf. This policy condition doesn't allow the user to call these grant operations directly. This policy condition can be applied to Key policies as well as IAM policies.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q64-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource\">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</strong> - <code>kms:EnableKey</code> controls permission to view detailed information about an AWS KMS key. This option is not relevant to the given use case.</p>\n\n<p><strong>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</strong> - <code>kms:CreateKey</code> controls permission to create an AWS KMS key that can be used to protect data keys and other sensitive information. Since the key is already created, this option is not relevant.</p>\n\n<p><strong>Add the condition as <code>{ \"Bool\": { \"kms:ViaService\": \"ec2.&lt;region&gt;.amazonaws.com\"}</code> to the IAM user policy</strong> - The <code>kms:ViaService</code> condition key limits the use of a KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key. The operation must be a KMS key resource operation, that is, an operation that is authorized for a particular KMS key. This option has been added as a distractor since the issue is with the EBS volumes (and not the EC2 service) that were encrypted using a CMK and the lack of sufficient key-specific grants thereof.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/\">https://aws.amazon.com/premiumsupport/knowledge-center/encrypted-volumes-stops-immediately/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource\">https://docs.aws.amazon.com/kms/latest/developerguide/conditions-kms.html#conditions-kms-grant-is-for-aws-resource</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-default.html</a></p>\n", "answers": ["<p>Add the &lt;action&gt; as <code>kms:CreateGrant</code> to the IAM user policy</p>", "<p>Add the condition as <code>{ \"Bool\": { \"kms:GrantIsForAWSResource\": true }</code> to the IAM user policy</p>", "<p>Add the &lt;action&gt; as <code>kms:DescribeKey</code> to the IAM user policy</p>", "<p>Add the &lt;action&gt; as <code>kms:CreateKey</code> to the IAM user policy</p>", "<p>Add the condition as <code>{ \"Bool\": { \"kms:ViaService\": \"ec2.&lt;region&gt;.amazonaws.com\"}</code> to the IAM user policy</p>"]}, "correct_response": ["a", "b"], "section": "Identity and Access Management", "question_plain": "The security team at a company has set up an IAM user with full permissions for the EC2 service, yet the user is unable to start an Amazon EC2 instance after it was stopped for maintenance purposes. The instance would change its state to \"Pending\" but would eventually switch back to \"Stopped\" with the error \"client error on launch\". Upon investigating the issue, it was discovered that the EC2 instance had attached Amazon EBS volumes that were encrypted using a Customer Master Key (CMK). Detaching the encrypted volumes from the EC2 instance resolved the issue and allowed the user to start the instance successfully.\n\nFollowing is a snippet of the existing IAM user policy:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                    &lt;action&gt;\n            ],\n            \"Resource\": \"arn:aws:kms:&lt;region&gt;:&lt;accountId&gt;:key/kms-encryption-key-for-ebs\",\n            \"Condition\": &lt;condition&gt;\n        }\n    ]\n}\n\n\nYou have been tasked to build a solution to fix this issue. What do you recommend? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 70394946, "assessment_type": "multi-select", "prompt": {"question": "<p>A financial services company wants to develop a solution called Financial Information System (FIS) on AWS Cloud that would allow the financial institutions and government agencies to collaborate, anticipate and navigate the changing finance landscape. While pursuing this endeavor, the company would like to decrease its IT operational overhead. The solution should help the company eliminate the bottleneck created by manual provisioning of development pipelines while adhering to crucial governance and control requirements. As a means to this end, the company has set up \"AWS Organizations\" to manage several of these scenarios and would like to use Service Control Policies (SCP) for central control over the maximum available permissions for the various accounts in their organization. This allows the organization to ensure that all accounts stay within the organization\u2019s access control guidelines.</p>\n\n<p>Which of the following scenarios would you identify as correct regarding the given use-case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, including the root user</strong></p>\n\n<p><strong>SCPs do not affect service-linked role</strong></p>\n\n<p>Service control policies (SCPs) are one type of policy that can be used to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization\u2019s access control guidelines.</p>\n\n<p>In SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.</p>\n\n<p>Please note the following effects on permissions vis-a-vis the SCPs:</p>\n\n<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action.</p>\n\n<p>SCPs affect all users and roles in the attached accounts, including the root user.</p>\n\n<p>SCPs do not affect any service-linked role.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-scs-pt/assets/pt2-q65-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</strong></p>\n\n<p><strong>SCPs affect all users and roles in attached accounts, excluding the root user</strong></p>\n\n<p><strong>SCPs affect service-linked roles</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p>\n", "answers": ["<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</p>", "<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</p>", "<p>SCPs affect all users and roles in attached accounts, including the root user</p>", "<p>SCPs affect all users and roles in attached accounts, excluding the root user</p>", "<p>SCPs affect service-linked roles</p>", "<p>SCPs do not affect service-linked role</p>"]}, "correct_response": ["a", "c", "f"], "section": "Identity and Access Management", "question_plain": "A financial services company wants to develop a solution called Financial Information System (FIS) on AWS Cloud that would allow the financial institutions and government agencies to collaborate, anticipate and navigate the changing finance landscape. While pursuing this endeavor, the company would like to decrease its IT operational overhead. The solution should help the company eliminate the bottleneck created by manual provisioning of development pipelines while adhering to crucial governance and control requirements. As a means to this end, the company has set up \"AWS Organizations\" to manage several of these scenarios and would like to use Service Control Policies (SCP) for central control over the maximum available permissions for the various accounts in their organization. This allows the organization to ensure that all accounts stay within the organization\u2019s access control guidelines.\n\nWhich of the following scenarios would you identify as correct regarding the given use-case? (Select three)", "related_lectures": []}]}
5750708
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 69669476, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer created an Amazon S3 bucket and attached the following bucket policy.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-04_02-13-58-26157aa0edcfdbf06a08442548015df5.jpg\"><p>What is the effect of this bucket policy?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The first statement in the bucket policy denies all S3 API actions for the AWS account specified with the Effect, Action, and Principal elements. This blocks all access to the account\u2019s IAM principals, regardless of the IAM policy attached to those users or roles.</p><p>This policy prevents unintended access to the secure S3 bucket that could be introduced by IAM policies or ACLs. The policy then uses a Condition element to make exceptions for the allowed principals.</p><p>The Deny statement only applies when the principal whose access is being evaluated is <em>not</em> one of the specified users.</p><p><br></p><p><strong>CORRECT: </strong>\"The specified users are not denied S3 permissions but must be granted permissions through IAM user policies or ACLs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The specified users are granted all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs\" is incorrect.</p><p>The policy does not grant any permissions, it simply does not deny any permissions for the specified users. The users will need to have permissions granted elsewhere.</p><p><strong>INCORRECT:</strong> \"The specified users are denied all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs\" is incorrect.</p><p>The specified users are not denied permissions based on the Condition element in the policy.</p><p><strong>INCORRECT:</strong> \"The specified account will be denied all S3 permissions but specified users from other accounts are excepted\" is incorrect.</p><p>The policy denies permissions for all users except those specified within the account specified. It does not grant or deny permissions for other AWS accounts.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-create-a-policy-that-whitelists-access-to-sensitive-amazon-s3-buckets/\">https://aws.amazon.com/blogs/security/how-to-create-a-policy-that-whitelists-access-to-sensitive-amazon-s3-buckets/</a></p>", "answers": ["<p>The specified users are granted all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs.</p>", "<p>The specified users are denied all S3 permissions to the bucket and objects within the bucket regardless of IAM user policies and ACLs.</p>", "<p>The specified users are not denied S3 permissions but must be granted permissions through IAM user policies or ACLs.</p>", "<p>The specified account will be denied all S3 permissions but specified users from other accounts are excepted.</p>"]}, "correct_response": ["c"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A security engineer created an Amazon S3 bucket and attached the following bucket policy.What is the effect of this bucket policy?", "related_lectures": []}, {"_class": "assessment", "id": 69669478, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying Amazon EC2 instances into a new VPC. The instances must be scanned to detect any known software vulnerabilities. The instances should also be checked for compliance with CIS benchmarks.</p><p>Which solution addresses these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Note: the CIS scans are not available in the new Amazon Inspector but are still mentioned in the exam in relation to Amazon Inspector Classic.</p><p>Amazon Inspector Classic can be used to scan the instances and detect known software vulnerabilities and compliance with CIS benchmarks.</p><p>The \u201cCommon vulnerabilities and exposures\u201d assessment verifies whether the EC2 instances in your assessment targets are exposed to common vulnerabilities and exposures (CVEs).</p><p>Attacks can exploit unpatched vulnerabilities to compromise the confidentiality, integrity, or availability of your service or data. The CVE system provides a reference method for publicly known information security vulnerabilities and exposures.</p><p>The \u201cCenter for Internet Security (CIS) Benchmarks\u201d helps to establish secure configuration postures for several operating system versions.</p><p><strong>CORRECT: </strong>Use Amazon Inspector and run the \u201cCommon vulnerabilities and exposures\u201d assessment and the \u201cCenter for Internet Security (CIS) Benchmarks\u201d assessment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector and run the \u201cNetwork Reachability\u201d assessment and the \u201cCenter for Internet Security (CIS) Benchmarks\u201d assessment\" is incorrect.</p><p>The \u201cCommon vulnerabilities and exposures\u201d assessment should be used to identify unpatched software vulnerabilities. The \u201cNetwork Reachability\u201d assessment checks which ports are open and how your network interfaces are configured.</p><p><strong>INCORRECT:</strong> \"Use AWS Config and configure the \u201crestricted-common-ports\u201d and \u2018wafv2-logging-enabled\u201d managed rules\" is incorrect.</p><p>AWS Config cannot be used to scan for software vulnerabilities or CIS benchmarks.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail and monitor the \u201cPutEventSelectors\u201d and \u201cPutInsightSelectors\u201d API actions\" is incorrect.</p><p>These CloudTrail API actions are totally unrelated to Amazon EC2.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_rule-packages.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_rule-packages.html</a></p>", "answers": ["<p>Use Amazon Inspector and run the \u201cCommon vulnerabilities and exposures\u201d assessment and the \u201cCenter for Internet Security (CIS) Benchmarks\u201d assessment.</p>", "<p>Use AWS Config and configure the \u201crestricted-common-ports\u201d and \u2018wafv2-logging-enabled\u201d managed rules.</p>", "<p>Use Amazon Inspector and run the \u201cNetwork Reachability\u201d assessment and the \u201cCenter for Internet Security (CIS) Benchmarks\u201d assessment.</p>", "<p>Use AWS CloudTrail and monitor the \u201cPutEventSelectors\u201d and \u201cPutInsightSelectors\u201d API actions.</p>"]}, "correct_response": ["a"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company is deploying Amazon EC2 instances into a new VPC. The instances must be scanned to detect any known software vulnerabilities. The instances should also be checked for compliance with CIS benchmarks.Which solution addresses these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69669480, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An attack left several Amazon EC2 Windows instances unresponsive. A security engineer has been asked to collect any memory dumps that may exist on the EC2 instances attached Amazon EBS volumes.</p><p>How should the security collect memory dumps for forensic analysis?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The EC2Rescue for Windows Server command line interface (CLI) allows you to run an EC2Rescue for Windows Server plugin (referred as an \"action\") programmatically.</p><p>The EC2Rescue for Windows Server tool has two execution modes:</p><ul><li><p><strong>/online</strong>\u2014This allows you to take action on the instance that EC2Rescue for Windows Server is installed on, such as collect log files.</p></li><li><p><strong>/offline:&lt;device_id&gt;</strong>\u2014This allows you to take action on the offline root volume that is attached to a separate Amazon EC2 Windows instance, on which you have installed EC2Rescue for Windows Server.</p></li></ul><p>In this case the instances are unresponsive so the /offline mode must be used and the device ID of the EBS volumes must be specified when running the CLI command.</p><p><strong>CORRECT: </strong>\"Run the EC2Rescue CLI using the /offline mode and specify the device ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run the EC2Rescue CLI using the /online mode and specify the instance ID\" is incorrect.</p><p>Online mode cannot be used on an unresponsive instance as the commands are run on the instance itself. You also do not specify the instance ID as the commands are run locally.</p><p><strong>INCORRECT:</strong> \"Install the SSM agent and stream log data to Amazon CloudWatch Logs\" is incorrect.</p><p>You cannot install the SSM agent on an unresponsive instance and it does not collect memory dumps.</p><p><strong>INCORRECT:</strong> \"Reboot the EC2 Windows Server, enter safe mode, and select memory dump\" is incorrect.</p><p>This is not a valid method of exporting memory dumps from a Windows server.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html</a></p>", "answers": ["<p>Install the SSM agent and stream log data to Amazon CloudWatch Logs.</p>", "<p>Run the EC2Rescue CLI using the /online mode and specify the instance ID.</p>", "<p>Reboot the EC2 Windows Server, enter safe mode, and select memory dump.</p>", "<p>Run the EC2Rescue CLI using the /offline mode and specify the device ID.</p>"]}, "correct_response": ["d"], "section": "Domain 1 - Incident Response", "question_plain": "An attack left several Amazon EC2 Windows instances unresponsive. A security engineer has been asked to collect any memory dumps that may exist on the EC2 instances attached Amazon EBS volumes.How should the security collect memory dumps for forensic analysis?", "related_lectures": []}, {"_class": "assessment", "id": 69669482, "assessment_type": "multi-select", "prompt": {"question": "<p>An application is being deployed on Amazon EC2 instances behind a Network Load Balancer (NLB). The EC2 instances are failing health checks and are not entering the InService state.</p><p>What could be the cause of this issue? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>If a target is taking longer than expected to enter the InService state, it might be failing health checks. Your target is not in service until it passes one health check.</p><p>Unlike ALBs, NLBs do not have security groups associated with them. The security group of the EC2 instances must be configured to allow inbound traffic on the health check port and protocol from the IP addresses of the NLB. The network ACL for the subnets of the instances and load balancer nodes must also be configured to allow this traffic.</p><p><strong>CORRECT: </strong>\"The EC2 instance security group does not allow inbound traffic from the NLB IP addresses\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The network ACL associated with the instance subnets does not allow traffic from the NLB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The security group associated with the NLB does not allow outbound traffic to the EC2 instance security group\" is incorrect.</p><p>NLBs do not have security groups.</p><p><strong>INCORRECT:</strong> \"The security group associated with the NLB does not allow inbound traffic from the internet\" is incorrect.</p><p>NLBs do not have security groups.</p><p><strong>INCORRECT:</strong> \"The EC2 instance security group does not allow inbound traffic from the NLB DNS name\" is incorrect.</p><p>The IP addresses of the NLB nodes must be used in the inbound rules of the EC2 instance security group, not the DNS name of the NLB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#target-security-groups\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html</a></p>", "answers": ["<p>The security group associated with the NLB does not allow outbound traffic to the EC2 instance security group.</p>", "<p>The EC2 instance security group does not allow inbound traffic from the NLB IP addresses.</p>", "<p>The network ACL associated with the instance subnets does not allow traffic from the NLB.</p>", "<p>The security group associated with the NLB does not allow inbound traffic from the internet.</p>", "<p>The EC2 instance security group does not allow inbound traffic from the NLB DNS name.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "An application is being deployed on Amazon EC2 instances behind a Network Load Balancer (NLB). The EC2 instances are failing health checks and are not entering the InService state.What could be the cause of this issue? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69669484, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Due to compliance requirements, a company must rotate encryption keys every year. An AWS KMS key was created using imported key material. A security engineer needs a process to rotate the KMS key.</p><p>Which key rotation process is MOST efficient?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS KMS keys with imported key material are not eligible for automatic rotation. Therefore, a new KMS key must be created with new key material. An Alias is a friendly name for an AWS KMS key and can be updated to point to a different KMS key. This is the most efficient method as it means any applications configured to use the Alias do not need to be updated.</p><p><strong>CORRECT: </strong>\"Create a new KMS key and update the existing Key Alias to point to the new KMS key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Select the option to enable automatic rotation for the existing KMS key\" is incorrect.</p><p>AWS KMS keys with imported key material are not eligible for automatic rotation.</p><p><strong>INCORRECT:</strong> \"Upload new key material into the existing KMS key\" is incorrect.</p><p>You cannot upload new key material to an AWS KMS key that was created with imported key material.</p><p><strong>INCORRECT:</strong> \"Create a new KMS key and change the application to point to the new KMS key\" is incorrect.</p><p>This is less efficient compared to using an Alias, so applications do not need to be updated to point to the new KMS key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/kms-alias.html\">https://docs.aws.amazon.com/kms/latest/developerguide/kms-alias.html</a></p>", "answers": ["<p>Select the option to enable automatic rotation for the existing KMS key.</p>", "<p>Upload new key material into the existing KMS key.</p>", "<p>Create a new KMS key and change the application to point to the new KMS key.</p>", "<p>Create a new KMS key and update the existing Key Alias to point to the new KMS key.</p>"]}, "correct_response": ["d"], "section": "Domain 5 - Data Protection", "question_plain": "Due to compliance requirements, a company must rotate encryption keys every year. An AWS KMS key was created using imported key material. A security engineer needs a process to rotate the KMS key.Which key rotation process is MOST efficient?", "related_lectures": []}, {"_class": "assessment", "id": 69669486, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has launched a web application running on port 80 on Amazon EC2 instances. The instances have been launched in a private subnet. An Application Load Balancer (ALB) is configured in front of the instances with an HTTP listener.</p><p>The instances are assigned to a security group named WebAppSG and the ALB is assigned to a security group named ALB-SG. The security team requires that the security group rules are locked down according to best practice.</p><p>What rules should be configured in the security groups? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The most secure configuration that will allow the required traffic is as follows:</p><p>ALB-SG:</p><ul><li><p>Inbound rule to allow port 80 from 0.0.0.0/0.</p></li><li><p>Outbound rule to allow port 80 to WebAppSG (and the health check port if different).</p></li></ul><p>WebAppSG:</p><ul><li><p>Inbound rule to allow port 80 from the security group ID for ALB-SG.</p></li><li><p>Outbound rules are not necessary as the response traffic to the ALB is allowed by default (may require rules for security updates etc.)</p></li></ul><p><strong>CORRECT: </strong>\"An inbound rule in WebAppSG allowing port 80 from source ALB-SG\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"An inbound rule in ALB-SG allowing port 80 from source 0.0.0.0/0\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"An outbound rule in ALB-SG allowing port 80 to WebAppSG\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"An inbound rule in ALB-SG allowing port 80 from WebAppSG\" is incorrect.</p><p>The ALB receives traffic from the internet so it should allow incoming traffic from 0.0.0.0/0. The ALB sends traffic to the web application outbound on port 80</p><p><strong>INCORRECT:</strong> \"An outbound rule in WebAppSG allowing ports 1024-65535 to destination ALB-SG\" is incorrect.</p><p>The web application security group does not need an outbound rule as response traffic is allowed. Ephemeral ports as specified above do not need to be opened.</p><p><strong>INCORRECT:</strong> \"An outbound rule in ALB-SG allowing ports 1024-65535 to destination 0.0.0.0/\" is incorrect.</p><p>There\u2019s no need for an outbound rule to ephemeral ports as security groups are stateful and will allow response traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>", "answers": ["<p>An inbound rule in WebAppSG allowing port 80 from source ALB-SG.</p>", "<p>An inbound rule in ALB-SG allowing port 80 from WebAppSG.</p>", "<p>An outbound rule in WebAppSG allowing ports 1024-65535 to destination ALB-SG.</p>", "<p>An inbound rule in ALB-SG allowing port 80 from source 0.0.0.0/0.</p>", "<p>An outbound rule in ALB-SG allowing ports 1024-65535 to destination 0.0.0.0/0.</p>", "<p>An outbound rule in ALB-SG allowing port 80 to WebAppSG.</p>"]}, "correct_response": ["a", "d", "f"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company has launched a web application running on port 80 on Amazon EC2 instances. The instances have been launched in a private subnet. An Application Load Balancer (ALB) is configured in front of the instances with an HTTP listener.The instances are assigned to a security group named WebAppSG and the ALB is assigned to a security group named ALB-SG. The security team requires that the security group rules are locked down according to best practice.What rules should be configured in the security groups? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69669488, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A developer recently left a company, and the company wants to ensure that any code the developer wrote cannot be deployed to AWS Lambda functions. The company uses AWS Signer for all Lambda functions.</p><p>Which solution will meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Code signing for Lambda provides four signature checks. First, the <em>integrity</em> check confirms that the deployment artifact hasn\u2019t been modified after it was signed using AWS Signer. Lambda performs this check by matching the hash of the artifact with the hash from the signature.</p><p>The second check is the <em>source mismatch</em> check, which detects if a signature isn\u2019t present or if the artifact is signed by a signing profile that isn\u2019t specified in the CSC. The third, <em>expiry</em> check, will fail if a signature is past its point of expiration.</p><p>The fourth is the <em>revocation</em> check, which is used to see if anyone has explicitly marked the signing profile used for signing or the signing job as invalid by revoking it.</p><p>The integrity check must succeed, or Lambda will not run the artifact. The other three checks can be configured to either block invocation or generate a warning. These checks are performed in order until one check fails, or all checks succeed.</p><p>In this case if the signing profile is revoked and the revocation check is configured to block on failure, the code cannot be deployed to Lambda functions.</p><p><strong>CORRECT: </strong>\"Revoke all versions of the signing profile assigned to the developer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Remove all IAM permissions that grant access to AWS Signer\" is incorrect.</p><p>This would prevent the developer from using Signer but would not stop the existing code from being deployed.</p><p><strong>INCORRECT:</strong> \"Change the AWS KMS key that is used to encrypt the source code\" is incorrect.</p><p>This would not prevent the code from being deployed, it would change the keys used to encrypt the code.</p><p><strong>INCORRECT:</strong> \"Use Amazon CodeGuru to review the code before it is deployed\" is incorrect.</p><p>CodeGuru is used for reviewing source code and provides recommendations for improvements but will not prevent the code from being deployed.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/best-practices-and-advanced-patterns-for-lambda-code-signing/\">https://aws.amazon.com/blogs/security/best-practices-and-advanced-patterns-for-lambda-code-signing/</a></p>", "answers": ["<p>Change the AWS KMS key that is used to encrypt the source code.</p>", "<p>Remove all IAM permissions that grant access to AWS Signer.</p>", "<p>Revoke all versions of the signing profile assigned to the developer.</p>", "<p>Use Amazon CodeGuru to review the code before it is deployed.</p>"]}, "correct_response": ["c"], "section": "Domain 5 - Data Protection", "question_plain": "A developer recently left a company, and the company wants to ensure that any code the developer wrote cannot be deployed to AWS Lambda functions. The company uses AWS Signer for all Lambda functions.Which solution will meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69669490, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is implementing an application that will use AWS KMS encryption keys. The company plans to create customer managed KMS keys within KMS and will not import any key material. The encryption keys should be rotated every 12 months.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you enable <em>automatic key rotation</em> for a customer managed key, AWS KMS generates new cryptographic material for the KMS key every year. You can enable or disable automatic key rotation for customer managed KMS keys at any time. You cannot modify the rotation configuration for AWS managed KMS keys which are automatically rotated every year.</p><p>Key rotation changes only the KMS key's <em>key material</em>, which is the cryptographic material that is used in encryption operations. The KMS key is the same logical resource, regardless of whether or how many times its key material changes. The properties of the KMS key do not change, as shown in the following image.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_00-34-09-a05cd64102acfb99c6f3e498dfdafdf3.jpg\"></p><p><strong>CORRECT: </strong>\"Enable the option to automatically rotate each KMS key every year\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS managed KMS keys instead so that AWS will rotate the keys automatically\" is incorrect.</p><p>You cannot modify the rotation configuration for AWS managed KMS keys which are automatically rotated every year.</p><p><strong>INCORRECT:</strong> \"Schedule an AWS Lambda function to rotate the backing key of each KMS key\" is incorrect.</p><p>Customer managed KMS keys are being used so there is no need to create a custom solution as you can simply enable automatic key rotation.</p><p><strong>INCORRECT:</strong> \"Change the customer managed KMS key policy to enable automatic key rotation\" is incorrect.</p><p>You do not enable key rotation through a key policy. Key policies are used to grant permissions to use a KMS key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>", "answers": ["<p>Change the customer managed KMS key policy to enable automatic key rotation.</p>", "<p>Use AWS managed KMS keys instead so that AWS will rotate the keys automatically.</p>", "<p>Enable the option to automatically rotate each KMS key every year.</p>", "<p>Schedule an AWS Lambda function to rotate the backing key of each KMS keys.</p>"]}, "correct_response": ["c"], "section": "Domain 5 - Data Protection", "question_plain": "A company is implementing an application that will use AWS KMS encryption keys. The company plans to create customer managed KMS keys within KMS and will not import any key material. The encryption keys should be rotated every 12 months.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69669492, "assessment_type": "multi-select", "prompt": {"question": "<p>An administrative user accidentally exposed an access key ID and secret access key to a public support forum. The user notified the security team about the incident after removing the exposed credentials from the forum.</p><p>Which initial steps should a security engineer take to mitigate the exposure without interrupting operations? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>There are several steps you can take if credentials are inadvertently exposed (see article below). The initial steps the engineer should take include invalidating any temporary security credentials that may have been issued and deleting or disabling the access key ID and secret access key.</p><p>These actions will ensure that any malicious actor who obtains the access key will not be able to use it and if temporary security credentials have been obtained they will no longer be valid for use.</p><p>One method of invalidating temporary security credentials is to attach a policy that denies all access to temporary security credentials that were issued before the specified date and time. Here is an example of such a policy:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_00-38-39-c2422bcd01ee5286f611b96458006f6c.jpg\"><p><strong>CORRECT: </strong>\"Invalidate any temporary security credentials\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Delete the access key ID and secret access key\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Reset the password for the root user on the account\" is incorrect.</p><p>This is unnecessary as the root user account has not been compromised.</p><p><strong>INCORRECT:</strong> \"Implement a deny policy blocking access to resources\" is incorrect.</p><p>This would cause interruption as resources would be inaccessible.</p><p><strong>INCORRECT:</strong> \"Terminate or disable any resources the user has access to\" is incorrect.</p><p>This would be very disruptive and may result in lost data and downtime.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/\">https://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/</a></p>", "answers": ["<p>Reset the password for the root user on the account.</p>", "<p>Implement a deny policy blocking access to resources.</p>", "<p>Terminate or disable any resources the user has access to.</p>", "<p>Invalidate any temporary security credentials.</p>", "<p>Delete the access key ID and secret access key.</p>"]}, "correct_response": ["d", "e"], "section": "Domain 1 - Incident Response", "question_plain": "An administrative user accidentally exposed an access key ID and secret access key to a public support forum. The user notified the security team about the incident after removing the exposed credentials from the forum.Which initial steps should a security engineer take to mitigate the exposure without interrupting operations? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69669494, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application running in a private subnet needs outbound connectivity to an internet service using the IPv6 protocol. A security engineer has created a separate route table for the private subnet.</p><p>The security engineer needs to enable outbound connectivity to the internet service. The solution should ensure inbound connections from the internet cannot be initiated.</p><p>Which actions should the network engineer take to meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet and prevents the internet from initiating an IPv6 connection with your instances.</p><p><strong>CORRECT: </strong>\"Create an egress-only internet gateway and update the route table in the private subnet\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a NAT gateway in a public subnet and update the route table in the private subnet\" is incorrect.</p><p>NAT gateways are used for IPv4 not IPv6.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway in a private subnet and update the route table in the private subnet\" is incorrect.</p><p>Internet gateways are used for routing traffic out of the VPC and are attached at the VPC level. To enable outbound IPv6 an egress-only internet gateway is also needed.</p><p><strong>INCORRECT:</strong> \"Create an internet gateway in a public subnet and update the route table in the private subnet\" is incorrect.</p><p>Internet gateways are used for routing traffic out of the VPC and are attached at the VPC level. To enable outbound IPv6 an egress-only internet gateway is also needed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>", "answers": ["<p>Create a NAT gateway in a public subnet and update the route table in the private subnet.</p>", "<p>Create an internet gateway in a private subnet and update the route table in the private subnet.</p>", "<p>Create an egress-only internet gateway and update the route table in the private subnet.</p>", "<p>Create an internet gateway in a public subnet and update the route table in the private subnet.</p>"]}, "correct_response": ["c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "An application running in a private subnet needs outbound connectivity to an internet service using the IPv6 protocol. A security engineer has created a separate route table for the private subnet.The security engineer needs to enable outbound connectivity to the internet service. The solution should ensure inbound connections from the internet cannot be initiated.Which actions should the network engineer take to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69669496, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer has deployed a virtual security appliance inline. The virtual security appliance will be used to inspect traffic that is forwarded between subnets.</p><p>What configuration is necessary to allow the virtual security appliance to route the traffic?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An inline security appliance running on an Amazon EC2 instance can be configured with multiple network interfaces. These interfaces may reside in different subnets within the VPC.</p><p>Each EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives. However, an inline security appliance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on these instances.</p><p><strong>CORRECT: </strong>\"Disable the Network Source/Destination check on the security appliance's elastic network interface\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the security appliance's elastic network interfaces for promiscuous mode and attach elastic IP addresses\" is incorrect.</p><p>Promiscuous mode (not supported) is a networking configuration where you are able to capture all the traffic from a network interface for inspection.</p><p><strong>INCORRECT:</strong> \"Attach an elastic network adapter (ENA), install the ENA module, and enable ENA support\" is incorrect.</p><p>This is a high-performance adapter and does not need to be used in this configuration.</p><p><strong>INCORRECT:</strong> \"Place the security appliance in the public subnet with the internet gateway and route traffic to the appliance\" is incorrect.</p><p>The security appliance can be attached to private or public subnets and does not need an internet gateway.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/scenarios-enis.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/scenarios-enis.html</a></p>", "answers": ["<p>Attach an elastic network adapter (ENA), install the ENA module, and enable ENA support.</p>", "<p>Disable the Network Source/Destination check on the security appliance's elastic network interface.</p>", "<p>Configure the security appliance's elastic network interfaces for promiscuous mode and attach elastic IP addresses.</p>", "<p>Place the security appliance in the public subnet with the internet gateway and route traffic to the appliance.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer has deployed a virtual security appliance inline. The virtual security appliance will be used to inspect traffic that is forwarded between subnets.What configuration is necessary to allow the virtual security appliance to route the traffic?", "related_lectures": []}, {"_class": "assessment", "id": 69669498, "assessment_type": "multi-select", "prompt": {"question": "<p>A solutions architect is designing a secure, distributed application that will run on Amazon EC2 instances across multiple Availability Zones and AWS Regions and on-premises servers. The has asked a security engineer how encryption will be applied between the EC2 instances and on-premises servers.</p><p>Which statements are correct about encryption in transit? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>All data flowing across AWS Regions over the AWS global network is automatically encrypted at the physical layer before it leaves AWS secured facilities. All traffic between AZs is also encrypted.</p><p>Traffic between instances may be encrypted in some circumstances. The instances must use a supported instance type and be within the same Region and VPC (or in a peered VPC.)</p><p><strong>CORRECT: </strong>\"All inter-region traffic over the AWS global network is automatically encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"All traffic between Availability Zones is encrypted by default\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"All intra-region traffic is encrypted between instances for all instance types\" is incorrect.</p><p>This is not true. Traffic may be encrypted between instances with certain constraints as mentioned above and in the reference link below.</p><p><strong>INCORRECT:</strong> \"All traffic between Availability Zones is unencrypted by default\" is incorrect.</p><p>This is not true as AWS does encrypt all traffic between AZs.</p><p><strong>INCORRECT:</strong> \"All traffic across an AWS Direct Connect connection is automatically encrypted\" is incorrect.</p><p>This is not true as AWS Direct Connect does not provide encryption. You must create an encrypted VPN tunnel over your DX connection to provide encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/data-protection.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/data-protection.html</a></p>", "answers": ["<p>All inter-region traffic over the AWS global network is automatically encrypted.</p>", "<p>All intra-region traffic is encrypted between instances for all instance types.</p>", "<p>All traffic between Availability Zones is unencrypted by default.</p>", "<p>All traffic across an AWS Direct Connect connection is automatically encrypted.</p>", "<p>All traffic between Availability Zones is encrypted by default.</p>"]}, "correct_response": ["a", "e"], "section": "Domain 5 - Data Protection", "question_plain": "A solutions architect is designing a secure, distributed application that will run on Amazon EC2 instances across multiple Availability Zones and AWS Regions and on-premises servers. The has asked a security engineer how encryption will be applied between the EC2 instances and on-premises servers.Which statements are correct about encryption in transit? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69669500, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses AWS Organizations and federation between and on-premises identity provider (IdP). Users authenticate to AWS using credentials in the IdP. A security engineer needs to audit requests to AWS Organizations for creating new AWS accounts.</p><p>What should the engineer review to determine who made the request?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Organizations enables you to create new accounts through the console or programmatically using the organizations API. When you create accounts through organizations the API calls are logged in AWS CloudTrail.</p><p>In this case AWS CloudTrail can be used to track the activity of the federated users. CloudTrail records the following AWS Security Token Service (AWS STS) API calls: AssumeRoleWithWebIdentity and AssumeRoleWithSAML.</p><p>Records of these API calls will be stored in CloudTrail and the user name of the federated user who made the call can be identified.</p><p><strong>CORRECT: </strong>\"AWS CloudTrail for the federated identity user name\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS IAM Access Analyzer for the federated user name\" is incorrect.</p><p>This service helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity.</p><p><strong>INCORRECT:</strong> \"AWS X-Ray traces for the federated identity user name\" is incorrect.</p><p>This service is used for analyzing and debugging production, distributed applications. It is not used for auditing account activity.</p><p><strong>INCORRECT:</strong> \"Federated identity provider logs for the user name\" is incorrect.</p><p>The request to AWS Organizations will be recorded by CloudTrail and the federated identity user name will be recorded in the log entry.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ct.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ct.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-easily-identify-your-federated-users-by-using-aws-cloudtrail/\">https://aws.amazon.com/blogs/security/how-to-easily-identify-your-federated-users-by-using-aws-cloudtrail/</a></p>", "answers": ["<p>AWS CloudTrail for the federated identity user name.</p>", "<p>AWS IAM Access Analyzer for the federated user name.</p>", "<p>AWS X-Ray traces for the federated identity user name.</p>", "<p>Federated identity provider logs for the user name.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company uses AWS Organizations and federation between and on-premises identity provider (IdP). Users authenticate to AWS using credentials in the IdP. A security engineer needs to audit requests to AWS Organizations for creating new AWS accounts.What should the engineer review to determine who made the request?", "related_lectures": []}, {"_class": "assessment", "id": 69669502, "assessment_type": "multi-select", "prompt": {"question": "<p>A company is deploying a solution that includes an Application Load Balancer in front of many Amazon EC2 instances. A caching and distribution solution is required for the content. Customers will connect to the application using a custom domain name and subdomains and must be secured with TLS encryption.</p><p>Which combination of actions will achieve the requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The CloudFront distribution will cache and distribute content. When creating the distribution you can add alternate domain names (CNAMEs). However, note that a CNAME cannot be used for an apex (or root) domain name in Amazon Route 53. Therefore, Alias records should be created in Route 53 instead.</p><p>For TLS encryption AWS Certificate Manager can be used to create SSL/TLS certificates which you can select in the CloudFront distribution. You can add multiple domain names to the certificate.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution, configure the alternate domain name and subdomains, and select a certificate from AWS Certificate Manager (ACM)\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a certificate in AWS Certificate Manager (ACM) and add the domain name and subdomains. Create Alias records for the distribution in Amazon Route 53\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution, configure the alternate domain name and subdomains, and select a key from AWS KMS\" is incorrect.</p><p>AWS KMS is used for creating and managing encryption keys for encryption at rest, not encryption in transit which uses certificates.</p><p><strong>INCORRECT:</strong> \"Create an encryption key in AWS Key Management Service (KMS) and add CloudFront to the key policy. Create CNAME. records in Amazon Route 53\" is incorrect.</p><p>AWS KMS is used for creating and managing encryption keys for encryption at rest, not encryption in transit which uses certificates.</p><p><strong>INCORRECT:</strong> \"Create a certificate in AWS Certificate Manager (ACM) and add the domain name and subdomains. Create CNAME. records in Amazon Route 53\" is incorrect.</p><p>A CNAME cannot be used for an apex domain name in Route 53.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/CNAMEs.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/CNAMEs.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-certificate.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html</a></p>", "answers": ["<p>Create an Amazon CloudFront distribution, configure the alternate domain name and subdomains, and select a key from AWS KMS.</p>", "<p>Create an Amazon CloudFront distribution, configure the alternate domain name and subdomains, and select a certificate from AWS Certificate Manager (ACM).</p>", "<p>Create a certificate in AWS Certificate Manager (ACM) and add the domain name and subdomains. Create Alias records for the distribution in Amazon Route 53.</p>", "<p>Create an encryption key in AWS Key Management Service (KMS) and add CloudFront to the key policy. Create CNAME. records in Amazon Route 53.</p>", "<p>Create a certificate in AWS Certificate Manager (ACM) and add the domain name and subdomains. Create CNAME. records in Amazon Route 53.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 1 - Incident Response", "question_plain": "A company is deploying a solution that includes an Application Load Balancer in front of many Amazon EC2 instances. A caching and distribution solution is required for the content. Customers will connect to the application using a custom domain name and subdomains and must be secured with TLS encryption.Which combination of actions will achieve the requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69669504, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses AWS across multiple Regions. A security audit highlighted some issues that must be addressed. The company must track all configuration changes affecting AWS resources and have detailed records of who has accessed the AWS environment. The data should include information such as which user has logged in and which API calls they made</p><p>What actions should be taken to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Config is a service used to track and remediation any unauthorized configuration changes made with your AWS Account. AWS Config could be used in this example with AWS AWS CloudTrail which keeps detailed logs of all API calls made within the account such as who logged in, which AWS Identity and Access Management (IAM) role is being used and also how they interact with the AWS Cloud.</p><p><strong>CORRECT: </strong>\"Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon CloudWatch does not make track configuration changes, it tracks performance metrics and AWS Config does not track API calls, it tracks configuration changes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud\" is incorrect. Although AWS Config would work in this scenario, <em>Amazon EventBridge</em> is a serverless event bus used to build event-driven- architectures so it cannot be used for tracking API calls.</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud\" is incorrect. Amazon Macie is used with Amazon S3 to detect sensitive PII data, which has nothing to do with tracking configuration changes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/config\">https://aws.amazon.com/config</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>", "answers": ["<p>Use Amazon CloudWatch to track configuration changes and AWS Config to record API calls and track access patterns in the AWS Cloud.</p>", "<p>Use AWS Config to track configuration changes and AWS CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>", "<p>Use AWS Config to track configuration changes and Amazon EventBridge to record API calls and track access patterns in the AWS Cloud.</p>", "<p>Use Amazon Macie to track configuration changes and Amazon CloudTrail to record API calls and track access patterns in the AWS Cloud.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company uses AWS across multiple Regions. A security audit highlighted some issues that must be addressed. The company must track all configuration changes affecting AWS resources and have detailed records of who has accessed the AWS environment. The data should include information such as which user has logged in and which API calls they madeWhat actions should be taken to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69669506, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a multi-account setup in AWS Organizations with 50 member accounts. The company uses AWS Security Hub to aggregate security findings across all accounts, with one account acting as a Security Hub administrator. The company's security operations team wants to receive real-time email alerts whenever there's a high-priority AWS Inspector finding across any of the accounts.</p><p>Which solution will fulfill these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>With an EventBridge rule in the Security Hub administrator account, the team can detect high-severity AWS Inspector findings across the organization. They can then use SNS to send real-time email alerts to the security operations team.</p><p><strong>CORRECT: </strong>\"In the Security Hub administrator account, create an Amazon EventBridge rule to react to AWS Inspector findings with a high priority level. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS Config across the organization and create a rule to detect AWS Inspector findings. Then, configure an EventBridge rule in the management account to react when AWS Config registers a compliance change. Have the EventBridge rule target an Amazon SNS topic and subscribe the security operations team's email addresses to it\" is incorrect.</p><p>AWS Config is used for resource configuration tracking and compliance auditing, not for responding to AWS Inspector findings. This approach would not meet the requirements.</p><p><strong>INCORRECT:</strong> \"In each member account, set up a rule in Amazon EventBridge that reacts to AWS Inspector findings with high severity. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to the SNS topic\" is incorrect.</p><p>This solution could technically work but would be operationally intensive to implement and maintain across 50 member accounts. It's more efficient to manage this centrally from the Security Hub administrator account.</p><p><strong>INCORRECT:</strong> \"Enable AWS CloudTrail for the organization in the management account. Set up an Amazon EventBridge rule to react to ListFindings API calls from AWS Inspector. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to it\" is incorrect.</p><p>AWS CloudTrail tracks API calls but does not directly interact with AWS Inspector findings. Using CloudTrail for this purpose would not meet the requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html\">https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html</a></p>", "answers": ["<p>In the Security Hub administrator account, create an Amazon EventBridge rule to react to AWS Inspector findings with a high priority level. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to the SNS topic.</p>", "<p>Enable AWS Config across the organization and create a rule to detect AWS Inspector findings. Then, configure an EventBridge rule in the management account to react when AWS Config registers a compliance change. Have the EventBridge rule target an Amazon SNS topic and subscribe the security operations team's email addresses to it.</p>", "<p>In each member account, set up a rule in Amazon EventBridge that reacts to AWS Inspector findings with high severity. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to the SNS topic.</p>", "<p>Enable AWS CloudTrail for the organization in the management account. Set up an Amazon EventBridge rule to react to ListFindings API calls from AWS Inspector. Configure the rule to target an Amazon SNS topic and subscribe the security operations team's email addresses to it.</p>"]}, "correct_response": ["a"], "section": "Domain 6: Management and Security Governance", "question_plain": "A company has a multi-account setup in AWS Organizations with 50 member accounts. The company uses AWS Security Hub to aggregate security findings across all accounts, with one account acting as a Security Hub administrator. The company's security operations team wants to receive real-time email alerts whenever there's a high-priority AWS Inspector finding across any of the accounts.Which solution will fulfill these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69669508, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has a range of highly sensitive data stored in an Amazon S3 bucket in the eu-west-1 Region. Certain objects in this S3 bucket are protected with server-side encryption using AWS KMS keys (SSE-KMS). To enable disaster recovery, a security architect sets up an additional S3 bucket in the eu-central-1 Region within the same AWS account.</p><p>A customer-managed key is established in the eu-central-1 region to ensure the encryption at rest of objects in the backup S3 bucket. The replication configuration is configured to utilize this key for encryption in the destination bucket. An IAM role has been granted to the S3 replication configuration to execute replication actions.</p><p>However, after some time, the security architect notices that the encrypted objects from the source S3 bucket are not being replicated to the destination S3 bucket, while the unencrypted objects are replicated without issue.</p><p>What sequence of steps should the security architect take to resolve this issue? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The IAM role needs the kms:Encrypt permission for the key in the eu-west-1 region that is used to encrypt the source objects. This permission is necessary so the role can encrypt objects before they are transferred.</p><p>The s3:GetObjectVersionForReplication permission is required for the IAM role to retrieve the version of an object for replication from the source S3 bucket. Without this permission, the role may not be able to access object versions for replication.</p><p>The kms:Decrypt permission for the key in eu-west-1 that is used to encrypt the source objects is necessary for the IAM role. This is because the role needs to decrypt the source objects before they can be transferred and encrypted again for the destination bucket.</p><p><strong>CORRECT: </strong>\"Provide the IAM role with the kms:Encrypt permission for the key in the eu-west-1 region that encrypts the source objects\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Grant the IAM role the s3:GetObjectVersionForReplication permission for the objects in the source S3 bucket\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Give the IAM role the kms:Decrypt permission for the eu-west-1 key encrypting the source objects\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the replication configuration to use the eu-west-1 key for the encryption of objects within the destination S3 bucket\" is incorrect.</p><p>It's not a best practice to use the key from the source region to encrypt objects in the destination region. The destination region should have its own key for encrypting objects at rest. Moreover, using the source key wouldn't resolve the issue, as the problem is not about the key used for encryption at the destination but rather the IAM role's permissions to decrypt (source) and encrypt (destination) using the respective keys.</p><p><strong>INCORRECT:</strong> \"Alter the key policy for the eu-west-1 key to grant the security architect's IAM account the kms:Decrypt permission\" is incorrect.</p><p>Granting the security architect's IAM account the kms:Decrypt permission won't solve the issue. The issue lies with the permissions of the IAM role used by the S3 replication, not the permissions of the security architect's IAM account. The IAM role is the entity performing the replication and hence needs to have the correct permissions.</p><p><strong>INCORRECT:</strong> \"Assign the IAM role the kms:Encrypt permission for the key in the eu-central-1 region that encrypts objects in the destination S3 bucket\" is incorrect.</p><p>The IAM role used for S3 replication doesn't need to explicitly encrypt the objects in the destination bucket, as Amazon S3 handles the encryption process on its side once the objects arrive. It\u2019s more important for the IAM role to have kms:Decrypt permission for the key in the source bucket so it can read and replicate the objects. The objects get encrypted at the destination bucket automatically using the key specified in the destination bucket's configuration.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html</a></p>", "answers": ["<p>Update the replication configuration to use the eu-west-1 key for the encryption of objects within the destination S3 bucket.</p>", "<p>Provide the IAM role with the kms:Encrypt permission for the key in the eu-west-1 region that encrypts the source objects.</p>", "<p>Grant the IAM role the s3:GetObjectVersionForReplication permission for the objects in the source S3 bucket.</p>", "<p>Give the IAM role the kms:Decrypt permission for the eu-west-1 key encrypting the source objects.</p>", "<p>Alter the key policy for the eu-west-1 key to grant the security architect's IAM account the kms:Decrypt permission.</p>", "<p>Assign the IAM role the kms:Encrypt permission for the key in the eu-central-1 region that encrypts objects in the destination S3 bucket.</p>"]}, "correct_response": ["b", "c", "d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A company has a range of highly sensitive data stored in an Amazon S3 bucket in the eu-west-1 Region. Certain objects in this S3 bucket are protected with server-side encryption using AWS KMS keys (SSE-KMS). To enable disaster recovery, a security architect sets up an additional S3 bucket in the eu-central-1 Region within the same AWS account.A customer-managed key is established in the eu-central-1 region to ensure the encryption at rest of objects in the backup S3 bucket. The replication configuration is configured to utilize this key for encryption in the destination bucket. An IAM role has been granted to the S3 replication configuration to execute replication actions.However, after some time, the security architect notices that the encrypted objects from the source S3 bucket are not being replicated to the destination S3 bucket, while the unencrypted objects are replicated without issue.What sequence of steps should the security architect take to resolve this issue? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69669510, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has several microservices running on Amazon EC2 instances in multiple AWS accounts. Each microservice generates logs containing performance and utilization data. The company wants to centralize the logs in one account for further analysis and detection of unusual patterns. A solutions architect is assigned to ensure that logs from all the company's AWS accounts are aggregated and ingested in real-time into a data processing system for anomaly detection.</p><p>Which solution will meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>CloudWatch Logs subscription filters can be used to stream logs in real-time. By sending logs to an Amazon Kinesis Data Firehose stream in the central account, you can efficiently aggregate logs from multiple accounts. Kinesis Data Firehose can then forward these logs directly to the data processing system for real-time analysis.</p><p><strong>CORRECT: </strong>\"Set up CloudWatch Logs subscription filters in each account. Use the subscription filters to stream the logs to an Amazon Kinesis Data Firehose stream in the central account, which then forwards the logs to the data processing system\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure Amazon CloudWatch Logs agent on EC2 instances in each account to send logs to an Amazon S3 bucket in the central account. Set up a Lambda function to process the logs from the S3 bucket and send them to the data processing system\" is incorrect.</p><p>This option is not efficient for real-time log ingestion as it involves storing logs in an S3 bucket first and then processing them through a Lambda function. This additional step can cause delays and isn\u2019t optimized for real-time data processing.</p><p><strong>INCORRECT:</strong> \"Set up AWS Config in each account to monitor changes in EC2 instances and aggregate the data in the central account. Use AWS Lambda to process the aggregated data and send it to the data processing system\" is incorrect.</p><p>AWS Config is primarily for configuration management and compliance. While it can monitor changes in EC2 instances, it isn't designed for log aggregation or real-time analysis of performance and utilization data from application logs.</p><p><strong>INCORRECT:</strong> \"Write a custom script on each EC2 instance to push logs to Amazon S3 in the respective accounts. Set up cross-account access for S3 buckets and use AWS Glue to aggregate and process logs in the central account\" is incorrect.</p><p>This option involves writing logs to S3 and then processing them. This is not optimized for real-time ingestion and processing, and writing custom scripts can add additional complexity and maintenance overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample</a></p>", "answers": ["<p>Configure Amazon CloudWatch Logs agent on EC2 instances in each account to send logs to an Amazon S3 bucket in the central account. Set up a Lambda function to process the logs from the S3 bucket and send them to the data processing system.</p>", "<p>Set up CloudWatch Logs subscription filters in each account. Use the subscription filters to stream the logs to an Amazon Kinesis Data Firehose stream in the central account, which then forwards the logs to the data processing system.</p>", "<p>Set up AWS Config in each account to monitor changes in EC2 instances and aggregate the data in the central account. Use AWS Lambda to process the aggregated data and send it to the data processing system.</p>", "<p>Write a custom script on each EC2 instance to push logs to Amazon S3 in the respective accounts. Set up cross-account access for S3 buckets and use AWS Glue to aggregate and process logs in the central account.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A company has several microservices running on Amazon EC2 instances in multiple AWS accounts. Each microservice generates logs containing performance and utilization data. The company wants to centralize the logs in one account for further analysis and detection of unusual patterns. A solutions architect is assigned to ensure that logs from all the company's AWS accounts are aggregated and ingested in real-time into a data processing system for anomaly detection.Which solution will meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69669512, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multinational enterprise uses AWS Organizations to manage several AWS accounts spread across different regions. The company's IT department centrally manages the creation of IAM roles. Recently, the company decided to delegate the IAM role creation to various regional teams to speed up the process and reduce the IT department's workload. However, it is critical to prevent privilege escalation and ensure the scope of IAM roles remains within the defined limits.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>In AWS Organizations, Service Control Policies (SCPs) are one type of policy that you can use to manage permissions. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help to ensure your accounts stay within your organization\u2019s access control guidelines.</p><p>When you attach an SCP to the root, it affects all the OUs, accounts, and users under the root. Any permissions that are not explicitly granted by the SCP are implicitly denied.</p><p>A permissions boundary is an advanced feature in which you set the maximum permissions that an entity (user or role) can have. By setting a permissions boundary for IAM roles, the security team can delegate the role creation to application teams but still control the maximum permissions any IAM role can have.</p><p>Using a combination of SCPs and permissions boundaries provides a mechanism to delegate IAM role creation to the application teams while limiting the permissions those roles can have, thus preventing privilege escalation and controlling the scope of IAM roles.</p><p>By attaching the SCP to the root OU, you ensure that these restrictions apply to all accounts in your organization, not just the ones you specifically identify. And by using a permissions boundary, you can provide a 'safety net' that limits the permissions of any roles that are created, regardless of who creates them or what permissions they attempt to assign to the role. This mechanism significantly reduces the operational overhead and ensures security compliance across all teams and roles.</p><p><strong>CORRECT: </strong>\"Establish an SCP and a permissions boundary for IAM roles. Apply the SCP to the root OU so that only roles with the attached permissions boundary can create any new IAM roles\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Assign an IAM group for each regional team, attach relevant policies to these groups, and then create IAM users for each regional team member. Add the respective IAM users to their relevant IAM group using Role-Based Access Control (RBAC)\" is incorrect.</p><p>Although IAM groups and RBAC can help in managing permissions, this solution does not inherently prevent privilege escalation or limit the scope of IAM roles.</p><p><strong>INCORRECT:</strong> \"Empower regional team leaders to create IAM roles for their teams and conduct bi-annual audits of the IAM roles they have created. Provide necessary training to these leaders about the rules and best practices of IAM role creation\" is incorrect.</p><p>This approach places a lot of responsibility and potential risk with the team leaders, and bi-annual audits could allow for extended periods of misconfigured access. It also adds operational overhead due to training needs.</p><p><strong>INCORRECT:</strong> \"Organize each AWS account into an Organizational Unit (OU) based on the regions. Attach Service Control Policies (SCP) to each OU, granting access only to the AWS services required by the regional teams\" is incorrect.</p><p>While this can limit access to certain services, it does not necessarily limit the scope of IAM roles or prevent privilege escalation within the allowed services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>", "answers": ["<p>Assign an IAM group for each regional team, attach relevant policies to these groups, and then create IAM users for each regional team member. Add the respective IAM users to their relevant IAM group using Role-Based Access Control (RBAC).</p>", "<p>Empower regional team leaders to create IAM roles for their teams and conduct bi-annual audits of the IAM roles they have created. Provide necessary training to these leaders about the rules and best practices of IAM role creation.</p>", "<p>Organize each AWS account into an Organizational Unit (OU) based on the regions. Attach Service Control Policies (SCP) to each OU, granting access only to the AWS services required by the regional teams.</p>", "<p>Establish an SCP and a permissions boundary for IAM roles. Apply the SCP to the root OU so that only roles with the attached permissions boundary can create any new IAM roles.</p>"]}, "correct_response": ["d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A multinational enterprise uses AWS Organizations to manage several AWS accounts spread across different regions. The company's IT department centrally manages the creation of IAM roles. Recently, the company decided to delegate the IAM role creation to various regional teams to speed up the process and reduce the IT department's workload. However, it is critical to prevent privilege escalation and ensure the scope of IAM roles remains within the defined limits.Which solution will meet these requirements with the LEAST operational overhead?", "related_lectures": []}, {"_class": "assessment", "id": 69669514, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A university is using AWS Organizations to manage several AWS accounts for different departments. Recently, there was an incident of misuse with one of the departmental accounts. To prevent any misuse or accidental changes, the university wants to limit the access level of the AWS root account across all member accounts.</p><p>Which solution will help meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Service Control Policies (SCPs) can limit the maximum permission level for all AWS accounts in an organization, including the root user. By implementing SCPs, the university can prevent the root user in any account from exceeding the permissions defined in the SCP.</p><p>The following example policy restricts all access to the specified actions for the root user in a member account:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-06_22-06-36-0ebfba80d678d4db0d0e8f8b26ba8db4.jpg\"><p><strong>CORRECT: </strong>\"Use Service Control Policies (SCPs) to limit root account permissions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS IAM policies to restrict root account access\" is incorrect.</p><p>IAM policies are used to grant permissions to IAM users, groups, or roles. They cannot be used to directly restrict the permissions of the root user account.</p><p><strong>INCORRECT:</strong> \"Implement multi-factor authentication (MFA) on the root account\" is incorrect.</p><p>Although MFA adds an extra layer of security to the root account, it does not limit the access level or permissions of the root account. The root user always has full access to all resources in the AWS account.</p><p><strong>INCORRECT:</strong> \"Activate AWS CloudTrail for monitoring root account activities\" is incorrect.</p><p>While AWS CloudTrail allows you to log and monitor activities in your AWS environment, it does not restrict or limit the permissions of the root account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-root-user\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-root-user</a></p>", "answers": ["<p>Use AWS IAM policies to restrict root account access.</p>", "<p>Implement multi-factor authentication (MFA) on the root account.</p>", "<p>Use Service Control Policies (SCPs) to limit root account permissions.</p>", "<p>Activate AWS CloudTrail for monitoring root account activities.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A university is using AWS Organizations to manage several AWS accounts for different departments. Recently, there was an incident of misuse with one of the departmental accounts. To prevent any misuse or accidental changes, the university wants to limit the access level of the AWS root account across all member accounts.Which solution will help meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69669516, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer must identify any Amazon EC2 instances that are running a vulnerable version of a common web framework. The security team need to quickly identify all compute resources running the specific version so they can install patches.</p><p>Which approach should the team take to accomplish this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Systems Manager Patch Manager can be used to scan systems and identify vulnerable versions of software and then install the patches on the systems. Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to.</p><p>Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage.</p><p><strong>CORRECT: </strong>\"Scan all the EC2 instances with AWS Systems Manager to identify the vulnerable version of the web framework\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Scan all the EC2 instances with the Amazon Inspector Network Reachability rules package\" is incorrect.</p><p>This service is used more for determining if ports are open and accessible leading to vulnerabilities. The question is specifically asking to identify the vulnerable software.</p><p><strong>INCORRECT:</strong> \"Scan all the EC2 instances for noncompliance with AWS Config and an AWS Lambda function\" is incorrect.</p><p>AWS Config can be used for compliance management but is not ideal for identifying vulnerable versions of software.</p><p><strong>INCORRECT:</strong> \"Scan all the EC2 instances with Amazon GuardDuty to identify the vulnerable version of the web framework\" is incorrect.</p><p>GuardDuty provides continuous threat detection but does not scan software to identify vulnerable software versions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>", "answers": ["<p>Scan all the EC2 instances for noncompliance with AWS Config and an AWS Lambda function.</p>", "<p>Scan all the EC2 instances with the Amazon Inspector Network Reachability rules package.</p>", "<p>Scan all the EC2 instances with Amazon GuardDuty to identify the vulnerable version of the web framework.</p>", "<p>Scan all the EC2 instances with AWS Systems Manager to identify the vulnerable version of the web framework.</p>"]}, "correct_response": ["d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer must identify any Amazon EC2 instances that are running a vulnerable version of a common web framework. The security team need to quickly identify all compute resources running the specific version so they can install patches.Which approach should the team take to accomplish this task?", "related_lectures": []}, {"_class": "assessment", "id": 69669518, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Amazon S3 is used for storing sensitive data that is generated by a serverless application. The data must be encrypted, and the company plans to use the AWS Key Management Service (KMS) to create and manage the encryption keys. The company\u2019s security policies require that the company\u2019s own key material is imported, and custom expiration dates are configured.</p><p>How should the company configure AWS KMS?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can import your own key material into customer managed KMS key. With customer managed KMS keys you can also manage the keys, configure your own key policies, and enable automatic rotation every 365 days.</p><p>Note that with AWS managed KMS keys you do not have the ability to import your own key material, manage the keys, or set automatic yearly rotation. The table below shows the key differences:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_15-03-53-2779214bba90f85b8b4070c4a5055a96.jpg\"><p>Custom key stores cannot be used for this solution as a custom key store is backed by AWS CloudHSM and imposes certain limitations. For example you cannot import your own key material into KMS keys or enable automatic rotation. Automatic key rotation is also not possible for KMS keys with imported key material.</p><p><strong>CORRECT: </strong>\"Use the default key store and import the company\u2019s keys into a customer managed KMS key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a custom key store and import the company\u2019s keys into a customer managed KMS key\" is incorrect (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a custom key store and import the company\u2019s keys into an AWS managed KMS key\" is incorrect (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the default key store and import the company\u2019s keys into an AWS managed KMS key\" is incorrect (as explained above.)</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html\">https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html</a></p>", "answers": ["<p>Create a custom key store and import the company\u2019s keys into a customer managed KMS key.</p>", "<p>Use the default key store and import the company\u2019s keys into a customer managed KMS key.</p>", "<p>Create a custom key store and import the company\u2019s keys into an AWS managed KMS key.</p>", "<p>Use the default key store and import the company\u2019s keys into an AWS managed KMS key.</p>"]}, "correct_response": ["b"], "section": "Domain 5 - Data Protection", "question_plain": "Amazon S3 is used for storing sensitive data that is generated by a serverless application. The data must be encrypted, and the company plans to use the AWS Key Management Service (KMS) to create and manage the encryption keys. The company\u2019s security policies require that the company\u2019s own key material is imported, and custom expiration dates are configured.How should the company configure AWS KMS?", "related_lectures": []}, {"_class": "assessment", "id": 69669520, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Amazon CloudFront is used as the front-end of the application an AWS WAF is used to protect the front-end with the AWS Managed Rules rule group.</p><p>A security architect is concerned that the infrastructure is vulnerable to layer 7 DDoS attacks. What improvements can be made to the solution to protect against this type of attack?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests. By default, AWS WAF aggregates requests based on the IP address from the web request origin, but you can configure the rule to use an IP address from an HTTP header, like X-Forwarded-For, instead.</p><p>This rule can help prevent layer 7 DDoS attacks as the IP addresses of bots would be automatically blocked once they exceed the rate defined.</p><p><strong>CORRECT: </strong>\"Configure a rate-based rule on AWS WAF that puts a temporary block on requests from IP addresses that send excessive requests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure a Lambda@Edge function that imposes a rate limit on CloudFront viewer requests and blocks traffic that exceeds the limits\" is incorrect.</p><p>AWS WAF can do this using rate-based rules and is much better suited to the job than writing your own custom code and running it using Lambda.</p><p><strong>INCORRECT:</strong> \"Configure an IP set match rule on AWS WAF that blocks web requests based on the IP address of the web request origin\" is incorrect.</p><p>An IP set match rule uses a list of known IP addresses. With a DDoS attack you don\u2019t know the IP addresses of the bots ahead of time so this would not be effective.</p><p><strong>INCORRECT:</strong> \"Configure field-level encryption for the distribution and upload an SSL/TLS certificate from Amazon Certificate Manager (ACM)\" is incorrect.</p><p>Field level encryption adds protection for certain data in transit and is not useful for protecting against DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>", "answers": ["<p>Configure a Lambda@Edge function that imposes a rate limit on CloudFront viewer requests and blocks traffic that exceeds the limits.</p>", "<p>Configure a rate-based rule on AWS WAF that puts a temporary block on requests from IP addresses that send excessive requests.</p>", "<p>Configure an IP set match rule on AWS WAF that blocks web requests based on the IP address of the web request origin.</p>", "<p>Configure field-level encryption for the distribution and upload an SSL/TLS certificate from Amazon Certificate Manager (ACM).</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Amazon CloudFront is used as the front-end of the application an AWS WAF is used to protect the front-end with the AWS Managed Rules rule group.A security architect is concerned that the infrastructure is vulnerable to layer 7 DDoS attacks. What improvements can be made to the solution to protect against this type of attack?", "related_lectures": []}, {"_class": "assessment", "id": 69669522, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying an application on Amazon EC2 instances with an Amazon RDS MySQL database. The application will store sensitive data and a security engineer has been tasked with recommending measures to protect the sensitive data against security breaches. The solution must minimize operational overhead and credentials must be regularly and automatically rotated.</p><p>Which measures should the security engineer suggest?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The key requirements are to ensure data protection for the application running on EC2 and RDS and to ensure credentials are automatically rotated. For data protection we must ensure data is protected both in transit and at rest.</p><p>At rest encryption can be implemented for Amazon EBS volumes and RDS databases. Whether we use the default AWS managed keys or keys managed in AWS KMS is not important in this scenario if encryption is enabled.</p><p>Encryption in transit is not relevant to Amazon EBS as the volumes are attached at the block level and all data is encrypted in transit by default. For RDS we can use a TLS certificate to protect the data in transit.</p><p>Lastly, rotation of credentials can be implemented by using Secrets Manager which allows RDS database credentials to be rotated automatically without any custom development.</p><p><strong>CORRECT: </strong>\"Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in AWS Secrets Manager and configure automatic rotation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in instance metadata and automatically rotate with AWS Lambda\" is incorrect.</p><p>Credentials should not be stored in instance metadata as this is insecure.</p><p><strong>INCORRECT:</strong> \"Enable Amazon RDS encryption using AWS CloudHSM keys for the database and snapshots. Implement TLS connections to the Amazon EBS volumes and RDS instance. Import the credentials in AWS KMS and enable rotation every 365 days\" is incorrect.</p><p>KMS cannot be used for storing and rotating database credentials, it works only for encryption keys.</p><p><strong>INCORRECT:</strong> \"Enable Amazon RDS encryption using AWS KMS keys for the database and snapshots. Implement encryption at rest for Amazon EBS volumes. Use an ACM certificate for TLS connectivity to the database instance and use Systems Manager Parameter store to automatically rotate credentials\" is incorrect.</p><p>Systems Manager Parameter Store does not automatically rotate credentials, you must write your own Lambda function which increases operational overhead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p>", "answers": ["<p>Enable Amazon RDS encryption using AWS CloudHSM keys for the database and snapshots. Implement TLS connections to the Amazon EBS volumes and RDS instance. Import the credentials in AWS KMS and enable rotation every 365 days.</p>", "<p>Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in AWS Secrets Manager and configure automatic rotation.</p>", "<p>Enable Amazon RDS encryption using AWS KMS keys for the database and snapshots. Implement encryption at rest for Amazon EBS volumes. Use an ACM certificate for TLS connectivity to the database instance and use Systems Manager Parameter store to automatically rotate credentials.</p>", "<p>Implement encryption at rest for the Amazon EBS volumes attached to EC2 instances. Encrypt the RDS database and use a TLS secured connection. Store the database credentials in instance metadata and automatically rotate with AWS Lambda.</p>"]}, "correct_response": ["b"], "section": "Domain 5 - Data Protection", "question_plain": "A company is deploying an application on Amazon EC2 instances with an Amazon RDS MySQL database. The application will store sensitive data and a security engineer has been tasked with recommending measures to protect the sensitive data against security breaches. The solution must minimize operational overhead and credentials must be regularly and automatically rotated.Which measures should the security engineer suggest?", "related_lectures": []}, {"_class": "assessment", "id": 69669524, "assessment_type": "multi-select", "prompt": {"question": "<p>A security team is concerned about a possible vulnerability affecting the instance metadata service. The team requires that all existing and new Amazon EC2 instances must use version 2 of the instance metadata service (IMDSV2).</p><p>Which combination of steps should the security team take to complete the migration to IMDSV2 in the AWS environment? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can access instance metadata from a running instance using one of the following methods:</p><ul><li><p>Instance Metadata Service Version 1 (IMDSv1) \u2013 a request/response method</p></li><li><p>Instance Metadata Service Version 2 (IMDSv2) \u2013 a session-oriented method</p></li></ul><p>By default, you can use either IMDSv1 or IMDSv2, or both. The instance metadata service distinguishes between IMDSv1 and IMDSv2 requests based on whether, for any given request, either the PUT or GET headers, which are unique to IMDSv2, are present in that request.</p><p>You can configure the instance metadata service on each instance such that local code or users must use IMDSv2. When you specify that IMDSv2 must be used, IMDSv1 no longer works.</p><p>IMDSv2 uses session-oriented requests. With session-oriented requests, you create a session token that defines the session duration, which can be a minimum of one second and a maximum of six hours.</p><p>You can require the use of IMDSv2 by setting the HttpTokens parameter to \u201crequired\u201d for the ec2 run instances action and by modifying existing instances using the CLI. Please check the reference link below for more detailed information on transitioning to IMDSv2.</p><p><strong>CORRECT: </strong>\"When using the ec2:runinstances API action set the \u201c--metadata-options HttpTokens\u201d option to \u201crequired\u201d\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Update existing instances using the \u201cec2 modify-instance-metadata-options\u201d commands from the AWS CLI with the \u201cHttpTokens required\u201d option\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update existing instances using the \u201cec2 modify-instance-metadata-options\u201d commands from the AWS CLI with the \u201cHttpEndpoint disabled\u201d option\" is incorrect.</p><p>Setting the endpoint to disabled will prevent the instance metadata service from working completely.</p><p><strong>INCORRECT:</strong> \"Update Network ACLs to deny all inbound traffic to the 169.254.169.254 IP address for the HTTP protocol\" is incorrect.</p><p>This is the IP address on which the instance metadata service runs and is a local address used from within the instance itself. Therefore, updating security groups and network ACLs will not achieve anything.</p><p><strong>INCORRECT:</strong> \"When using the ec2:runinstances API action set the \u201c--metadata-options HttpTokens\u201d option to \u201cdisabled\" is incorrect.</p><p>This is not a valid option and does not achieve the stated objectives.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html</a></p>", "answers": ["<p>When using the ec2:runinstances API action set the \u201c--metadata-options HttpTokens\u201d option to \u201crequired\u201d.</p>", "<p>Update existing instances using the \u201cec2 modify-instance-metadata-options\u201d commands from the AWS CLI with the \u201cHttpEndpoint disabled\u201d option.</p>", "<p>Update existing instances using the \u201cec2 modify-instance-metadata-options\u201d commands from the AWS CLI with the \u201cHttpTokens required\u201d option.</p>", "<p>Update Network ACLs to deny all inbound traffic to the 169.254.169.254 IP address for the HTTP protocol.</p>", "<p>When using the ec2:runinstances API action set the \u201c--metadata-options HttpTokens\u201d option to \u201cdisabled\u201d.</p>"]}, "correct_response": ["a", "c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security team is concerned about a possible vulnerability affecting the instance metadata service. The team requires that all existing and new Amazon EC2 instances must use version 2 of the instance metadata service (IMDSV2).Which combination of steps should the security team take to complete the migration to IMDSV2 in the AWS environment? (Select TWO.)", "related_lectures": []}]}
5750710
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 63654976, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In response to an incident a security engineer locked down an Amazon S3 bucket with a policy that denies access to all users. Subsequently, the engineer attempted to grant access to a forensic analyst. After updating the bucket policy the forensic analyst still cannot access the bucket and is receiving access denied messages.</p><p>What is the most likely explanation for the denial?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>There is an explicit deny in the bucket policy that denies all users. Explicit denies will always override any explicit allow statements so the new update to the policy does not grant access to the forensic analyst</p><p><strong>CORRECT: </strong>\"An explicit deny will always override an explicit allow\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Amazon S3 Block Public Access feature is enabled\" is incorrect.</p><p>The forensic analyst is not attempting to access the bucket anonymously but is using an AWS account, so this is not relevant.</p><p><strong>INCORRECT:</strong> \"An IAM user policy may be denying access to the bucket\" is incorrect.</p><p>That could be the case but the bucket policy is incorrect so the most likely issue is the bucket policy, and this would be the first thing to check.</p><p><strong>INCORRECT:</strong> \"S3 is eventually consistent, and the changes are not yet updated\" is incorrect.</p><p>This is not true, S3 is strongly consistent for all operations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>", "answers": ["<p>The Amazon S3 Block Public Access feature is enabled.</p>", "<p>An IAM user policy may be denying access to the bucket.</p>", "<p>S3 is eventually consistent, and the changes are not yet updated.</p>", "<p>An explicit deny will always override an explicit allow.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "In response to an incident a security engineer locked down an Amazon S3 bucket with a policy that denies access to all users. Subsequently, the engineer attempted to grant access to a forensic analyst. After updating the bucket policy the forensic analyst still cannot access the bucket and is receiving access denied messages.What is the most likely explanation for the denial?", "related_lectures": []}, {"_class": "assessment", "id": 63654978, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has thousands of employees that use a single Microsoft Active Directory on-premises identity provider. The company is deploying several dozen AWS accounts and needs to provide its employees with access to the AWS accounts. The solution should maximize scalability and operational efficiency.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>There are several ways you can federate with existing identity providers and establish trust relationships with existing identity providers. The key to answering this question correctly is to determine which options are workable AND represent the simplest solutions.</p><p>The best option is to use a combination of AWS Control Tower for centralized management of many AWS accounts and AWS SSO for single sign-on leveraging the existing identity provider. This would be by far the simplest solution in terms of scalability and operational efficiency.</p><p><strong>CORRECT: </strong>\"Create a landing zone using AWS Control Tower. Integrate AWS Single Sign-On (SSO) with the company\u2019s existing identity provider. Grant Active Directory users access to accounts and applications\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement an AD connector in each AWS account that redirects requests to the on-premises Active Directory. Grant permissions to AWS resources using IAM role-based access control\" is incorrect.</p><p>This would require many AD connectors and complex permissions policies and would not be efficient.</p><p><strong>INCORRECT:</strong> \"Create a centralized account with IAM roles that employees can assume through federation with their existing identity provider. Establish trust relationships between the central account and the resource accounts\" is incorrect.</p><p>Trust relationships are not something you establish between AWS accounts; this is a term used in relation to Active Directory. Roles can be assumed across accounts instead.</p><p><strong>INCORRECT:</strong> \"Within each AWS account, create dedicated IAM users that employees can assume through federation based upon group membership in the existing Active Directory identity provider\" is incorrect.</p><p>This would be highly complex for both administration and usability.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html</a></p><p><a href=\"https://aws.amazon.com/controltower/features/\">https://aws.amazon.com/controltower/features/</a></p>", "answers": ["<p>Create a centralized account with IAM roles that employees can assume through federation with their existing identity provider. Establish trust relationships between the central account and the resource accounts.</p>", "<p>Within each AWS account, create dedicated IAM users that employees can assume through federation based upon group membership in the existing Active Directory identity provider.</p>", "<p>Create a landing zone using AWS Control Tower. Integrate AWS Single Sign-On (SSO) with the company\u2019s existing identity provider. Grant Active Directory users access to accounts and applications.</p>", "<p>Implement an AD connector in each AWS account that redirects requests to the on-premises Active Directory. Grant permissions to AWS resources using IAM role-based access control.</p>"]}, "correct_response": ["c"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has thousands of employees that use a single Microsoft Active Directory on-premises identity provider. The company is deploying several dozen AWS accounts and needs to provide its employees with access to the AWS accounts. The solution should maximize scalability and operational efficiency.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63654980, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company requires that all traffic to a specific application is captured and inspected for network and security anomalies. The application runs on several Amazon EC2 instances. The detection software has been installed on an intrusion detection instance running on EC2.</p><p>What should a security engineer do next to route traffic to the intrusion detection instance?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. You can then send the traffic to out-of-band security and monitoring appliances for:</p><ul><li><p>Content inspection</p></li><li><p>Threat monitoring</p></li><li><p>Troubleshooting</p></li></ul><p>The security and monitoring appliances can be deployed as individual instances, or as a fleet of instances behind a Network Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation, so that you only extract the traffic of interest to monitor by using monitoring tools of your choice.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_01-45-41-29d5cf5b4c0baf12b0baf99559c4beb6.jpg\"><p><strong>CORRECT: </strong>\"Configure VPC traffic mirroring to send traffic to the intrusion detection EC2 instance using a Network Load Balancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Disable source/destination checks on the Amazon EC2 instances and enable VPC Flow Logs on the ENIs\" is incorrect.</p><p>Disabling source/destination checks is required for NAT instances but is not a step required to setup traffic mirroring. VPC Flow Logs can capture log information relating to traffic flows but not the entire packet.</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to capture and inspect traffic and trigger an AWS Lambda function to send route anomalous traffic to the EC2 instance\" is incorrect.</p><p>Amazon Inspector does not perform traffic capturing.</p><p><strong>INCORRECT:</strong> \"Configure VPC Flow Logs at the VPC level and write logs to Amazon S3. Use event notifications to trigger an AWS Lambda function to inspect the logs\" is incorrect.</p><p>VPC Flow Logs can capture log information relating to traffic flows but not the entire packet so will not be sufficient for intrusion detection. There is also not solution for sending the traffic to the intrusion detection instance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html</a></p>", "answers": ["<p>Disable source/destination checks on the Amazon EC2 instances and enable VPC Flow Logs on the ENIs.</p>", "<p>Configure VPC traffic mirroring to send traffic to the intrusion detection EC2 instance using a Network Load Balancer.</p>", "<p>Use Amazon Inspector to capture and inspect traffic and trigger an AWS Lambda function to send route anomalous traffic to the EC2 instance.</p>", "<p>Configure VPC Flow Logs at the VPC level and write logs to Amazon S3. Use event notifications to trigger an AWS Lambda function to inspect the logs.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company requires that all traffic to a specific application is captured and inspected for network and security anomalies. The application runs on several Amazon EC2 instances. The detection software has been installed on an intrusion detection instance running on EC2.What should a security engineer do next to route traffic to the intrusion detection instance?", "related_lectures": []}, {"_class": "assessment", "id": 63654982, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has deployed an organization in AWS Organizations with several member accounts. The security team requires that there is at least on AWS CloudTrail trail configured for all existing accounts and any accounts that are created in the future. The logs should be sent to a single centralized Amazon S3 bucket and administrators in member accounts should not be able to modify the configuration.</p><p>Which actions should be taken to accomplish this?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>If you have created an organization in AWS Organizations, you can create a trail that will log all events for all AWS accounts in that organization. This is sometimes referred to as an <em>organization trail</em>.</p><p>You can also choose to edit an existing trail in the management account and apply it to an organization, making it an organization trail. Organization trails log events for the management account and all member accounts in the organization.</p><p>When you create an organization trail, a trail with the name that you give it will be created in every AWS account that belongs to your organization. Users with CloudTrail permissions in member accounts will be able to see this trail when they log into the AWS CloudTrail console from their AWS accounts, or when they run AWS CLI commands such as describe-trail.</p><p>However, users in member accounts will not have sufficient permissions to delete the organization trail, turn logging on or off, change what types of events are logged, or otherwise alter the organization trail in any way.</p><p><strong>CORRECT: </strong>\"Create an organization trail in the management account and specify a central S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new trail in each account and use a cross-account role to log to a central S3 bucket\" is incorrect.</p><p>An organization trail is easier to implement and cannot be edited in member accounts which is more secure.</p><p><strong>INCORRECT:</strong> \"Enable CloudTrail Insights in the management account and run the \u201caws cloudtrail lookup-events\u201d CLI command\" is incorrect.</p><p>CloudTrail Insights detects unusual API or error rate activity. This is not used for capturing and logging API events.</p><p><strong>INCORRECT:</strong> \"Create a new trail in each account and use an SCP to deny the \u201ccloudtrail:Delete\u201d API action\" is incorrect.</p><p>An organization trail is easier to implement and cannot be edited in member accounts which is more secure.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>", "answers": ["<p>Create a new trail in each account and use a cross-account role to log to a central S3 bucket.</p>", "<p>Enable CloudTrail Insights in the management account and run the \u201caws cloudtrail lookup-events\u201d CLI command.</p>", "<p>Create a new trail in each account and use an SCP to deny the \u201ccloudtrail:Delete\u201d API action.</p>", "<p>Create an organization trail in the management account and specify a central S3 bucket.</p>"]}, "correct_response": ["d"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company has deployed an organization in AWS Organizations with several member accounts. The security team requires that there is at least on AWS CloudTrail trail configured for all existing accounts and any accounts that are created in the future. The logs should be sent to a single centralized Amazon S3 bucket and administrators in member accounts should not be able to modify the configuration.Which actions should be taken to accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 63654984, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A developer who was recently fired by a company has a personal laptop that contains the SSH keys used to access multiple Amazon EC2 instances. The security team need to ensure the developer is unable to access the EC2 instances.</p><p>How can a security engineer protect the running EC2 instances?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can change the key pair that is used to access the default system account of your instance by adding a new public key on the instance, or by replacing the public key (deleting the existing public key and adding a new one) on the instance. You might do this for the following reasons:</p><ul><li><p>If a user in your organization requires access to the system user account using a separate key pair, you can add the public key to your instance.</p></li><li><p>If someone has a copy of the private key (.pem file) and you want to prevent them from connecting to your instance (for example, if they've left your organization), you can delete the public key on the instance and replace it with a new one.</p></li></ul><p>The public keys are in the .ssh/authorized_keys file on the instance. To replace the key pair, generate a new key pair using the EC2 console and then paste the public key information into the authorized_keys file. The original key information should also be deleted.</p><p><strong>CORRECT: </strong>\" Connect to each EC2 instance and replace the public key information in the authorized_keys file\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Delete the key pair, create a new key pair, and use the EC2 console to modify the EC2 instances to use the new key pair\" is incorrect.</p><p>You cannot use the EC2 console to change the key pair used by an EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use the modify-instance-attribute API to change the key on any EC2 instance that is using the key\" is incorrect.</p><p>You cannot use this API to modify the key pair used by EC2 instances.</p><p><strong>INCORRECT:</strong> \"Modify the key pair in the AMIs used to launch the EC2 instances and then restart the EC2 instances\" is incorrect.</p><p>Changing an AMI does not modify existing instances that were launched from the AMI.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-key-pair</a></p>", "answers": ["<p>Delete the key pair, create a new key pair, and use the EC2 console to modify the EC2 instances to use the new key pair.</p>", "<p>Connect to each EC2 instance and replace the public key information in the authorized_keys file.</p>", "<p>Use the modify-instance-attribute API to change the key on any EC2 instance that is using the key.</p>", "<p>Modify the key pair in the AMIs used to launch the EC2 instances and then restart the EC2 instances.</p>"]}, "correct_response": ["b"], "section": "Domain 1 - Incident Response", "question_plain": "A developer who was recently fired by a company has a personal laptop that contains the SSH keys used to access multiple Amazon EC2 instances. The security team need to ensure the developer is unable to access the EC2 instances.How can a security engineer protect the running EC2 instances?", "related_lectures": []}, {"_class": "assessment", "id": 63654986, "assessment_type": "multi-select", "prompt": {"question": "<p>A bespoke application consisting of three tiers is being deployed in a VPC. You need to create three security groups. You have configured the WebSG (web server) security group and now need to configure the AppSG (application tier) and DBSG (database tier). The application runs on port 1030 and the database runs on 3306.</p><p>Which rules should be created according to security best practice? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With security groups rules are always allow rules. The best practice is to configure the source as another security group which is attached to the EC2 instances that traffic will come from. In this case you need to configure a rule that allows TCP 1030 and configure the source as the web server security group (WebSG).</p><p>This allows traffic from the web servers to reach the application servers. You then need to allow communications on port 3306 (MYSQL/Aurora) from the AppSG security group to enable access to the database from the application servers.</p><p><strong>CORRECT: </strong>\"On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the AppSG security group as the source\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the WebSG security group as the source\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the DBSG security group as the source\" is incorrect.</p><p>The app tier will receive traffic from the web tier.</p><p><strong>INCORRECT:</strong> \"On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the WebSG security group as the source\" is incorrect.</p><p>The databases will be receiving traffic from the app servers.</p><p><strong>INCORRECT:</strong> \"On the WebSG security group, create a custom TCP rule for TCP 1030 and configure the AppSG security group as the source\" is incorrect.</p><p>The web service will be receiving traffic from internet, presumably on standard HTTP/HTTPS ports. This web server security group has already been configured.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>", "answers": ["<p>On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the DBSG security group as the source.</p>", "<p>On the AppSG security group, create a custom TCP rule for TCP 1030 and configure the WebSG security group as the source.</p>", "<p>On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the AppSG security group as the source.</p>", "<p>On the DBSG security group, create a custom TCP rule for TCP 3306 and configure the WebSG security group as the source.</p>", "<p>On the WebSG security group, create a custom TCP rule for TCP 1030 and configure the AppSG security group as the source.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A bespoke application consisting of three tiers is being deployed in a VPC. You need to create three security groups. You have configured the WebSG (web server) security group and now need to configure the AppSG (application tier) and DBSG (database tier). The application runs on port 1030 and the database runs on 3306.Which rules should be created according to security best practice? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63654988, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has several AWS accounts that use a combination of the following identity provider:</p><p>\u00b7 Users in AWS Identity and Access Management (IAM)</p><p>\u00b7 Federated sign-in with Active Directory and IAM</p><p>\u00b7 Users in Amazon Cognito user pools</p><p>The company security team requires that password policies are configured for all identity providers to require a minimum password length and password complexity.</p><p>Which configuration settings should the company update? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>This question simply requires an understanding of where the relevant password policies should be configured:</p><p>\u00b7 When using federation with Active Directory and AWS IAM the user account is created and managed in Active Directory so the password policy should be configured there.</p><p>\u00b7 When using AWS IAM to create user accounts you can specify a password policy in IAM.</p><p>\u00b7 When using Amazon Cognito user pools to create user accounts you can specify a password policy within the user pool.</p><p><strong>CORRECT: </strong>\"Configure a password policy in Active Directory for the federation scenario\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure an IAM password policy for the IAM user scenario\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a password policy in the Amazon Cognito user pool\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an IAM password policy for the federation scenario\" is incorrect.</p><p>In a federation scenario the user account is not in IAM so the password policy in IAM would not affect the users.</p><p><strong>INCORRECT:</strong> \"Configure a password policy in an Amazon Cognito identity pool\" is incorrect.</p><p>An identity pool is different to a user pool and is used to gain temporary security credentials through IAM roles. A user pool can be configured as an identity provider with user accounts that are created within the pool.</p><p><strong>INCORRECT:</strong> \"Configure a password policy in AWS Organizations for the IAM user scenario\" is incorrect.</p><p>AWS Organizations cannot be used for creating password policies for AWS IAM.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-policies.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-policies.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/\">https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/</a></p>", "answers": ["<p>Configure a password policy in Active Directory for the federation scenario.</p>", "<p>Configure an IAM password policy for the federation scenario.</p>", "<p>Configure an IAM password policy for the IAM user scenario.</p>", "<p>Configure a password policy in the Amazon Cognito user pool.</p>", "<p>Configure a password policy in an Amazon Cognito identity pool.</p>", "<p>Configure a password policy in AWS Organizations for the IAM user scenario.</p>"]}, "correct_response": ["a", "c", "d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has several AWS accounts that use a combination of the following identity provider:\u00b7 Users in AWS Identity and Access Management (IAM)\u00b7 Federated sign-in with Active Directory and IAM\u00b7 Users in Amazon Cognito user poolsThe company security team requires that password policies are configured for all identity providers to require a minimum password length and password complexity.Which configuration settings should the company update? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 63654990, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company has an organization in AWS organizations with several member accounts. Amazon S3 buckets are used to store sensitive data backups from common applications within each AWS account. The company needs to restrict users from deleting any S3 buckets or objects across the organization.</p><p>What is the MOST scalable solution that meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most scalable solution is to use a service control policy as this will automatically apply to any additional accounts that are added to the organization. The following example SCP prevents users or roles in any affected account from deleting any S3 bucket or objects.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_01-53-54-bc2c1b333ee26fbd313e29260e1334b2.jpg\"><p><strong>CORRECT: </strong>\"Service Control Policies (SCPs)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Permissions boundaries in AWS IAM\" is incorrect.</p><p>Permissions boundaries are implemented within IAM and therefore must be configured in each AWS account within the organization. This is a much less scalable solution.</p><p><strong>INCORRECT:</strong> \"S3 bucket policies\" is incorrect.</p><p>Bucket policies must be configured on every bucket in every account within the organization. This is also a much less scalable solution.</p><p><strong>INCORRECT:</strong> \"S3 bucket ACLs\" is incorrect.</p><p>Bucket ACLs offer only limited permission options and would also not represent a scalable solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>", "answers": ["<p>Permissions boundaries in AWS IAM</p>", "<p>S3 bucket policies</p>", "<p>S3 bucket ACLs</p>", "<p>Service Control Policies (SCPs)</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A financial services company has an organization in AWS organizations with several member accounts. Amazon S3 buckets are used to store sensitive data backups from common applications within each AWS account. The company needs to restrict users from deleting any S3 buckets or objects across the organization.What is the MOST scalable solution that meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63654992, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A new application requires an AWS KMS key for encrypting sensitive data. The security policy requires that separate keys are used for different AWS services.</p><p>How can the AWS KMS key be constrained to work with only Amazon S3?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The kms:ViaService condition key can be used in a key policy of an AWS KMS key. This condition limits use of an AWS KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key. The operation must be a <em>KMS key resource operation</em>, that is, an operation that is authorized for a particular KMS key.</p><p><strong>CORRECT: </strong>\"Configure the key policy with a kms:ViaService condition key that limits use of the KMS key to the Amazon S3 service name\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the key policy with a <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-grantee-principal\">kms:GranteePrincipal</a> condition key that limits use of the KMS key to the Amazon S3 principal ID\" is incorrect.</p><p>This condition controls access to the CreateGrant operation based on the value of the GranteePrincipal parameter in the request.</p><p><strong>INCORRECT:</strong> \"Configure the key policy with a <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-request-alias\">kms:RequestAlias</a> condition key that limits use of the KMS key to requests that use a specific alias\" is incorrect.</p><p>You can use this condition key to allow an operation only when the request uses a particular alias to identify the KMS key.</p><p><strong>INCORRECT:</strong> \"Configure the key policy with a <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-caller-account\">kms:CallerAccount</a> condition key that limits use of the KMS key to identities in the specific AWS account\" is incorrect.</p><p>You can use this condition key to allow or deny access to all identities (IAM users and roles) in an AWS account. This does not assist with limiting use of the KMS key to specific AWS services.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html\">https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html</a></p>", "answers": ["<p>Configure the key policy with a kms:ViaService condition key that limits use of the KMS key to the Amazon S3 service name.</p>", "<p>Configure the key policy with a kms:GranteePrincipal condition key that limits use of the KMS key to the Amazon S3 principal ID.</p>", "<p>Configure the key policy with a kms:RequestAlias condition key that limits use of the KMS key to requests that use a specific alias.</p>", "<p>Configure the key policy with a kms:CallerAccount condition key that limits use of the KMS key to identities in the specific AWS account.</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A new application requires an AWS KMS key for encrypting sensitive data. The security policy requires that separate keys are used for different AWS services.How can the AWS KMS key be constrained to work with only Amazon S3?", "related_lectures": []}, {"_class": "assessment", "id": 63654994, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs an application behind an Application Load Balancer (ALB). A security engineer has noticed many suspicious HTTP requests hitting the ALB. There is an Amazon CloudFront distribution in front of the ALB. Users are reporting performance problems.</p><p>A security engineer discovers that the website is receiving a high rate of unwanted requests to the CloudFront distribution originating from a series of source IP addresses.</p><p>How should the security engineer address this problem with the LEAST effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.</p><p>Rate-based Rules are a new type of Rule that can be configured in AWS WAF. This feature allows you to specify the number of web requests that are allowed by a client IP in a trailing, continuously updated, 5-minute period. If an IP address breaches the configured limit, new requests will be blocked until the request rate falls below the configured threshold.</p><p><strong>CORRECT: </strong>\"Create an AWS WAF rate-based rule to block this traffic when it exceeds a defined threshold\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution to cache content and automatically block access to the suspicious source IP addresses\" is incorrect.</p><p>CloudFront cannot automatically detect malicious traffic and block access to the source IP addresses.</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to analyze network activity, detect anomalies, and trigger a Lambda function to prevent access\" is incorrect.</p><p>GuardDuty can analyze logs and network flows and trigger a function to perform remediation. However, this does not represent the option that requires the least effort as you would need to write the Lambda function.</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda to analyze a VPC Flow Log, detect the suspicious traffic, and block the IP address in the security groups\" is incorrect.</p><p>This is not the option that requires the least effort (must write a Lambda function), and you cannot block IP addresses in security groups (they do not support deny rules).<strong>References:</strong></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/faq/\">https://aws.amazon.com/waf/faq/</a></p>", "answers": ["<p>Create an Amazon CloudFront distribution to cache content and automatically block access to the suspicious source IP addresses.</p>", "<p>Use Amazon GuardDuty to analyze network activity, detect anomalies, and trigger a Lambda function to prevent access.</p>", "<p>Use AWS Lambda to analyze a VPC Flow Log, detect the suspicious traffic, and block the IP address in the security groups.</p>", "<p>Create an AWS WAF rate-based rule to block this traffic when it exceeds a defined threshold.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A company runs an application behind an Application Load Balancer (ALB). A security engineer has noticed many suspicious HTTP requests hitting the ALB. There is an Amazon CloudFront distribution in front of the ALB. Users are reporting performance problems.A security engineer discovers that the website is receiving a high rate of unwanted requests to the CloudFront distribution originating from a series of source IP addresses.How should the security engineer address this problem with the LEAST effort?", "related_lectures": []}, {"_class": "assessment", "id": 63654996, "assessment_type": "multi-select", "prompt": {"question": "<p>A developer is deploying a website hosted in an Amazon S3 bucket. An Amazon CloudFront distribution will be deployed in front of the S3 bucket to cache the content. The developer requires that users may only access the website using the CloudFront distribution and should not be able to access the website directly by using the S3 URL.</p><p>Which configurations should a security engineer make to support these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To restrict access to content that you serve from Amazon S3 buckets you can create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.</p><p>You must then configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. This also ensures that users can\u2019t use a direct URL to the S3 bucket to access its contents.</p><p>The diagram below depicts this scenario.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_01-58-12-6e9e8198a3998a91bd83de6a835f942f.jpg\"><p><strong>CORRECT: </strong>\"Create an origin access identity (OAI) and associate it with the CloudFront distribution\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the S3 bucket permissions so that only the origin access identity can access the bucket contents\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement a condition in the S3 bucket policy that limits access to \"Principal\": \"cloudfront.amazonaws.com\" is incorrect.</p><p>An OAI should be used and referenced in the policy condition rather than the principal for the AWS service.</p><p><strong>INCORRECT:</strong> \"Configure a gateway endpoint for the S3 bucket and attach the CloudFront distribution to the VPC\" is incorrect.</p><p>You cannot attach a CloudFront distribution to a VPC, so this does not work.</p><p><strong>INCORRECT:</strong> \"Configure CloudFront to add a custom HTTP header to requests for the S3 bucket and configure the bucket to only accept requests with the custom header\" is incorrect.</p><p>This technique should be used with elastic load balancers and does not work with S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html</a></p>", "answers": ["<p>Implement a condition in the S3 bucket policy that limits access to \"Principal\": \"cloudfront.amazonaws.com\".</p>", "<p>Create an origin access identity (OAI) and associate it with the CloudFront distribution.</p>", "<p>Configure a gateway endpoint for the S3 bucket and attach the CloudFront distribution to the VPC.</p>", "<p>Configure the S3 bucket permissions so that only the origin access identity can access the bucket contents.</p>", "<p>Configure CloudFront to add a custom HTTP header to requests for the S3 bucket and configure the bucket to only accept requests with the custom header.</p>"]}, "correct_response": ["b", "d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A developer is deploying a website hosted in an Amazon S3 bucket. An Amazon CloudFront distribution will be deployed in front of the S3 bucket to cache the content. The developer requires that users may only access the website using the CloudFront distribution and should not be able to access the website directly by using the S3 URL.Which configurations should a security engineer make to support these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63654998, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company manages all access to Amazon S3 buckets using identity-based policies. A security engineer needs to receive an alert if any user adds a bucket policy to any Amazon S3 bucket.</p><p>Which approach meets the requirements MOST efficiently?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon EventBridge is a serverless event bus for building event-driven applications. In this scenario the EventBridge rule can be configured to monitor API calls being recorded in AWS CloudTrail that relate to policies being applied to an Amazon S3 bucket. EventBridge can then generate a notification using Amazon SNS.</p><p>A diagram depicting this solution is provided below and includes an example of the event pattern that would be used in the EventBridge rule:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_02-01-28-74393a32d90c744c87cea57686675d94.jpg\"><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule uses the \u201cAWS API Call via CloudTrail\u201d event source and the \u201cs3:PutBucketPolicy\u201d event pattern. Generate an alert using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule uses the \u201cAWS CloudWatch Alarm State Change\u201d event source and the \u201cs3:PutBucketPolicy\u201d event pattern. Generate an alert using Amazon SNS\" is incorrect.</p><p>The incorrect event source is used in this answer. The event source should be \u201cAWS API Call via CloudTrail\u201d.</p><p><strong>INCORRECT:</strong> \"Set up a rule in AWS Config to trigger based on Amazon S3 bucket change events. Trigger an AWS Lambda function to process the events. Generate an alert using Amazon SNS\" is incorrect.</p><p>This is not the most efficient solution as an AWS Lambda function is not required to process the events.</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to monitor for changes to Amazon S3 bucket policies. Generate an alert using Amazon SNS\" is incorrect.</p><p>Inspector does not monitor for changes to S3 bucket policy configurations.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/\">https://aws.amazon.com/blogs/compute/using-dynamic-amazon-s3-event-handling-with-amazon-eventbridge/</a></p>", "answers": ["<p>Set up a rule in AWS Config to trigger based on Amazon S3 bucket change events. Trigger an AWS Lambda function to process the events. Generate an alert using Amazon SNS.</p>", "<p>Create an Amazon EventBridge rule uses the \u201cAWS API Call via CloudTrail\u201d event source and the \u201cs3:PutBucketPolicy\u201d event pattern. Generate an alert using Amazon SNS.</p>", "<p>Use Amazon Inspector to monitor for changes to Amazon S3 bucket policies. Generate an alert using Amazon SNS.</p>", "<p>Create an Amazon EventBridge rule uses the \u201cAWS CloudWatch Alarm State Change\u201d event source and the \u201cs3:PutBucketPolicy\u201d event pattern. Generate an alert using Amazon SNS.</p>"]}, "correct_response": ["b"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company manages all access to Amazon S3 buckets using identity-based policies. A security engineer needs to receive an alert if any user adds a bucket policy to any Amazon S3 bucket.Which approach meets the requirements MOST efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 63655000, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is extending a secure development environment from an on-premises data center into AWS. They have secured the VPC by removing the Internet Gateway and configuring security groups and network ACLs. An AWS Direct Connect connection has been established between the data center and the Amazon VPC.</p><p>What else needs to be done to add encryption in transit?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Direct Connect (DX) does not offer encryption in transit. To encrypt data that is sent over a DX connection you can configure an AWS Virtual Private Network (VPN). To configure a VPN you must first create a virtual private gateway (VGW) which is the AWS side of the VPN connection.</p><p>You can run the VPN across the Direct Connect connection to encrypt all data that traverses the Direct Connect link. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p><p><strong>CORRECT: </strong>\"Setup a Virtual Private Gateway (VGW)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AWS KMS key to the Direct Connect configuration\" is incorrect.</p><p>You cannot enable encryption through AWS Direct Connect.</p><p><strong>INCORRECT:</strong> \"Enable IPSec encryption on the Direct Connect connection\" is incorrect.</p><p>There is no option to enable IPSec encryption on the Direct Connect connection.</p><p><strong>INCORRECT:</strong> \"Configure an AWS Direct Connect Gateway\" is incorrect.</p><p>An AWS Direct Connect Gateway is used to connect to VPCs across multiple AWS regions. It is not involved with encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p>", "answers": ["<p>Add an AWS KMS key to the Direct Connect configuration.</p>", "<p>Enable IPSec encryption on the Direct Connect connection.</p>", "<p>Configure an AWS Direct Connect Gateway.</p>", "<p>Setup a Virtual Private Gateway (VGW).</p>"]}, "correct_response": ["d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company is extending a secure development environment from an on-premises data center into AWS. They have secured the VPC by removing the Internet Gateway and configuring security groups and network ACLs. An AWS Direct Connect connection has been established between the data center and the Amazon VPC.What else needs to be done to add encryption in transit?", "related_lectures": []}, {"_class": "assessment", "id": 63655002, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer is attempting to setup automatic notifications that alert administrators about any changes that are made to an Amazon S3 bucket. The engineer has configured AWS Config and created an SNS topic. Changes have been made to the S3 bucket, but the SNS notifications have not been sent.</p><p>Which combination of steps should the security engineer take to resolve the issue? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>This could be a permissions issue so the security engineer must ensure the correct permissions are configured to allow AWS Config to assume the role assigned to the Config service, write to the S3 bucket, and publish an SNS notification.</p><p>The trust policy on the IAM role assigned to Config must allow \u201cconfig.amazonaws.com\u201d to assume the role. The role must also have PutObject and PutObjectAcl permissions to the S3 bucket.</p><p>For Amazon SNS the access policy must allow \u201cconfig.amazonaws.com\u201d to publish notifications.</p><p><strong>CORRECT: </strong>\"Configure the trust policy on the IAM role AWS Config uses to allow \u201cconfig.amazonaws.com\u201d to assume the role\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the role policy on the IAM role AWS Config uses to allow write access to the Amazon S3 bucket\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the access policy for the Amazon SNS topic to allow \u201csns:publish\u201d access to \u201cconfig.amazonaws.com\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Amazon S3 bucket ACLs to allow AWS Config to record any changes made to the S3 bucket\" is incorrect.</p><p>Bucket ACLs are not used for granting access to AWS Config.</p><p><strong>INCORRECT:</strong> \"Configure the access policy for the Amazon SNS topic to allow \u201csns:write\u201d access to \u201cconfig.amazonaws.com\" is incorrect.</p><p>The SNS:Publish API action should be specified as this will allow Config to publish notifications using SNS.</p><p><strong>INCORRECT:</strong> \"Configure the trust policy on the IAM role AWS Config uses to allow \u201cs3.amazonaws.com\u201d to assume the role\" is incorrect.</p><p>S3 does not assume the role, Config does. Therefore, the principal specified in this answer is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/iamrole-permissions.html\">https://docs.aws.amazon.com/config/latest/developerguide/iamrole-permissions.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/sns-topic-policy.html#required-permissions-snstopic-in-another-account\">https://docs.aws.amazon.com/config/latest/developerguide/sns-topic-policy.html#required-permissions-snstopic-in-another-account</a></p>", "answers": ["<p>Configure the Amazon S3 bucket ACLs to allow AWS Config to record any changes made to the S3 bucket.</p>", "<p>Configure the trust policy on the IAM role AWS Config uses to allow \u201cconfig.amazonaws.com\u201d to assume the role.</p>", "<p>Configure the role policy on the IAM role AWS Config uses to allow write access to the Amazon S3 bucket.</p>", "<p>Configure the access policy for the Amazon SNS topic to allow \u201csns:publish\u201d access to \u201cconfig.amazonaws.com\u201d.</p>", "<p>Configure the access policy for the Amazon SNS topic to allow \u201csns:write\u201d access to \u201cconfig.amazonaws.com\u201d.</p>", "<p>Configure the trust policy on the IAM role AWS Config uses to allow \u201cs3.amazonaws.com\u201d to assume the role.</p>"]}, "correct_response": ["b", "c", "d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A security engineer is attempting to setup automatic notifications that alert administrators about any changes that are made to an Amazon S3 bucket. The engineer has configured AWS Config and created an SNS topic. Changes have been made to the S3 bucket, but the SNS notifications have not been sent.Which combination of steps should the security engineer take to resolve the issue? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 63655004, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has four private subnets within a VPC. Two of the subnets are used for running database instances and the other two are used for application instances. Separate route tables are used for the database and application subnets. A NAT gateway is defined in the route tables to provide internet connectivity for the subnets.</p><p>The security team requires that the database subnets should not have internet access. A security engineer must remove internet connectivity for the database subnets without affecting the application subnets.</p><p>Which approach should the security engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>As seen in the diagram below, the NAT gateway is deployed in a public subnet and the route tables of private subnets are updated with a route pointing to the NAT gateway for all traffic for which another more specific route is not defined.</p><p>Therefore, the only change that needs to be made is to remove the route table entry for the NAT gateway from the route table of the private subnets in which the database instances are running.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_02-09-04-025c259cad29b0f883baadd67aac5394.jpg\"><p><strong>CORRECT: </strong>\"Modify the route table of the database subnets to remove the default route to the NAT gateway\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Remove the existing NAT gateway. Create a new NAT gateway that only the application subnets can use\" is incorrect.</p><p>There is no need to do this as only the route table needs to be updated.</p><p><strong>INCORRECT:</strong> \"Configure the database subnets\u2019 inbound network ACL to deny traffic from the security group ID of the NAT gateway\" is incorrect.</p><p>You cannot deny access based on a security group ID within a network ACL.</p><p><strong>INCORRECT:</strong> \"Configure the route table of the NAT gateway to deny connections to the database subnets\" is incorrect.</p><p>You cannot configure deny rules in a route table.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>", "answers": ["<p>Remove the existing NAT gateway. Create a new NAT gateway that only the application subnets can use.</p>", "<p>Configure the database subnets\u2019 inbound network ACL to deny traffic from the security group ID of the NAT gateway.</p>", "<p>Configure the route table of the NAT gateway to deny connections to the database subnets.</p>", "<p>Modify the route table of the database subnets to remove the default route to the NAT gateway.</p>"]}, "correct_response": ["d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company has four private subnets within a VPC. Two of the subnets are used for running database instances and the other two are used for application instances. Separate route tables are used for the database and application subnets. A NAT gateway is defined in the route tables to provide internet connectivity for the subnets.The security team requires that the database subnets should not have internet access. A security engineer must remove internet connectivity for the database subnets without affecting the application subnets.Which approach should the security engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 63655006, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer has deployed several custom-built images provided by the development team using Amazon Elastic Container Service (ECS) with the Fargate launch type. The engineer now needs to aggregate the logs from all the containers into a pre-existing CloudWatch log group.</p><p>Which solution will satisfy these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The Fargate launch type supports the awslogs log driver. You need to specify the awslogs-group (CloudWatch log group name) and awslogs-region (AWS Region of the log group) parameters in the LogConfiguration property in the task definition for Amazon ECS.</p><p><strong>CORRECT: </strong>\"Enable the awslogs log driver by including awslogs-group and awslogs-region parameters in the LogConfiguration property\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install and configure the CloudWatch agent on the running container instances\" is incorrect.</p><p>The CloudWatch agent cannot be installed directly on the containers running on AWS Fargate. Instead, the awslogs log driver needs to be used.</p><p><strong>INCORRECT:</strong> \"Implement Fluent Bit and FluentD in a DaemonSet configuration to direct logs to Amazon CloudWatch Logs\" is incorrect.</p><p>While Fluent Bit and FluentD are often used for log aggregation in Kubernetes environments, Amazon ECS with Fargate doesn't natively support this type of configuration. Instead, you would use the awslogs driver in your task definition.</p><p><strong>INCORRECT:</strong> \"Define an IAM policy that encompasses the logs:CreateLogGroup action and assign this policy to the running container instances\" is incorrect.</p><p>Just providing an IAM policy with the logs:CreateLogGroup action to the running container instances would not be enough. The task definition needs to be configured to use the awslogs log driver to send logs to CloudWatch Logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p>", "answers": ["<p>Enable the awslogs log driver by including awslogs-group and awslogs-region parameters in the LogConfiguration property.</p>", "<p>Install and configure the CloudWatch agent on the running container instances.</p>", "<p>Implement Fluent Bit and FluentD in a DaemonSet configuration to direct logs to Amazon CloudWatch Logs.</p>", "<p>Define an IAM policy that encompasses the logs:CreateLogGroup action and assign this policy to the running container instances.</p>"]}, "correct_response": ["a"], "section": "Domain 6: Management and Security Governance", "question_plain": "A DevOps engineer has deployed several custom-built images provided by the development team using Amazon Elastic Container Service (ECS) with the Fargate launch type. The engineer now needs to aggregate the logs from all the containers into a pre-existing CloudWatch log group.Which solution will satisfy these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655008, "assessment_type": "multi-select", "prompt": {"question": "<p>A financial institution uses Amazon API Gateway to provide REST APIs for their mobile application. A data analyst wants to study the usage patterns of the APIs without having to sift through log files.</p><p>Which pair of actions will fulfill these requirements with minimal effort? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Enabling access logging for the API stage in question allows for the collection of detailed API Gateway logs, which would aid in the analysis of API usage patterns.</p><p>Amazon CloudWatch Logs Insights enables you to explore, analyze, and visualize your logs instantly. This is a good solution for analyzing API usage data as it doesn't require parsing log files manually.</p><p><strong>CORRECT: </strong>\"Enable access logging for the appropriate API stage\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use Amazon CloudWatch Logs Insights for analyzing API usage data\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up an AWS CloudTrail trail destination for API Gateway events. Filter based on the userIdentity, userAgent, and sourceIPAddress fields\" is incorrect.</p><p>AWS CloudTrail primarily focuses on auditing and tracking API calls across AWS services for governance, compliance, operational auditing, and risk auditing. For this specific use case, it's not the most efficient option.</p><p><strong>INCORRECT:</strong> \"Specify an Amazon S3 destination for API Gateway logs. Utilize Amazon Athena to execute queries and analyze the API usage data\" is incorrect.</p><p>This involves extra steps - configuring an S3 bucket for log storage and then running Athena queries on those logs - which does not meet the criterion of least effort.</p><p><strong>INCORRECT:</strong> \"Activate the Enable Detailed CloudWatch Metrics option on the required API stage\" is incorrect.</p><p>While enabling detailed CloudWatch Metrics for the API stage would provide additional data, it wouldn't provide the usage pattern insights required without additional analysis or tools.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html</a></p>", "answers": ["<p>Enable access logging for the appropriate API stage.</p>", "<p>Set up an AWS CloudTrail trail destination for API Gateway events. Filter based on the userIdentity, userAgent, and sourceIPAddress fields.</p>", "<p>Specify an Amazon S3 destination for API Gateway logs. Utilize Amazon Athena to execute queries and analyze the API usage data.</p>", "<p>Use Amazon CloudWatch Logs Insights for analyzing API usage data.</p>", "<p>Activate the Enable Detailed CloudWatch Metrics option on the required API stage.</p>"]}, "correct_response": ["a", "d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A financial institution uses Amazon API Gateway to provide REST APIs for their mobile application. A data analyst wants to study the usage patterns of the APIs without having to sift through log files.Which pair of actions will fulfill these requirements with minimal effort? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655010, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial firm receives a warning from the AWS Trust and Safety team about a potential security threat. An IAM access key linked to an IT administrator seems to have been compromised. This key is employed in an automated process that uses AWS Lambda functions to launch AWS Elastic Beanstalk environments.</p><p>The firm's security engineer is tasked with addressing this security issue, preventing further use of the exposed access key, and bolstering security practices.</p><p>Which of the following steps would be the most appropriate in this scenario?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This solution aligns with AWS's recommended security practices. Instead of static access keys, it employs IAM roles, which provide a secure way to grant permissions to AWS services. Responding directly to the AWS Trust and Safety team with the remediation steps taken demonstrates proactive action towards resolving the issue.</p><p><strong>CORRECT: </strong>\"Disable or delete the compromised IAM access key. Stop using static IAM access keys and instead, create a new IAM role for the Lambda automation process. Assign this role to the AWS Lambda functions. Respond to the AWS Trust and Safety team detailing the remediation actions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Track down the exposed IAM access key and deactivate or remove it. Establish new access keys for the Lambda automation process and integrate them into the system. In the AWS account associated with the exposed key, create a new support case in AWS Support detailing the remediation measures\" is incorrect.</p><p>Although it addresses the immediate issue of the exposed key, it doesn't adhere to best practices as it generates new static access keys, which could also potentially be compromised.</p><p><strong>INCORRECT:</strong> \"Find the exposed IAM access key and remove the associated IAM user. Generate a new access key, store it in AWS Systems Manager Parameter Store, and encrypt it using an AWS KMS customer-managed key. Modify the AWS Lambda functions to fetch the access key from Parameter Store during execution. Log a new support case with AWS Support providing the details of the remediation steps\" is incorrect.</p><p>While this option uses AWS Systems Manager Parameter Store for secret management, it unnecessarily deletes the IAM user. Also, it involves creating a new static access key, which does not align with AWS's best security practices.</p><p><strong>INCORRECT:</strong> \"Deactivate or delete the compromised IAM access key. Generate a new access key and save it as an environment variable within the configuration settings of the automation system's Lambda functions. Respond to the AWS Trust and Safety team detailing the remediation actions taken\" is incorrect.</p><p>Even though this option addresses the immediate issue of the exposed key, storing access keys as environment variables in Lambda functions is not a secure practice.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p>", "answers": ["<p>Track down the exposed IAM access key and deactivate or remove it. Establish new access keys for the Lambda automation process and integrate them into the system. In the AWS account associated with the exposed key, create a new support case in AWS Support detailing the remediation measures.</p>", "<p>Disable or delete the compromised IAM access key. Stop using static IAM access keys and instead, create a new IAM role for the Lambda automation process. Assign this role to the AWS Lambda functions. Respond to the AWS Trust and Safety team detailing the remediation actions.</p>", "<p>Find the exposed IAM access key and remove the associated IAM user. Generate a new access key, store it in AWS Systems Manager Parameter Store, and encrypt it using an AWS KMS customer-managed key. Modify the AWS Lambda functions to fetch the access key from Parameter Store during execution. Log a new support case with AWS Support providing the details of the remediation steps.</p>", "<p>Deactivate or delete the compromised IAM access key. Generate a new access key and save it as an environment variable within the configuration settings of the automation system's Lambda functions. Respond to the AWS Trust and Safety team detailing the remediation actions taken.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A financial firm receives a warning from the AWS Trust and Safety team about a potential security threat. An IAM access key linked to an IT administrator seems to have been compromised. This key is employed in an automated process that uses AWS Lambda functions to launch AWS Elastic Beanstalk environments.The firm's security engineer is tasked with addressing this security issue, preventing further use of the exposed access key, and bolstering security practices.Which of the following steps would be the most appropriate in this scenario?", "related_lectures": []}, {"_class": "assessment", "id": 63655012, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare organization is using Amazon EC2 instances to host an application that stores sensitive patient records. In compliance with healthcare regulations, the organization must restrict access to these records. A system engineer needs to establish a secure connection to the EC2 instances without opening any inbound ports, managing SSH keys, or maintaining bastion hosts.</p><p>The organization also requires that all session activity logs are monitored, stored, and accessible in an encrypted format.</p><p>Which solution would satisfy these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Systems Manager Session Manager provides secure and auditable instance management without the need for bastion hosts, SSH, or open inbound ports. By configuring Amazon CloudWatch logging and enabling the upload of session logs only to encrypted CloudWatch Logs log groups, the organization can satisfy all its requirements.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Session Manager to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is incorrect.</p><p>Amazon Inspector is a security assessment service. It can't be used for connecting to EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed\" is incorrect.</p><p>Amazon GuardDuty is a threat detection service. It can't be used for connecting to EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Session Manager to connect to the EC2 instances. Configure Amazon CloudWatch Logs to record sessions. Choose the option to store session logs for the relevant CloudWatch Logs log groups\" is incorrect.</p><p>While AWS Systems Manager Session Manager is the correct tool to connect to EC2 instances securely, this option does not mention the encryption requirement for CloudWatch Logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>", "answers": ["<p>Use Amazon Inspector to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed.</p>", "<p>Use Amazon GuardDuty to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed.</p>", "<p>Use AWS Systems Manager Session Manager to connect to the EC2 instances. Configure Amazon CloudWatch Logs to record sessions. Choose the option to store session logs for the relevant CloudWatch Logs log groups.</p>", "<p>Use AWS Systems Manager Session Manager to access the EC2 instances. Set up Amazon CloudWatch Logs for session logging. Choose the option to upload session logs and ensure that only encrypted CloudWatch Logs log groups are allowed.</p>"]}, "correct_response": ["d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A healthcare organization is using Amazon EC2 instances to host an application that stores sensitive patient records. In compliance with healthcare regulations, the organization must restrict access to these records. A system engineer needs to establish a secure connection to the EC2 instances without opening any inbound ports, managing SSH keys, or maintaining bastion hosts.The organization also requires that all session activity logs are monitored, stored, and accessible in an encrypted format.Which solution would satisfy these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655014, "assessment_type": "multi-select", "prompt": {"question": "<p>A fintech company has an application that relies on AWS Systems Manager Parameter Store for managing secure string parameters. This is done using the standard tier and an AWS Key Management Service (AWS KMS) custom-managed key for encryption and decryption.</p><p>Upon attempting to modify a parameter, the team has been encountering a series of error messages.</p><p>What might be the causes of these error messages? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The application must have the kms:Encrypt permission for the custom-managed key. Without this permission, error messages will arise when trying to use the key. If the state of the customer-managed key specified in the application is 'Disabled', the application won't be able to use it, leading to error messages.</p><p><strong>CORRECT: </strong>\"The application lacks the kms:Encrypt permission for the custom-managed key\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The state of the customer-managed key specified within the application is set to 'Disabled'\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The custom-managed key is already encrypting another secure string parameter\" is incorrect.</p><p>A customer-managed key can be used to encrypt more than one secure string parameter. This would not lead to the errors.</p><p><strong>INCORRECT:</strong> \"A customer-managed key cannot be used for encryption with a standard tier secure string parameter\" is incorrect.</p><p>A customer-managed key can be used for encryption with standard tier secure string parameters. Therefore, this wouldn't cause the error messages.</p><p><strong>INCORRECT:</strong> \"In the application, the specified customer-managed key uses a key alias as opposed to a key ID\" is incorrect.</p><p>A key alias can be used interchangeably with a key ID. This wouldn't result in the mentioned error messages.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p>", "answers": ["<p>The application lacks the kms:Encrypt permission for the custom-managed key.</p>", "<p>The custom-managed key is already encrypting another secure string parameter.</p>", "<p>A customer-managed key cannot be used for encryption with a standard tier secure string parameter.</p>", "<p>The state of the customer-managed key specified within the application is set to 'Disabled'.</p>", "<p>In the application, the specified customer-managed key uses a key alias as opposed to a key ID.</p>"]}, "correct_response": ["a", "d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A fintech company has an application that relies on AWS Systems Manager Parameter Store for managing secure string parameters. This is done using the standard tier and an AWS Key Management Service (AWS KMS) custom-managed key for encryption and decryption.Upon attempting to modify a parameter, the team has been encountering a series of error messages.What might be the causes of these error messages? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655016, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is archiving sensitive data to Amazon S3 Glacier. A security engineer has created a new vault lock policy for 1 TB of data and called the initiate-vault-lock operation 8 hours ago. When reviewing the policy the security engineer noticed and error that should be corrected.</p><p>What is the MOST cost-effective method of correcting the error?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The initiate-vault-lock operation initiates the vault locking process by doing the following:</p><ul><li><p>Installing a vault lock policy on the specified vault.</p></li><li><p>Setting the lock state of vault lock to InProgress .</p></li><li><p>Returning a lock ID, which is used to complete the vault locking process.</p></li></ul><p>You must complete the vault locking process within 24 hours after the vault lock enters the InProgress state. After the 24-hour window ends, the lock ID expires, the vault automatically exits the InProgress state, and the vault lock policy is removed from the vault.</p><p>You call CompleteVaultLock to complete the vault locking process by setting the state of the vault lock to Locked. You can abort the vault locking process by calling AbortVaultLock. When the vault lock is in the InProgress state you must call AbortVaultLock before you can initiate a new vault lock policy.</p><p>Therefore, the security engineer will need to call the AbortVaultLock operation before updating the policy and can then call the operation to lock the vault again.</p><p><br></p><p><strong>CORRECT: </strong>\"Call the AbortVaultLock operation. Update the policy. Call the initiate-vault-lock operation again\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The policy cannot be updated after the initiate-vault-lock operation has entered the InProgress state\" is incorrect.</p><p>This is not true as explained above.</p><p><strong>INCORRECT:</strong> \"Modify the policy and then call the initiate-vault-lock operation to apply the updated policy\" is incorrect.</p><p>You cannot modify the policy without first calling the AbortVaultLock operation.</p><p><strong>INCORRECT:</strong> \"Copy the data to a new vault and call the initiate-vault-lock operation. Delete the old vault\" is incorrect.</p><p>There is no need to copy data to a new vault and this will be more costly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/glacier/initiate-vault-lock.html\">https://docs.aws.amazon.com/cli/latest/reference/glacier/initiate-vault-lock.html</a></p>", "answers": ["<p>The policy cannot be updated after the initiate-vault-lock operation has entered the InProgress state.</p>", "<p>Modify the policy and then call the initiate-vault-lock operation to apply the updated policy.1. Copy the data to a new vault and call the initiate-vault-lock operation. Delete the old vault.</p>", "<p>Copy the data to a new vault and call the initiate-vault-lock operation. Delete the old vault.</p>", "<p>Call the AbortVaultLock operation. Update the policy. Call the initiate-vault-lock operation again.</p>"]}, "correct_response": ["d"], "section": "Domain 5 - Data Protection", "question_plain": "A company is archiving sensitive data to Amazon S3 Glacier. A security engineer has created a new vault lock policy for 1 TB of data and called the initiate-vault-lock operation 8 hours ago. When reviewing the policy the security engineer noticed and error that should be corrected.What is the MOST cost-effective method of correcting the error?", "related_lectures": []}, {"_class": "assessment", "id": 63655018, "assessment_type": "multi-select", "prompt": {"question": "<p>A security team has requested that all existing and new Amazon RDS databases are encrypted at rest using AWS Key Management Service (KMS) encryption keys. A security engineer must identify which RDS databases are currently unencrypted and devise a plan for enabling encryption.</p><p>Which combination of steps should the security engineer take to accomplish this? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With AWS Config, you can continuously monitor and record configuration changes of your AWS resources. There are several predefined managed rules you can use. The rds-storage-encrypted managed rule checks whether storage encryption is enabled for your RDS DB instances. You can then configure a notification based on the result using Amazon SNS.</p><p>Once the unencrypted databases have been discovered the next task is to enable encryption. The key fact to remember here is that you cannot alter the encryption state of an RDS database after you have deployed it. You also cannot create encrypted replicas from unencrypted instances.</p><p>The only solution is to create a snapshot (which will be unencrypted) and subsequently create an encrypted copy of the snapshot. You can then create a new database instance from the encrypted snapshot. The new database will be encrypted and will have a new endpoint address.</p><p><strong>CORRECT: </strong>\"Create a snapshot of unencrypted databases. Copy the unencrypted snapshots to created encrypted snapshots. Restore the databases from the encrypted snapshots\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Config to detect any existing and new unencrypted databases. Configure an Amazon SNS notification to alert the security team\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable encryption for the Amazon RDS database instances by adding an AWS KMS key to the storage settings\" is incorrect.</p><p>You cannot enable encryption after creation of the database, and this includes for any instances created from the database such as replicas and multi-AZ standby instances.</p><p><strong>INCORRECT:</strong> \"Create an encrypted read replica of unencrypted RDS database instances. Promote the replicas to become primary and terminate the unencrypted database instances\" is incorrect.</p><p>You cannot enable encryption after creation of the database, and this includes for any instances created from the database such as replicas and multi-AZ standby instances.</p><p><strong>INCORRECT:</strong> \"Use AWS System Manager State Manager to detect the RDS database encryption status. Create an Amazon SNS notification to alert the security team\" is incorrect.</p><p>AWS Systems Manager State Manager is used to manage the configuration on EC2 instances and on-premises servers. It does not detect the status of RDS encryption.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html\">https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/rds-storage-encrypted.html\">https://docs.aws.amazon.com/config/latest/developerguide/rds-storage-encrypted.html</a></p>", "answers": ["<p>Enable encryption for the Amazon RDS database instances by adding an AWS KMS key to the storage settings.</p>", "<p>Create a snapshot of unencrypted databases. Copy the unencrypted snapshots to created encrypted snapshots. Restore the databases from the encrypted snapshots.</p>", "<p>Use AWS Config to detect any existing and new unencrypted databases. Configure an Amazon SNS notification to alert the security team.</p>", "<p>Create an encrypted read replica of unencrypted RDS database instances. Promote the replicas to become primary and terminate the unencrypted database instances.</p>", "<p>Use AWS System Manager State Manager to detect the RDS database encryption status. Create an Amazon SNS notification to alert the security team.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 5 - Data Protection", "question_plain": "A security team has requested that all existing and new Amazon RDS databases are encrypted at rest using AWS Key Management Service (KMS) encryption keys. A security engineer must identify which RDS databases are currently unencrypted and devise a plan for enabling encryption.Which combination of steps should the security engineer take to accomplish this? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655020, "assessment_type": "multi-select", "prompt": {"question": "<p>A static website runs on an Amazon EC2 instance. The security engineer has been asked to suggest improvements to mitigate the risk of DDoS attacks.</p><p>Which of the following may assist with this goal? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>There are several ways to mitigate DDoS attacks including using AWS WAF, AWS Shield, and Amazon Route 53. For this scenario the static website can easily be migrated to an Amazon S3 bucket and placed behind a CloudFront distribution. With CloudFront you can attach an AWS WAF Web ACL and also get the advantages of AWS Shield Standard.</p><p><strong>CORRECT: </strong>\"Migrate the static content to an Amazon S3 bucket and create an Amazon CloudFront distribution\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use the AWS Web Application Firewall (WAF) service to inspect and manage web requests\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Place an Application Load Balancer (ALB) in front of the Amazon EC2 instance and create an HTTPS listener\" is incorrect.</p><p>An ALB with an HTTPS listener does not assist with mitigating DDoS attacks, especially when there is only one instance behind the ALB. This could be implemented with Auto Scaling and an ALB which would offer some advantages and you could also then add a Web ACL.</p><p><strong>INCORRECT:</strong> \"Use AWS X-Ray to inspect and analyze traffic going to the Amazon EC2 instances to identify latency\" is incorrect.</p><p>X-Ray is used for tracing and debugging applications and does not assist with mitigating DDoS attacks.</p><p><strong>INCORRECT:</strong> \"Update the security group attached to the Amazon EC2 instance to block inbound traffic\" is incorrect.</p><p>Security groups do not support deny rules so are not useful for blocking addresses that are involved with a DDoS attack. A network ACL would be more useful here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/\">https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-against-ddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/</a></p>", "answers": ["<p>Place an Application Load Balancer (ALB) in front of the Amazon EC2 instance and create an HTTPS listener.</p>", "<p>Migrate the static content to an Amazon S3 bucket and create an Amazon CloudFront distribution.</p>", "<p>Use AWS X-Ray to inspect and analyze traffic going to the Amazon EC2 instances to identify latency.</p>", "<p>Use the AWS Web Application Firewall (WAF) service to inspect and manage web requests.</p>", "<p>Update the security group attached to the Amazon EC2 instance to block inbound traffic.</p>"]}, "correct_response": ["b", "d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A static website runs on an Amazon EC2 instance. The security engineer has been asked to suggest improvements to mitigate the risk of DDoS attacks.Which of the following may assist with this goal? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655022, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company requires data encryption for sensitive data. The security has requested that the solution must allow cryptographic erasure of all resources protected by the encryption key within 15 minutes.</p><p>Which AWS Key Management Service (AWS KMS) key solution will allow the security engineer to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Note that the term CMK which may appear in the exam has been replaced with the term KMS key though the concepts are identical.</p><p>Cryptographic erasure is when the encryption material used to encrypt the data is deleted. This results in the data being unrecoverable as it cannot be decrypted. A security team may use this technique if the encryption keys used to encrypt data have been compromised. Of course, you would want to ensure that the key materials are backed up offline so you can perform a restore of the data.</p><p>In this case the only option available that will meet the requirements is to import key material into an AWS KMS key. You cannot immediately delete a KMS key; you must schedule it for deletion with a waiting period of a minimum of 7 days. With imported key material you can speed up the process by deleting the key material which renders the KMS key unusable. This effect is immediate.</p><p><strong>CORRECT: </strong>\"Use imported key material with an AWS KMS key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS KMS customer managed KMS key\" is incorrect.</p><p>The default is to use AWS generated key material and in this case you must schedule the deletion of the key with a minimum waiting period of 7 days.</p><p><strong>INCORRECT:</strong> \"Use an AWS managed KMS key\" is incorrect.</p><p>You cannot manage or delete these keys.</p><p><strong>INCORRECT:</strong> \"Use an AWS owned KMS key\" is incorrect.</p><p>You have no ability to manage or even use AWS owned KMS keys.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-delete-key-material.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-delete-key-material.html</a></p>", "answers": ["<p>Use imported key material with an AWS KMS key.</p>", "<p>Use an AWS owned KMS key.</p>", "<p>Use an AWS managed KMS key.</p>", "<p>Use an AWS KMS customer managed KMS key.</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A company requires data encryption for sensitive data. The security has requested that the solution must allow cryptographic erasure of all resources protected by the encryption key within 15 minutes.Which AWS Key Management Service (AWS KMS) key solution will allow the security engineer to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655024, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has multiple accounts that are managed using AWS Organizations. A security engineer must setup a shared S3 bucket in a central account and grant read-only access for all users in any account within the AWS Organization. There should be no public access to the S3 bucket data.</p><p>Which parameters should the security engineer use to accomplish this goal MOST efficiently?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use a condition key, aws:PrincipalOrgID, in policies to require all principals accessing the resource to be from an account (including the master account) in the organization. To set this up for this scenario you must specify '*' as the principal, to allow any user access, and then restrict only to users within the AWS Organization using the condition key. The aws:PrincipalOrgId condition key should be used with the organization ID value specified.</p><p>The example policy below could be used for this scenario (allows s3:GetObject only):</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_16-24-26-62807ca108ffe0c329c2c6687828ecac.jpg\"><p><strong>CORRECT: </strong>\"Specify '*' as the principal and aws:PrincipalOrgld as a condition\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Specify aws:PrincipalOrgld as the principal with the organization ID value\" is incorrect.</p><p>The value mentioned is used in a condition, not in the principal.</p><p><strong>INCORRECT:</strong> \"Specify all account numbers within an array as the principal\" is incorrect.</p><p>This is less efficient as you must specify all account numbers and you must come back and add account numbers if new accounts are added to the organization.</p><p><strong>INCORRECT:</strong> \"Specify the organization's master account as the principal\" is incorrect.</p><p>This is the not the correct method and will not grant access for users from other accounts within the organization.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/\">https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/</a></p>", "answers": ["<p>1. Specify all account numbers within an array as the principal.</p>", "<p>1. Specify '*' as the principal and aws:PrincipalOrgld as a condition.</p>", "<p>1. Specify the organization's master account as the principal.</p>", "<p>Specify aws:PrincipalOrgld as the principal with the organization ID value.</p>"]}, "correct_response": ["b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has multiple accounts that are managed using AWS Organizations. A security engineer must setup a shared S3 bucket in a central account and grant read-only access for all users in any account within the AWS Organization. There should be no public access to the S3 bucket data.Which parameters should the security engineer use to accomplish this goal MOST efficiently?", "related_lectures": []}]}
5750712
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 63655028, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A new employee is joining a security team. The employee initially requires access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. All security team members are added to the security team IAM group that provides additional permissions to manage all other AWS services.</p><p>The team lead wants to limit the permissions the new employee has access to until the employee takes on additional responsibilities, and then be able to easily add permissions as required, eventually providing the same access as all other security team employees.</p><p>How can the team lead limit the permissions assigned to the new user account whilst minimizing complexity?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS supports <em>permissions boundaries</em> for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p><p>In this case the permissions boundary means the user can remain in the security team IAM group. This will minimize complexity and set the configuration up for the future state where the user will have access to all privileges assigned to that group. In the meantime whilst the employee has limited responsibilities the permissions boundary can be used to limit the maximum available permissions.</p><p>This scenario is easy to implement and manage as a single policy statement can be updated with additional service permissions when required.</p><p>The diagram below depicts a similar scenario:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_05-14-13-7140cadf2ce30aecdeded141f8e92381.jpg\"><p><strong>CORRECT: </strong>\"Create an IAM account for the new employee and add the account to the security team IAM group. Set a permissions boundary that grants access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add the additional services to the permissions boundary IAM policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM account for the new employee. Create a new IAM group for the employee and add a permissions policy that grants access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add the additional services to the IAM policy\" is incorrect.</p><p>This will have the desired effect but will increase complexity as the employee must be moved to a separate IAM group to other team members until all permissions are assigned to the user and then moved back over.</p><p><strong>INCORRECT:</strong> \"Create an IAM account for the new employee and add the account to the security team IAM group. Use a Service Control Policy (SCP) to limit the maximum available permissions to Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, remove the SCP\" is incorrect.</p><p>SCPs control the maximum available permissions in an AWS account, not to individual user accounts.</p><p><strong>INCORRECT:</strong> \"Create an IAM account for the new employee in a dedicated account. Use cross-account access to manage resources. Limit the permissions on the cross-account access role to only allow management of Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add permissions to the cross-account access IAM role\" is incorrect.</p><p>This adds lots of complexity as the user is created in a separate account to other security team users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>", "answers": ["<p>Create an IAM account for the new employee and add the account to the security team IAM group. Set a permissions boundary that grants access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add the additional services to the permissions boundary IAM policy.</p>", "<p>Create an IAM account for the new employee. Create a new IAM group for the employee and add a permissions policy that grants access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add the additional services to the IAM policy.</p>", "<p>Create an IAM account for the new employee and add the account to the security team IAM group. Use a Service Control Policy (SCP) to limit the maximum available permissions to Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, remove the SCP.</p>", "<p>Create an IAM account for the new employee in a dedicated account. Use cross-account access to manage resources. Limit the permissions on the cross-account access role to only allow management of Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. When the employee takes on new management responsibilities, add permissions to the cross-account access IAM role.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A new employee is joining a security team. The employee initially requires access to manage Amazon DynamoDB, Amazon RDS, and Amazon CloudWatch. All security team members are added to the security team IAM group that provides additional permissions to manage all other AWS services.The team lead wants to limit the permissions the new employee has access to until the employee takes on additional responsibilities, and then be able to easily add permissions as required, eventually providing the same access as all other security team employees.How can the team lead limit the permissions assigned to the new user account whilst minimizing complexity?", "related_lectures": []}, {"_class": "assessment", "id": 63655030, "assessment_type": "multi-select", "prompt": {"question": "<p>An application running on Amazon EC2 instances reads secrets stored in AWS Systems Manager Parameter Store. The application issued GetParameter API calls for secure string parameters and the calls failed.</p><p>Which factors could be the cause of this failure? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To perform a GetParameter API call to a secure string parameter there are two permissions required. Firstly, the IAM role assigned to the EC2 instance must have the permission to execute the ssm:GetParameter API action which is an AWS Systems Manager Parameter Store permission.</p><p>The parameters are stored as secure strings which means they will be encrypted with an AWS KMS key. To read the values they must be decrypted so the EC2 instance will also need the Decrypt API permission which is associated with AWS KMS.</p><p><strong>CORRECT: </strong>\"The IAM role assigned to the EC2 instance profile does not have decrypt permissions on the AWS KMS key used to encrypt the parameter\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The IAM role assigned to the EC2 instance profile does not have permissions to retrieve parameters in Systems Manager Parameter Store\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The IAM role assigned to the EC2 instance profile does not have encrypt permissions on the AWS KMS key used to encrypt the parameter\" is incorrect.</p><p>In this case the instance is attempting to retrieve and therefore decrypt the secure string, not encrypt it.</p><p><strong>INCORRECT:</strong> \"Systems Manager Parameter Store does not have decrypt permissions on the AWS KMS key used to encrypt the parameter\" is incorrect.</p><p>Parameter store is not executing the API action to decrypt the secure string, the EC2 instance is executing the API action.</p><p><strong>INCORRECT:</strong> \"Systems Manager Parameter Store does not have encrypt permissions on the AWS KMS key used to encrypt the parameter\" is incorrect.</p><p>Parameter store is not executing the API action to encrypt or decrypt the secure string, the EC2 instance is executing the API action.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-access.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-access.html</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html\">https://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html</a></p>", "answers": ["<p>The IAM role assigned to the EC2 instance profile does not have decrypt permissions on the AWS KMS key used to encrypt the parameter.</p>", "<p>The IAM role assigned to the EC2 instance profile does not have encrypt permissions on the AWS KMS key used to encrypt the parameter.</p>", "<p>Systems Manager Parameter Store does not have decrypt permissions on the AWS KMS key used to encrypt the parameter.</p>", "<p>Systems Manager Parameter Store does not have encrypt permissions on the AWS KMS key used to encrypt the parameter.</p>", "<p>The IAM role assigned to the EC2 instance profile does not have permissions to retrieve parameters in Systems Manager Parameter Store.</p>"]}, "correct_response": ["a", "e"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An application running on Amazon EC2 instances reads secrets stored in AWS Systems Manager Parameter Store. The application issued GetParameter API calls for secure string parameters and the calls failed.Which factors could be the cause of this failure? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655032, "assessment_type": "multi-select", "prompt": {"question": "<p>A company is using AWS CloudTrail is being used to monitor API calls. An audit revealed that CloudTrail is failing to deliver events to Amazon S3 as expected. A security engineer is attempting to resolve the issue. What initial actions should be taken to allow delivery of CloudTrail events to S3? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS CloudTrail captures API activity and can be configured to store records of activity in an Amazon S3 bucket. If you specify an existing bucket you must make sure that the bucket policy grants AWS CloudTrail the privileges to write objects into the bucket. The specific API action that must be granted is s3:PutObject.</p><p>Additionally, the correct bucket and prefix must be specified in the CloudTrail configuration for this to work correctly. If you modify a prefix then CloudTrail and the bucket policy will need to be updated.</p><p><strong>CORRECT: </strong>\"Verify that the S3 bucket policy grants CloudTrail the s3:PutObject permission\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Verify that the S3 bucket and prefix defined in CloudTrail exists\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Verify that the IAM role used by CloudTrail has access to write to S3\" is incorrect.</p><p>CloudTrail does not use an IAM Role it uses the service principal \u201ccloudtrail.amazonaws.com\u201d.</p><p><strong>INCORRECT:</strong> \"Verify that the S3 bucket ACL grants CloudTrail access to write objects\" is incorrect.</p><p>The bucket ACL cannot grant write access to the CloudTrail service principal; you must use a bucket policy.</p><p><strong>INCORRECT:</strong> \"Verify that versioning is enabled for the s3 bucket\" is incorrect.</p><p>Versioning is not required to be enabled or disabled as it has not relevance here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html</a></p>", "answers": ["<p>Verify that the S3 bucket policy grants CloudTrail the s3:PutObject permission.</p>", "<p>Verify that the IAM role used by CloudTrail has access to write to S3.</p>", "<p>Verify that the S3 bucket ACL grants CloudTrail access to write objects.</p>", "<p>Verify that versioning is enabled for the s3 bucket.</p>", "<p>Verify that the S3 bucket and prefix defined in CloudTrail exists.</p>"]}, "correct_response": ["a", "e"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company is using AWS CloudTrail is being used to monitor API calls. An audit revealed that CloudTrail is failing to deliver events to Amazon S3 as expected. A security engineer is attempting to resolve the issue. What initial actions should be taken to allow delivery of CloudTrail events to S3? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655034, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has created an organization within AWS Organizations. A security engineer created an organizational unit (OU) and moved several AWS accounts into the OU. The Amazon EC2 service is restricted with the following SCP:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_05-24-35-5df426c2532f3c2f6463479f6546a8b1.jpg\"><p>One of the AWS accounts in the OU is used for data analytics and the data analysts require access to Amazon EC2 instances for running analytics software.</p><p>How can the security engineer provide the data analysts with Amazon EC2 access without affecting the other accounts in the OU?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An explicit deny will always override an explicit allow and SCPs apply to all users including root in member accounts. Therefore, the only way to get around the restrictions is to ensure that the SCP does not apply to the data analytics account.</p><p>An easy way to achieve this outcome is to create a new OU that does not have the policy applied to it. The OU must not be beneath the OU with the restrictive policy applied or it will inherit the deny statement.</p><p><strong>CORRECT: </strong>\"Create a new OU without the SCP restricting EC2 access. Move the data analytics account to the new OU\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Move the SCP that denies the EC2 service to the root OU of Organizations to limit the accounts it applies to\" is incorrect.</p><p>This would ensure that the SCP denies EC2 for all accounts within the organization and in all OUs.</p><p><strong>INCORRECT:</strong> \"Instruct the data analysts to login to the data analytics account using root credentials to avoid the restrictions\" is incorrect.</p><p>The root user in member accounts will also be restricted by the SCP.</p><p><strong>INCORRECT:</strong> \"Add an allow statement for the EC2 service to the SCP with a condition that limits it to the data analytics account\" is incorrect.</p><p>An explicit deny will always override an explicit allow. You also cannot use a condition element within an allow statement of an SCP.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html</a></p>", "answers": ["<p>Move the SCP that denies the EC2 service to the root OU of Organizations to limit the accounts it applies to.</p>", "<p>Instruct the data analysts to login to the data analytics account using root credentials to avoid the restrictions.</p>", "<p>Add an allow statement for the EC2 service to the SCP with a condition that limits it to the data analytics account.</p>", "<p>Create a new OU without the SCP restricting EC2 access. Move the data analytics account to the new OU.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has created an organization within AWS Organizations. A security engineer created an organizational unit (OU) and moved several AWS accounts into the OU. The Amazon EC2 service is restricted with the following SCP:One of the AWS accounts in the OU is used for data analytics and the data analysts require access to Amazon EC2 instances for running analytics software.How can the security engineer provide the data analysts with Amazon EC2 access without affecting the other accounts in the OU?", "related_lectures": []}, {"_class": "assessment", "id": 63655036, "assessment_type": "multi-select", "prompt": {"question": "<p>A company requires that only trusted code can be deployed to AWS Lambda functions. A method of validating the integrity of the code should be implemented and developers should not be able to bypass the solution.</p><p>Which combination of steps should a security engineer take to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To verify code integrity, use AWS Signer to create digitally signed code packages for functions and layers. When a user attempts to deploy a code package, Lambda performs validation checks on the code package before accepting the deployment</p><p>You can use IAM to control who can create code signing configurations. Typically, you allow only specific administrative users to have this ability. Additionally, you can set up IAM policies to enforce that developers only create functions that have code signing enabled.</p><p><strong>CORRECT: </strong>\"Use AWS Signer to verify code integrity when code packages are deployed to Lambda\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use IAM policies to enforce that developers can only create functions that have code signing enabled\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AWS Key Management Service (AWS KMS) to encrypt the code and enable automatic key rotation\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Use Amazon S3 to store the code packages and configure default encryption for the S3 bucket\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Use IAM policies to enforce that developers can only deploy code packages with encrypted source code\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-codesigning.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-codesigning.html</a></p>", "answers": ["<p>Use AWS Signer to verify code integrity when code packages are deployed to Lambda.</p>", "<p>Use IAM policies to enforce that developers can only create functions that have code signing enabled.</p>", "<p>Use the AWS Key Management Service (AWS KMS) to encrypt the code and enable automatic key rotation.</p>", "<p>Use Amazon S3 to store the code packages and configure default encryption for the S3 bucket.</p>", "<p>Use IAM policies to enforce that developers can only deploy code packages with encrypted source code.</p>"]}, "correct_response": ["a", "b"], "section": "Domain 5 - Data Protection", "question_plain": "A company requires that only trusted code can be deployed to AWS Lambda functions. A method of validating the integrity of the code should be implemented and developers should not be able to bypass the solution.Which combination of steps should a security engineer take to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655038, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a critical web application running on a fleet of auto scaling Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is associated with an AWS WAF web ACL. The security team has identified suspicious port scans coming from a specific range of internet IP addresses. A security engineer needs to block access from the identified addresses.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A web access control list (web ACL) gives you fine-grained control over all the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, and AWS AppSync resources.</p><p>The IP set match statement inspects the IP address of a web request against a set of IP addresses and address ranges. Use this to allow or block web requests based on the IP addresses that the requests originate from.</p><p>By default, AWS WAF uses the IP address from the web request origin, but you can configure the rule to use an HTTP header like X-Forwarded-For instead. The Block action will deny any requests that originate from the offending IP address range.</p><p><strong>CORRECT: </strong>\"Modify the web ACL with an IP set match rule statement and a block action to deny incoming requests from the IP address range\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Modify the web ACL with a rate-based rule statement and a block action to deny incoming requests from the IP address range\" is incorrect.</p><p>A rate-based rule is used when you want to apply a rule action if the rate of requests exceeds a specified limit.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution in front of the ALB and use geo restrictions to block access from the IP address range\" is incorrect.</p><p>CloudFront geo restriction can be used to restrict access based on geography but not by a specified list of IP addresses.</p><p><strong>INCORRECT:</strong> \"Add a rule to the ALB security group to deny incoming requests from the IP address range\" is incorrect.</p><p>You cannot add deny rules to security groups, use network ACLs instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p>", "answers": ["<p>Modify the web ACL with an IP set match rule statement and a block action to deny incoming requests from the IP address range.</p>", "<p>Modify the web ACL with a rate-based rule statement and a block action to deny incoming requests from the IP address range.</p>", "<p>Create an Amazon CloudFront distribution in front of the ALB and use geo restrictions to block access from the IP address range.</p>", "<p>Add a rule to the ALB security group to deny incoming requests from the IP address range.</p>"]}, "correct_response": ["a"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company has a critical web application running on a fleet of auto scaling Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is associated with an AWS WAF web ACL. The security team has identified suspicious port scans coming from a specific range of internet IP addresses. A security engineer needs to block access from the identified addresses.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655040, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer requires a solution for allowing employees to connect to a command line interface on Amazon EC2 Linux instances without using SSH keys or ports.</p><p>Which solutions meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs).</p><p>You can use either an interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>Session Manager helps you improve your security posture by letting you close SSH ports, freeing you from managing SSH keys and certificates, bastion hosts, and jump boxes.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Session Manager. Grant the IAM user accounts permissions to use Systems Manager Session Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Run Command to open an SSH connection to the EC2 Linux instances. Grant the IAM user accounts permissions to use Run Command\" is incorrect.</p><p>Run Command is used to automate common administrative tasks and perform one-time configuration changes at scale.</p><p><strong>INCORRECT:</strong> \"Use a bastion host EC2 instance in a public subnet. Use the bastion instance to connect to the EC2 Linux instances using an X.509 certificate\" is incorrect.</p><p>X.509 certificates are SSL/TLS certificates and cannot be used for gaining command line access to an EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to store SSH keys. Instruct the employees to use the AWS CLI to retrieve the SSH key and connect to the EC2 Linux instances\" is incorrect.</p><p>Secrets Manager can be used for storing secrets but storing SSH keys does not provide a solution as once retrieved the users would still need to connect via the SSH protocol. The public keys must also be stored on the server.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>", "answers": ["<p>Use a bastion host EC2 instance in a public subnet. Use the bastion instance to connect to the EC2 Linux instances using an X.509 certificate.</p>", "<p>Use AWS Systems Manager Session Manager. Grant the IAM user accounts permissions to use Systems Manager Session Manager.</p>", "<p>Use AWS Secrets Manager to store SSH keys. Instruct the employees to use the AWS CLI to retrieve the SSH key and connect to the EC2 Linux instances.</p>", "<p>Use AWS Systems Manager Run Command to open an SSH connection to the EC2 Linux instances. Grant the IAM user accounts permissions to use Run Command.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer requires a solution for allowing employees to connect to a command line interface on Amazon EC2 Linux instances without using SSH keys or ports.Which solutions meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655042, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer needs to automate SSH key pair management for many Amazon EC2 instances. The security engineer must create a solution that automatically stores and rotates SSH key pairs that are more than 90 days old. There must also be an audit trail of the rotation recorded in an Amazon S3 bucket.</p><p>Which solution that meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Secrets Manager can be used to store secrets, and this includes SSH key pairs. The Secrets Manager API can be used programmatically to rotate secrets using an AWS Lambda function. The Lambda function can be configured to rotate the secrets every 90 days and then deliver them to the EC2 instances.</p><p>Secrets Manager API calls can also be logged for auditing purposes by creating an AWS CloudTrail trail. The trail can be configured to store the log files in an Amazon S3 bucket where they can be viewed at any point in time.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store the SSH key pairs. Create an AWS Lambda function that rotates the SSH keys every 90 days. Create an AWS CloudTrail trail that logs to an S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to store the SSH key pairs. Enable automatic rotation for the SSH keys every 90 days. Enable audit logging in Secrets Manager and select an S3 bucket\" is incorrect.</p><p>Automatic rotation using Secrets Manager only works for select databases and does not work for SSH keys. Auditing is not enabled through Secrets Manager; the activity must be captured using CloudTrail.</p><p><strong>INCORRECT:</strong> \"Use the EC2 dashboard in the AWS Management Console to enable automatic rotation for the SSH keys every 90 days. Create an AWS CloudTrail trail that logs to an S3 bucket\" is incorrect.</p><p>You cannot automatically rotate SSH keys using the EC2 management console.</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudWatch alarm for SSH keys that are more than 90 days old. Invoke an AWS Lambda function that will delete the old SSH key pairs. Enable audit logging for each EC2 instance to an S3 bucket\" is incorrect.</p><p>You cannot create a CloudWatch alarm that checks the age of SSH keys. You also cannot enable audit logging at the EC2 instance level.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/\">https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-securely-store-rotate-ssh-key-pairs/</a></p><p><a href=\"https://aws.amazon.com/secrets-manager/features/\">https://aws.amazon.com/secrets-manager/features/</a></p>", "answers": ["<p>Use AWS Secrets Manager to store the SSH key pairs. Enable automatic rotation for the SSH keys every 90 days. Enable audit logging in Secrets Manager and select an S3 bucket.</p>", "<p>Use the EC2 dashboard in the AWS Management Console to enable automatic rotation for the SSH keys every 90 days. Create an AWS CloudTrail trail that logs to an S3 bucket.</p>", "<p>Use AWS Secrets Manager to store the SSH key pairs. Create an AWS Lambda function that rotates the SSH keys every 90 days. Create an AWS CloudTrail trail that logs to an S3 bucket.</p>", "<p>Configure an Amazon CloudWatch alarm for SSH keys that are more than 90 days old. Invoke an AWS Lambda function that will delete the old SSH key pairs. Enable audit logging for each EC2 instance to an S3 bucket.</p>"]}, "correct_response": ["c"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A security engineer needs to automate SSH key pair management for many Amazon EC2 instances. The security engineer must create a solution that automatically stores and rotates SSH key pairs that are more than 90 days old. There must also be an audit trail of the rotation recorded in an Amazon S3 bucket.Which solution that meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A developer who recently left a company was found to have published many access keys IDs to a public source code repository. A list of the exposed access key IDs has been created. A security engineer needs to quickly identify which users the access key IDs belong to so the credentials can be immediately rotated. The company uses multiple accounts in an AWS Organization.</p><p>Which approach should the security engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can generate and download a <em>credential report</em> that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices.</p><p>You can use credential reports to assist in your auditing and compliance efforts. You can use the report to audit the effects of credential lifecycle requirements, such as password and access key rotation.</p><p>In this case the credential report must be created in each account within the AWS Organization. Then, the reports can be consolidated, and the compromised access key IDs can be identified. Finally, the access key IDs can be rotated.</p><p><strong>CORRECT: </strong>\"Generate a credential report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Generate a credential report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>You cannot create a single credential report for an organization.</p><p><strong>INCORRECT:</strong> \"Generate an IAM Access Analyzer report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>IAM access analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, shared with an external entity. It does not identify access key IDs that may have been compromised.</p><p><strong>INCORRECT:</strong> \"Generate an IAM Access Analyzer report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs\" is incorrect.</p><p>As above, the IAM access analyzer is the wrong tool for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html</a></p>", "answers": ["<p>Generate a credential report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs.</p>", "<p>Generate an IAM Access Analyzer report in the root account in the Organization. Identify the users the access key IDs belong to. Rotate the access key IDs.</p>", "<p>Generate an IAM Access Analyzer report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs.</p>", "<p>Generate a credential report in each account in the Organization. Consolidate the reports and identify the users the access key IDs belong to. Rotate the access key IDs.</p>"]}, "correct_response": ["d"], "section": "Domain 1 - Incident Response", "question_plain": "A developer who recently left a company was found to have published many access keys IDs to a public source code repository. A list of the exposed access key IDs has been created. A security engineer needs to quickly identify which users the access key IDs belong to so the credentials can be immediately rotated. The company uses multiple accounts in an AWS Organization.Which approach should the security engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 63655046, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Amazon EC2 instance requires permissions to read and write data in an Amazon S3 bucket. A security engineer is creating an IAM role that will be assumed by the EC2 instance.</p><p>When creating the role using the AWS CLI create-role command, which policy must be added to allow the instance to assume the role?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The IAM role must have a policy attached that provides the permissions necessary to read and write data in the S3 bucket. Additionally, a trust policy must be attached. This policy defines which principals can assume the role, and under which conditions. This is sometimes referred to as a <em>resource-based policy</em> for the IAM role.</p><p>The example trust policy below allows Amazon EC2 instances to assume the role:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_05-57-31-36a944457eb8dc4c3767ab82cd43ad2a.jpg\"><p><strong>CORRECT: </strong>\"Trust policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Bucket policy\" is incorrect. A bucket policy is applied to a bucket, it does not allow the EC2 instance to assume the IAM role.</p><p><strong>INCORRECT:</strong> \"Inline policy\" is incorrect. Inline policies are permissions policies, not trust policies, that are applied directly to principals.</p><p><strong>INCORRECT:</strong> \"Managed policy\" is incorrect. A managed policy is a permissions policy managed by AWS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/\">https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/</a></p>", "answers": ["<p>Bucket policy</p>", "<p>Inline policy</p>", "<p>Trust policy</p>", "<p>Managed policy</p>"]}, "correct_response": ["c"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An Amazon EC2 instance requires permissions to read and write data in an Amazon S3 bucket. A security engineer is creating an IAM role that will be assumed by the EC2 instance.When creating the role using the AWS CLI create-role command, which policy must be added to allow the instance to assume the role?", "related_lectures": []}, {"_class": "assessment", "id": 63655048, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer received a notification that an administrative user account may have been compromised. The engineer wants to immediately rotate the access key for the user whilst ensuring that applications that use the access key are not affected.</p><p>What is the BEST approach in this situation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The correct procedure for rotating access keys is to first create a second key which you can then use for your applications. Once your applications have been updated to use the new access key you can disable the old access key and test. If the applications continue to work successfully then you can delete the old access key.</p><p><strong>CORRECT: </strong>\"Create a second access key and modify applications to use the new key. Disable the old access key and check applications are working correctly before deleting the old access key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a second access key and modify applications to use the new key. Modify resource security policies to disable permissions for the temporary security credentials associated with the access key\" is incorrect.</p><p>This answer does not include disabling or deleting the old key so it can still be used.</p><p><strong>INCORRECT:</strong> \"Disable and then delete the old access key. Create a new access key and modify the applications to use the new access key\" is incorrect.</p><p>This is not the best approach as the old access key will have been deleted before testing occurs to ensure the applications continue to function.</p><p><strong>INCORRECT:</strong> \"Create a second access key and modify applications to use the new key. Delete the old access key and revoke all active sessions\" is incorrect.</p><p>The old access key should be disabled first so testing can occur. This provides a rollback path if issues occur.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/</a></p>", "answers": ["<p>Disable and then delete the old access key. Create a new access key and modify the applications to use the new access key.</p>", "<p>Create a second access key and modify applications to use the new key. Modify resource security policies to disable permissions for the temporary security credentials associated with the access key.</p>", "<p>Create a second access key and modify applications to use the new key. Delete the old access key and revoke all active sessions.</p>", "<p>Create a second access key and modify applications to use the new key. Disable the old access key and check applications are working correctly before deleting the old access key.</p>"]}, "correct_response": ["d"], "section": "Domain 1 - Incident Response", "question_plain": "A security engineer received a notification that an administrative user account may have been compromised. The engineer wants to immediately rotate the access key for the user whilst ensuring that applications that use the access key are not affected.What is the BEST approach in this situation?", "related_lectures": []}, {"_class": "assessment", "id": 63655050, "assessment_type": "multi-select", "prompt": {"question": "<p>An application runs on Amazon EC2 instances that use an Amazon SQS queue and an Amazon DynamoDB table. The application processes highly confidential information and the connectivity between these AWS services should be private.</p><p>Which combination of steps should the security engineer take to meet this requirement? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, your VPC is not exposed to the public internet.</p><p>For Amazon S3 and DynamoDB you must use a gateway VPC endpoint and for other AWS services you create an interface VPC endpoint. In both cases you can apply policies to control access to the service(s) to which you are connecting.</p><p><strong>CORRECT: </strong>\"Create an interface VPC endpoint for Amazon SQS\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a gateway VPC endpoint for Amazon DynamoDB\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the endpoint policies on all VPC endpoints. Specify the SQS and DynamoDB resources that the application uses\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure a connection to Amazon S3 through AWS Direct Connect\" is incorrect.</p><p>You cannot connect from a VPC to S3 using Direct Connect (DX). You create DX connections between on-premises data centers and a VPC.</p><p><strong>INCORRECT:</strong> \"Configure a connection to Amazon S3 through an AWS Managed VPN\" is incorrect.</p><p>You cannot create a VPN between S3 and a VPC. You must use VPC endpoints instead.</p><p><strong>INCORRECT:</strong> \"Modify the IAM role applied to the EC2 instance profile to allow outbound traffic to the interface endpoints\" is incorrect.</p><p>The IAM role attached to an instance profile specifies the permissions the instance has for other AWS services. For controlling traffic across a VPC endpoint you should instead use endpoint policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html</a></p>", "answers": ["<p>Create an interface VPC endpoint for Amazon SQS.</p>", "<p>Create a gateway VPC endpoint for Amazon DynamoDB.</p>", "<p>Configure a connection to Amazon S3 through AWS Direct Connect.</p>", "<p>Configure a connection to Amazon S3 through an AWS Managed VPN.</p>", "<p>Modify the IAM role applied to the EC2 instance profile to allow outbound traffic to the interface endpoints.</p>", "<p>Modify the endpoint policies on all VPC endpoints. Specify the SQS and DynamoDB resources that the application uses.</p>"]}, "correct_response": ["a", "b", "f"], "section": "Domain 3 - Infrastructure Security", "question_plain": "An application runs on Amazon EC2 instances that use an Amazon SQS queue and an Amazon DynamoDB table. The application processes highly confidential information and the connectivity between these AWS services should be private.Which combination of steps should the security engineer take to meet this requirement? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 63655026, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer is troubleshooting a connectivity issue with an Amazon EC2 Linux instance. The engineer is trying to connect from the internet, but the connection attempt times out. Other instances in the VPC are contactable over the internet.</p><p>Which of the following could be causes of this issue? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>There are several potential issues here. Firstly, the VPC must have an internet gateway attached and the subnet must have a route to the internet gateway. We know an internet gateway is attached as other instances in the VPC are contactable over the internet. Therefore, it only remains to ensure there is a route in the route table pointing to the internet gateway.</p><p>Security groups must be configured to allow the inbound connection and will automatically allow any response traffic. Therefore, outbound ephemeral ports need not be opened in security groups. For network ACLs however the connections in both directions must be allowed as they are not stateful.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-04_02-44-35-2b8256311cdfbbf0c71a0691f30f732c.jpg\"><p>Lastly, a host-based firewall may be configured that blocks access. This should be checked to ensure this is not the cause.</p><p><strong>CORRECT: </strong>\"The route table of the subnet is missing a route to the internet gateway\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The network ACL denies outbound traffic on ephemeral ports\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The host-based firewall of the instance operating system is denying traffic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The security group of the internet gateway is misconfigured\" is incorrect (as explained above.)</p><p><strong>INCORRECT:</strong> \"The internet gateway is no longer attached to the VPC\" is incorrect (as explained above.)</p><p><strong>INCORRECT:</strong> \"The instance security group denies outbound traffic on ephemeral ports\" is incorrect (as explained above.)</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-connect-internet-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-connect-internet-gateway/</a></p>", "answers": ["<p>The route table of the subnet is missing a route to the internet gateway.</p>", "<p>The security group of the internet gateway is misconfigured.</p>", "<p>The internet gateway is no longer attached to the VPC.</p>", "<p>The instance security group denies outbound traffic on ephemeral ports.</p>", "<p>The network ACL denies outbound traffic on ephemeral ports.</p>", "<p>The host-based firewall of the instance operating system is denying traffic.</p>"]}, "correct_response": ["a", "e", "f"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer is troubleshooting a connectivity issue with an Amazon EC2 Linux instance. The engineer is trying to connect from the internet, but the connection attempt times out. Other instances in the VPC are contactable over the internet.Which of the following could be causes of this issue? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 63655052, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company enforces encryption for all Amazon EBS volumes. Following security incidents, EBS snapshots sometimes need to be shared with a forensics account for analysis. The security team must ensure the volumes remain encrypted as much as possible throughout the process.</p><p>Which steps are required to share the encrypted snapshots with least privilege?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Note that the term CMK which may appear in the exam has been replaced with the term KMS key though the concepts are identical.</p><p>The correct process for sharing an encrypted snapshot across AWS accounts is to first ensure the snapshot is encrypted with a customer managed KMS key. You cannot use an AWS managed or default KMS key. The key policy should allow the Decrypt and CreateGrant actions for the target account at a minimum. The snapshot will be decrypted by the target account users and then reencrypted. IAM users in the target account will require additional privileges on the key they create to reencrypt the snapshot.</p><p>Please refer to the reference link below for further instructions.</p><p><strong>CORRECT: </strong>\"Share an encrypted snapshot, use a customer managed KMS key, and allow the Decrypt and CreateGrant actions for the target account in the key policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Copy an encrypted EBS volume to the target account, use a customer managed KMS key, and allow the Encrypt, Decrypt, and CreateGrant actions in the key policy\" is incorrect.</p><p>You cannot copy encrypted EBS volumes across accounts, snapshots must be used.</p><p><strong>INCORRECT:</strong> \"Create an unencrypted copy of the encrypted snapshot, share the snapshot with the target account, encrypt the copied snapshot with a customer managed KMS key\" is incorrect.</p><p>This does not maintain encryption throughout the process. The snapshot will spend more time in an unencrypted state than the correct method.</p><p><strong>INCORRECT:</strong> \"Share an encrypted snapshot, use an AWS managed KMS key, and allow the kms:* actions for the target account in the key policy\" is incorrect.</p><p>This does not maintain least privilege as more permissions that necessary are granted.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-ebs-volume/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-ebs-volume/</a></p>", "answers": ["<p>Share an encrypted snapshot, use a customer managed KMS key, and allow the Decrypt and CreateGrant actions for the target account in the key policy.</p>", "<p>Copy an encrypted EBS volume to the target account, use a customer managed KMS key, and allow the Encrypt, Decrypt, and CreateGrant actions in the key policy.</p>", "<p>Create an unencrypted copy of the encrypted snapshot, share the snapshot with the target account, encrypt the copied snapshot with a customer managed KMS key.</p>", "<p>Share an encrypted snapshot, use an AWS managed KMS key, and allow the kms:* actions for the target account in the key policy.</p>"]}, "correct_response": ["a"], "section": "Domain 1 - Incident Response", "question_plain": "A company enforces encryption for all Amazon EBS volumes. Following security incidents, EBS snapshots sometimes need to be shared with a forensics account for analysis. The security team must ensure the volumes remain encrypted as much as possible throughout the process.Which steps are required to share the encrypted snapshots with least privilege?", "related_lectures": []}, {"_class": "assessment", "id": 63655054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A website runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) which serves as an origin for an Amazon CloudFront distribution. An AWS WAF is being used to protect against SQL injection attacks. A review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.</p><p>What&nbsp;steps should be taken to protect the application?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The IP match condition / IP set match statement inspects the IP address of a web request's origin against a set of IP addresses and address ranges. Use this to allow or block web requests based on the IP addresses that the requests originate from.</p><p>AWS WAF supports all IPv4 and IPv6 address ranges. An IP set can hold up to 10,000 IP addresses or IP address ranges to check.</p><p><strong>CORRECT: </strong>\"Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address\" is the correct answer.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address\" is incorrect as CloudFront does not sit within a subnet so network ACLs do not apply to it.</p><p><strong>INCORRECT:</strong> \"Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address\" is incorrect as the source IP addresses of the data in the EC2 instances\u2019 subnets will be the ELB IP addresses.</p><p><strong>INCORRECT:</strong> \"Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.\" is incorrect as you cannot create deny rules with security groups.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-ipset-match.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>", "answers": ["<p>Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.</p>", "<p>Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.</p>", "<p>Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.</p>", "<p>Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.</p>"]}, "correct_response": ["b"], "section": "Domain 1 - Incident Response", "question_plain": "A website runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) which serves as an origin for an Amazon CloudFront distribution. An AWS WAF is being used to protect against SQL injection attacks. A review of security logs revealed an external malicious IP that needs to be blocked from accessing the website.What&nbsp;steps should be taken to protect the application?", "related_lectures": []}, {"_class": "assessment", "id": 63655056, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company receives an AWS Abuse notification stating that an IAM user's access key, used by an inventory management system, may have been compromised. The security manager needs to address the potential security breach while ensuring minimal service interruption to the inventory system.</p><p>What would be the optimal strategy to address this situation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>By creating a new access key and updating the application to use it before deleting the old key, you ensure minimal downtime. The application won't fail when the old key is deleted because it has already been configured to use the new one.</p><p><strong>CORRECT: </strong>\"Generate a new access key for the IAM user. Update the inventory management system to utilize the new access key. Subsequently, deactivate the compromised access key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Immediately delete the IAM user's access key. Then, produce a new access key and implement it within the inventory management system\" is incorrect.</p><p>Immediately deleting the access key will disrupt the system until the new key is generated and configured. This could result in unnecessary downtime for the system.</p><p><strong>INCORRECT:</strong> \"Implement an IAM policy that terminates all sessions before the moment specified in the AWS Abuse notification\" is incorrect. Terminating sessions doesn't resolve the issue of the potentially compromised key. The key could still be used to initiate new sessions until it's deactivated.</p><p><strong>INCORRECT:</strong> \"Modify the inventory management system to utilize an IAM role with identical permissions as the IAM user\" is incorrect.</p><p>Updating the system to use an IAM role might demand considerable alterations in the system code which could potentially result in longer downtime.</p><p><strong>References</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey</a></p>", "answers": ["<p>Immediately delete the IAM user's access key. Then, produce a new access key and implement it within the inventory management system.</p>", "<p>Generate a new access key for the IAM user. Update the inventory management system to utilize the new access key. Subsequently, deactivate the compromised access key.</p>", "<p>Implement an IAM policy that terminates all sessions before the moment specified in the AWS Abuse notification.</p>", "<p>Modify the inventory management system to utilize an IAM role with identical permissions as the IAM user.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "An e-commerce company receives an AWS Abuse notification stating that an IAM user's access key, used by an inventory management system, may have been compromised. The security manager needs to address the potential security breach while ensuring minimal service interruption to the inventory system.What would be the optimal strategy to address this situation?", "related_lectures": []}, {"_class": "assessment", "id": 63655058, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multinational corporation has a diversified range of services deployed on Amazon EC2 instances. The company has AWS Systems Manager Agent (SSM Agent) installed on their EC2 instances and utilizes AWS Security Hub for consolidating their security alerts and findings.</p><p>The company uses AWS Organizations with numerous managed AWS accounts. They wish to regularly monitor their workloads for potential software vulnerabilities and unexpected network exposure.</p><p>The company seeks a solution that will seamlessly deploy across all member accounts, including any future accounts, and will automatically scan new workloads as they become operational.</p><p>What solution would fulfill these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Inspector performs automated security assessments to help improve the security and compliance of applications deployed on AWS. By configuring a delegated admin for Amazon Inspector across the organization, the company can ensure automatic scanning across all accounts including new ones.</p><p><strong>CORRECT: </strong>\"Designate a delegated administrator for Amazon Inspector for the entire organization. Set up automatic scanning for all existing and new member accounts\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement Service Control Policies (SCPs) to facilitate the scanning of EC2 instances across all organizational accounts\" is incorrect.</p><p>SCPs are used to set fine-grained permissions and manage AWS service actions across an AWS organization. They can't directly trigger workload scanning on EC2.</p><p><strong>INCORRECT:</strong> \"Appoint a delegated administrator for Amazon GuardDuty across the organization and set up an Amazon EventBridge rule to trigger the analysis of Amazon EC2 instances\" is incorrect.</p><p>Amazon GuardDuty provides threat detection for your AWS environment, but it doesn't perform vulnerability assessment tasks like software vulnerabilities detection on EC2 instances.</p><p><strong>INCORRECT:</strong> \"Establish a delegated administrator for Amazon Inspector throughout the organization and create an AWS Config rule to initiate the analysis of Amazon EC2 instances\" is incorrect.</p><p>AWS Config provides configuration, compliance, and auditing features. It can't directly initiate workload scanning on Amazon EC2 instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/user/adding-member-accounts.html\">https://docs.aws.amazon.com/inspector/latest/user/adding-member-accounts.html</a></p>", "answers": ["<p>Implement Service Control Policies (SCPs) to facilitate the scanning of EC2 instances across all organizational accounts.</p>", "<p>Appoint a delegated administrator for Amazon GuardDuty across the organization and set up an Amazon EventBridge rule to trigger the analysis of Amazon EC2 instances.</p>", "<p>Designate a delegated administrator for Amazon Inspector for the entire organization. Set up automatic scanning for all existing and new member accounts.</p>", "<p>Establish a delegated administrator for Amazon Inspector throughout the organization and create an AWS Config rule to initiate the analysis of Amazon EC2 instances.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A multinational corporation has a diversified range of services deployed on Amazon EC2 instances. The company has AWS Systems Manager Agent (SSM Agent) installed on their EC2 instances and utilizes AWS Security Hub for consolidating their security alerts and findings.The company uses AWS Organizations with numerous managed AWS accounts. They wish to regularly monitor their workloads for potential software vulnerabilities and unexpected network exposure.The company seeks a solution that will seamlessly deploy across all member accounts, including any future accounts, and will automatically scan new workloads as they become operational.What solution would fulfill these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial institution employs an on-premises hardware security module (HSM) to generate and administer its encryption keys, according to its stringent security policies. Their transaction processing application uses Amazon RDS to store data, and all data must be encrypted at rest.</p><p>A security specialist has generated an encryption key using the on-premises HSM.</p><p>What should the security specialist do next to adhere to these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS KMS allows the import of key material from an external source into a new customer-managed key. Amazon RDS can then use this key for encryption at rest if it has the necessary permissions. You can create a new RDS instance with this customer-managed key as the encryption key and then import your data.</p><p><strong>CORRECT: </strong>\"Create a new customer-managed key in AWS KMS and import the new key material. Provide Amazon RDS permissions to use the key. Create a new RDS instance and choose the new key as the encryption key. Migrate the data into RDS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new customer-managed key in AWS KMS, importing the new key material. Create a new RDS instance, choosing the new key as the encryption key. Deactivate the KMS key following the RDS instance creation. Migrate the data into RDS\" is incorrect.</p><p>Disabling the KMS key after creating the RDS instance is not advisable, as it would prevent future cryptographic operations with that key, making the encrypted data inaccessible.</p><p><strong>INCORRECT:</strong> \"Create a new AWS-managed key in AWS KMS and import the new key material. Give Amazon RDS permission to use the key. Create a new RDS instance and choose the new key as the encryption key. Migrate the data into RDS\" is incorrect.</p><p>AWS KMS does not allow importing key material into AWS-managed keys. The key material import feature is only available for customer-managed keys.</p><p><strong>INCORRECT:</strong> \"Create a new AWS-managed key in AWS KMS, importing the new key material. Use the AWS SDK to integrate with AWS KMS for local data encryption using the new KMS key. Create a new RDS instance and choose the new key as the encryption key. Deactivate the KMS key following RDS instance creation. Migrate the data into RDS\" is incorrect.</p><p>AWS KMS does not allow importing key material into AWS-managed keys. Also, disabling the KMS key after creating the RDS instance would prevent access to the encrypted data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys.html</a></p>", "answers": ["<p>Create a new customer-managed key in AWS KMS and import the new key material. Provide Amazon RDS permissions to use the key. Create a new RDS instance and choose the new key as the encryption key. Migrate the data into RDS.</p>", "<p>Create a new customer-managed key in AWS KMS, importing the new key material. Create a new RDS instance, choosing the new key as the encryption key. Deactivate the KMS key following the RDS instance creation. Migrate the data into RDS.</p>", "<p>Create a new AWS-managed key in AWS KMS and import the new key material. Give Amazon RDS permission to use the key. Create a new RDS instance and choose the new key as the encryption key. Migrate the data into RDS.</p>", "<p>Create a new AWS-managed key in AWS KMS, importing the new key material. Use the AWS SDK to integrate with AWS KMS for local data encryption using the new KMS key. Create a new RDS instance and choose the new key as the encryption key. Deactivate the KMS key following RDS instance creation. Migrate the data into RDS.</p>"]}, "correct_response": ["a"], "section": "Domain 6: Management and Security Governance", "question_plain": "A financial institution employs an on-premises hardware security module (HSM) to generate and administer its encryption keys, according to its stringent security policies. Their transaction processing application uses Amazon RDS to store data, and all data must be encrypted at rest.A security specialist has generated an encryption key using the on-premises HSM.What should the security specialist do next to adhere to these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655062, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare institution has developed a cloud-based application that collects and stores confidential patient records in an Amazon DynamoDB table. They need to adopt a solution that ensures end-to-end data protection and the ability to identify any unauthorized changes to the data.</p><p>What solution would cater to these needs?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The use of a client-side encryption library, like the DynamoDB Encryption Client, ensures that data is encrypted before it leaves the client and decrypted only after it returns. By signing the table items, the application can detect any unauthorized changes made to the data.</p><p><strong>CORRECT: </strong>\"Use the DynamoDB Encryption Client for client-side encryption and to digitally sign the table items\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Key Management Service (AWS KMS) customer managed key to encrypt the data at rest\" is incorrect.</p><p>Using an AWS KMS customer managed key can provide encryption for data at rest, but it does not provide a method for detecting unauthorized data changes.</p><p><strong>INCORRECT:</strong> \"Use an AWS Certificate Manager SSL/TLS certificate to ensure data is encrypted during transit\" is incorrect.</p><p>While AWS Certificate Manager can be used to secure data in transit by providing managed SSL/TLS certificates, it doesn't provide end-to-end protection and doesn't detect unauthorized data changes.</p><p><strong>INCORRECT:</strong> \"Use the AWS Encryption SDK to execute client-side encryption and to digitally sign the table items\" is incorrect.</p><p>The AWS Encryption SDK can be used for client-side encryption, but it's not directly associated with DynamoDB, and it requires more manual work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/what-is-database-encryption-sdk.html\">https://docs.aws.amazon.com/database-encryption-sdk/latest/devguide/what-is-database-encryption-sdk.html</a></p>", "answers": ["<p>Use an AWS Key Management Service (AWS KMS) customer managed key to encrypt the data at rest.</p>", "<p>Use an AWS Certificate Manager SSL/TLS certificate to ensure data is encrypted during transit.</p>", "<p>Use the DynamoDB Encryption Client for client-side encryption and to digitally sign the table items.</p>", "<p>Use the AWS Encryption SDK to execute client-side encryption and to digitally sign the table items.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A healthcare institution has developed a cloud-based application that collects and stores confidential patient records in an Amazon DynamoDB table. They need to adopt a solution that ensures end-to-end data protection and the ability to identify any unauthorized changes to the data.What solution would cater to these needs?", "related_lectures": []}, {"_class": "assessment", "id": 63655064, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A FinTech company wants to shield its online banking portal from man-in-the-middle attacks, and it's using Amazon CloudFront for content delivery.</p><p>What would be the most effective method to accomplish this goal with minimal administrative intervention?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Utilizing the SecurityHeadersPolicy managed response headers policy can provide protection against several types of security vulnerabilities, including man-in-the-middle attacks, with very little administrative effort.</p><p><strong>CORRECT: </strong>\"Use the SecurityHeadersPolicy managed response headers policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the BasicCORS predefined response headers policy\" is incorrect.</p><p>The BasicCORS predefined response headers policy is mainly aimed at enabling Cross-Origin Resource Sharing (CORS) requests. It's not tailored to thwart man-in-the-middle attacks.</p><p><strong>INCORRECT:</strong> \"Create a Lambda@Edge function to include the HTTP Strict Transport Security (HSTS) response header\" is incorrect.</p><p>Even though using a Lambda@Edge function to include the HSTS header is feasible, this approach involves managing additional components like the Lambda function, leading to higher operational overhead.</p><p><strong>INCORRECT:</strong> \"Use the Content-Security-Policy header within a custom-made response headers policy\" is incorrect.</p><p>The Content-Security-Policy header can help safeguard against cross-site scripting (XSS) and other code-injection attacks, but it doesn't directly mitigate man-in-the-middle attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-response-headers-policies.html</a></p>", "answers": ["<p>Use the BasicCORS predefined response headers policy.</p>", "<p>Create a Lambda@Edge function to include the HTTP Strict Transport Security (HSTS) response header.</p>", "<p>Use the SecurityHeadersPolicy managed response headers policy.</p>", "<p>Use the Content-Security-Policy header within a custom-made response headers policy.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A FinTech company wants to shield its online banking portal from man-in-the-middle attacks, and it's using Amazon CloudFront for content delivery.What would be the most effective method to accomplish this goal with minimal administrative intervention?", "related_lectures": []}, {"_class": "assessment", "id": 63655066, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security architect is designing a highly secure application and must determine the best solution for storage of encryption keys. The encryption keys must be accessible only from within a VPC on single-tenant hardware security modules (HSMs). The solution must also include access logging and high availability.</p><p>Which of the following services meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. CloudHSM runs on single-tenant hardware security modules (HSMs) in your Amazon VPC. In addition to the logging features built into the Client SDK, you can also use AWS CloudTrail, Amazon CloudWatch Logs, and Amazon CloudWatch to monitor AWS CloudHSM.</p><p>AWS CloudHSM automatically load balances requests and securely duplicates keys stored in any HSM to all the other HSMs in the cluster. This provides additional cryptographic capacity and improves the durability of the keys. By storing multiple copies of your keys across HSMs located in different Availability Zones (AZs), your keys will be available and protected in the event that a single HSM becomes unavailable. Using at least two HSMs across multiple AZs is Amazon\u2019s recommended configuration for availability and durability.</p><p><strong>CORRECT: </strong>\"AWS CloudHSM\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Key Management Service (KMS)\" is incorrect.</p><p>AWS KMS is a service you can use for creating and managing encryption keys, but it is not single-tenant and does not run within your VPC.</p><p><strong>INCORRECT:</strong> \"Amazon Certificate Manager (ACM)\" is incorrect.</p><p>ACM is used for creating and managing SSL/TLS certificates only and does not run on single-tenant HSMs within your VPC.</p><p><strong>INCORRECT:</strong> \"AWS Secrets Manager \" is incorrect.</p><p>AWS Secrets Manager is not used for creating and managing encryption keys, it is used for storing secrets such as passwords and database connection strings.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/features/\">https://aws.amazon.com/cloudhsm/features/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/get-logs.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/get-logs.html</a></p>", "answers": ["<p>Amazon Certificate Manager (ACM).</p>", "<p>AWS CloudHSM.</p>", "<p>AWS Key Management Service (KMS).</p>", "<p>AWS Secrets Manager.</p>"]}, "correct_response": ["b"], "section": "Domain 5 - Data Protection", "question_plain": "A security architect is designing a highly secure application and must determine the best solution for storage of encryption keys. The encryption keys must be accessible only from within a VPC on single-tenant hardware security modules (HSMs). The solution must also include access logging and high availability.Which of the following services meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655068, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer has been asked to review an Amazon S3 bucket policy to determine if the data is properly secured against public access. The policy statement is as follows:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-07-07_16-29-18-a24a7eeaa428ac9fecbdc1bfb1dddcc6.jpg\"><p>What should the response be from the security engineer?</p><p>Is this bucket policy sufficient to ensure that the data is not publicly accessible?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The bucket policy has been written with the goal of granting access only to users who come from the 10.0.0.0/24 IP address space. This address space is internal so should not allow public access from any public IP addresses.</p><p>However, if the bucket ACL or object ACL settings allow access then access will be granted. This is because whenever an AWS principal issues a request to S3, the authorization decision depends on the union of all the IAM policies, S3 bucket policies, and S3 ACLs that apply.</p><p>One fix that could be made to the policy to ensure that ACLs do not allow access is to change the Effect to Deny and the condition from IpAddress to NotIPAddress as in the code below:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_16-29-18-2dfb0c316a72682a74c053e648b71c0e.jpg\"><p>This will ensure that access is denied to all requests except those originating from the 10.0.0.0/24 address space as an explicit deny will override any allows.</p><p><strong>CORRECT: </strong>\"The S3 bucket ACL and object ACLs will need to be checked to determine if public access is possible\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The bucket policy will block all access from public IP addresses and will only allow connections from the specified private CIDR\" is incorrect.</p><p>This is not true as it will depend on the bucket ACL and object ACL settings.</p><p><strong>INCORRECT:</strong> \"IAM user policies will need to be checked to determine if public access is granted through these policies\" is incorrect.</p><p>IAM user policies do not apply to public access as this would be from unauthenticated sources.</p><p><strong>INCORRECT:</strong> \"The bucket policy does not block access from public IP addresses regardless of the bucket ACL and object ACL settings\" is incorrect.</p><p>This is not true as it will depend on the bucket ACL and object ACL settings.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>", "answers": ["<p>The bucket policy will block all access from public IP addresses and will only allow connections from the specified private CIDR.</p>", "<p>The S3 bucket ACL and object ACLs will need to be checked to determine if public access is possible.</p>", "<p>IAM user policies will need to be checked to determine if public access is granted through these policies.</p>", "<p>The bucket policy does not block access from public IP addresses regardless of the bucket ACL and object ACL settings.</p>"]}, "correct_response": ["b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A security engineer has been asked to review an Amazon S3 bucket policy to determine if the data is properly secured against public access. The policy statement is as follows:What should the response be from the security engineer?Is this bucket policy sufficient to ensure that the data is not publicly accessible?", "related_lectures": []}, {"_class": "assessment", "id": 63655070, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has a serverless application that is accessed by internal users. The application consists of an AWS Lambda function that accesses an Amazon DynamoDB table. The security team are concerned that the Lambda function has internet access and the endpoints for Lambda and DynamoDB are both public.</p><p>How can a security engineer improve the security of the application? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your AWS account. When you do this you can invoke your function internally within the VPC without accessing the public address space. The function will also not have internet access unless you add a NAT gateway.</p><p>To secure access to DynamoDB a Gateway VPC Endpoint can be created within the VPC. This will enable the Lambda function to access the DynamoDB table using private addresses which meets the requirements of the question.</p><p><strong>CORRECT: </strong>\"Configure the Lambda function to connect to private subnets in an Amazon VPC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a VPC endpoint for accessing the DynamoDB table using private addresses\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a resource-based policy for Lambda to restrict internet access\" is incorrect.</p><p>You cannot create resource based IAM policies on Lambda and so this is not a method of restricting permissions or internet access.</p><p><strong>INCORRECT:</strong> \"Configure the DynamoDB table to connect to private subnets in an Amazon VPC\" is incorrect.</p><p>You cannot configure DynamoDB tables to connect to private subnets. You can connect <em>from </em>a private subnet to DynamoDB using a VPC endpoint.</p><p><strong>INCORRECT:</strong> \"Create a resource-based policy for DynamoDB to restrict access to the Amazon VPC\" is incorrect.</p><p>DynamoDB doesn't support resource-based policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/access-control-overview.html#access-control-manage-access-resource-based\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/access-control-overview.html#access-control-manage-access-resource-based</a></p>", "answers": ["<p>Create a resource-based policy for Lambda to restrict internet access.</p>", "<p>Configure the DynamoDB table to connect to private subnets in an Amazon VPC.</p>", "<p>Configure the Lambda function to connect to private subnets in an Amazon VPC.</p>", "<p>Create a resource-based policy for DynamoDB to restrict access to the Amazon VPC.</p>", "<p>Configure a VPC endpoint for accessing the DynamoDB table using private addresses.</p>"]}, "correct_response": ["c", "e"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company has a serverless application that is accessed by internal users. The application consists of an AWS Lambda function that accesses an Amazon DynamoDB table. The security team are concerned that the Lambda function has internet access and the endpoints for Lambda and DynamoDB are both public.How can a security engineer improve the security of the application? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655072, "assessment_type": "multi-select", "prompt": {"question": "<p>A security team are designing a plan to respond to incidents of compromised Amazon EC2 instances. The incident response plan should include the automated provisioning of a secure forensic environment and orchestration of incident response processes.</p><p>Which AWS services should be included in the plan? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS CloudFormation can be used to automatically provision a secure forensic environment based on a template. CloudFormation will provision the infrastructure required based on the definitions in the JSON or YAML template file.</p><p>AWS Step Functions coordinates processes such as AWS Lambda functions. This can be used to orchestrate the processes required to execute forensic analysis within the secure forensic environment. CloudFormation can also be used to provision an AWS Step Functions state machine.</p><p><strong>CORRECT: </strong>\"AWS CloudFormation\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"AWS Step Functions\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Fargate\" is incorrect.</p><p>Fargate is a serverless service for running Docker containers and does not automate infrastructure deployment or orchestrate processes.</p><p><strong>INCORRECT:</strong> \"Amazon GuardDuty\" is incorrect.</p><p>GuardDuty is an intelligent threat detection service. It can mitigate threats through automated responses but is not used for creating forensic environments or orchestrating processes.</p><p><strong>INCORRECT:</strong> \"AWS Shield\" is incorrect.</p><p>AWS Shield is used for mitigating DDoS attacks.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-stepfunctions-statemachine.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-stepfunctions-statemachine.html</a></p>", "answers": ["<p>AWS CloudFormation</p>", "<p>AWS Fargate</p>", "<p>Amazon GuardDuty</p>", "<p>AWS Shield</p>", "<p>AWS Step Functions</p>"]}, "correct_response": ["a", "e"], "section": "Domain 1 - Incident Response", "question_plain": "A security team are designing a plan to respond to incidents of compromised Amazon EC2 instances. The incident response plan should include the automated provisioning of a secure forensic environment and orchestration of incident response processes.Which AWS services should be included in the plan? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655074, "assessment_type": "multi-select", "prompt": {"question": "<p>A company is experiencing a layer 3 and layer 4 DDoS attack on its web servers running on AWS.</p><p>Which combination of AWS services and features will provide protection in this scenario? (Select THREE).</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>There are several services that can assist with protecting against the impacts of a DDoS attack. AWS Shield is designed exactly for this purpose. Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency.</p><p>Amazon Route 53 can scale seamlessly to support large numbers of queries without any interruption to your legitimate users. Route 53 also uses <em>shuffle sharding</em> and <em>anycast striping</em> to increase availability.</p><p>Large DDoS attacks can overwhelm the capacity of a single Amazon EC2 instance. With Elastic Load Balancing (ELB), you can reduce the risk of overloading your application by distributing traffic across many backend instances. Elastic Load Balancing can scale automatically, allowing you to manage larger volumes when you have unanticipated extra traffic, for example, due to flash crowds or DDoS attacks.</p><p><strong>CORRECT: </strong>\"Amazon Route 53\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"AWS Shield\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Elastic Load Balancer\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS KMS\" is incorrect.</p><p>KMS is used for generating and managing encryption keys and does not offer protection to L3/L4 DDoS attacks.</p><p><strong>INCORRECT:</strong> \"Amazon EBS\" is incorrect.</p><p>Amazon EBS is the Elastic Block Store and is the block-based storage system used with Amazon EC2. It does not offer protection against DDoS attacks.</p><p><strong>INCORRECT:</strong> \"Amazon GuardDuty\" is incorrect.</p><p>Amazon GuardDuty can detect attacks such as application-level attacks. However, to offer protection you would need to integrate with other services such as CloudWatch Events and Lambda to respond to incidents. The question specifically asks for the services that WILL offer protection and as further configuration would be needed, GuardDuty is not a good answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load-balancing-bp6.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load-balancing-bp6.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/reduce-ddos-risks-using-amazon-route-53-and-aws-shield/\">https://aws.amazon.com/blogs/aws/reduce-ddos-risks-using-amazon-route-53-and-aws-shield/</a></p>", "answers": ["<p>Amazon Route 53</p>", "<p>AWS KMS</p>", "<p>AWS Shield</p>", "<p>Amazon EBS</p>", "<p>Elastic Load Balancer</p>", "<p>Amazon GuardDuty</p>"]}, "correct_response": ["a", "c", "e"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company is experiencing a layer 3 and layer 4 DDoS attack on its web servers running on AWS.Which combination of AWS services and features will provide protection in this scenario? (Select THREE).", "related_lectures": []}]}
5750714
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70265668, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer is attempts to encrypt a secure string parameter value in AWS Systems Manager Parameter Store with an AWS KMS key and receives an <em>InvalidKeyId</em> error message.</p><p>Why was this error message generated?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To perform any operation on a secure string parameter, Parameter Store must be able to use the Amazon KMS key that you specify for your intended operation. Most of the Parameter Store failures related to KMS keys are caused by the following problems:</p><ul><li><p>The credentials that an application is using do not have permission to perform the specified action on the KMS key.</p></li><li><p>The KMS key is not found. This typically happens when you use an incorrect identifier for the KMS key.</p></li><li><p>The KMS key is not enabled. When this occurs, Parameter Store returns an <strong>InvalidKeyId</strong> exception with a detailed error message from Amazon KMS.</p></li></ul><p>The specific error message received indicates that the issue is due to the KMS key being disabled.</p><p><strong>CORRECT: </strong>\"The KMS key specified is not enabled\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The KMS key specified is not compliant\" is incorrect.</p><p>There is no compliance requirement for a key to work with Parameter Store.</p><p><strong>INCORRECT:</strong> \"The KMS key specified does not exist\" is incorrect.</p><p>The specific error generated indicates that the key is not enabled.</p><p><strong>INCORRECT:</strong> \"The KMS key specified is currently in use\" is incorrect.</p><p>KMS keys do not get locked to a single process and can be used by multiple processes at the same time.</p><p><strong>References:</strong></p><p><a href=\"https://docs.amazonaws.cn/en_us/kms/latest/developerguide/services-parameter-store.html\">https://docs.amazonaws.cn/en_us/kms/latest/developerguide/services-parameter-store.html</a></p>", "answers": ["<p>The KMS key specified is not compliant.</p>", "<p>The KMS key specified is not enabled.</p>", "<p>The KMS key specified does not exist.</p>", "<p>The KMS key specified is currently in use.</p>"]}, "correct_response": ["b"], "section": "Domain 5 - Data Protection", "question_plain": "A security engineer is attempts to encrypt a secure string parameter value in AWS Systems Manager Parameter Store with an AWS KMS key and receives an InvalidKeyId error message.Why was this error message generated?", "related_lectures": []}, {"_class": "assessment", "id": 70265670, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has created an organization within AWS Organizations that has all features enabled. Several resource accounts have been added to the organization. The security team requires that the privileges of root user accounts within the member accounts are restricted.</p><p>How can the security administrator restrict usage of member root user accounts across the organization?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. The SCP that is attached to the OU will ensure that permissions are not available for the root user account which limits the API actions the root user can perform, improving overall security.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_02-55-25-d5347171bc7df0d2676e57410b071343.jpg\"><p><strong>CORRECT: </strong>\"Create an OU in AWS Organizations and add all the member accounts. Attach an SCP that controls the permissions available for the root user\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Disable the root user account for member accounts through the organization at the root account level\" is incorrect.</p><p>It is not possible to disable the root user account through AWS Organizations (or any other method).</p><p><strong>INCORRECT:</strong> \"Within each member account attach an IAM user policy to a group that restricts available permissions. Add the root user account to the group\" is incorrect.</p><p>You cannot add the root user account to an IAM group as it is not an IAM user account.</p><p><strong>INCORRECT:</strong> \"Within each member account attach an inline IAM policy to the root user account that restricts available permissions\" is incorrect.</p><p>You cannot attach inline IAM policies to the root user account as it is not an IAM user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>", "answers": ["<p>Disable the root user account for member accounts through the organization at the root account level.</p>", "<p>Within each member account attach an IAM user policy to a group that restricts available permissions. Add the root user account to the group.</p>", "<p>Within each member account attach an inline IAM policy to the root user account that restricts available permissions.</p>", "<p>Create an OU in AWS Organizations and add all the member accounts. Attach an SCP that controls the permissions available for the root user.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has created an organization within AWS Organizations that has all features enabled. Several resource accounts have been added to the organization. The security team requires that the privileges of root user accounts within the member accounts are restricted.How can the security administrator restrict usage of member root user accounts across the organization?", "related_lectures": []}, {"_class": "assessment", "id": 70265672, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer is tasked with securing the network access for an application that uses an AWS Lambda function and an Amazon RDS database. The Lambda function and database both run in the same AWS account.</p><p>Which network configuration is the MOST secure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Note that with AWS exam questions you must choose the BEST answer, though the solution often may not be perfect!</p><p>In this case the best answer is to configure the Lambda function to connect to a VPC and update the security group attached to the function configuration to allow outbound access to the VPC CIDR block. An improvement would be to allow outbound traffic to the DB security group only. The Lambda security group should be configured to allow inbound access on the DB port/protocol from the Lambda security group.</p><p><strong>CORRECT: </strong>\"Connect the Lambda function to a private subnet within the VPC. Attach a security group to the function that allows outbound access to the VPC CIDR block only. Update the DB instance security group to allow traffic from the Lambda security group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Connect the Lambda function to a public subnet within the VPC. Attach a security group to the function that allows outbound access to the DB security group only. Update the DB instance security group to allow traffic from the Lambda public address space\" is incorrect.</p><p>The Lambda function is attached to a VPC so the inbound rule on the DB security group should allow access from the Lambda security group, not the Lambda public address space.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to the Lambda function. Attach a security group to the function that allows outbound access to the DB security group. Update the DB instance security group to allow traffic from the Lambda security group\" is incorrect.</p><p>You cannot attach Elastic IP addresses to Lambda functions. When you connect a Lambda function to a VPC, Lambda assigns your function to a Hyperplane ENI (elastic network interface) for each subnet in your function's VPC configuration.</p><p><strong>INCORRECT:</strong> \"Create an interface VPC endpoint for Lambda and update a private subnet route table to point to the endpoint. Update the DB to connect to the private subnet and access the Lambda function. Use endpoint policies to secure network access between the function and DB instance\" is incorrect.</p><p>This is backwards as the VPC endpoint is allowing access from the VPC into Lambda. The database does not connect TO Lambda, the Lambda function connects TO the database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</a></p>", "answers": ["<p>Connect the Lambda function to a private subnet within the VPC. Attach a security group to the function that allows outbound access to the VPC CIDR block only. Update the DB instance security group to allow traffic from the Lambda security group.</p>", "<p>Attach an Elastic IP to the Lambda function. Attach a security group to the function that allows outbound access to the DB security group. Update the DB instance security group to allow traffic from the Lambda security group.</p>", "<p>Connect the Lambda function to a public subnet within the VPC. Attach a security group to the function that allows outbound access to the DB security group only. Update the DB instance security group to allow traffic from the Lambda public address space.</p>", "<p>Create an interface VPC endpoint for Lambda and update a private subnet route table to point to the endpoint. Update the DB to connect to the private subnet and access the Lambda function. Use endpoint policies to secure network access between the function and DB instance.</p>"]}, "correct_response": ["a"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer is tasked with securing the network access for an application that uses an AWS Lambda function and an Amazon RDS database. The Lambda function and database both run in the same AWS account.Which network configuration is the MOST secure?", "related_lectures": []}, {"_class": "assessment", "id": 70265674, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer is building an application that is running on Amazon EC2. The application communicates with an Amazon RDS MySQL instance and authenticates with a user name and password. The credentials should be encrypted and rotated every 60 days.</p><p>Which steps should the engineer take to protect the credentials and ensure they can be automatically rotated?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Secrets Manager can automatically rotate credentials for Oracle, Microsoft SQL Server, or MariaDB databases hosted on Amazon Relational Database Service (Amazon RDS). Secrets Manager also automatically encrypts credentials using a default key or an AWS KMS key that you specify. Applications must be configured to retrieve the secrets from Secrets Manager using the API.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager and choose an AWS KMS key. Enable automatic rotation every 60 days and configure the application to retrieve the secret programmatically\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials as an encrypted string parameter in AWS Systems Manager Parameter Store. Enable automatic rotation every 60 days and grant permission to the EC2 instance role to retrieve the parameter programmatically\" is incorrect.</p><p>Parameter Store does not automatically rotate credentials.</p><p><strong>INCORRECT:</strong> \"Store the credentials in an Amazon S3 bucket configured with SSE-KMS encryption. Grant permission to the EC2 instance role to retrieve the credentials from S3 programmatically. Update the credential files every 60 days\" is incorrect.</p><p>This is not an example of automatic rotation; Secrets Manager is a better solution.</p><p><strong>INCORRECT:</strong> \"Store the credentials on an encrypted Amazon EFS volume. Configure the application instances to mount the volume. Use an AWS Lambda function to update the credentials on the EFS volume every 60 days\" is incorrect.</p><p>This is an expensive and unnecessary solution. Secrets Manager is built for this purpose and will be lower cost and more efficient.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/\">https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-amazon-rds-database-types-oracle/</a></p>", "answers": ["<p>Store the credentials as an encrypted string parameter in AWS Systems Manager Parameter Store. Enable automatic rotation every 60 days and grant permission to the EC2 instance role to retrieve the parameter programmatically.</p>", "<p>Store the credentials in AWS Secrets Manager and choose an AWS KMS key. Enable automatic rotation every 60 days and configure the application to retrieve the secret programmatically.</p>", "<p>Store the credentials in an Amazon S3 bucket configured with SSE-KMS encryption. Grant permission to the EC2 instance role to retrieve the credentials from S3 programmatically. Update the credential files every 60 days.</p>", "<p>Store the credentials on an encrypted Amazon EFS volume. Configure the application instances to mount the volume. Use an AWS Lambda function to update the credentials on the EFS volume every 60 days.</p>"]}, "correct_response": ["b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A security engineer is building an application that is running on Amazon EC2. The application communicates with an Amazon RDS MySQL instance and authenticates with a user name and password. The credentials should be encrypted and rotated every 60 days.Which steps should the engineer take to protect the credentials and ensure they can be automatically rotated?", "related_lectures": []}, {"_class": "assessment", "id": 70265676, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer was reviewing AWS KMS key policies and found this statement in several key policies within the AWS account.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_03-03-35-a7a32145359cd1d49f285e8bbfad3dc3.jpg\"></p><p>What is the purpose of this statement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Key policies are the primary way to control access to AWS KMS keys. Every KMS key must have exactly one key policy. The statements in the key policy document determine who has permission to use the KMS key and how they can use it. You can also use IAM policies and grants to control access to the KMS key, but every KMS key must have a key policy.</p><p>By default, a policy statement like this one in this question is present in the key policy document when you create a new KMS key with the AWS Management Console. It is also present when you create a new KMS key programmatically but do not provide a key policy.</p><p>A key policy document with a statement that allows access to the AWS account (root user) enables IAM policies in the account to allow access to the KMS key. This means that IAM users and roles in the account might have access to the KMS key even if they are not explicitly listed as principals in the key policy document. It is therefore important to check IAM policies for grants.</p><p><strong>CORRECT: </strong>\"Enables IAM policies in the 554422336677 account to allow access to the key\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Restricts key usage to the root user in account 554422336677\" is incorrect.</p><p>The root user will be granted access to the key and IAM users <em>may</em> have access if an IAM policy grants it to them.</p><p><strong>INCORRECT:</strong> \"Allows all principals from account 554422336677 to use the key with Amazon S3\" is incorrect.</p><p>Amazon S3 is not mentioned at all and as explained previously all principals will not be granted access based on the key policy.</p><p><strong>INCORRECT:</strong> \"Enables the root user to grant key usage permissions to principals in account 554422336677\" is incorrect.</p><p>This is not the purpose of this key policy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/determining-access-key-policy.html\">https://docs.aws.amazon.com/kms/latest/developerguide/determining-access-key-policy.html</a></p>", "answers": ["<p>Enables IAM policies in the 554422336677 account to allow access to the key.</p>", "<p>Restricts key usage to the root user in account 554422336677.</p>", "<p>Allows all principals from account 554422336677 to use the key with Amazon S3.</p>", "<p>Enables the root user to grant key usage permissions to principals in account 554422336677.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A security engineer was reviewing AWS KMS key policies and found this statement in several key policies within the AWS account.What is the purpose of this statement?", "related_lectures": []}, {"_class": "assessment", "id": 70265678, "assessment_type": "multi-select", "prompt": {"question": "<p>A company needs a solution for running analytics on the log files generated by hundreds of applications running on Amazon EC2. The solution must offer real-time analytics, support the replay of messages, and store the logs persistently.</p><p>Which AWS services can be used to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon Kinesis is a service that can be used for collecting, processing, and analyzing real-time streaming data. Kinesis can be used to ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. This service is suitable for collecting and processing the log files.</p><p>OpenSearch is the successor to Amazon Elasticsearch and is a distributed, open-source search and analytics suite used for a broad set of use cases like real-time application monitoring, log analytics, and website search. This service can receive data from Kinesis and can then analyze and store the data.</p><p><strong>CORRECT: </strong>\"Amazon Kinesis\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Amazon OpenSearch\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon Athena\" is incorrect.</p><p>Athena is used for running SQL queries on datasets in data stores such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Amazon SQS\" is incorrect.</p><p>Amazon SQS is used for storing and retrieving messages. It is a message queue service and does not process or analyze the data.</p><p><strong>INCORRECT:</strong> \"Amazon ElastiCache\" is incorrect.</p><p>Amazon ElastiCache is an in-memory database and is not used for streaming data or performing computational processes such as analytics.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/\">https://aws.amazon.com/kinesis/</a></p><p><a href=\"https://aws.amazon.com/opensearch-service/the-elk-stack/what-is-opensearch/\">https://aws.amazon.com/opensearch-service/the-elk-stack/what-is-opensearch/</a></p>", "answers": ["<p>Amazon Kinesis</p>", "<p>Amazon Athena</p>", "<p>Amazon ElastiCache</p>", "<p>Amazon SQS</p>", "<p>Amazon OpenSearch</p>"]}, "correct_response": ["a", "e"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company needs a solution for running analytics on the log files generated by hundreds of applications running on Amazon EC2. The solution must offer real-time analytics, support the replay of messages, and store the logs persistently.Which AWS services can be used to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265680, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer was asked to configure an automated alert that notifies the security team when configuration changes occur on security groups. The engineer has created an AWS CloudTrail trail, specified a log group, and assigned appropriate IAM permissions to CloudTrail. The solution must be simple and cost-effective.</p><p>Which additional actions should the security engineer take? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can create a solution that sends automatic notifications when security group changes occur. The solution in this scenario uses AWS CloudTrail to send information about the API actions that occur in the account to an Amazon CloudWatch Logs log group.</p><p>CloudTrail must be granted sufficient IAM permissions to be able to create a CloudWatch Logs log stream in the log group that you specify and to deliver CloudTrail events to that log stream.</p><p>When the logs are being correctly sent to the specified log group a metric filter should be created that filters out the log events which the security engineer is looking for. An alarm can then be created that is based on the filter. The alarm should send a notification to the security team using an Amazon SNS topic.</p><p><strong>CORRECT: </strong>\"Create a metric filter and define a metric pattern that matches security group changes\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an alarm that sends an Amazon SNS notification if security group changes are identified\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Stream the CloudWatch Logs to Amazon Kinesis Data Streams and use Kinesis Data Analytics to identify security group changes in near real-time\" is incorrect.</p><p>This solution is more expensive, and complex compared to using metric filters with CloudWatch Logs. It also does not specify a method of sending a notification.</p><p><strong>INCORRECT:</strong> \"Create a query in Amazon CloudWatch Logs Insights and write an AWS Lambda function that runs the query on a schedule\" is incorrect.</p><p>CloudWatch Logs Insights can be used to interactively search and analyze data in CloudWatch Logs. However, metric filters are automatic and free which is a simpler and cheaper solution.</p><p><strong>INCORRECT:</strong> \"Create a subscription to an AWS Lambda function that analyses the logs and a subscription filter to filter the log events that are forwarded\" is incorrect.</p><p>This is a workable solution but is more complex and costly compared to using metric filters. It also does not specify a method of sending a notification.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html</a></p>", "answers": ["<p>Create an alarm that sends an Amazon SNS notification if security group changes are identified.</p>", "<p>Stream the CloudWatch Logs to Amazon Kinesis Data Streams and use Kinesis Data Analytics to identify security group changes in near real-time.</p>", "<p>Create a query in Amazon CloudWatch Logs Insights and write an AWS Lambda function that runs the query on a schedule.</p>", "<p>Create a subscription to an AWS Lambda function that analyses the logs and a subscription filter to filter the log events that are forwarded.</p>", "<p>Create a metric filter and define a metric pattern that matches security group changes.</p>"]}, "correct_response": ["a", "e"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A security engineer was asked to configure an automated alert that notifies the security team when configuration changes occur on security groups. The engineer has created an AWS CloudTrail trail, specified a log group, and assigned appropriate IAM permissions to CloudTrail. The solution must be simple and cost-effective.Which additional actions should the security engineer take? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265682, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has two AWS accounts: A production account and a development account. Developers with user accounts in the production account need to be able to access artifacts stored in an Amazon S3 bucket in the development account when deploying resources.</p><p>A cross-account role has been created in the development account with access to the S3 bucket. The security team requires that the users can assume the role only if they are authenticated with multi-factor authentication (MFA).</p><p>Which step should the security engineer take to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A cross-account role has been created and must be assumed by the developers to access the S3 bucket. To enforce a restriction that sessions must be authenticated using MFA, there are two options. Either you can configure the bucket policy, or you can configure the trust policy of the role (or both).</p><p>In this case the requirement is to ensure that users can assume the role only if they are authenticated with MFA. Therefore, the best place to add the condition is in the trust policy of the role. If the user is not authenticated with MFA, they will not be able to assume the role.</p><p>An example role trust policy is provided below:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-09-28-e174abd7c53e95bf1feceb9013ca6cd9.jpg\"></p><p><strong>CORRECT: </strong>\"Add an aws:MultiFactorAuthPresent : true condition to the role's trust policy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the role\u2019s permissions policy\" is incorrect.</p><p>The trust policy defines who is allowed to assume the role. The permissions policy defines the permissions they are granted.</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the session policy\" is incorrect.</p><p>Session policies limit the permissions that the role or user's identity-based policies grant to the session. The role\u2019s trust policy defines who is allowed to assume the role</p><p><strong>INCORRECT:</strong> \"Add an aws:MultiFactorAuthPresent : true condition to the S3 bucket policy\" is incorrect.</p><p>This will restrict access to the bucket if the user is not authenticated with MFA. However, the requirement is to prevent the user from assuming the role if they are not authenticated with MFA.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-do-i-protect-cross-account-access-using-mfa-2/\">https://aws.amazon.com/blogs/security/how-do-i-protect-cross-account-access-using-mfa-2/</a></p>", "answers": ["<p>Add a aws:MultiFactorAuthPresent : true condition to the role\u2019s permissions policy.</p>", "<p>Add a aws:MultiFactorAuthPresent : true condition to the session policy.</p>", "<p>Add a aws:MultiFactorAuthPresent : true condition to the S3 bucket policy.</p>", "<p>Add a aws:MultiFactorAuthPresent : true condition to the role's trust policy.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has two AWS accounts: A production account and a development account. Developers with user accounts in the production account need to be able to access artifacts stored in an Amazon S3 bucket in the development account when deploying resources.A cross-account role has been created in the development account with access to the S3 bucket. The security team requires that the users can assume the role only if they are authenticated with multi-factor authentication (MFA).Which step should the security engineer take to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265684, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer is working with the development team to design an application that encrypts data using an AWS KMS key. Various users with accounts in AWS IAM will need to be provided with temporary access to decrypt data using the KMS key.</p><p>What is the MOST efficient way to manage access control for the KMS key?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A <em>grant</em> is a policy instrument that allows AWS principals to use KMS keys in cryptographic operations. When authorizing access to a KMS key, grants are considered along with key policies and IAM policies. Grants are often used for temporary permissions because you can create one, use its permissions, and delete it without changing your key policies or IAM policies.</p><p><strong>CORRECT: </strong>\"Use KMS grants. Programmatically create and revoke grants to manage access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use KMS key policies. Programmatically update the KMS key policies to manage access\" is incorrect.</p><p>Grants are better suited for temporary permissions compared to key policies.</p><p><strong>INCORRECT:</strong> Use an IAM role. Programmatically update the IAM role policies to manage access\" is incorrect.</p><p>Permissions to use a specific KMS key can be assigned through key policies, IAM policies, grants or VPC endpoint policies. However, grants are the best option for temporary access.</p><p><strong>INCORRECT:</strong> \"Use an IAM group. Programmatically add and remove users from the group and attach a policy that grants key access\" is incorrect.</p><p>As above, for temporary access and KMS grant is a better option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html\">https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html</a></p>", "answers": ["<p>Use an IAM role. Programmatically update the IAM role policies to manage access.</p>", "<p>Use KMS key policies. Programmatically update the KMS key policies to manage access.</p>", "<p>Use an IAM group. Programmatically add and remove users from the group and attach a policy that grants key access.</p>", "<p>Use KMS grants. Programmatically create and revoke grants to manage access.</p>"]}, "correct_response": ["d"], "section": "Domain 5 - Data Protection", "question_plain": "A security engineer is working with the development team to design an application that encrypts data using an AWS KMS key. Various users with accounts in AWS IAM will need to be provided with temporary access to decrypt data using the KMS key.What is the MOST efficient way to manage access control for the KMS key?", "related_lectures": []}, {"_class": "assessment", "id": 70265686, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer needs to access log files generated by AWS CloudTrail. The trail stores log files in an Amazon S3 bucket that is encrypted with AWS KMS managed keys (SSE-KMS). The logs should be accessed by assuming an IAM role. When attempting to access the log files the security engineer experienced an access denied error.</p><p>What is the MOST likely cause of this issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To use SSE-KMS with CloudTrail, you create and manage a KMS key. You attach a policy to the key that determines which users can use the key for encrypting and decrypting CloudTrail log files. The decryption is seamless through S3. When authorized users of the key read CloudTrail log files, S3 manages the decryption, and the authorized users can read log files in unencrypted form.</p><p>Permissions to use a key can be granted in IAM permissions policies attached to users, groups, or roles, or they can be granted on the S3 bucket policy. In this case the engineer assumes a role and the role may not have the permissions needed to decrypt the data.</p><p><strong>CORRECT: </strong>\"The KMS key policy does not grant the IAM role permissions to use the key for decryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The KMS key policy does not grant AWS CloudTrail the permissions to write encrypted log files\" is incorrect.</p><p>The log files exist so they must have been written already.</p><p><strong>INCORRECT:</strong> \"The S3 bucket policy does not grant AWS CloudTrail permissions to use the key for decryption\" is incorrect.</p><p>CloudTrail should be granted decrypt permissions but in this case the log files exist, so CloudTrail has been able to write and encrypt the files. Also, when the user is trying to access the files decryption happens through S3, not CloudTrail.</p><p><strong>INCORRECT:</strong> \"The log files in the S3 bucket can only be decrypted and viewed using the AWS CloudTrail console\" is incorrect.</p><p>This is not true. You can access the files directly through S3 if you have read permissions to S3 and decrypt permissions to use the KMS key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html</a></p>", "answers": ["<p>The KMS key policy does not grant the IAM role permissions to use the key for decryption.</p>", "<p>The KMS key policy does not grant AWS CloudTrail the permissions to write encrypted log files.</p>", "<p>The S3 bucket policy does not grant AWS CloudTrail permissions to use the key for decryption.</p>", "<p>The log files in the S3 bucket can only be decrypted and viewed using the AWS CloudTrail console.</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A security engineer needs to access log files generated by AWS CloudTrail. The trail stores log files in an Amazon S3 bucket that is encrypted with AWS KMS managed keys (SSE-KMS). The logs should be accessed by assuming an IAM role. When attempting to access the log files the security engineer experienced an access denied error.What is the MOST likely cause of this issue?", "related_lectures": []}, {"_class": "assessment", "id": 70265688, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Temporary security credentials that were issued by the AWS Security Token Service (STS) may have been compromised. A security engineer needs to immediately revoke the credentials so they cannot be used with any AWS service.</p><p>Which action should the security engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>We can determine that the credentials were issued to a role rather than a user as users will use access keys rather than obtaining the credentials from AWS STS. When a role needs permissions to access an AWS service it must call the AWS STS service to obtain temporary security credentials.</p><p>The best way to immediately revoke the credentials is to use the AWS management console to revoke all active sessions for the IAM role. IAM immediately attaches a policy named AWSRevokeOlderSessions to the role. The policy denies all access to users who assumed the role before the moment you chose Revoke active sessions.</p><p><strong>CORRECT: </strong>\"Use the AWS management console to revoke active sessions for the IAM role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AWS management console to revoke active sessions for the IAM user\" is incorrect.</p><p>As explained above, an IAM role was used in this scenario, not an IAM user.</p><p><strong>INCORRECT:</strong> \"Use the AWS management console to delete the IAM role\" is incorrect.</p><p>The best way to prevent the credentials from being used is to revoke all active sessions. Any user who assumes the role <em>after </em>you chose to revoke active sessions is not affected by this action.</p><p><strong>INCORRECT:</strong> \"Use the AWS management console to delete the IAM user\" is incorrect.</p><p>As explained above, an IAM role rather than a user was used in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_revoke-sessions.html</a></p>", "answers": ["<p>Use the AWS management console to revoke active sessions for the IAM role.</p>", "<p>Use the AWS management console to delete the IAM role.</p>", "<p>Use the AWS management console to revoke active sessions for the IAM user.</p>", "<p>Use the AWS management console to delete the IAM user.</p>"]}, "correct_response": ["a"], "section": "Domain 1 - Incident Response", "question_plain": "Temporary security credentials that were issued by the AWS Security Token Service (STS) may have been compromised. A security engineer needs to immediately revoke the credentials so they cannot be used with any AWS service.Which action should the security engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 70265690, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has as an AWS Organization for developers. The organization includes several accounts and SCPs are used to control access to AWS services. A single SCP exists at the root of the organization and has the following policy statements:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_03-20-17-63fdc92057a0a8cb88e757cd2193068f.jpg\"><p>A group of developers are working on a project that requires an Amazon RDS database. These developers use an account that is in a child OU with an SCP attached that allows all Amazon RDS API actions. The developers have full IAM permissions for RDS but are unable to launch RDS database instances.</p><p>Which change must a security engineer implement so that the developers can access Amazon RDS?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Inheritance for service control policies behaves like a filter through which permissions flow to all parts of the tree below. If an action is blocked by a Deny statement, then all OUs and accounts affected by that SCP are denied access to that action. An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can <strong><em>only</em></strong> filter; they never add permissions.</p><p>In this case the root SCP denies all actions for Amazon RDS. This deny statement will flow down to all OUs and accounts in the hierarchy. Therefore it is not possible to allow access to RDS anywhere in the organization if this policy statement exists.</p><p>The simplest solution to this issue is to simply remove the deny statement for RDS at the root level.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-20-17-59111c1ef9f0c147d83cdfe31c3f7dfb.jpg\"><p><strong>CORRECT: </strong>\"Remove the deny statement for Amazon RDS from the root SCP\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Remove the root SCP from the root of the organization\" is incorrect.</p><p>You must always have a policy attached at the root and any OU level. You can remove the root SCP but only after you have attached another SCP.</p><p><strong>INCORRECT:</strong> \"Add a statement to the SCP of the child OU that allows all actions on all resources\" is incorrect.</p><p>The deny statement at the root SCP level overrides any allows anywhere else in the organization.</p><p><strong>INCORRECT:</strong> \"Attach a resource-based policy to Amazon RDS granting launch permissions\" is incorrect.</p><p>You cannot attach resource-based policies to Amazon RDS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html</a></p>", "answers": ["<p>Add a statement to the SCP of the child OU that allows all actions on all resources.</p>", "<p>Remove the deny statement for Amazon RDS from the root SCP.</p>", "<p>Attach a resource-based policy to Amazon RDS granting launch permissions.</p>", "<p>Remove the root SCP from the root of the organization.</p>"]}, "correct_response": ["b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has as an AWS Organization for developers. The organization includes several accounts and SCPs are used to control access to AWS services. A single SCP exists at the root of the organization and has the following policy statements:A group of developers are working on a project that requires an Amazon RDS database. These developers use an account that is in a child OU with an SCP attached that allows all Amazon RDS API actions. The developers have full IAM permissions for RDS but are unable to launch RDS database instances.Which change must a security engineer implement so that the developers can access Amazon RDS?", "related_lectures": []}, {"_class": "assessment", "id": 70265692, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has configured federation between an on-premises identity provider (IdP) and AWS. Developers authenticate into an identity account and assume an IAM role named IdPUsersRole. The developers then access a production account by assuming a role named ProdDevRole in the production account.</p><p>Developers are unable to assume the IAM role in the production account. The policy attached to the role in the identity account is:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_03-21-58-1df8c3f4bcc13f6e7590603358e86056.jpg\"><p>What needs to be done to enable the developers to assume the appropriate role in the production account?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The developer users have been granted the permissions they need in the identity account to be able to assume a role in the production account. Additionally, the developers will need permissions in the production account to be able to assume the role there.</p><p>The \u201csts: AssumeRole\u201d API action is performed when assuming a role and the IAM role\u2019s trust policy determines who is allowed to perform this API action. In this case the trust policy for the IAM role in the production account must be configured to allow this API action for the principal name of the IAM role in the identity account.</p><p>The trust policy should be configured as per the example below. The production account number in this case is \u201c123412341234\u201d.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-21-58-d12a35f71e93197a579e4467adb10ac8.jpg\"><p><strong>CORRECT: </strong>\"Update the trust policy on the role in the production account to allow the \u201csts: AssumeRole\u201d API action for the IdPUsersRole principal in the identity account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the IAM policy attached to the role in the identity account to allow the \u201csts: AssumeRole\u201d API action for the ProdDevRole principal in the production account\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Update the trust policy on the role in the identity account to allow the \u201csts: AssumeRole\u201d API action for the IdPUsersRole principal in the identity account\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Update the IAM policy attached to the role in the target account to allow the \u201csts: AssumeRole\u201d API action for the ProdDevRole principal in the production account\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html</a></p>", "answers": ["<p>Update the trust policy on the role in the production account to allow the \u201csts: AssumeRole\u201d API action for the IdPUsersRole principal in the identity account.</p>", "<p>Update the IAM policy attached to the role in the identity account to allow the \u201csts: AssumeRole\u201d API action for the ProdDevRole principal in the production account.</p>", "<p>Update the trust policy on the role in the identity account to allow the \u201csts: AssumeRole\u201d API action for the IdPUsersRole principal in the identity account.</p>", "<p>Update the IAM policy attached to the role in the target account to allow the \u201csts: AssumeRole\u201d API action for the ProdDevRole principal in the production account.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has configured federation between an on-premises identity provider (IdP) and AWS. Developers authenticate into an identity account and assume an IAM role named IdPUsersRole. The developers then access a production account by assuming a role named ProdDevRole in the production account.Developers are unable to assume the IAM role in the production account. The policy attached to the role in the identity account is:What needs to be done to enable the developers to assume the appropriate role in the production account?", "related_lectures": []}, {"_class": "assessment", "id": 70265694, "assessment_type": "multi-select", "prompt": {"question": "<p>An operations engineer plans to launch a collection of Amazon EC2 instances. The instances will run a custom application which will be managed by operations users who are members of a group. The operations users should be granted access only to the custom application instances.</p><p>Which actions should a security engineer take to control access? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The condition element of an IAM policy can be used to identify the EC2 instances to which access should be granted. The IAM policy can be attached to the users or groups that require access. The best practice is to add the users to a group and attach the policy to the group. In this case the Condition element can be used to identify the instances by tag.</p><p><strong>CORRECT: </strong>\"Add specific tags to the Amazon EC2 instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM policy to the operations users that grants access to the instances with the specific tag using the Condition element\" is a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an IAM role to the Amazon EC2 instances\" is incorrect.</p><p>An IAM role would give the instance permissions to other AWS services. In this case the question asks how the operations users can be granted access to the instances.</p><p><strong>INCORRECT:</strong> \"Attach an instance profile to the Amazon EC2 instances\" is incorrect.</p><p>When you attach an IAM role to an instance you do so using an instance profile. As per the previous explanation this is assigning permissions to EC2 rather than the users.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy that grants access to the instances based on the Principal element\" is incorrect.</p><p>The condition element should be used to identify the tags. The principal element is not relevant when attaching a policy to a user, role, or group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html#amazonec2-policy-keys\">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html#amazonec2-policy-keys</a></p>", "answers": ["<p>Attach an IAM role to the Amazon EC2 instances.</p>", "<p>Attach an instance profile to the Amazon EC2 instances.</p>", "<p>Create an IAM policy that grants access to the instances based on the Principal element.</p>", "<p>Attach an IAM policy to the operations users that grants access to the instances with the specific tag using the Condition element.</p>", "<p>Add specific tags to the Amazon EC2 instances.</p>"]}, "correct_response": ["d", "e"], "section": "Domain 5 - Data Protection", "question_plain": "An operations engineer plans to launch a collection of Amazon EC2 instances. The instances will run a custom application which will be managed by operations users who are members of a group. The operations users should be granted access only to the custom application instances.Which actions should a security engineer take to control access? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265696, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has several AWS Lambda functions. While reviewing the Lambda functions a security engineer discovers that sensitive information is being stored in environment variables and is viewable as plaintext in the Lambda console. The values of the sensitive information are 8 KB in size and there are over 10,000 values stored across the functions.</p><p>What is the MOST cost-effective way to address this security issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Secrets Manager is well suited to this use case. Secrets can be stored with values up to 10KB in size and the maximum number of secrets within a Region is 500,000.</p><p>Both Systems Manager Parameter Store and Secrets Manager support controlling access to the information stored using IAM policies.</p><p>Pricing for secrets manager includes the cost of storing the secret value as well as API calls, refer to the pricing page linked below for more information.</p><p><strong>CORRECT: </strong>\"Store the environment variables in AWS Secrets Manager and access them at runtime. Use IAM permissions to restrict access to the secrets to only the Lambda functions that require access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the environment variables in AWS Systems Manager Parameter Store as standard parameters and access them at runtime. Use IAM permissions to restrict access to the parameters to only the Lambda functions that require access\" is incorrect.</p><p>Standard parameters support up to 4KB for the parameter value and so cannot be used for this use case. Also, the limit of 10,000 parameters would be reached leaving no room for future growth.</p><p><strong>INCORRECT:</strong> \"Store the environment variables in an encrypted Amazon EFS file system and access them at runtime. Use POSIX permissions to restrict access to only the Lambda functions that require access\" is incorrect.</p><p>EFS would be an expensive solution and Lambda cannot mount an EFS file system.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to store the environment variables. Access the environment variables at runtime. Use IAM permissions to restrict access to the environment variables to only the Lambda functions that require access\" is incorrect.</p><p>Config cannot be used to store information such as environment variables.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access.html</a></p><p><a href=\"https://aws.amazon.com/secrets-manager/pricing/\">https://aws.amazon.com/secrets-manager/pricing/</a></p>", "answers": ["<p>Store the environment variables in an encrypted Amazon EFS file system and access them at runtime. Use POSIX permissions to restrict access to only the Lambda functions that require access.</p>", "<p>Use AWS Config to store the environment variables. Access the environment variables at runtime. Use IAM permissions to restrict access to the environment variables to only the Lambda functions that require access.</p>", "<p>Store the environment variables in AWS Secrets Manager and access them at runtime. Use IAM permissions to restrict access to the secrets to only the Lambda functions that require access.</p>", "<p>Store the environment variables in AWS Systems Manager Parameter Store as standard parameters and access them at runtime. Use IAM permissions to restrict access to the parameters to only the Lambda functions that require access.</p>"]}, "correct_response": ["c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company has several AWS Lambda functions. While reviewing the Lambda functions a security engineer discovers that sensitive information is being stored in environment variables and is viewable as plaintext in the Lambda console. The values of the sensitive information are 8 KB in size and there are over 10,000 values stored across the functions.What is the MOST cost-effective way to address this security issue?", "related_lectures": []}, {"_class": "assessment", "id": 70265698, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An online fitness platform based in Germany uses Amazon Cognito with the Cognito Hosted UI to manage user registrations and sign-ins. Recently, the platform's security team has noticed an unusual number of fraudulent sign-ups originating from outside Germany.</p><p>The security team wants to implement a mechanism that can add a layer of custom validation during the registration process that checks the location of the customer. The mechanism should be able to accept or reject user registration requests based on the outcome of the validation process.</p><p>Which solution should the security team implement to fulfill these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Lambda triggers for Amazon Cognito can add custom validation to user pool workflows. A Pre sign-up Lambda trigger can perform custom validation during sign-up, such as checking the geographical origin of the registration request.</p><p><strong>CORRECT: </strong>\"Configure a Pre Sign-Up AWS Lambda trigger and associate it with the Amazon Cognito user pool to execute custom validation during sign-up\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement AWS WAF with a geographic match statement and connect it to the Amazon Cognito user pool's domain to filter requests based on geographical origin\" is incorrect.</p><p>AWS WAF can protect the Cognito User Pool's Hosted UI from web exploits, but it does not control or influence user registration or authentication directly in the user pool.</p><p><strong>INCORRECT:</strong> \"Enable a multi-factor authentication (MFA) method in the Cognito user pool to add an extra layer of security during the user authentication process\" is incorrect.</p><p>While multi-factor authentication (MFA) adds an additional layer of security during user authentication, it does not provide control over the registration process or validation based on geographical location.</p><p><strong>INCORRECT:</strong> \"Configure a social identity provider (IdP) in Amazon Cognito to validate requests through the hosted UI\" is incorrect.</p><p>A social identity provider (IdP) can simplify the authentication process but does not provide a mechanism to prevent fraudulent sign-ups from specific geographic locations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools-working-with-aws-lambda-triggers.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools-working-with-aws-lambda-triggers.html</a></p>", "answers": ["<p>Implement AWS WAF with a geographic match statement and connect it to the Amazon Cognito user pool's domain to filter requests based on geographical origin.</p>", "<p>Configure a Pre Sign-Up AWS Lambda trigger and associate it with the Amazon Cognito user pool to execute custom validation during sign-up.</p>", "<p>Enable a multi-factor authentication (MFA) method in the Cognito user pool to add an extra layer of security during the user authentication process.</p>", "<p>Configure a social identity provider (IdP) in Amazon Cognito to validate requests through the hosted UI.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "An online fitness platform based in Germany uses Amazon Cognito with the Cognito Hosted UI to manage user registrations and sign-ins. Recently, the platform's security team has noticed an unusual number of fraudulent sign-ups originating from outside Germany.The security team wants to implement a mechanism that can add a layer of custom validation during the registration process that checks the location of the customer. The mechanism should be able to accept or reject user registration requests based on the outcome of the validation process.Which solution should the security team implement to fulfill these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265700, "assessment_type": "multi-select", "prompt": {"question": "<p>A multinational corporation uses Amazon S3 for storing various types of files across a multitude of S3 buckets, each of which contains an extensive number of objects. The company's security team is keen on analyzing object access patterns, such as pinpointing the top 50 most accessed objects, the 20 largest downloaded files, and the objects with the lengthiest download time.</p><p>The team intends to display this information in an intuitive, interactive dashboard, utilizing SQL queries for efficient analysis.</p><p>Which combination of AWS services should a security engineer use to fulfill these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon S3 server access logging is an essential feature for recording detailed logs of the requests made to S3 buckets. The logs will contain the data needed for analysis, such as the objects accessed, the size of the data transferred, and the time taken.</p><p>Amazon Athena is an interactive query service that enables you to analyze data directly in Amazon S3 using standard SQL.</p><p>Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service for building visualizations, performing ad-hoc analysis, and quickly getting business insights from your data.</p><p>Combining Athena with QuickSight, you can execute SQL queries on the server access logs and visualize the results in an interactive dashboard.</p><p><strong>CORRECT: </strong>\"Enable Amazon S3 server access logging to monitor and record detailed logs of the requests made to the S3 buckets\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use Amazon Athena to perform SQL queries on the server access logs in S3 and employ Amazon QuickSight for visualizing the analyzed data in an interactive dashboard\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy Amazon GuardDuty to assess potential security threats and unauthorized access to the stored data\" is incorrect.</p><p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior, but it doesn\u2019t allow for detailed analysis or visualization of S3 access patterns, as required in this scenario.</p><p><strong>INCORRECT:</strong> \"Utilize Amazon CloudWatch Logs Insights to perform SQL queries and analyze the logs from AWS services\" is incorrect.</p><p>Amazon CloudWatch Logs Insights is primarily used for analyzing and visualizing logs from AWS services. While it can be used for querying logs, in this scenario, the combination of Athena for querying and QuickSight for visualization is more appropriate for analyzing S3 access logs.</p><p><strong>INCORRECT:</strong> \"Leverage AWS Lambda to automatically process and filter the server access logs and store the results in an Amazon DynamoDB table\" is incorrect.</p><p>While AWS Lambda can be used to process logs, it is not ideal for performing complex SQL queries on large datasets or for visualizing the data. In this scenario, Amazon Athena and Amazon QuickSight are better suited for analyzing and visualizing the S3 server access logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html\">https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html</a></p>", "answers": ["<p>Enable Amazon S3 server access logging to monitor and record detailed logs of the requests made to the S3 buckets.</p>", "<p>Use Amazon Athena to perform SQL queries on the server access logs in S3 and employ Amazon QuickSight for visualizing the analyzed data in an interactive dashboard.</p>", "<p>Deploy Amazon GuardDuty to assess potential security threats and unauthorized access to the stored data.</p>", "<p>Utilize Amazon CloudWatch Logs Insights to perform SQL queries and analyze the logs from AWS services.</p>", "<p>Leverage AWS Lambda to automatically process and filter the server access logs and store the results in an Amazon DynamoDB table.</p>"]}, "correct_response": ["a", "b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A multinational corporation uses Amazon S3 for storing various types of files across a multitude of S3 buckets, each of which contains an extensive number of objects. The company's security team is keen on analyzing object access patterns, such as pinpointing the top 50 most accessed objects, the 20 largest downloaded files, and the objects with the lengthiest download time.The team intends to display this information in an intuitive, interactive dashboard, utilizing SQL queries for efficient analysis.Which combination of AWS services should a security engineer use to fulfill these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265702, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A business is developing a cloud-native application on AWS and has selected AWS CodeBuild for automating the process of building, testing, and packaging their software code. To meet their security requirements, the company needs to ensure that all CodeBuild API operations executed within the VPC do not traverse the public internet.</p><p>What should a security engineer do to meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An interface VPC endpoint (powered by AWS PrivateLink) enables you to connect your VPC to supported AWS services, including CodeBuild, and services hosted by other AWS accounts. This setup enables you to keep the network traffic within the AWS network without traversing the public internet.</p><p><strong>CORRECT: </strong>\"Deploy an interface VPC endpoint for CodeBuild API operations\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch a NAT gateway in a public subnet in the VPC\" is incorrect.</p><p>NAT gateways enable instances in a private subnet to connect to the internet or other AWS services but prevent the internet from initiating a connection with those instances. It does not provide the private link needed for AWS CodeBuild API operations to occur within the VPC.</p><p><strong>INCORRECT:</strong> \"Implement a gateway VPC endpoint for CodeBuild API operations\" is incorrect.</p><p>Gateway VPC endpoints are used to connect VPCs to Amazon S3 and DynamoDB. They cannot be used to provide access to CodeBuild, which requires an interface VPC endpoint.</p><p><strong>INCORRECT:</strong> \"Establish a connection to the VPC using AWS Direct Connect\" is incorrect.</p><p>AWS Direct Connect is a service that establishes a dedicated network connection from your premises to AWS. However, it is not required or directly related to enabling private API communication within a VPC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/use-vpc-endpoints-with-codebuild.html</a></p>", "answers": ["<p>Launch a NAT gateway in a public subnet in the VPC.</p>", "<p>Deploy an interface VPC endpoint for CodeBuild API operations.</p>", "<p>Implement a gateway VPC endpoint for CodeBuild API operations.</p>", "<p>Establish a connection to the VPC using AWS Direct Connect.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A business is developing a cloud-native application on AWS and has selected AWS CodeBuild for automating the process of building, testing, and packaging their software code. To meet their security requirements, the company needs to ensure that all CodeBuild API operations executed within the VPC do not traverse the public internet.What should a security engineer do to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 70265704, "assessment_type": "multi-select", "prompt": {"question": "<p>A fintech company offers a web application that stores files on Amazon S3 and processes transactions on Amazon EC2. Users are complaining about slow response times, and recent cybersecurity audits have raised concerns about web content security.</p><p>The company needs to accelerate content delivery while enhancing security and privacy, without altering the application code.</p><p>What combination of actions should the company undertake to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon CloudFront can speed up the distribution of static and dynamic web content, such as .html, .css, .js, and image files, to users. CloudFront delivers content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency.</p><p>Lambda@Edge allows you to run AWS Lambda functions in response to CloudFront events, which enables dynamic, personalized content generation close to your users. You can write Lambda@Edge functions to modify headers, which would increase security without changing application code.</p><p><strong>CORRECT: </strong>\"Configure Amazon CloudFront and set up a distribution for the S3 and EC2 origins to accelerate content delivery\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use an AWS Lambda function configured with CloudFront (Lambda@Edge) to add HTTP security headers on origin responses\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS WAF web ACL with predefined AWS managed rules to add HTTP security headers to origin responses\" is incorrect.</p><p>AWS WAF is a web application firewall that helps protect web applications from common web exploits. However, it does not add HTTP security headers to responses.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon ElastiCache to cache the responses of the application endpoints\" is incorrect.</p><p>Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. While it can help with caching, it wouldn't directly address the issue of adding HTTP security headers.</p><p><strong>INCORRECT:</strong> \"Utilize an Application Load Balancer (ALB) with enabled Sticky Sessions to preserve the connection header from incoming client requests after forwarding the response back to the client\" is incorrect.</p><p>While an ALB can improve load balancing of incoming application traffic, it does not directly address the need for improved content delivery speed or enhanced HTTP security.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/</a></p>", "answers": ["<p>Configure Amazon CloudFront and set up a distribution for the S3 and EC2 origins to accelerate content delivery.</p>", "<p>Use an AWS WAF web ACL with predefined AWS managed rules to add HTTP security headers to origin responses.</p>", "<p>Use an AWS Lambda function configured with CloudFront (Lambda@Edge) to add HTTP security headers on origin responses.</p>", "<p>Deploy Amazon ElastiCache to cache the responses of the application endpoints.</p>", "<p>Utilize an Application Load Balancer (ALB) with enabled Sticky Sessions to preserve the connection header from incoming client requests after forwarding the response back to the client.</p>"]}, "correct_response": ["a", "c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A fintech company offers a web application that stores files on Amazon S3 and processes transactions on Amazon EC2. Users are complaining about slow response times, and recent cybersecurity audits have raised concerns about web content security.The company needs to accelerate content delivery while enhancing security and privacy, without altering the application code.What combination of actions should the company undertake to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265706, "assessment_type": "multi-select", "prompt": {"question": "<p>A streaming media company is using an Application Load Balancer (ALB) to manage the traffic of its on-demand video service. Recently, they deployed Amazon CloudFront to accelerate content delivery. Despite this, the company noticed that some requests are still bypassing CloudFront and reaching the ALB directly.</p><p>The company needs to ensure that its Amazon EC2 instances, which are behind the ALB, only process traffic coming from CloudFront.</p><p>Which combination of steps should the company undertake to fulfill these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By setting up CloudFront to include a custom HTTP header in requests, you can create a unique identifier that the ALB can check to validate traffic coming from CloudFront.</p><p>After setting up CloudFront to include a custom HTTP header in requests, you would then configure the ALB to deny any requests that do not include that custom header. This ensures that only traffic from CloudFront is processed by the EC2 instances.</p><p><strong>CORRECT: </strong>\"Configure CloudFront to add a custom HTTP header to requests that CloudFront sends to the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the ALB to deny requests that do not contain the custom HTTP header\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Set up CloudFront to implement a unique User-Agent header to be validated by the ALB\" is incorrect.</p><p>While using a unique User-Agent header could theoretically be used as a method of identifying traffic, this header is easily spoofed and would not provide a reliable means of security.</p><p><strong>INCORRECT:</strong> \"Configure the ALB and CloudFront to utilize the X-Forwarded-For header to validate client IP addresses\" is incorrect.</p><p>The X-Forwarded-For header is used to identify the originating IP address of a client connecting to a web server through an HTTP proxy or load balancer. This would not be applicable in this situation, as the goal is to prevent direct traffic to the ALB, not validate client IP addresses.</p><p><strong>INCORRECT:</strong> \"Implement the same SSL/TLS certificate, issued by AWS Certificate Manager (ACM), on both the ALB and CloudFront\" is incorrect.</p><p>While using the same SSL/TLS certificate for both ALB and CloudFront would be beneficial from a security standpoint, it would not contribute to preventing direct traffic to the ALB.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/add-origin-custom-headers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/add-origin-custom-headers.html</a></p>", "answers": ["<p>Configure CloudFront to add a custom HTTP header to requests that CloudFront sends to the ALB.</p>", "<p>Configure the ALB to deny requests that do not contain the custom HTTP header.</p>", "<p>Set up CloudFront to implement a unique User-Agent header to be validated by the ALB.</p>", "<p>Configure the ALB and CloudFront to utilize the X-Forwarded-For header to validate client IP addresses.</p>", "<p>Implement the same SSL/TLS certificate, issued by AWS Certificate Manager (ACM), on both the ALB and CloudFront.</p>"]}, "correct_response": ["a", "b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A streaming media company is using an Application Load Balancer (ALB) to manage the traffic of its on-demand video service. Recently, they deployed Amazon CloudFront to accelerate content delivery. Despite this, the company noticed that some requests are still bypassing CloudFront and reaching the ALB directly.The company needs to ensure that its Amazon EC2 instances, which are behind the ALB, only process traffic coming from CloudFront.Which combination of steps should the company undertake to fulfill these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265708, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses Microsoft Active Directory (AD) for access management for on-premises resources. They wish to use the same Microsoft AD for authenticating to AWS including accessing the AWS Management Console. All identity data must remain on-premises</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This solution uses federated single sign-on (SSO), which lets users sign into the AWS Management Console or make programmatic calls to AWS APIs by using assertions from a SAML-compliant identity provider (IdP) like ADFS. With this solution all identity data is stored in the on-premises directory and only authentication tokens are used within AWS. This meets the security requirements for the authentication solution.</p><p>The following image depicts the steps involved in identity federation:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-07-49-c070d107993b24f47075b941c6b93b35.jpg\"><p><strong>CORRECT: </strong>\"Set up federated sign-in to AWS through ADFS and SAML\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy domain controllers on Amazon EC2\" is incorrect.</p><p>This would involve replicating the AD to the EC2 DCs which would mean identities are replicated to AWS.</p><p><strong>INCORRECT:</strong> \"Create an AWS Managed Microsoft AD\" is incorrect.</p><p>This would involve creating an Active Directory within AWS so the identity data would be stored there. You would then need to setup trust relationships.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Cognito Identity Pool\" is incorrect.</p><p>Amazon Cognito is used for authenticating to web and mobile applications and is not related to Microsoft AD.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/enabling-federation-to-aws-using-windows-active-directory-adfs-and-saml-2-0/\">https://aws.amazon.com/blogs/security/enabling-federation-to-aws-using-windows-active-directory-adfs-and-saml-2-0/</a></p>", "answers": ["<p>Deploy domain controllers on Amazon EC2.</p>", "<p>Create an AWS Managed Microsoft AD.</p>", "<p>Create an Amazon Cognito Identity Pool.</p>", "<p>Set up federated sign-in to AWS through ADFS and SAML.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company uses Microsoft Active Directory (AD) for access management for on-premises resources. They wish to use the same Microsoft AD for authenticating to AWS including accessing the AWS Management Console. All identity data must remain on-premisesWhich solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265710, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security vulnerability has been discovered that could lead to sensitive data being leaked on TCP port 5601. The development team is working on updating the code, but it could take several days. A security engineer must identify any hosts attempting to send data over port 5601 and prevent the traffic leaving the network.</p><p>How can the security engineer accomplish this goal?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Flow logs can publish flow log data directly to Amazon CloudWatch. When publishing to CloudWatch Logs, flow log data is published to a log group, and each network interface has a unique log stream in the log group.</p><p>A metric filter can be created that searches for specific event patterns such as connection attempts to TCP port 5601. The metric filter can trigger an alarm that sends an Amazon SNS notification to the security team.</p><p>The security team should then update the Network ACL of any affected subnets to block outbound access on the specific port.</p><p><strong>CORRECT: </strong>\"Capture IP traffic using VPC Flow Logs and create a metric filter with an alarm that notifies the security team if connection attempts are made. Then, update NACLs to block the traffic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the unified CloudWatch agent on Amazon EC2 instances and collect log files in CloudWatch Logs. Create a metric filter with an alarm that notifies the security team if connection attempts are made. Then, update NACLs to block the traffic\" is incorrect.</p><p>The unified CloudWatch agent does not send metrics reporting which ports the instance connects to.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to log API actions in an Amazon S3 bucket. Use Amazon Athena to query the log files and identify attempts to connect. Then, update security groups to block the traffic\" is incorrect.</p><p>CloudTrail API actions are not initiated for sending network packets so the required information will not be recorded in the log files. Also, you cannot create deny rules with security groups so NACLs should be updated instead.</p><p><strong>INCORRECT:</strong> \"Run an Amazon Inspector custom network assessment to identify Amazon EC2 instances that have TCP port 5601 open. Then, update security groups to block the traffic\" is incorrect.</p><p>Amazon Inspector checks for network reachability by checking which ports your instance is listening on. It does not analyze which ports your instance connects to outbound.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-cwl.html</a></p>", "answers": ["<p>Use AWS CloudTrail to log API actions in an Amazon S3 bucket. Use Amazon Athena to query the log files and identify attempts to connect. Then, update security groups to block the traffic.</p>", "<p>Capture IP traffic using VPC Flow Logs and create a metric filter with an alarm that notifies the security team if connection attempts are made. Then, update NACLs to block the traffic.</p>", "<p>Install the unified CloudWatch agent on Amazon EC2 instances and collect log files in CloudWatch Logs. Create a metric filter with an alarm that notifies the security team if connection attempts are made. Then, update NACLs to block the traffic.</p>", "<p>Run an Amazon Inspector custom network assessment to identify Amazon EC2 instances that have TCP port 5601 open. Then, update security groups to block the traffic.</p>"]}, "correct_response": ["b"], "section": "Domain 1 - Incident Response", "question_plain": "A security vulnerability has been discovered that could lead to sensitive data being leaked on TCP port 5601. The development team is working on updating the code, but it could take several days. A security engineer must identify any hosts attempting to send data over port 5601 and prevent the traffic leaving the network.How can the security engineer accomplish this goal?", "related_lectures": []}, {"_class": "assessment", "id": 70265712, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer must implement a solution to allow the company's SysOps team to have interactive command line access to Amazon EC2 Linux instances using the AWS Management Console. The solution should minimize the attack surface of the EC2 instances.</p><p>Which steps should the security engineer take to satisfy this requirement while maintaining least privilege? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS Systems Manager Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI).</p><p>Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>To use Session Manager you must use a supported operating system and have the SSM agent installed. Session Manager also requires specific permissions which can be granted by attaching an IAM policy to an instance profile.</p><p>Role based access control for users can be implemented by using IAM user policies. In this case the SysOps team can be granted access to use Session Manager to connect to the EC2 instances.</p><p><strong>CORRECT: </strong>\"Install the Systems Manager SSM Agent on the EC2 Linux instances and attach an IAM role that grants permissions\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an IAM user policy for Session Manager access granting the SysOps team access to the EC2 Linux instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM user policy granting the SysOps team access to the EC2 management console\" is incorrect.</p><p>This is not a step that is required for connecting using Session Manager. Instead the user policy should grant access to connect using Session Manager to the instances.</p><p><strong>INCORRECT:</strong> \"Configure the security group for the EC2 Linux instances to allow inbound traffic on SSH port 22 from AWS IP addresses\" is incorrect.</p><p>With Session Manager you do not need to open ports to connect.</p><p><strong>INCORRECT:</strong> \"Enable EC2 instance connect and generate new private and public keys using the AWS CLI\" is incorrect.</p><p>EC2 instance connect relies on opening ports and security groups to connect. Instead the solution should use Session Manager to minimize the attack surface.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>", "answers": ["<p>Install the Systems Manager SSM Agent on the EC2 Linux instances and attach an IAM role that grants permissions.</p>", "<p>Create an IAM user policy granting the SysOps team access to the EC2 management console.</p>", "<p>Configure the security group for the EC2 Linux instances to allow inbound traffic on SSH port 22 from AWS IP addresses.</p>", "<p>Enable EC2 instance connect and generate new private and public keys using the AWS CLI.</p>", "<p>Create an IAM user policy for Session Manager access granting the SysOps team access to the EC2 Linux instances.</p>"]}, "correct_response": ["a", "e"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer must implement a solution to allow the company's SysOps team to have interactive command line access to Amazon EC2 Linux instances using the AWS Management Console. The solution should minimize the attack surface of the EC2 instances.Which steps should the security engineer take to satisfy this requirement while maintaining least privilege? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265714, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has created an organization in AWS Organizations. The company has several accounts and OUs and uses the default FullAWSAccess SCP. A security engineer needs to ensure that no one in member accounts can disable specific AWS services. The security engineer must ensure that permissions granted by IAM policies defined in member accounts are not overridden.</p><p>What will be the effect of adding the following SCP to the root of the organization?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-07-07_17-12-00-44ee2150989bfe7292d14ec11741d508.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A deny list strategy makes use of the FullAWSAccess SCP that is attached by default to every OU and account. This SCP overrides the default implicit deny, and explicitly allows all permissions to flow down from the root to every account, unless you explicitly deny a permission with an additional SCP that you create and attach to the appropriate OU or account.</p><p>This strategy works because an explicit deny in a policy always overrides any kind of allow. No account below the level of the OU with the deny policy can use the denied API, and there is no way to add the permission back lower in the hierarchy.</p><p>In this scenario the SCP will deny modification of the specified services for all member accounts. This will affect users and administrators in member accounts. The SCP does not affect IAM policies defined in member accounts for specifying privileges to other AWS services.</p><p>Note that SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p><p><strong>CORRECT: </strong>\"All users in member accounts will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will not be overridden\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"All users in the root account will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will be overridden\" is incorrect.</p><p>In fact the administrators in the root account are not restricted by SCPs. The SCP will affect all users in member accounts. IAM policies that specify privileges to other AWS services will not be affected.</p><p><strong>INCORRECT:</strong> \"All users in member accounts will be able to disable AWS SecurityHub and delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will be overridden\" is incorrect.</p><p>IAM policies that specify privileges to other AWS services will not be affected. The goal of restricting the specific services specified in the SCP is achieved without affecting permissions or available permissions for other AWS services.</p><p><strong>INCORRECT:</strong> \"Users (but not administrators) in member accounts will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will not be overridden\" is incorrect.</p><p>The SCPs will affect all users including administrators.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html</a></p>", "answers": ["<p>All users in member accounts will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will not be overridden.</p>", "<p>All users in the root account will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will be overridden.</p>", "<p>All users in member accounts will be able to disable AWS SecurityHub and delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will be overridden.</p>", "<p>Users (but not administrators) in member accounts will not be able to disable AWS SecurityHub or delete or modify the Amazon GuardDuty configuration. IAM policies defined in member accounts will not be overridden.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has created an organization in AWS Organizations. The company has several accounts and OUs and uses the default FullAWSAccess SCP. A security engineer needs to ensure that no one in member accounts can disable specific AWS services. The security engineer must ensure that permissions granted by IAM policies defined in member accounts are not overridden.What will be the effect of adding the following SCP to the root of the organization?", "related_lectures": []}, {"_class": "assessment", "id": 70265716, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances. Two instances were launched and each instance runs in a separate Availability Zone within an Amazon VPC. Each instance must communicate with the Elastic IP address of the other instance. The instances can connect using private IP addresses and can access external internet addresses. However, they are unable to communicate with each other using public IP addresses.</p><p>How can this issue be resolved?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most likely explanation for this issue is that the security groups have not been setup to allow inbound communication from the source Elastic IP addresses. The instances are communicating across AZs and using the public (Elastic) IP address for connectivity.</p><p>For this to work the security group should have an inbound rule allowing the relevant protocol with the source specified as the Elastic IP address of the instance that is attempting to connect.</p><p>This is because outbound connections to public IP addresses will be routed to the internet gateway and the source address will show as the public IP address of the instance making the connection.</p><p><strong>CORRECT: </strong>\"Add the Elastic IP addresses to the ingress rules of the instance security groups\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the instance IDs to the ingress rules of the instance security groups\" is incorrect.</p><p>If the instance ID is specified this will allow communication from the private IP of the instances but not if the source address is a public address.</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway and add a route to the subnet route tables\" is incorrect.</p><p>An internet gateway must already be attached as the instances are able to communicate with internet addresses.</p><p><strong>INCORRECT:</strong> \"Add a public IP address and detach the Elastic IP address from each instance\" is incorrect.</p><p>An Elastic IP address is a static public IP address. There is no benefit in changing from Elastic to public.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>", "answers": ["<p>Add a public IP address and detach the Elastic IP address from each instance.</p>", "<p>Add the Elastic IP addresses to the ingress rules of the instance security groups.</p>", "<p>Attach an internet gateway and add a route to the subnet route tables.</p>", "<p>Add the instance IDs to the ingress rules of the instance security groups.</p>"]}, "correct_response": ["b"], "section": "Domain 1 - Incident Response", "question_plain": "An application runs on Amazon EC2 instances. Two instances were launched and each instance runs in a separate Availability Zone within an Amazon VPC. Each instance must communicate with the Elastic IP address of the other instance. The instances can connect using private IP addresses and can access external internet addresses. However, they are unable to communicate with each other using public IP addresses.How can this issue be resolved?", "related_lectures": []}]}
5750716
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 63655128, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has a group of Amazon EC2 instances in a private subnet that does not have a NAT gateway attached. A security engineer needs to capture logs from an application and collect the log files in Amazon CloudWatch Logs.</p><p>Which steps should the security engineer take to securely meet the requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can establish a private connection between your VPC and CloudWatch Logs by creating an interface VPC endpoint. You can use this connection to send logs to CloudWatch Logs without sending them through the internet.</p><p>The log files can be collected from the instances and automatically sent to CloudWatch Logs by using the unified CloudWatch Agent. The agent must be installed and configured on each EC2 instance. The instances will also need an instance profile that supplies the permissions needed to write to CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"Create an interface VPC endpoint for CloudWatch Logs. Configure the endpoint policy to allow the EC2 instances to use the endpoint\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Install and configure the unified CloudWatch agent on the EC2 instances and attach an instance profile with permissions to write to CloudWatch Logs\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an internet gateway to the subnet and update the subnet route table to point to the internet gateway\" is incorrect.</p><p>This would be a less secure solution compared to using a VPC endpoint as the traffic would go via an internet gateway. Also, the instances are in a private subnet so cannot even use an internet gateway without a NAT gateway which is not mentioned.</p><p><strong>INCORRECT:</strong> \"Schedule a local cron job on each EC2 instance that copies the log files to an Amazon S3 bucket. Point CloudWatch Logs to the S3 bucket\" is incorrect.</p><p>This is unnecessary as the CloudWatch agent will collect the log files and stream them to CloudWatch Logs which is a much better solution. Also CloudWatch Logs cannot be \u201cpointed\u201d to an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Create a gateway VPC endpoint for Amazon S3. Configure the bucket policy to allow access only for connections from the VPC endpoint\" is incorrect.</p><p>This is unnecessary as the S3 solution does not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch-logs-and-interface-VPC.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/cloudwatch-logs-and-interface-VPC.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>", "answers": ["<p>Create an interface VPC endpoint for CloudWatch Logs. Configure the endpoint policy to allow the EC2 instances to use the endpoint.</p>", "<p>Install and configure the unified CloudWatch agent on the EC2 instances and attach an instance profile with permissions to write to CloudWatch Logs.</p>", "<p>Attach an internet gateway to the subnet and update the subnet route table to point to the internet gateway.</p>", "<p>Schedule a local cron job on each EC2 instance that copies the log files to an Amazon S3 bucket. Point CloudWatch Logs to the S3 bucket.</p>", "<p>Create a gateway VPC endpoint for Amazon S3. Configure the bucket policy to allow access only for connections from the VPC endpoint.</p>"]}, "correct_response": ["a", "b"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company has a group of Amazon EC2 instances in a private subnet that does not have a NAT gateway attached. A security engineer needs to capture logs from an application and collect the log files in Amazon CloudWatch Logs.Which steps should the security engineer take to securely meet the requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655130, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A new application runs on Amazon EC2 instances behind an Application Load Balancer. Some of the company\u2019s other applications have recently seen attacks with high rates of requests from single IP addresses. A security engineer wants to ensure the new application is protected from such attacks.</p><p>How can the security engineer add protection to the application without permanently blocking the IP address?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A rate-based rule tracks the rate of requests for each originating IP address and triggers the rule action on IPs with rates that go over a limit. You set the limit as the number of requests per 5-minute time span. You can use this type of rule to put a temporary block on requests from an IP address that's sending excessive requests.</p><p><strong>CORRECT: </strong>\"Use AWS WAF to create a rate-based rule\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Generate a custom error page in Amazon CloudFront\" is incorrect.</p><p>Custom error pages cannot be generated based on the rate of requests from a specific IP address.</p><p><strong>INCORRECT:</strong> \"Add AWS Shield protection to the Application Load Balancer\" is incorrect.</p><p>AWS Shield Advanced can be used to protect ALBs but it will still leverage AWS WAF for rate-based rules.</p><p><strong>INCORRECT:</strong> \"Enable geo restriction in Amazon CloudFront\" is incorrect.</p><p>Geo restriction does not restrict based on the rate of requests from a specific IP address, it restricts based on the geographic location of the originating user.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>", "answers": ["<p>Generate a custom error page in Amazon CloudFront.</p>", "<p>Use AWS WAF to create a rate-based rule.</p>", "<p>Add AWS Shield protection to the Application Load Balancer.</p>", "<p>Enable geo restriction in Amazon CloudFront.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A new application runs on Amazon EC2 instances behind an Application Load Balancer. Some of the company\u2019s other applications have recently seen attacks with high rates of requests from single IP addresses. A security engineer wants to ensure the new application is protected from such attacks.How can the security engineer add protection to the application without permanently blocking the IP address?", "related_lectures": []}, {"_class": "assessment", "id": 63655132, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company plans to migrate some confidential data to Amazon S3. A security engineer must ensure that the data is encrypted at rest. The encryption solution must enable the company to generate its own keys without needing to manage key storage or the encryption process.</p><p>What should the security engineer use to accomplish this?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created. The data keys used to encrypt your data are also encrypted and stored alongside the data that they protect.</p><p>If you don't specify a customer managed key, Amazon S3 automatically creates an AWS KMS key in your AWS account the first time that you add an object encrypted with SSE-KMS to a bucket. By default, Amazon S3 uses this KMS key for SSE-KMS.</p><p>If you want to use a customer managed key for SSE-KMS, create the customer managed key before you configure SSE-KMS. Then, when you configure SSE-KMS for your bucket, specify the existing customer managed key.</p><p><strong>CORRECT: </strong>\"Server-side encryption with AWS KMS-managed keys (SSE-KMS)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Server-side encryption with customer-provided keys (SSE-C)\" is incorrect.</p><p>With SSE-C you must manage your encryption keys and provide them in encrypt and decrypt operations. The question states that the company does not want to manage storage of keys or the encryption process so KMS should be used.</p><p><strong>INCORRECT:</strong> \"Server-side encryption with Amazon S3-managed keys (SSE-S3)\" is incorrect.</p><p>This does not provide you an option to supply your own keys and uses AWS managed keys only.</p><p><strong>INCORRECT:</strong> \"Client-side encryption with an AWS KMS-managed KMS key\" is incorrect.</p><p>With this option AWS does not perform any cryptographic operations; you must encrypt and decrypt data on the client side. This does not meet the requirement to not manage key storage or encryption processes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p>", "answers": ["<p>Server-side encryption with Amazon S3-managed keys (SSE-S3).</p>", "<p>Server-side encryption with customer-provided keys (SSE-C).</p>", "<p>Server-side encryption with AWS KMS-managed keys (SSE-KMS).</p>", "<p>Client-side encryption with an AWS KMS-managed KMS key.</p>"]}, "correct_response": ["c"], "section": "Domain 5 - Data Protection", "question_plain": "A company plans to migrate some confidential data to Amazon S3. A security engineer must ensure that the data is encrypted at rest. The encryption solution must enable the company to generate its own keys without needing to manage key storage or the encryption process.What should the security engineer use to accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 63655134, "assessment_type": "multi-select", "prompt": {"question": "<p>A security engineer is deploying an application that will read and write data to an Amazon S3 bucket. The application data must be encrypted both in transit and at rest.</p><p>Which of the following actions should the security engineer take to enforce the encryption requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon S3 allows both HTTP and HTTPS requests. By default, requests are made through the AWS Management Console, AWS Command Line Interface (AWS CLI), or HTTPS. To only allow HTTPS connections, use a condition that checks for the key \"aws:SecureTransport\". When this key is true, the request is allowed. This enforces encryption in transit.</p><p>To encrypt an object at the time of upload, you need to add a header called x<strong>-amz-server-side-encryption</strong> to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS. To enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. This enforces encryption at rest.</p><p><strong>CORRECT: </strong>\"Add a condition to the S3 bucket policy that allows actions if the request meets the condition \"aws:SecureTransport\": \"true\"\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure default encryption for the S3 bucket. Add a condition to the S3 bucket policy that denies PUT requests that don\u2019t include the \u201cx-amz-server-side-encryption\u201d header\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a security group for the S3 bucket that allows HTTPS but not HTTP from the application instance security group\" is incorrect.</p><p>You cannot attach security groups to Amazon S3 buckets.</p><p><strong>INCORRECT:</strong> \"Configure default encryption for the S3 bucket. Add a condition to the IAM policy attached to the EC2 instance profile to deny PUT requests that don\u2019t include the \u201cx-amz-server-side-encryption\u201d header\" is incorrect.</p><p>The condition should be in the S3 bucket policy, not in an IAM policy attached to an EC2 instance profile.</p><p><strong>INCORRECT:</strong> \"Use Amazon Certificate Manager (ACM) to create a public SSL/TLS certificate. Specify the SSL/TLS certificate in the configuration of the S3 bucket\" is incorrect.</p><p>You cannot select the certificate that is used for Amazon S3, AWS provide the certificate.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p>", "answers": ["<p>Add a condition to the S3 bucket policy that allows actions if the request meets the condition \"aws:SecureTransport\": \"true\".</p>", "<p>Create a security group for the S3 bucket that allows HTTPS but not HTTP from the application instance security group.</p>", "<p>Configure default encryption for the S3 bucket. Add a condition to the S3 bucket policy that denies PUT requests that don\u2019t include the \u201cx-amz-server-side-encryption\u201d header.</p>", "<p>Configure default encryption for the S3 bucket. Add a condition to the IAM policy attached to the EC2 instance profile to deny PUT requests that don\u2019t include the \u201cx-amz-server-side-encryption\u201d header.</p>", "<p>Use Amazon Certificate Manager (ACM) to create a public SSL/TLS certificate. Specify the SSL/TLS certificate in the configuration of the S3 bucket.</p>"]}, "correct_response": ["a", "c"], "section": "Domain 5 - Data Protection", "question_plain": "A security engineer is deploying an application that will read and write data to an Amazon S3 bucket. The application data must be encrypted both in transit and at rest.Which of the following actions should the security engineer take to enforce the encryption requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655136, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying a solution that will allow users to encrypt Amazon S3 objects seamlessly. The solution must be cost effective, highly scalable, and use a managed service. The company must also be able to immediately delete the encryption keys if necessary.</p><p>Which solution is suitable and will allow immediate deletion of encryption keys?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS KMS is a fully managed and highly scalable service for key management. It is also more cost-effective compared to AWS CloudHSM, so it is the better option for storing the keys. With AWS KMS you cannot schedule key deletion in 0 days, you must enter a time period between 7 and 30 days. Therefore, it is not possible to immediately delete encryption keys as per the requirements.</p><p>The solution to enable to ability to immediately delete encryption keys is to use AWS KMS with imported key materials. When you use imported key material it is possible to use the DeletelmportedKeyMaterial API to delete key material that you previously imported. This operation makes the specified KMS key unusable.</p><p><strong>CORRECT: </strong>\"Use AWS KMS with AWS imported key material and then use the DeletelmportedKeyMaterial API to remove the key material\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudHSM to store the keys and then use the CloudHSM API for key deletion\" is incorrect.</p><p>CloudHSM would be less cost-effective compared to using AWS KMS.</p><p><strong>INCORRECT:</strong> \"Use AWS KMS with AWS managed keys and then use the ScheduleKeyDeletion API with a waiting period of 0 days for key deletion\" is incorrect.</p><p>You cannot specify a waiting period of 0 days. The waiting period must be between 7 and 30 days.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudHSM with the importPrivateKey API and then use the deleteKey API for key deletion\" is incorrect.</p><p>CloudHSM would be less cost-effective compared to using AWS KMS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-delete-key-material.html\">https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-delete-key-material.html</a></p>", "answers": ["<p>Use AWS KMS with AWS managed keys and then use the ScheduleKeyDeletion API with a waiting period of 0 days for key deletion.</p>", "<p>Use AWS KMS with AWS imported key material and then use the DeletelmportedKeyMaterial API to remove the key material.</p>", "<p>Use AWS CloudHSM to store the keys and then use the CloudHSM API for key deletion.</p>", "<p>Use AWS CloudHSM with the importPrivateKey API and then use the deleteKey API for key deletion.</p>"]}, "correct_response": ["b"], "section": "Domain 5 - Data Protection", "question_plain": "A company is deploying a solution that will allow users to encrypt Amazon S3 objects seamlessly. The solution must be cost effective, highly scalable, and use a managed service. The company must also be able to immediately delete the encryption keys if necessary.Which solution is suitable and will allow immediate deletion of encryption keys?", "related_lectures": []}, {"_class": "assessment", "id": 63655138, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying a web application that runs in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB will be configured to terminate a TLS connection from clients. Security requirements mandate that all TLS traffic to the ALB must remain secure even if the certificate private key is compromised.</p><p>How can a security engineer meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Elastic Load Balancing uses a Secure Socket Layer (SSL) negotiation configuration, known as a security policy, to negotiate SSL connections between a client and the load balancer. A security policy is a combination of protocols and ciphers.</p><p>The protocol establishes a secure connection between a client and a server and ensures that all data passed between the client and your load balancer is private. A cipher is an encryption algorithm that uses encryption keys to create a coded message. Protocols use several ciphers to encrypt data over the internet.</p><p>During the connection negotiation process, the client and the load balancer present a list of ciphers and protocols that they each support, in order of preference. By default, the first cipher on the server's list that matches any one of the client's ciphers is selected for the secure connection.</p><p>Forward Secrecy (FS) uses a derived session key to provide additional safeguards against the eavesdropping of encrypted data. This prevents the decoding of captured data, even if the secret long-term key is compromised.</p><p>In this case the security engineer must select a predefined security policy that supports FS to meet the requirements of the scenario.</p><p><strong>CORRECT: </strong>\"Create an HTTPS listener that uses a predefined security policy that supports forward secrecy (FS)\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a HTTPS listener that uses a custom security policy supports forward secrecy (FS)\" is incorrect.</p><p>The ALB does not support custom security policies.</p><p><strong>INCORRECT:</strong> \"Create an HTTPS listener that uses a certificate that was imported into AWS Certificate Manager (ACM)\" is incorrect.</p><p>It doesn\u2019t make any difference whether the certificate was added manually, through ACM, or whether it was imported or generated by ACM.</p><p><strong>INCORRECT:</strong> \"Create an HTTPS listener that uses the Server Order Preference security feature\" is incorrect.</p><p>This is not relevant to the ALB. You must select a security policy and the ALB will perform the negotiation for protocols and ciphers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p>", "answers": ["<p>Create an HTTPS listener that uses a certificate that was imported into AWS Certificate Manager (ACM).</p>", "<p>Create an HTTPS listener that uses a predefined security policy that supports forward secrecy (FS).</p>", "<p>Create an HTTPS listener that uses the Server Order Preference security feature.</p>", "<p>Create a HTTPS listener that uses a custom security policy supports forward secrecy (FS).</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company is deploying a web application that runs in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB will be configured to terminate a TLS connection from clients. Security requirements mandate that all TLS traffic to the ALB must remain secure even if the certificate private key is compromised.How can a security engineer meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 63655140, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is building an application that uses Amazon EC2 instances and an Amazon RDS database. The solution must be highly secure, and encryption will be implemented within the application and database using an AWS KMS customer managed KMS key. The security team wants to prevent any other services from using the KMS key.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The kms:ViaService condition key limits use of an AWS KMS key to requests from specified AWS services. You can specify one or more services in each kms:ViaService condition key.</p><p>For example, the following key policy statement uses the kms:ViaService condition key to allow a customer managed KMS key to be used for the specified actions only when the request comes from Amazon EC2 or Amazon RDS in the US West (Oregon) region on behalf of ExampleUser.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_03-52-34-2f7d96a165ddff57343266d3d2cc7078.jpg\"><p><strong>CORRECT: </strong>\"Create a custom key policy for the KMS key. Use the kms:ViaService condition key to allow the KMS key to be used only when the request comes from Amazon EC2 or Amazon RDS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an instance profile and attach it to the Amazon EC2 instances and Amazon RDS database. Attach an IAM policy to the instance profile that allows use of the KMS key\" is incorrect.</p><p>You cannot attach instance profiles to RDS databases, and this solution does not explicitly deny access to the KMS key from other services.</p><p><strong>INCORRECT:</strong> \"Create a custom key policy for the KMS key. Use the kms:GrantOperations condition key to grant access to the KMS key only when the request comes from Amazon EC2 or Amazon RDS\" is incorrect.</p><p>The GrantOperations condition key is used to control access to the CreateGrant operation and does not restrict or grant access to a KMS key by service.</p><p><strong>INCORRECT:</strong> \"Create an SCP the explicitly allows permission to the KMS key for Amazon EC2 and Amazon RDS and explicitly denies permission to the KMS key for all other services. Attach the SCP to the AWS account\" is incorrect.</p><p>You cannot use conditions with allow statements in an SCP.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-via-service\">https://docs.aws.amazon.com/kms/latest/developerguide/policy-conditions.html#conditions-kms-via-service</a></p>", "answers": ["<p>Create a custom key policy for the KMS key. Use the kms:ViaService condition key to allow the KMS key to be used only when the request comes from Amazon EC2 or Amazon RDS.</p>", "<p>Create an instance profile and attach it to the Amazon EC2 instances and Amazon RDS database. Attach an IAM policy to the instance profile that allows use of the KMS key.</p>", "<p>Create a custom key policy for the KMS key. Use the kms:GrantOperations condition key to grant access to the KMS key only when the request comes from Amazon EC2 or Amazon RDS.</p>", "<p>Create an SCP the explicitly allows permission to the KMS key for Amazon EC2 and Amazon RDS and explicitly denies permission to the KMS key for all other services. Attach the SCP to the AWS account.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company is building an application that uses Amazon EC2 instances and an Amazon RDS database. The solution must be highly secure, and encryption will be implemented within the application and database using an AWS KMS customer managed KMS key. The security team wants to prevent any other services from using the KMS key.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655142, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is implementing a web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The company requires that all traffic must be over HTTPS and any connections made to the HTTP port should be redirected to HTTPS.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A HTTPS listener uses an X.509 certificate to create a secure channel for communication. You can create an HTTPS listener on an ALB with a certificate that is created/imported in AWS Certificate Manager or that is imported into IAM.</p><p>To redirect connection attempts from HTTP to HTTPS another listener is required. This listener listens for requests to the HTTP port and is configured with a rule that redirects connections to the HTTPS port.</p><p><strong>CORRECT: </strong>\"Add an HTTP listener with a rule that redirects HTTP requests to HTTPS. Add an HTTPS listener and choose an AWS Certificate Manager (ACM) certificate\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an HTTPS listener with a rule that redirects HTTP requests to HTTPS. Choose an AWS Certificate Manager (ACM) certificate for the listener\" is incorrect.</p><p>The rule to redirect requests from HTTP to HTTPS should be added to an HTTP listener.</p><p><strong>INCORRECT:</strong> \"Add a TLS listener with a rule that redirects port 80 to port 443. Import an X.509 certificate directly into the listener configuration\" is incorrect.</p><p>You cannot create TLS listeners on an ALB, and you cannot import certificates directly into an ALB.</p><p><strong>INCORRECT:</strong> \"Add an HTTP listener and an HTTPS listener. Import an X.509 certificate directly into the listener configuration for both listeners\" is incorrect.</p><p>You cannot add a certificate to an HTTP listener. A rule is needed to redirect from HTTP to HTTPS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/</a></p>", "answers": ["<p>Add an HTTP listener with a rule that redirects HTTP requests to HTTPS. Add an HTTPS listener and choose an AWS Certificate Manager (ACM) certificate.</p>", "<p>Add a TLS listener with a rule that redirects port 80 to port 443. Import an X.509 certificate directly into the listener configuration.</p>", "<p>Add an HTTPS listener with a rule that redirects HTTP requests to HTTPS. Choose an AWS Certificate Manager (ACM) certificate for the listener.</p>", "<p>Add an HTTP listener and an HTTPS listener. Import an X.509 certificate directly into the listener configuration for both listeners.</p>"]}, "correct_response": ["a"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company is implementing a web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The company requires that all traffic must be over HTTPS and any connections made to the HTTP port should be redirected to HTTPS.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655144, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer has created an AWS Lambda function that checks AWS CloudTrail logs in an Amazon S3 bucket for security related issues. The Lambda function should record results in Amazon CloudWatch Logs. The security engineer has sufficient permissions to execute the function. Upon testing the function the execution fails.</p><p>The Lambda function execution role has the following permissions:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_04-00-28-e4e0767ef7f1c4ce3267f194998497f2.jpg\"><p>What is the most likely cause of the issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked.</p><p>The most likely issue in this scenario is that the policy assigned to the AWS Lambda function execution role does not specify any permissions for Amazon S3. Permissions for CloudTrail do not help here as the logs are stored in S3 and the Lambda function must read the logs from the S3 bucket.</p><p><strong>CORRECT: </strong>\"The Lambda function does not have permissions to access the S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have permissions to CloudWatch Logs\" is incorrect.</p><p>Full permissions to CloudWatch are provided in the policy.</p><p><strong>INCORRECT:</strong> \"The \u201cResource\u201d element in the policy does not include the S3 bucket objects\" is incorrect.</p><p>The wildcard for the resource \u201c*\u201d is all inclusive and would include bucket objects. However, S3 permissions are not provided.</p><p><strong>INCORRECT:</strong> \"The CloudTrail trail does not allow access in a resource-based policy\" is incorrect.</p><p>Lambda does not need to be provided with any permissions to CloudTrail in this scenario as it reads the logs from the S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p>", "answers": ["<p>The Lambda function does not have permissions to CloudWatch Logs.</p>", "<p>The \u201cResource\u201d element in the policy does not include the S3 bucket objects.</p>", "<p>The Lambda function does not have permissions to access the S3 bucket.</p>", "<p>The CloudTrail trail does not allow access in a resource-based policy.</p>"]}, "correct_response": ["c"], "section": "Domain 1 - Incident Response", "question_plain": "A security engineer has created an AWS Lambda function that checks AWS CloudTrail logs in an Amazon S3 bucket for security related issues. The Lambda function should record results in Amazon CloudWatch Logs. The security engineer has sufficient permissions to execute the function. Upon testing the function the execution fails.The Lambda function execution role has the following permissions:What is the most likely cause of the issue?", "related_lectures": []}, {"_class": "assessment", "id": 63655146, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has on on-premises corporate identity provider (IdP) with thousands of corporate users. The company needs to allow the users to access a set of AWS services from the corporate network. The security engineer has been instructed that the company would prefer to avoid having multiple sets of identities and credentials to manage for each user.</p><p>Which actions will meet the requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The best solution for this scenario is to enable federated access between the corporate IdP and the AWS account using IAM. In this configuration the users can assume roles in the AWS account and can access resources they are granted access to. This solution ensures that there is no duplication of identities or credentials as per the requirements.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-01-42-b979b7b6853a7bfe0bb8533e40c93a3c.jpg\"><p><strong>CORRECT: </strong>\"Enable federated access between the corporate IdP and the AWS account using IAM. Use IAM roles to provide access to AWS resources\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon Cognito identity pool and attach the corporate IdP. Use IAM user accounts to provide access to AWS resources\" is incorrect.</p><p>With Cognito identity pools users can access AWS resources by assuming IAM roles and gaining temporary security credentials. They do not get an IAM user account.</p><p><strong>INCORRECT:</strong> \"Create an AWS Managed Microsoft AD and create a two-way trust relationship. Run ADSync and assign IAM permissions to the synchronized user accounts in AWS\" is incorrect.</p><p>This would create a duplicate copy of the user accounts in the Microsoft AD (though ADSync is used for Azure, not an on-premises IdP).</p><p><strong>INCORRECT:</strong> \"Establish an AWS Managed VPN connection to AWS. Assign permissions to AWS resources in the corporate IdP through resource-based policies\" is incorrect.</p><p>You cannot assign resource-based policies to users in an external IdP.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/identity/federation/\">https://aws.amazon.com/identity/federation/</a></p>", "answers": ["<p>Create an Amazon Cognito identity pool and attach the corporate IdP. Use IAM user accounts to provide access to AWS resources.</p>", "<p>Create an AWS Managed Microsoft AD and create a two-way trust relationship. Run ADSync and assign IAM permissions to the synchronized user accounts in AWS.</p>", "<p>Establish an AWS Managed VPN connection to AWS. Assign permissions to AWS resources in the corporate IdP through resource-based policies.</p>", "<p>Enable federated access between the corporate IdP and the AWS account using IAM. Use IAM roles to provide access to AWS resources.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has on on-premises corporate identity provider (IdP) with thousands of corporate users. The company needs to allow the users to access a set of AWS services from the corporate network. The security engineer has been instructed that the company would prefer to avoid having multiple sets of identities and credentials to manage for each user.Which actions will meet the requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655148, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company's security team wants to use Amazon Detective to generate visualizations that help with security investigations. The company has enabled AWS CloudTrail and VPC Flow Logs. The security team cannot enable Detective.</p><p>Which steps should be taken to enable Amazon Detective?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of security findings or suspicious activities. Detective automatically collects log data from your AWS resources. It then uses machine learning, statistical analysis, and graph theory to generate visualizations that help you to conduct faster and more efficient security investigations.</p><p>When you try to enable Detective, Detective checks whether GuardDuty has been enabled for your account for at least 48 hours. If you are not a GuardDuty customer or have been a GuardDuty customer for less than 48 hours, you cannot enable Detective. You must either enable GuardDuty or wait for 48 hours. This allows GuardDuty to assess the data volume that your account produces.</p><p><strong>CORRECT: </strong>\"Enable Amazon GuardDuty. After 48 hours, enable Amazon Detective\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable Amazon Inspector. After 48 hours, enable Amazon Detective\" is incorrect.</p><p>GuardDuty rather than Inspector must first be enabled.</p><p><strong>INCORRECT:</strong> \"Attach a role to Amazon Detective with permissions to CloudTrail and VPC Flow Logs\" is incorrect.</p><p>Detective does not need a role with these permissions.</p><p><strong>INCORRECT:</strong> \"Login with the account root user credentials and enable Amazon Detective\" is incorrect.</p><p>You do not need to login with the account root user credentials to enabled Detective.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/detective/latest/adminguide/detective-prerequisites.html\">https://docs.aws.amazon.com/detective/latest/adminguide/detective-prerequisites.html</a></p>", "answers": ["<p>Enable Amazon Inspector. After 48 hours, enable Amazon Detective.</p>", "<p>Attach a role to Amazon Detective with permissions to CloudTrail and VPC Flow Logs.</p>", "<p>Enable Amazon GuardDuty. After 48 hours, enable Amazon Detective.</p>", "<p>Login with the account root user credentials and enable Amazon Detective.</p>"]}, "correct_response": ["c"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company's security team wants to use Amazon Detective to generate visualizations that help with security investigations. The company has enabled AWS CloudTrail and VPC Flow Logs. The security team cannot enable Detective.Which steps should be taken to enable Amazon Detective?", "related_lectures": []}, {"_class": "assessment", "id": 63655150, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An organization has a SAML 2.0-compliant corporate identity provider (IdP) that is federated with AWS IAM. Users from the corporate IdP can use the AWS management console. A security engineer has been asked to identify which federated user terminated an Amazon EC2 instance a few days ago.</p><p>What is the FASTEST method of identifying the federated user who terminated the instance?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS CloudTrail can be used to track the activity of your federated users (web identity federation and Security Assertion Markup Language [SAML]). For example, you can use CloudTrail to identify a SAML federated user who terminated an Amazon EC2 instance.</p><p>To capture the activity of these federated users, CloudTrail records the AssumeRoleWithWebIdentity and AssumeRoleWithSAML AWS STS API calls. CloudTrail logging must be enabled to capture these AWS STS API calls.</p><p>The security engineer can search CloudTrail for the TerminateInstances event and identify the IAM role (not user) ARN. Then, the engineer should search the logs for the AssumeRoleWithSAML event and identify the role ARN. The engineer can then identify the federated user by looking at the username attribute in the event.</p><p><strong>CORRECT: </strong>\"Search CloudTrail event logs for the TerminateInstances event and identify the assumed IAM role ARN. Then, search CloudTrail event logs for the AssumeRoleWithSAML event that includes the role ARN and note the federated username\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Search CloudTrail event logs for the TerminateInstances event and identify the IAM user ARN. Then, search CloudTrail event logs for the AssumeRoleWithWebIdentity event that includes the user ARN and note the federated username\" is incorrect.</p><p>There are two issues with this answer. Firstly, through federation the user from the IdP will assume an IAM role, not a user account. Secondly, with a SAML federation the AssumeRoleWithSAML API call will be made.</p><p><strong>INCORRECT:</strong> \"Run an SQL query on the CloudTrail logs with Amazon Athena and search for the TerminateInstances event. Identify the IAM role ARN and run another query to find the AssumeRoleWithWebIdentity event that includes the role ARN and note the federated username\" is incorrect.</p><p>AWS CloudTrail should be used to find the information in the CloudTrail logs as it will be much quicker. Also, the incorrect API action is specified for assuming the role.</p><p><strong>INCORRECT:</strong> \"Run an SQL query on the CloudTrail logs with Amazon Athena and search for the TerminateInstances event. Identify the IAM user ARN and run another query to find the AssumeRoleWithSAML event that includes the user ARN and note the federated username\" is incorrect.</p><p>As above, CloudTrail should be used. Also, a role will be assumed, not a user account.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-easily-identify-your-federated-users-by-using-aws-cloudtrail/\">https://aws.amazon.com/blogs/security/how-to-easily-identify-your-federated-users-by-using-aws-cloudtrail/</a></p>", "answers": ["<p>Search CloudTrail event logs for the TerminateInstances event and identify the assumed IAM role ARN. Then, search CloudTrail event logs for the AssumeRoleWithSAML event that includes the role ARN and note the federated username.</p>", "<p>Search CloudTrail event logs for the TerminateInstances event and identify the IAM user ARN. Then, search CloudTrail event logs for the AssumeRoleWithWebIdentity event that includes the user ARN and note the federated username.</p>", "<p>Run an SQL query on the CloudTrail logs with Amazon Athena and search for the TerminateInstances event. Identify the IAM role ARN and run another query to find the AssumeRoleWithWebIdentity event that includes the role ARN and note the federated username.</p>", "<p>Run an SQL query on the CloudTrail logs with Amazon Athena and search for the TerminateInstances event. Identify the IAM user ARN and run another query to find the AssumeRoleWithSAML event that includes the user ARN and note the federated username.</p>"]}, "correct_response": ["a"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An organization has a SAML 2.0-compliant corporate identity provider (IdP) that is federated with AWS IAM. Users from the corporate IdP can use the AWS management console. A security engineer has been asked to identify which federated user terminated an Amazon EC2 instance a few days ago.What is the FASTEST method of identifying the federated user who terminated the instance?", "related_lectures": []}, {"_class": "assessment", "id": 63655152, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on a fleet of Amazon EC2 instances in a private subnet. The EC2 instances read and write data to an Amazon S3 bucket. The data is highly confidential and a private and secure connection is required between the EC2 instances and the S3 bucket.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A VPC gateway endpoint can be used to access an Amazon S3 bucket using private IP addresses. To further secure the solution an S3 bucket policy can be created that restricts access to the VPC endpoint so connections cannot be made to the bucket from other sources.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-06-32-015823a2883db0e154d0f381098561cb.jpg\"><p><strong>CORRECT: </strong>\"Set up S3 bucket policies to allow access from a VPC endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure encryption for the S3 bucket using an AWS KMS key\" is incorrect.</p><p>This will encrypt data at rest but does not secure the connection to the bucket or ensure private connections must be made.</p><p><strong>INCORRECT:</strong> \"Configure a custom SSL/TLS certificate on the S3 bucket\" is incorrect.</p><p>You cannot add a custom SSL/TLS certificate to Amazon S3.</p><p><strong>INCORRECT:</strong> \"Set up an IAM policy to grant read-write access to the S3 bucket\" is incorrect.</p><p>This does not enable private access from EC2. A gateway VPC endpoint is required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html</a></p>", "answers": ["<p>Configure encryption for the S3 bucket using an AWS KMS key.</p>", "<p>Configure a custom SSL/TLS certificate on the S3 bucket.</p>", "<p>Set up S3 bucket policies to allow access from a VPC endpoint.</p>", "<p>Set up an IAM policy to grant read-write access to the S3 bucket.</p>"]}, "correct_response": ["c"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An application runs on a fleet of Amazon EC2 instances in a private subnet. The EC2 instances read and write data to an Amazon S3 bucket. The data is highly confidential and a private and secure connection is required between the EC2 instances and the S3 bucket.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655154, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Several AWS accounts belonging to different business units are used for development purposes. An additional account is used by the security team. To ensure security best practices are being followed, the security team requires access to review the configuration of the Amazon EC2 instances in the development accounts.</p><p>Which solution will meet these requirements in the MOST secure manner?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This question is checking that you know how to configure cross-account access and how to do so securely using the principle of least privilege. This can be achieved through the creation of a policy providing permissions to the resources in each development account, associating the policies to roles, and then assuming those roles from the security admins account.</p><p>To ensure this solution is secure, read-only permissions should be assigned to the permissions policy as per the requirements of the security team.</p><p><strong>CORRECT: </strong>\"Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has administrator access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account\" is incorrect.</p><p>Administrator access provides more permissions than are required by the security team.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to an IAM user. Share the user credentials with the security team\" is incorrect.</p><p>Sharing credentials is much less secure than using roles.</p><p><strong>INCORRECT:</strong> \"Create an IAM policy in each development account that has administrator access to all Amazon EC2 actions. Assign the policy to an IAM user. Share the user credentials with the security team\" is incorrect.</p><p>This answer provides too many permissions and an insecure method of authentication.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>", "answers": ["<p>Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to an IAM user. Share the user credentials with the security team.</p>", "<p>Create an IAM policy in each development account that has administrator access to all Amazon EC2 actions. Assign the policy to an IAM user. Share the user credentials with the security team.</p>", "<p>Create an IAM policy in each development account that has read-only access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account.</p>", "<p>Create an IAM policy in each development account that has administrator access to Amazon EC2 resources. Assign the policy to a cross-account IAM role. Ask the security team members to assume the role from their account.</p>"]}, "correct_response": ["c"], "section": "Domain 4 - Identity and Access Management", "question_plain": "Several AWS accounts belonging to different business units are used for development purposes. An additional account is used by the security team. To ensure security best practices are being followed, the security team requires access to review the configuration of the Amazon EC2 instances in the development accounts.Which solution will meet these requirements in the MOST secure manner?", "related_lectures": []}, {"_class": "assessment", "id": 63655168, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An AWS Lambda function has started to cause errors in an application and a security engineer must check the output of the function. The engineer checked Amazon CloudWatch Logs but could not find any log files for the Lambda function.</p><p>What is the best explanation for why the logs are not available?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked.</p><p>Lambda will record execution output to CloudWatch Logs if it has the permission to do so. You can add CloudWatch Logs permissions using the AWSLambdaBasicExecutionRole AWS managed policy provided by Lambda. The policy statement is shown below:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-15-29-b33d558437ba5a56cd12fe0bdf95c8b3.jpg\"><p><strong>CORRECT: </strong>\"The Lambda function execution role does not have permissions to write to CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Lambda function execution role does not have permissions to write to Amazon S3\" is incorrect.</p><p>Amazon S3 is unrelated to CloudWatch Logs. The execution role needs permissions for CloudWatch Logs only in this case.</p><p><strong>INCORRECT:</strong> \"The Lambda function does not have monitoring enabled to execution output is not being logged\" is incorrect.</p><p>Logging to CloudWatch Logs happens automatically if the execution role grants the necessary permissions.</p><p><strong>INCORRECT:</strong> \"The log output is stored in AWS X-Ray so the security engineer must check there instead\" is incorrect.</p><p>AWS X-Ray is used for tracing and is unrelated to CloudWatch Logs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html\">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html</a></p>", "answers": ["<p>The Lambda function execution role does not have permissions to write to CloudWatch Logs.</p>", "<p>The Lambda function does not have monitoring enabled to execution output is not being logged.</p>", "<p>The Lambda function execution role does not have permissions to write to Amazon S3.</p>", "<p>The log output is stored in AWS X-Ray so the security engineer must check there instead.</p>"]}, "correct_response": ["a"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "An AWS Lambda function has started to cause errors in an application and a security engineer must check the output of the function. The engineer checked Amazon CloudWatch Logs but could not find any log files for the Lambda function.What is the best explanation for why the logs are not available?", "related_lectures": []}, {"_class": "assessment", "id": 63655170, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer needs to secure an Amazon S3 bucket that will be used by many internal users who have AWS accounts. The security engineer enabled default encryption on the S3 bucket and needs to limit access to user-specific folders. Each user should only be able to access their own folder.</p><p>What should the security engineer configure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p><em>Policy variables</em> let you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.</p><p>The following example shows a policy for an Amazon S3 bucket that uses a policy variable.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-21-26-51db91de2dec0ae0424e3d257346e9e8.jpg\"><p>When this policy is evaluated, IAM replaces the variable ${aws:username}with the friendly name of the actual current user. This means that a single policy applied to a group of users can control access to a bucket. It does this by using the user name as part of the resource's name.</p><p><strong>CORRECT: </strong>\"Update the relevant IAM policy to grant access to the resource \"arn:aws:s3:::examplebucket/${aws:username}/*\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the relevant IAM policy to grant access to the resource \"arn:aws:s3:::bucket/${aws:PrincipalTag/username}/*\"\" is incorrect.</p><p>In this example part of the ARN would be replaced with a tag value rather than the requesting users\u2019 username.</p><p><strong>INCORRECT:</strong> \"Update the object ACL for the folder to grant access to the \u201cAuthenticated users group\" is incorrect.</p><p>There isn\u2019t an ACL for folders, only buckets and objects have ACLs. Also, the authenticated users group means any authenticated user rather than a specific user account.</p><p><strong>INCORRECT:</strong> \"Instruct each user to assume object ownership for their own S3 folder using the object writer option\" is incorrect.</p><p>You cannot assume ownership of individual folders within an S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html</a></p>", "answers": ["<p>Update the relevant IAM policy to grant access to the resource \"arn:aws:s3:::examplebucket/${aws:username}/*\u201d.</p>", "<p>Update the object ACL for the folder to grant access to the \u201cAuthenticated users group\u201d.</p>", "<p>Instruct each user to assume object ownership for their own S3 folder using the object writer option.</p>", "<p>Update the relevant IAM policy to grant access to the resource \"arn:aws:s3:::bucket/${aws:PrincipalTag/username}/*\".</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A security engineer needs to secure an Amazon S3 bucket that will be used by many internal users who have AWS accounts. The security engineer enabled default encryption on the S3 bucket and needs to limit access to user-specific folders. Each user should only be able to access their own folder.What should the security engineer configure?", "related_lectures": []}, {"_class": "assessment", "id": 63655172, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs a hybrid cloud with on-premises network that is connected to AWS using an AWS Direct Connect connection. The company also has an internet connection with significant bandwidth available. An application that runs on-premises needs to stream data to Amazon Kinesis Data Streams. The company's security policy requires that data be encrypted in transit using a private network.</p><p>How should the company meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The first thing to note is that Kinesis Data Streams uses TLS for all connections, so the data is encrypted in transit by default. Therefore, we don\u2019t need to think about using encrypted tunnels to connect (Direct Connect is not encrypted). The solution must ensure data is sent over a private connection, which in this case is the Direct Connect connection.</p><p>You can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Firehose from leaving the Amazon network. This will ensure that traffic received over the DX connection that is destined for KDS does not traverse the public internet.</p><p><strong>CORRECT: </strong>\"Create an interface VPC endpoint for Kinesis Data Streams. Configure the application to connect to the VPC endpoint\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IPSec VPN connection to the Amazon VPC. Configure the application to connect via the virtual private gateway\" is incorrect.</p><p>There is no need for an encrypted tunnel over a VPN as the data is already encrypted. A VPN would use the internet by default and therefore does not use a private network.</p><p><strong>INCORRECT:</strong> \"Configure Kinesis Data Streams as a target for a public facing Network Load Balancer (NLB) with a TLS listener\" is incorrect.</p><p>You cannot configure KDS as a target for an NLB. This also does not use a private network.</p><p><strong>INCORRECT:</strong> \"Enable server-side encryption for Kinesis Data Streams using an AWS KMS key. Configure the application to connect via the Direct Connect connection\" is incorrect.</p><p>Server-side encryption is used for encryption at rest rather than encryption in transit. There is also no way to use the DX connection unless an interface VPC endpoint is provisioned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/vpc.html\">https://docs.aws.amazon.com/streams/latest/dev/vpc.html</a></p>", "answers": ["<p>Create an interface VPC endpoint for Kinesis Data Streams. Configure the application to connect to the VPC endpoint.</p>", "<p>Create an IPSec VPN connection to the Amazon VPC. Configure the application to connect via the virtual private gateway.</p>", "<p>Configure Kinesis Data Streams as a target for a public facing Network Load Balancer (NLB) with a TLS listener.</p>", "<p>Enable server-side encryption for Kinesis Data Streams using an AWS KMS key. Configure the application to connect via the Direct Connect connection.</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A company runs a hybrid cloud with on-premises network that is connected to AWS using an AWS Direct Connect connection. The company also has an internet connection with significant bandwidth available. An application that runs on-premises needs to stream data to Amazon Kinesis Data Streams. The company's security policy requires that data be encrypted in transit using a private network.How should the company meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655174, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company must ensure that AWS CloudTrail is recording API activity across all AWS Regions within their account. An automated solution is required to check that CloudTrail is enabled and to turn it back on if it has been turned off.</p><p>What is the MOST efficient way to implement this solution?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To ensure that CloudTrail remains enabled in your account, AWS Config provides the cloudtrail-enabled managed rule<em>. </em>If CloudTrail is turned off, the cloudtrail-enabled rule automatically re-enables it by using automatic remediation.</p><p>This solution uses AWS Config to identify if CloudTrail logging is turned off and then an AWS Systems Manager Automation runbook to remediate the issue. The following diagram depicts this solution:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-24-48-c310f042a748ae5dd5d29448711bc062.jpg\"><p><strong>CORRECT: </strong>\"Use AWS Config with the managed rule cloudtrail-enabled to check that CloudTrail is enabled. If the rule is NON_COMPLIANT use Systems Manager Automation to automatically remediate the issue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge event with the event type \u201cAWS API Call via CloudTrail\u201d and configure AWS Lambda as a target. Configure the Lambda function to turn CloudTrail back on\" is incorrect.</p><p>This event will trigger when certain API calls are made via AWS CloudTrail. It does not check if CloudTrail itself is being modified unless specifically targeting those API calls. This would require more work to implement properly compared to the correct answer.</p><p><strong>INCORRECT:</strong> \"Use Amazon Athena to monitor the Amazon S3 buckets where CloudTrail logging occurs. If logging ceases trigger an automated action that executes Systems Manager Automation to remediate the issue\" is incorrect.</p><p>Athena is used for running SQL queries on S3 data. This is not a good way to check for changes to AWS CloudTrail.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the AWS CloudTrail StopLogging API action. Configure remediation using an AWS Step Functions State Machine with an AWS Lambda function that turns CloudTrail back on\" is incorrect.</p><p>You can alert based on metric filters that check for CloudTrail API actions. However, this would be more complex, and Step Functions would not be used as the target, Lambda would be.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-re-enable-aws-cloudtrail-by-using-a-custom-remediation-rule-in-aws-config.html</a></p>", "answers": ["<p>Create an Amazon CloudWatch alarm for the AWS CloudTrail StopLogging API action. Configure remediation using an AWS Step Functions State Machine with an AWS Lambda function that turns CloudTrail back on.</p>", "<p>Use Amazon Athena to monitor the Amazon S3 buckets where CloudTrail logging occurs. If logging ceases trigger an automated action that executes Systems Manager Automation to remediate the issue.</p>", "<p>Create an Amazon EventBridge event with the event type \u201cAWS API Call via CloudTrail\u201d and configure AWS Lambda as a target. Configure the Lambda function to turn CloudTrail back on.</p>", "<p>Use AWS Config with the managed rule cloudtrail-enabled to check that CloudTrail is enabled. If the rule is NON_COMPLIANT use Systems Manager Automation to automatically remediate the issue.</p>"]}, "correct_response": ["d"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company must ensure that AWS CloudTrail is recording API activity across all AWS Regions within their account. An automated solution is required to check that CloudTrail is enabled and to turn it back on if it has been turned off.What is the MOST efficient way to implement this solution?", "related_lectures": []}, {"_class": "assessment", "id": 63655176, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs many Amazon EC2 Linux instances. Communications between the instances are complex and rules governing ingress, egress, and inter-instance communications are beyond the limits of security groups and network ACLs.</p><p>What mechanism will allow the company to implement all required network rules without incurring additional cost?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The only option available that works and does not incur additional cost is to use the host-based firewall within the operating system of the EC2 instances. With Linux and Windows instances you can configure a host-based firewall to control communications.</p><p><strong>CORRECT: </strong>\"Configure the host-based firewall within the operating system\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS WAF web ACLs to implement the required rules\" is incorrect.</p><p>WAF is not free so would incur additional cost.</p><p><strong>INCORRECT:</strong> \"Use a third-party firewall appliance from the AWS Marketplace\" is incorrect.</p><p>Appliances with third-party firewall software built-in will typically cost extra as you must run the instances hosting the firewall and typically pay additional costs for the licensing within the EC2 pricing.</p><p><strong>INCORRECT:</strong> \"Use an AWS transit gateway to control inter-instance communications\" is incorrect.</p><p>Transit gateway is not a firewall and is not free.</p><p><strong>References:</strong></p><p><a href=\"https://en.wikipedia.org/wiki/Iptables\">https://en.wikipedia.org/wiki/Iptables</a></p>", "answers": ["<p>Configure AWS WAF web ACLs to implement the required rules.</p>", "<p>Use an AWS transit gateway to control inter-instance communications.</p>", "<p>Use a third-party firewall appliance from the AWS Marketplace.</p>", "<p>Configure the host-based firewall within the operating system.</p>"]}, "correct_response": ["d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company runs many Amazon EC2 Linux instances. Communications between the instances are complex and rules governing ingress, egress, and inter-instance communications are beyond the limits of security groups and network ACLs.What mechanism will allow the company to implement all required network rules without incurring additional cost?", "related_lectures": []}, {"_class": "assessment", "id": 63655156, "assessment_type": "multi-select", "prompt": {"question": "<p>A company's security engineer receives an abuse notification from AWS. The notification indicates that malware is being hosted in the AWS account. The security engineer investigated the issue and found an unauthorized Amazon S3 bucket.</p><p>Which combination of steps should the security engineer take to MINIMIZE the consequences of this compromise? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The AWS Trust &amp; Safety Team sends abuse reports to the security contact on your account. If there is no security contact listed, the AWS Trust &amp; Safety Team contacts you using the email address listed on your account.</p><p>If you observe unauthorized activity within your AWS account, or you believe that an unauthorized party accessed your account, then do the following:</p><ul><li><p>Rotate and delete all root and AWS Identity and Access Management (IAM) access keys.</p></li><li><p>Delete any potentially unauthorized IAM users, and then change the password for all other IAM users.</p></li><li><p>Check your bill. Your bill can help you identify resources that you didn't create.</p></li><li><p>Delete any resources on your account that you didn't create.</p></li><li><p>Enable MFS on the root user and any IAM users with console access.</p></li><li><p>Verify that your account information is correct.</p></li><li><p>Respond to the notifications that you received from AWS Support through the AWS Support Center.</p></li></ul><p><strong>CORRECT: </strong>\"Rotate and delete all root and IAM access keys\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Delete any unauthorized IAM users\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Delete any unauthorized resources\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Login as root and delete all IAM users\" is incorrect.</p><p>This would be highly disruptive and is not recommended as a response measure.</p><p><strong>INCORRECT:</strong> \"Enable the AWS Shield Advanced service\" is incorrect.</p><p>This service protects against DDoS attacks. This account has already been compromised with malware so Shield will not assist in this case.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of all Amazon EBS volumes\" is incorrect.</p><p>This is not one of the recommended response measures for this circumstance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/\">https://aws.amazon.com/premiumsupport/knowledge-center/potential-account-compromise/</a></p>", "answers": ["<p>Login as root and delete all IAM users.</p>", "<p>Rotate and delete all root and IAM access keys.</p>", "<p>Delete any unauthorized IAM users.</p>", "<p>Delete any unauthorized resources.</p>", "<p>Enable the AWS Shield Advanced service.</p>", "<p>Take a snapshot of all Amazon EBS volumes.</p>"]}, "correct_response": ["b", "c", "d"], "section": "Domain 1 - Incident Response", "question_plain": "A company's security engineer receives an abuse notification from AWS. The notification indicates that malware is being hosted in the AWS account. The security engineer investigated the issue and found an unauthorized Amazon S3 bucket.Which combination of steps should the security engineer take to MINIMIZE the consequences of this compromise? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 63655158, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company is using Amazon S3 to store its sales data that is encrypted with an AWS Key Management Service (AWS KMS) customer-managed key. The company uses several AWS Lambda functions, each needing to access the sales data in the S3 bucket independently.</p><p>A security engineer needs to ensure that each Lambda function has individual and restricted access permissions to the KMS key.</p><p>Which solution should the security engineer implement to fulfill this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A Lambda function should be assigned an execution role that provides the necessary permissions for its tasks. If each function needs access to a KMS key, assigning each function an execution role that provides these permissions is the correct approach.</p><p><strong>CORRECT: </strong>\"Assign a distinct Lambda execution role with specific KMS key access permissions to each Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Assign an individual IAM user to each Lambda function. Attach an IAM policy that grants precise access permissions to the KMS key\" is incorrect.</p><p>AWS IAM users should not be attached directly to AWS Lambda functions. Instead, Lambda functions should assume a role that provides the necessary permissions.</p><p><strong>INCORRECT:</strong> \"Create a KMS key policy that restricts permissions to the specific Lambda service principals\" is incorrect.</p><p>KMS key policy is not typically used to provide permissions to specific Lambda service principals. It's more suited to control overall access to the KMS key itself.</p><p><strong>INCORRECT:</strong> \"Configure each Lambda function to assume an IAM role with exact access permissions to the AWS managed KMS key for Amazon S3\" is incorrect.</p><p>It is not possible to configure each Lambda function to assume an IAM role that provides permissions to the AWS managed KMS key for Amazon S3. The key in question is a customer-managed key, not an AWS-managed key.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p>", "answers": ["<p>Assign an individual IAM user to each Lambda function. Attach an IAM policy that grants precise access permissions to the KMS key.</p>", "<p>Create a KMS key policy that restricts permissions to the specific Lambda service principals.</p>", "<p>Assign a distinct Lambda execution role with specific KMS key access permissions to each Lambda function.</p>", "<p>Configure each Lambda function to assume an IAM role with exact access permissions to the AWS managed KMS key for Amazon S3.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "A retail company is using Amazon S3 to store its sales data that is encrypted with an AWS Key Management Service (AWS KMS) customer-managed key. The company uses several AWS Lambda functions, each needing to access the sales data in the S3 bucket independently.A security engineer needs to ensure that each Lambda function has individual and restricted access permissions to the KMS key.Which solution should the security engineer implement to fulfill this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 63655160, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A software development firm operates a multi-account AWS environment managed by AWS Organizations and AWS IAM Identity Center. The firm needs to ensure that each development team can operate only within assigned AWS Regions and specific AWS services. The solution should aim to minimize management overhead.</p><p>Which solution will best meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Service Control Policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization, providing central control over the maximum available permissions for all accounts in your organization.</p><p>SCPs can restrict access to specific services and actions across all accounts in an AWS Organization, and they are the best way to enforce region and service restrictions at scale.</p><p><strong>CORRECT: </strong>\"Utilize Service Control Policies (SCPs) in AWS Organizations to limit each team's access to only their assigned Regions and services\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Config to set up service control policies that restrict access to only the necessary Regions and services\" is incorrect.</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources, but it cannot restrict access to specific regions or services.</p><p><strong>INCORRECT:</strong> \"Implement AWS WAF rules to restrict requests from IPs outside of the designated Regions\" is incorrect.</p><p>AWS WAF is a web application firewall service that helps to protect your web applications from common web exploits, it's not meant for restricting access to regions or services.</p><p><strong>INCORRECT:</strong> \"Establish IAM roles for each team with specific policies that restrict access to only their assigned Regions and services\" is incorrect.</p><p>While IAM roles can help manage access to AWS services, managing individual IAM roles for each team could lead to high operational overhead, especially if the number of teams and services is significant.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>", "answers": ["<p>Use AWS Config to set up service control policies that restrict access to only the necessary Regions and services.</p>", "<p>Implement AWS WAF rules to restrict requests from IPs outside of the designated Regions.</p>", "<p>Establish IAM roles for each team with specific policies that restrict access to only their assigned Regions and services.</p>", "<p>Utilize Service Control Policies (SCPs) in AWS Organizations to limit each team's access to only their assigned Regions and services.</p>"]}, "correct_response": ["d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A software development firm operates a multi-account AWS environment managed by AWS Organizations and AWS IAM Identity Center. The firm needs to ensure that each development team can operate only within assigned AWS Regions and specific AWS services. The solution should aim to minimize management overhead.Which solution will best meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655162, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A fintech company operates a suite of applications on Amazon EC2. The applications have intricate security needs, governed by a set of security groups. After an unintended modification in a security group disrupted the connectivity of some applications, the company wants to be alerted via a designated email whenever changes are made to these security groups.</p><p>Which solution can fulfill this requirement most efficiently?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS CloudTrail can capture all API calls, including those that modify security groups. The logs can be forwarded to Amazon CloudWatch, which can filter and raise an alarm on detecting specific patterns. This alarm can then be linked to an SNS topic for email notifications, making this the most operationally efficient option.</p><p><strong>CORRECT: </strong>\"Use AWS CloudTrail. Enable forwarding to Amazon CloudWatch Logs. Create a CloudWatch Logs metric filter to match patterns indicating security group changes. Configure a CloudWatch alarm to send alerts to an Amazon SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Config. Set up a custom AWS Config rule to track changes to security groups. Configure AWS Lambda for automated remediation and to send notifications to an Amazon SNS topic\" is incorrect.</p><p>While AWS Config can be set up to track changes in security groups, AWS Lambda doesn't inherently support direct remediation or sending notifications upon detection of changes in security groups. This solution would require more work and is therefore less efficient.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail. Forward logs to Amazon S3. Set up an Amazon Athena query to scan for event patterns indicating security group changes. Use AWS Lambda to send query results to Amazon SES for email notifications\" is incorrect.</p><p>Although AWS CloudTrail, Amazon S3, Amazon Athena, and Amazon SES could provide the required functionality, this process would be overly complicated and operationally inefficient compared to the other options.</p><p><strong>INCORRECT:</strong> \"Use Amazon Macie. Enable monitoring for changes in security groups. Configure Amazon Macie to send alerts to an Amazon SNS topic\" is incorrect.</p><p>Amazon Macie is primarily used for data protection and data privacy and doesn't directly support monitoring changes in security groups.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>", "answers": ["<p>Use AWS Config. Set up a custom AWS Config rule to track changes to security groups. Configure AWS Lambda for automated remediation and to send notifications to an Amazon SNS topic.</p>", "<p>Use AWS CloudTrail. Enable forwarding to Amazon CloudWatch Logs. Create a CloudWatch Logs metric filter to match patterns indicating security group changes. Configure a CloudWatch alarm to send alerts to an Amazon SNS topic.</p>", "<p>Use AWS CloudTrail. Forward logs to Amazon S3. Set up an Amazon Athena query to scan for event patterns indicating security group changes. Use AWS Lambda to send query results to Amazon SES for email notifications.</p>", "<p>Use Amazon Macie. Enable monitoring for changes in security groups. Configure Amazon Macie to send alerts to an Amazon SNS topic.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A fintech company operates a suite of applications on Amazon EC2. The applications have intricate security needs, governed by a set of security groups. After an unintended modification in a security group disrupted the connectivity of some applications, the company wants to be alerted via a designated email whenever changes are made to these security groups.Which solution can fulfill this requirement most efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 63655164, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An international media company has recently migrated their operations to AWS, operating across multiple accounts within AWS Organizations. They have a critical need to log all user actions across these accounts for audit purposes. For certain key actions, they want to be immediately notified through an email list.</p><p>Which solution best fits their needs?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An organizational trail with CloudTrail logs all user actions across accounts. Forwarding these logs to CloudWatch Logs and setting a metric filter for specific actions allows an alarm to be set, which sends notifications in real-time through an SNS topic when these actions occur.</p><p><strong>CORRECT: </strong>\"Configure an organizational trail with AWS CloudTrail, forwarding logs to CloudWatch Logs. Set a metric filter within CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SNS topic upon these actions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement CloudTrail and set it to direct logs to CloudWatch Logs. Create a metric filter in CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SQS queue\" is incorrect.</p><p>This answer isn't correct as it doesn't consider multiple accounts and the SQS queue does not meet the requirement of notifying an email list.</p><p><strong>INCORRECT:</strong> \"Establish an organizational trail with CloudTrail, storing logs in an S3 bucket. Set up an EC2 instance to scan the logs for specific actions and set it to publish messages to an SNS topic\" is incorrect.</p><p>This answer isn't correct as it isn't as efficient or real-time as using CloudWatch Logs and metric filters, although it does use an organizational trail.</p><p><strong>INCORRECT:</strong> \"Deploy CloudTrail and set it to store logs in an S3 bucket. Every hour, use Glue to create a Data Catalog that references the S3 bucket. Configure Athena to initiate queries against the Data Catalog to identify specific actions\" is incorrect.</p><p>This answer isn't correct as it does not use organizational trails and therefore does not accommodate the multiple accounts within the organization.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>", "answers": ["<p>Configure an organizational trail with AWS CloudTrail, forwarding logs to CloudWatch Logs. Set a metric filter within CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SNS topic upon these actions.</p>", "<p>Implement CloudTrail and set it to direct logs to CloudWatch Logs. Create a metric filter in CloudWatch Logs to catch specific actions and create a CloudWatch alarm to send messages to an SQS queue.</p>", "<p>Establish an organizational trail with CloudTrail, storing logs in an S3 bucket. Set up an EC2 instance to scan the logs for specific actions and set it to publish messages to an SNS topic.</p>", "<p>Deploy CloudTrail and set it to store logs in an S3 bucket. Every hour, use Glue to create a Data Catalog that references the S3 bucket. Configure Athena to initiate queries against the Data Catalog to identify specific actions.</p>"]}, "correct_response": ["a"], "section": "Domain 6: Management and Security Governance", "question_plain": "An international media company has recently migrated their operations to AWS, operating across multiple accounts within AWS Organizations. They have a critical need to log all user actions across these accounts for audit purposes. For certain key actions, they want to be immediately notified through an email list.Which solution best fits their needs?", "related_lectures": []}, {"_class": "assessment", "id": 63655166, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An online gaming company has a network of Amazon EC2 instances that are frequently targeted by rogue bots. The security team needs to implement an automated system to block traffic from identified malicious sources. The system needs to respond in near real-time and the security team decided to use AWS Step Functions to orchestrate this solution.</p><p>Which solution should the security engineer implement to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity. It can identify potentially harmful behavior, such as traffic from a botnet.</p><p>The suspicious IP addresses can be stored in a DynamoDB table.</p><p>Lambda can be used to update the DynamoDB table and to automatically update an AWS Network Firewall rule group to block traffic from these IP addresses.</p><p><strong>CORRECT: </strong>\"Use Amazon GuardDuty to identify malicious traffic. Store the identified IP addresses in a DynamoDB table. Use Lambda to update the DynamoDB table and modify an AWS Network Firewall rule group to block the traffic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to identify malicious traffic. Store the identified IP addresses in a DynamoDB table. Use Lambda to update the DynamoDB table and modify a Security Group rule to block the traffic from these IP addresses\" is incorrect.</p><p>Although this approach uses Amazon GuardDuty to identify malicious traffic and Lambda to update the DynamoDB table, it uses Security Group rules to block the traffic. Security Groups only support allow rules, so it is more challenging to implement this solution.</p><p><strong>INCORRECT:</strong> \"Use AWS WAF to detect malicious traffic. Use DynamoDB to store the identified IP addresses. Utilize Lambda to update the DynamoDB table and modify an AWS Network Firewall rule group to block the traffic\" is incorrect.</p><p>AWS WAF primarily protects against common web exploits that could affect application availability, compromise security, or consume excessive resources. However, it is not specifically designed to detect malicious traffic targeting EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudTrail to monitor for malicious traffic. Store the identified IP addresses in a DynamoDB table. Utilize Lambda to update the DynamoDB table and modify a WAF web ACL rule to block the traffic\" is incorrect.</p><p>AWS CloudTrail is used for logging and tracking API calls, not for identifying and blocking malicious traffic. Therefore, it is not suitable for this use case.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>", "answers": ["<p>Use Amazon GuardDuty to identify malicious traffic. Store the identified IP addresses in a DynamoDB table. Use Lambda to update the DynamoDB table and modify a Security Group rule to block the traffic from these IP addresses.</p>", "<p>Use AWS WAF to detect malicious traffic. Use DynamoDB to store the identified IP addresses. Utilize Lambda to update the DynamoDB table and modify an AWS Network Firewall rule group to block the traffic.</p>", "<p>Use Amazon GuardDuty to identify malicious traffic. Store the identified IP addresses in a DynamoDB table. Use Lambda to update the DynamoDB table and modify an AWS Network Firewall rule group to block the traffic.</p>", "<p>Use AWS CloudTrail to monitor for malicious traffic. Store the identified IP addresses in a DynamoDB table. Utilize Lambda to update the DynamoDB table and modify a WAF web ACL rule to block the traffic.</p>"]}, "correct_response": ["c"], "section": "Domain 6: Management and Security Governance", "question_plain": "An online gaming company has a network of Amazon EC2 instances that are frequently targeted by rogue bots. The security team needs to implement an automated system to block traffic from identified malicious sources. The system needs to respond in near real-time and the security team decided to use AWS Step Functions to orchestrate this solution.Which solution should the security engineer implement to meet these requirements?", "related_lectures": []}]}
5750718
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 63655180, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has deployed an application on Amazon EC2 instances with an Amazon RDS database. A security architect needs a secure solution for storing the database credentials and enabling automatic rotation on a regular basis. The credentials must be encrypted both in transit and at rest.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p><em>Rotation</em> is the process of periodically updating a secret. When you rotate a secret, you update the credentials in both the secret and the database or service. In Secrets Manager, you can set up automatic rotation for your secrets. Applications that retrieve the secret from Secrets Manager automatically get the new credentials after rotation.</p><p>Secrets Manager provides complete rotation templates for Amazon RDS, Amazon DocumentDB, and Amazon Redshift secrets. You can also enable encryption in transit and at rest for keys stored in AWS Secrets Manager.</p><p>The slide below provides more information on AWS Secrets Manager:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-35-26-a125b1dcc185a051d727086679ca6c06.jpg\"><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager and configure automatic rotation of the credentials\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store and configure automatic rotation of the credentials\" is incorrect.</p><p>Systems Manager Parameter Store can be used for storing encrypted secrets, but it does not have a feature for automatic secret rotation.</p><p><strong>INCORRECT:</strong> \"Use AWS Key Management Server (KMS) and rotate the keys using an AWS Lambda function\" is incorrect.</p><p>AWS KMS cannot be used for storing secrets, it is used for storing encryption keys.</p><p><strong>INCORRECT:</strong> \"Use IAM access keys in and rotate the access keys using an AWS Lambda function\" is incorrect.</p><p>IAM access keys cannot be used for authenticating to an Amazon RDS database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-db.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-db.html</a></p>", "answers": ["<p>Use AWS Key Management Server (KMS) and rotate the keys using an AWS Lambda function.</p>", "<p>Use AWS Systems Manager Parameter Store and configure automatic rotation of the credentials.</p>", "<p>Use IAM access keys in and rotate the access keys using an AWS Lambda function.</p>", "<p>Use AWS Secrets Manager and configure automatic rotation of the credentials.</p>"]}, "correct_response": ["d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A company has deployed an application on Amazon EC2 instances with an Amazon RDS database. A security architect needs a secure solution for storing the database credentials and enabling automatic rotation on a regular basis. The credentials must be encrypted both in transit and at rest.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655182, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security department in a company requires automatic discovery of any security groups that allow unrestricted inbound traffic on port 22 (SSH). The security administrators should be notified of any violations</p><p>Which solution meets these requirements with the MOST operational efficiency?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The AWS Config managed rule \u201crestricted-ssh\u201d checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0).</p><p>With AWS Config you can configure automatic remediations such as publishing a notification to an Amazon SNS topic. In this case if the rule is NON_COMPLIANT it means Config has detected a security group with unrestricted access on port 22. In this case it will trigger a notification.</p><p><strong>CORRECT: </strong>\"Configure the restricted-ssh managed rule in AWS Config. When the rule is NON_COMPLIANT, use the AWS Config remediation feature to publish a notification to an Amazon SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon GuardDuty to automatically detect threats. Integrate GuardDuty with Lambda for automated actions. Configure the Lambda function to identify security group assessment findings and send a notification to an Amazon SNS topic\" is incorrect.</p><p>GuardDuty detects threats and account compromise. It does not check security group configuration for unrestricted access.</p><p><strong>INCORRECT:</strong> \"Configure VPC Flow Logs for the VPC and specify a CloudWatch Logs group. Subscribe a Lambda function to the log group that parses the log entries, detects successful connections on port 22, and then sends notification to an Amazon SNS topic\" is incorrect.</p><p>This is a complex solution that is not necessary as the Config managed rule restricted-ssh can perform the same function with less operational overhead.</p><p><strong>INCORRECT:</strong> \"Install the SSM agent on all EC2 instances and run an Amazon Inspector network reachability assessment on a daily schedule. Create an AWS Lambda function that runs on a schedule, parses the assessment report, and sends a notification to an Amazon SNS topic\" is incorrect.</p><p>Configuring a function to parse an Inspector report would be complicated and, as with the previous answer, unnecessary as there is a much better solution available.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>", "answers": ["<p>Use Amazon GuardDuty to automatically detect threats. Integrate GuardDuty with Lambda for automated actions. Configure the Lambda function to identify security group assessment findings and send a notification to an Amazon SNS topic.</p>", "<p>Configure the restricted-ssh managed rule in AWS Config. When the rule is NON_COMPLIANT, use the AWS Config remediation feature to publish a notification to an Amazon SNS topic.</p>", "<p>Configure VPC Flow Logs for the VPC and specify a CloudWatch Logs group. Subscribe a Lambda function to the log group that parses the log entries, detects successful connections on port 22, and then sends notification to an Amazon SNS topic.</p>", "<p>Install the SSM agent on all EC2 instances and run an Amazon Inspector network reachability assessment on a daily schedule. Create an AWS Lambda function that runs on a schedule, parses the assessment report, and sends a notification to an Amazon SNS topic.</p>"]}, "correct_response": ["b"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "The security department in a company requires automatic discovery of any security groups that allow unrestricted inbound traffic on port 22 (SSH). The security administrators should be notified of any violationsWhich solution meets these requirements with the MOST operational efficiency?", "related_lectures": []}, {"_class": "assessment", "id": 63655184, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer is deploying a proxy server solution in an Amazon VPC. The engineer has deployed proxy software on multiple EC2 instances across Availability Zones. Route tables have been configured to forward traffic to the proxy instances. The proxy instance security groups have been configured to allow ports 80 and 443 inbound and outbound.</p><p>Upon testing the solution the engineer has discovered that the proxy instances are not forwarding traffic to the internet.</p><p>What else needs to be done for this solution to work?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Each EC2 instance performs source/destination checks by default. This means that the instance must be the source or destination of any traffic it sends or receives.</p><p>However, a NAT instance must be able to send and receive traffic when the source or destination is not itself. Therefore, you must disable source/destination checks on the NAT instance.</p><p>You can disable the SrcDestCheck attribute for a NAT instance that's either running or stopped using the console or the command line.</p><p><strong>CORRECT: </strong>\"Disable source/destination checks on the EC2 proxy instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new DHCP options set pointing to the proxy server\" is incorrect.</p><p>Proxy settings are not distributed using DHCP. Within a VPC you point your instances to proxy servers by updating the route table which has already been done.</p><p><strong>INCORRECT:</strong> \"Allow outbound traffic on all ports to any internet address\" is incorrect.</p><p>Only ports 80 and 443 are needed outbound for proxying traffic to internet web addresses.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Route 53 Resolver endpoint for proxy traffic\" is incorrect.</p><p>Route 53 Resolver is a DNS service and is not used for configuring a web proxy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p>", "answers": ["<p>Disable source/destination checks on the EC2 proxy instances.</p>", "<p>Create a new DHCP options set pointing to the proxy server.</p>", "<p>Allow outbound traffic on all ports to any internet address.</p>", "<p>Create an Amazon Route 53 Resolver endpoint for proxy traffic.</p>"]}, "correct_response": ["a"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A security engineer is deploying a proxy server solution in an Amazon VPC. The engineer has deployed proxy software on multiple EC2 instances across Availability Zones. Route tables have been configured to forward traffic to the proxy instances. The proxy instance security groups have been configured to allow ports 80 and 443 inbound and outbound.Upon testing the solution the engineer has discovered that the proxy instances are not forwarding traffic to the internet.What else needs to be done for this solution to work?", "related_lectures": []}, {"_class": "assessment", "id": 63655186, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security team has mandated that only approved Amazon Machine Images (AMIs) can be used for launching Amazon EC2 instances. The security team requires a method of automatically validating compliance with the new mandate.</p><p>Which solution can the security team use to find unapproved AMIs for new and existing Amazon EC2 instances?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Config rule can check that running EC2 instances are using approved Amazon Machine Images, or AMIs. You can specify a list of approved AMIs by ID or provide a tag to specify the list of AMI IDs.</p><p>The AWS Config rule \u201capproved_ami_by_id\u201d checks if running instances are using specified AMIs. You must specify a list of approved AMI IDs. Running instances with AMIs that are not on this list are NON_COMPLIANT.</p><p><strong>CORRECT: </strong>\"Deploy the AWS Config rule \u201capproved_ami_by_id\u201d and specify the approved AMI IDs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a user data script that checks the AMI ID in instance metadata for compliance\" is incorrect.</p><p>This would only work for new instances that are launched and would not help identify existing instances that are not compliant.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch custom metric that reports the AMI ID of EC2 instances\" is incorrect.</p><p>You cannot create a metric that reports the AMI ID of an EC2 instance.</p><p><strong>INCORRECT:</strong> \"Deploy the AWS Config rule \u201cebs-optimized-instance\u201d and specify the approved AMI ARNs\" is incorrect.</p><p>This rule checks if running instances are using specified AMIs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p>", "answers": ["<p>Deploy the AWS Config rule \u201capproved_ami_by_id\u201d and specify the approved AMI IDs.</p>", "<p>Create a user data script that checks the AMI ID in instance metadata for compliance.</p>", "<p>Create an Amazon CloudWatch custom metric that reports the AMI ID of EC2 instances.</p>", "<p>Deploy the AWS Config rule \u201cebs-optimized-instance\u201d and specify the approved AMI ARNs.</p>"]}, "correct_response": ["a"], "section": "Domain 5 - Data Protection", "question_plain": "A security team has mandated that only approved Amazon Machine Images (AMIs) can be used for launching Amazon EC2 instances. The security team requires a method of automatically validating compliance with the new mandate.Which solution can the security team use to find unapproved AMIs for new and existing Amazon EC2 instances?", "related_lectures": []}, {"_class": "assessment", "id": 63655188, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A security engineer must configure AWS WAF to store logs in a central location for later analysis.</p><p>What is the MOST operationally efficient solution that meets this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>With AWS WAF you can enable logging to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched.</p><p>You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose. In this case the most operationally efficient solution is to send the logs directly to Amazon S3.</p><p><strong>CORRECT: </strong>\"Configure AWS WAF to send its log files directly to an Amazon S3 bucket for later analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS WAF to send its log files directly to Amazon Kinesis Data Analytics for analysis\" is incorrect.</p><p>You cannot send log files from AWS WAF to Kinesis Data Analytics.</p><p><strong>INCORRECT:</strong> \"Configure AWS WAF to send its log files to an Amazon CloudWatch Logs log group and then export to an Amazon S3 bucket\" is incorrect.</p><p>This is less operationally efficient and more expensive as CloudWatch Logs is being used in addition to S3 rather than sending directly to Amazon S3.</p><p><strong>INCORRECT:</strong> \"Configure AWS WAF to send its log files to Amazon Kinesis Data Firehose and then to stream the logs to an Amazon S3 bucket\" is incorrect.</p><p>This is less operationally efficient and more expensive as Kinesis Data Firehose is being used in addition to S3 rather than sending directly to Amazon S3.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging.html</a></p>", "answers": ["<p>Configure AWS WAF to send its log files to an Amazon CloudWatch Logs log group and then export to an Amazon S3 bucket.</p>", "<p>Configure AWS WAF to send its log files directly to Amazon Kinesis Data Analytics for analysis.</p>", "<p>Configure AWS WAF to send its log files directly to an Amazon S3 bucket for later analysis.</p>", "<p>Configure AWS WAF to send its log files to Amazon Kinesis Data Firehose and then to stream the logs to an Amazon S3 bucket.</p>"]}, "correct_response": ["c"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A security engineer must configure AWS WAF to store logs in a central location for later analysis.What is the MOST operationally efficient solution that meets this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 63655190, "assessment_type": "multi-select", "prompt": {"question": "<p>A company runs an ecommerce website and is concerned about the risk of DDoS attacks. The company needs to identify methods to minimize the downtime associated with any attacks that might happen in the future.</p><p>Which steps would help achieve this? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS Shield Advanced offers protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources.</p><p>For customers on Business or Enterprise support plans, AWS Shield Advanced gives you 24/7 access to the SRT, which can be engaged before, during, or after a DDoS attack. The SRT will help triage the incidents, identify root causes, and apply mitigations on your behalf. The SRT has deep expertise in rapidly responding to and mitigating DDoS attacks across AWS customers.</p><p>With AWS WAF you can create rule statements in your web ACL with criteria that matches the unusual behavior. It is recommended to first use the rule action Count instead of Block. After you're comfortable that the new rule is identifying the correct requests, you can modify your rule to block the requests.</p><p><strong>CORRECT: </strong>\"Subscribe to AWS Shield Advanced and reach out to AWS Support in the event of an attack\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create rule statements in AWS WAF web ACLs to block matching requests relating to the attack traffic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access\" is incorrect.</p><p>GuardDuty monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. However, this is not the best service for protecting against DDoS attack. AWS Shield and AWS WAF should be used for this purpose.</p><p><strong>INCORRECT:</strong> \"Use VPC Flow Logs to monitor network traffic and an AWS Config auto-remediation to block the attack traffic\" is incorrect.</p><p>You can capture network activity within your VPC using Flow Logs. However, AWS Config is a configuration compliance service and though it does offer auto-remediation, this is not suitable for blocking DDoS attack traffic.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Logs to capture network traffic and AWS Lambda to block unauthorized access by updating Network ACLs\" is incorrect.</p><p>CloudWatch Logs does not capture network traffic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-responding.html</a></p><p><a href=\"https://aws.amazon.com/shield/features/\">https://aws.amazon.com/shield/features/</a></p>", "answers": ["<p>Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access.</p>", "<p>Subscribe to AWS Shield Advanced and reach out to AWS Support in the event of an attack.</p>", "<p>Create rule statements in AWS WAF web ACLs to block matching requests relating to the attack traffic.</p>", "<p>Use VPC Flow Logs to monitor network traffic and an AWS Config auto-remediation to block the attack traffic.</p>", "<p>Use Amazon CloudWatch Logs to capture network traffic and AWS Lambda to block unauthorized access by updating Network ACLs.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company runs an ecommerce website and is concerned about the risk of DDoS attacks. The company needs to identify methods to minimize the downtime associated with any attacks that might happen in the future.Which steps would help achieve this? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655192, "assessment_type": "multi-select", "prompt": {"question": "<p>A developer is attempting to access an Amazon S3 bucket in a member account in AWS Organizations. The developer is logged in to the account with user credentials and has received an access denied error with no bucket listed. The developer should have read-only access to all buckets in the account.</p><p>A security engineer has reviewed the permissions and found that the developer's IAM user has been granted read-only access to all S3 buckets in the account.</p><p>Which additional steps should the security engineer take to troubleshoot the issue? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>A service control policy (SCP) may have been implemented that limits the API actions that are available for Amazon S3. This will apply to all users in the account regardless of the permissions they have assigned to their user account.</p><p>Another potential cause of the issue is that the permissions boundary for the user limits the S3 API actions available to the user. A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_04-52-50-aa8d1a16e85159c7aec866ab7b5ffbaf.JPG\"><p><strong>CORRECT: </strong>\"Check the SCPs set at the organizational units (OUs)\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Check for the permissions boundaries set for the IAM user\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check if an appropriate IAM role is attached to the IAM user\" is incorrect.</p><p>The question states that the user is logged in with a user account so is not assuming a role.</p><p><strong>INCORRECT:</strong> \"Check the bucket policies for all S3 buckets\" is incorrect.</p><p>The user has not been granted access to any buckets, and the error does not list access denied to any specific bucket. Therefore, it is more likely that the user is not granted the API action to list the buckets.</p><p><strong>INCORRECT:</strong> \"Check the ACLs for all S3 buckets\" is incorrect.</p><p>With a bucket ACL the grantee is an AWS account or one of the predefined groups. With an ACL you can grant read/write at the bucket level, but list is restricted to the object level so would not apply to the bucket itself. The user has been unable to list any buckets in this case, so an ACL is unlikely to be the cause.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>", "answers": ["<p>Check the bucket policies for all S3 buckets.</p>", "<p>Check the ACLs for all S3 buckets.</p>", "<p>Check the SCPs set at the organizational units (OUs).</p>", "<p>Check for the permissions boundaries set for the IAM user.</p>", "<p>Check if an appropriate IAM role is attached to the IAM user.</p>"]}, "correct_response": ["c", "d"], "section": "Domain 4 - Identity and Access Management", "question_plain": "A developer is attempting to access an Amazon S3 bucket in a member account in AWS Organizations. The developer is logged in to the account with user credentials and has received an access denied error with no bucket listed. The developer should have read-only access to all buckets in the account.A security engineer has reviewed the permissions and found that the developer's IAM user has been granted read-only access to all S3 buckets in the account.Which additional steps should the security engineer take to troubleshoot the issue? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655194, "assessment_type": "multi-select", "prompt": {"question": "<p>A company hosts video files for a website in an Amazon S3 bucket that is configured as an origin for an Amazon CloudFront distribution. The company was recently notified that the videos were being accessed from unauthorized countries.</p><p>Which actions should a security engineer take the limit the distribution of the video files? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can use <em>geo restriction</em>, also known as <em>geo blocking</em>, to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront distribution. The CloudFront geo restriction feature can be used to restrict access to all the files that are associated with a distribution and to restrict access at the country level.</p><p>To ensure users in the restricted countries cannot bypass CloudFront and go straight to the Amazon S3 bucket, an origin access identity can be configured in the CloudFront distribution. This identity is then granted access in the S3 bucket policy, and all other connections are denied.</p><p><strong>CORRECT: </strong>\"Create an origin access identity (OAI) for the CloudFront distribution and update the S3 bucket policy to restrict access to the OAI\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Update the distribution settings in CloudFront and configure restrictions based on the geography of the request\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Restrict Viewer Access option in CloudFront and specify a deny list of unauthorized countries\" is incorrect.</p><p>This feature is used to configure signed URLs and signed cookies.</p><p><strong>INCORRECT:</strong> \"Update the S3 bucket policy with condition statements that deny access based on the source IP addresses of users\" is incorrect.</p><p>This would be hard to manage as the addresses may change and be hard to identify. The users should always go through the CloudFront distribution as well.</p><p><strong>INCORRECT:</strong> \"Configure a query string whitelist in CloudFront and specify a list of countries that should be denied access using query string parameters\" is incorrect.</p><p>This is not the correct usage for the query string whitelist feature which is used for determining the query string parameters that you want CloudFront to use as a basis for caching.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p>", "answers": ["<p>Configure the Restrict Viewer Access option in CloudFront and specify a deny list of unauthorized countries.</p>", "<p>Update the S3 bucket policy with condition statements that deny access based on the source IP addresses of users.</p>", "<p>Create an origin access identity (OAI) for the CloudFront distribution and update the S3 bucket policy to restrict access to the OAI.</p>", "<p>Update the distribution settings in CloudFront and configure restrictions based on the geography of the request.</p>", "<p>Configure a query string whitelist in CloudFront and specify a list of countries that should be denied access using query string parameters.</p>"]}, "correct_response": ["c", "d"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company hosts video files for a website in an Amazon S3 bucket that is configured as an origin for an Amazon CloudFront distribution. The company was recently notified that the videos were being accessed from unauthorized countries.Which actions should a security engineer take the limit the distribution of the video files? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655196, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has started to deploy resources to the AWS cloud. Initial resources have been deployed in the US West (Oregon) Region and an AWS CloudTrail trail has been created to record API activity in a bucket in the same Region.</p><p>The security team requires that API activity is captured from all Regions and stored in a central Region.</p><p>What is the SIMPLEST way to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can configure CloudTrail to deliver log files from multiple regions to a single S3 bucket for a single account. For example, you have a trail in the US West (Oregon) Region that is configured to deliver log files to an S3 bucket.</p><p>When you change an existing single-region trail to log all regions, CloudTrail logs events from all regions in your account. CloudTrail delivers log files to the same S3 bucket.</p><p>If CloudTrail has permissions to write to an S3 bucket, the bucket for a multi-region trail does not have to be in the trail's home region.</p><p><strong>CORRECT: </strong>\"Change the existing single-region trail to log all regions and capture API activity in a single central Amazon S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create individual trails for each region and capture API activity in a single central Amazon S3 bucket\" is incorrect.</p><p>The simplest solution is to use a single trail configured for all regions.</p><p><strong>INCORRECT:</strong> \"Create a new trail that applies to all regions and capture API activity in separate central S3 buckets for each region\" is incorrect.</p><p>When you create a trail that applies to all regions you specify a single S3 bucket.</p><p><strong>INCORRECT:</strong> \"Create individual trails for each region and capture API activity in separate central S3 buckets for each region\" is incorrect.</p><p>This is the most complex solution, so it does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html</a></p>", "answers": ["<p>Create individual trails for each region and capture API activity in a single central Amazon S3 bucket.</p>", "<p>Create a new trail that applies to all regions and capture API activity in separate central S3 buckets for each region.</p>", "<p>Create individual trails for each region and capture API activity in separate central S3 buckets for each region.</p>", "<p>Change the existing single-region trail to log all regions and capture API activity in a single central Amazon S3 bucket.</p>"]}, "correct_response": ["d"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company has started to deploy resources to the AWS cloud. Initial resources have been deployed in the US West (Oregon) Region and an AWS CloudTrail trail has been created to record API activity in a bucket in the same Region.The security team requires that API activity is captured from all Regions and stored in a central Region.What is the SIMPLEST way to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655198, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs across multiple Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer (ALB). The application is experiencing a DDoS attack from malicious software that is distributed across hosts around the world. The software can be identified in the User-Agent field of the request header.</p><p>A security engineer needs to mitigate the attack. Which actions should be taken?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you create string match conditions, you specify filters that identify the string that you want to search for and the part of web requests that you want AWS WAF to inspect for that string, such as the URI or the query string.</p><p>The string match condition can be configured to match the value in the User-Agent HTTP request header. In this scenario the rule can match the value in the request header of the malicious software and block that traffic only.</p><p><strong>CORRECT: </strong>\"Create a Web ACL with a string match condition that matches the value in the User-Agent header. Configure WAF to block requests that match the condition\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution in front of the ALB. Configure conditional requests and block requests that match the value in the User-Agent header\" is incorrect.</p><p>Conditional requests in Amazon CloudFront relate to the behavior of caching and cannot be used for this purpose.</p><p><strong>INCORRECT:</strong> \"Create a Web ACL with a size constraint that matches the byte size of the value in the User-Agent header. Configure WAF to block requests that match the condition\" is incorrect.</p><p>A size constraint should not be used in this situation. A string match condition should be used to match the value in the request header.</p><p><strong>INCORRECT:</strong> \"Enable AWS Shield and configure layer 7 protection to identify and block attacks based on the value in the User-Agent header\" is incorrect.</p><p>Shield provides layer 3 and 4 protection. AWS WAF is required for Layer 7 protection.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-string-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-string-conditions.html</a></p>", "answers": ["<p>Create an Amazon CloudFront distribution in front of the ALB. Configure conditional requests and block requests that match the value in the User-Agent header.</p>", "<p>Create a Web ACL with a string match condition that matches the value in the User-Agent header. Configure WAF to block requests that match the condition.</p>", "<p>Create a Web ACL with a size constraint that matches the byte size of the value in the User-Agent header. Configure WAF to block requests that match the condition.</p>", "<p>Enable AWS Shield and configure layer 7 protection to identify and block attacks based on the value in the User-Agent header.</p>"]}, "correct_response": ["b"], "section": "Domain 1 - Incident Response", "question_plain": "An application runs across multiple Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer (ALB). The application is experiencing a DDoS attack from malicious software that is distributed across hosts around the world. The software can be identified in the User-Agent field of the request header.A security engineer needs to mitigate the attack. Which actions should be taken?", "related_lectures": []}, {"_class": "assessment", "id": 63655200, "assessment_type": "multi-select", "prompt": {"question": "<p>An Amazon EC2 web server has been deployed into Subnet B within a VPC. An EC2 instance in Subnet A within the same VPC must be able to connect to the web service. A network administrator has created a security group and added both instances to it. Subnet A uses a default Network ACL. A custom Network ACL has been created and attached to Subnet B.</p><p>Which rules must be created to successfully connect to the web server? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>A <em>security group</em> acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.</p><p>For each security group, you add <em>rules</em> that control the inbound traffic to instances, and a separate set of rules that control the outbound traffic.</p><p>A <em>network access control list (ACL)</em> is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. Network ACLs act at the subnet level, not the instance level.</p><p>A key difference between a security group and a network ACL is that security groups are stateful whereas network ACLs are stateless. With a network ACL you must therefore create a rule that allows the return traffic for a connection using the ephemeral port range (1024-65535).</p><p>Even though both instances are in the same security group you still must create the inbound rule for the web server and an outbound rule for the connection to it. With the network ACL you must create an inbound rule allowing TCP port 80 and an outbound rule allowing the return traffic on the ephemeral ports.</p><p>The table below describes some important differences between security groups and network ACLs:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_05-00-49-dba9ec22c6ac2aecaaab236e338abf88.jpg\"><p><strong>CORRECT: </strong>\"Security group TCP port 80 inbound and TCP port 80 outbound\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Network ACL: TCP port 80 inbound and TCP ports 1024-65535 outbound\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Security group: TCP port 80 inbound and nothing outbound\" is incorrect.</p><p>You must have an outbound rule otherwise the connection to the web server will not be possible.</p><p><strong>INCORRECT:</strong> \"Security group: TCP port 80 inbound and TCP ports 1024-65535 outbound\" is incorrect.</p><p>There is no need to allow the ephemeral ports outbound as these will typically only be used in response traffic which is allowed due to the stateful nature of a security group. Only port 80 is required outbound.</p><p><strong>INCORRECT:</strong> \"Network ACL: TCP port 80 inbound and TCP port 80 outbound\" is incorrect.</p><p>Port 80 outbound is not required in this security group as it is attached to the subnet the web server runs in and the web server will be receiving traffic on port 80 and returning responses using ephemeral ports.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>", "answers": ["<p>Security group: TCP port 80 inbound and nothing outbound.</p>", "<p>Network ACL: TCP port 80 inbound and TCP ports 1024-65535 outbound.</p>", "<p>Security group TCP port 80 inbound and TCP port 80 outbound.</p>", "<p>Network ACL: TCP port 80 inbound and TCP port 80 outbound.</p>", "<p>Security group: TCP port 80 inbound and TCP ports 1024-65535 outbound.</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3 - Infrastructure Security", "question_plain": "An Amazon EC2 web server has been deployed into Subnet B within a VPC. An EC2 instance in Subnet A within the same VPC must be able to connect to the web service. A network administrator has created a security group and added both instances to it. Subnet A uses a default Network ACL. A custom Network ACL has been created and attached to Subnet B.Which rules must be created to successfully connect to the web server? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655202, "assessment_type": "multi-select", "prompt": {"question": "<p>An application uses Amazon EC2 instances to retrieve messages from an Amazon SQS queue. The EC2 instances have an instance profile assigned that uses an IAM role to provide permissions to the queue. A security engineer has been asked to investigate why the instances are not able to retrieve messages. The solution should follow the principle of least privilege.</p><p>What actions should be taken to identify the cause of the? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>There are two ways to give your users permissions to your Amazon SQS resources: using the Amazon SQS policy system and using the IAM policy system. You can use one or the other, or both.</p><p>The security engineer should check that the IAM role has the minimum permissions requires to receive messages from the queue. The SQS policy system should also be checked to ensure that more restrictive permissions are not assigned there.</p><p><strong>CORRECT: </strong>\"Check the configuration of the IAM role attached to the instance profile to ensure it has sufficient permissions\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Check if an Amazon SQS policy explicitly denies access to the IAM role used by the instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check that the AmazonSQSFullAccess managed policy is attached to the IAM role used by the instances\" is incorrect.</p><p>This managed policy provides more permissions than are required by the instances and does not follow the principle of least privilege.</p><p><strong>INCORRECT:</strong> \"Check that a policy is attached to the IAM role used by the instances that grants the \u201csqs:AddPermission\u201d permission\" is incorrect.</p><p>This permission is not required to receive messages from an Amazon SQS queue, it allows sharing access to the queue.</p><p><strong>INCORRECT:</strong> \"Check if server-side encryption is enabled using an AWS KMS managed key\" is incorrect.</p><p>Server-side encryption is not a requirement for receiving messages from the queue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-identity-based-policies.html</a></p>", "answers": ["<p>Check the configuration of the IAM role attached to the instance profile to ensure it has sufficient permissions.</p>", "<p>Check if an Amazon SQS policy explicitly denies access to the IAM role used by the instances.</p>", "<p>Check that the AmazonSQSFullAccess managed policy is attached to the IAM role used by the instances.</p>", "<p>Check that a policy is attached to the IAM role used by the instances that grants the \u201csqs:AddPermission\u201d permission.</p>", "<p>Check if server-side encryption is enabled using an AWS KMS managed key.</p>"]}, "correct_response": ["a", "b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An application uses Amazon EC2 instances to retrieve messages from an Amazon SQS queue. The EC2 instances have an instance profile assigned that uses an IAM role to provide permissions to the queue. A security engineer has been asked to investigate why the instances are not able to retrieve messages. The solution should follow the principle of least privilege.What actions should be taken to identify the cause of the? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655204, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IAM user is unable to assume an IAM role using the AWS CLI. The IAM policy assigned to the user includes the following statement:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-01-05_05-04-17-807f267ae8ebe3a6e04e065effa0209b.jpg\"><p>Which additional element MUST be included to allow the role to be assumed?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The user needs to verify that the IAM policy grants permission to call sts:AssumeRole for the role that they want to assume. The Action element of an IAM policy must allow you to call the AssumeRole action. In addition, the Resource element of your IAM policy must specify the role that you want to assume.</p><p>For this situation the IAM policy would need to include a statement such as this:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-01-05_05-04-17-58db370662b4c3e0cb0b87b178a0d31e.jpg\"><p><strong>CORRECT: </strong>\"Action\": \"sts:AssumeRole\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Action\": \"ec2:AssumeRole\" is incorrect.</p><p>The security token service (STS) is used for assuming roles so \u201cec2\u201d should be replaced with \u201csts\u201d.</p><p><strong>INCORRECT:</strong> \"Action\": \"sts:GetSessionToken\" is incorrect.</p><p>This API action returns a set of temporary credentials for an AWS account or IAM user.</p><p><strong>INCORRECT:</strong> \"Action\": \"sts:GetAccessKeyInfo\" is incorrect.</p><p>This API Action returns the account identifier for the specified access key ID.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>", "answers": ["<p>\"Action\": \"ec2:AssumeRole\"</p>", "<p>\"Action\": \"sts:AssumeRole\"</p>", "<p>\"Action\": \"sts:GetSessionToken\"</p>", "<p>\"Action\": \"sts:GetAccessKeyInfo\"</p>"]}, "correct_response": ["b"], "section": "Domain 4 - Identity and Access Management", "question_plain": "An IAM user is unable to assume an IAM role using the AWS CLI. The IAM policy assigned to the user includes the following statement:Which additional element MUST be included to allow the role to be assumed?", "related_lectures": []}, {"_class": "assessment", "id": 63655206, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company currently manages Amazon EC2 instances running Windows and Linux in public and private subnets. The operations team currently connects over the Internet to manage the instances as there is no private connection to the corporate network.</p><p>Security groups have been updated to allow the RDP and SSH protocols from any source IPv4 address. There have been reports of malicious attempts to access the resources and the company wishes to implement the most secure solution for managing the instances.</p><p>Which strategy should a security engineer recommend?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most secure option presented is to use AWS Systems Manager Session Manager. Session Manager is a fully managed AWS Systems Manager capability that lets you manage EC2 instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI).</p><p>Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your managed instances.</p><p><strong>CORRECT: </strong>\"Deploy the AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using Session Manager restricting access to users with permission to manage the instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy a server on the corporate network that can be used for managing EC2 instances. Update the security groups to allow connections over SSH and RDP from the on-premises management server only\" is incorrect.</p><p>This is less secure compared to using session manager as the SSH and RDP ports must still be open on instances and it does not offer the robust controls offered by session manager.</p><p><strong>INCORRECT:</strong> \"Deploy a Linux bastion host with an Elastic IP address in the public subnet. Allow access to the bastion host from 0.0.0.0/\" is incorrect.</p><p>This is less secure compared to using session manager as the SSH and RDP ports must still be open on instances and it does not offer the robust controls offered by session manager. This solution could be better secured by restricting access to the corporate IP ranges.</p><p><strong>INCORRECT:</strong> \"Configure an IPSec Virtual Private Network (VPN) connecting the corporate network to the Amazon VPC. Update security groups to allow connections over SSH and RDP from the corporate network only\" is incorrect.</p><p>This is less secure compared to using session manager as the SSH and RDP ports must still be open on instances and it does not offer the robust controls offered by session manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>", "answers": ["<p>Deploy a server on the corporate network that can be used for managing EC2 instances. Update the security groups to allow connections over SSH and RDP from the on-premises management server only.</p>", "<p>Deploy the AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using Session Manager restricting access to users with permission to manage the instances.</p>", "<p>Deploy a Linux bastion host with an Elastic IP address in the public subnet. Allow access to the bastion host from 0.0.0.0/0.</p>", "<p>Configure an IPSec Virtual Private Network (VPN) connecting the corporate network to the Amazon VPC. Update security groups to allow connections over SSH and RDP from the corporate network only.</p>"]}, "correct_response": ["b"], "section": "Domain 3 - Infrastructure Security", "question_plain": "A company currently manages Amazon EC2 instances running Windows and Linux in public and private subnets. The operations team currently connects over the Internet to manage the instances as there is no private connection to the corporate network.Security groups have been updated to allow the RDP and SSH protocols from any source IPv4 address. There have been reports of malicious attempts to access the resources and the company wishes to implement the most secure solution for managing the instances.Which strategy should a security engineer recommend?", "related_lectures": []}, {"_class": "assessment", "id": 63655208, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses an Amazon RDS MySQL database instance to store customer order data. The security team have requested that SSL/TLS encryption in transit must be used for encrypting connections to the database from application servers. The data in the database is currently encrypted at rest using an AWS KMS key.</p><p>How can a security engineer enable encryption in transit?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when Amazon RDS provisions the instance. These certificates are signed by a certificate authority. The SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks.</p><p>You can download a root certificate from AWS that works for all Regions, or you can download Region-specific intermediate certificates.</p><p><strong>CORRECT: </strong>\"Download the AWS-provided root certificates. Use the certificates when connecting to the RDS DB instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption in transit enabled\" is incorrect.</p><p>There is no need to do this as a certificate is created when the DB instance is launched.</p><p><strong>INCORRECT:</strong> \"Enable encryption in transit using the RDS Management console and obtain a key using AWS KMS\" is incorrect.</p><p>You can't enable/disable encryption in transit using the RDS management console or use a KMS key.</p><p><strong>INCORRECT:</strong> \"Add a self-signed certificate to the RDS DB instance. Use the certificates in all connections to the RDS DB instance\" is incorrect.</p><p>You can't use self-signed certificates with RDS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html</a></p>", "answers": ["<p>Enable encryption in transit using the RDS Management console and obtain a key using AWS KMS.</p>", "<p>Add a self-signed certificate to the RDS DB instance. Use the certificates in all connections to the RDS DB instance.</p>", "<p>Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption in transit enabled.</p>", "<p>Download the AWS-provided root certificates. Use the certificates when connecting to the RDS DB instance.</p>"]}, "correct_response": ["d"], "section": "Domain 5 - Data Protection", "question_plain": "A company uses an Amazon RDS MySQL database instance to store customer order data. The security team have requested that SSL/TLS encryption in transit must be used for encrypting connections to the database from application servers. The data in the database is currently encrypted at rest using an AWS KMS key.How can a security engineer enable encryption in transit?", "related_lectures": []}, {"_class": "assessment", "id": 63655210, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media company is streaming their content globally via AWS, but due to legal constraints, it needs to restrict data storage to a specific AWS region using AWS Organizations. A security engineer is tasked with preventing users from storing data in any other region.</p><p>Which approach will allow the security engineer to implement these requirements with the MINIMUM operational overhead?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The SCP policy with the \"aws:RequestedRegion\" condition denying actions outside the approved region, when attached to the AWS account under AWS Organizations, will effectively prevent any user in the organization from storing data outside the approved region.</p><p>It reduces the operational overhead as it doesn't need to be individually attached to every user but can be applied at the organizational level.</p><p><strong>CORRECT: </strong>\"Implement an SCP that uses the \"aws:RequestedRegion\" condition to deny actions outside the approved region. Attach the SCP to the AWS account under AWS Organizations\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM policy with the \"aws:RequestedRegion\" condition to permit actions only in the approved region. Attach the policy to all IAM users\" is incorrect.</p><p>This approach could work, but it adds unnecessary operational overhead as it requires attaching the policy to every IAM user individually. Additionally, new users or those missed could still potentially store data outside the approved region.</p><p><strong>INCORRECT:</strong> \"Design an IAM policy that uses the \"aws:RequestedRegion\" condition to deny operations outside the preferred region. Attach this policy to every IAM user\" is incorrect.</p><p>This solution also entails higher operational overhead as it requires the policy to be manually attached to each user.</p><p><strong>INCORRECT:</strong> \"Develop an SCP with the \"aws:RequestedRegion\" condition to allow operations solely in the specified region. Attach the SCP to each IAM user in the organization\" is incorrect.</p><p>SCPs can't be attached to individual IAM users. They are designed to set fine-grained permissions for the whole organization or organizational units, not individual IAM users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-requestedregion\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-requestedregion</a></p>", "answers": ["<p>Implement an SCP that uses the \"aws:RequestedRegion\" condition to deny actions outside the approved region. Attach the SCP to the AWS account under AWS Organizations.</p>", "<p>Create an IAM policy with the \"aws:RequestedRegion\" condition to permit actions only in the approved region. Attach the policy to all IAM users.</p>", "<p>Design an IAM policy that uses the \"aws:RequestedRegion\" condition to deny operations outside the preferred region. Attach this policy to every IAM user.</p>", "<p>Develop an SCP with the \"aws:RequestedRegion\" condition to allow operations solely in the specified region. Attach the SCP to each IAM user in the organization.</p>"]}, "correct_response": ["a"], "section": "Domain 6: Management and Security Governance", "question_plain": "A media company is streaming their content globally via AWS, but due to legal constraints, it needs to restrict data storage to a specific AWS region using AWS Organizations. A security engineer is tasked with preventing users from storing data in any other region.Which approach will allow the security engineer to implement these requirements with the MINIMUM operational overhead?", "related_lectures": []}, {"_class": "assessment", "id": 63655212, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An enterprise has two VPCs in the ap-south-1 Region: vpc-alpha and vpc-beta. The enterprise has recently created an Amazon API Gateway REST API with the endpoint type set to PRIVATE. It also created a VPC endpoint for the REST API in vpc-alpha, providing the resources in vpc-alpha successful access to the REST API.</p><p>The enterprise wants to allow resources in vpc-beta to access the REST API. A VPC endpoint for the REST API was created in vpc-beta, but the resources in vpc-beta still can't access the REST API.</p><p>A security engineer must enable access to the REST API for resources in vpc-beta while adhering to the principle of least privilege.</p><p>What solution will achieve these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon API Gateway allows you to create, deploy, and manage a REST API as a collection of resources and methods. When an API Gateway REST API is set to PRIVATE, it's only accessible from a specified VPC endpoint.</p><p>In this scenario, the API is initially configured with a VPC endpoint in vpc-alpha, enabling resources in vpc-alpha to access it. Now, the goal is to allow resources in vpc-beta to access the API. Although a VPC endpoint for the REST API is created in vpc-beta, the resources in vpc-beta are still unable to access the API.</p><p>This issue is typically due to missing permissions. When a VPC endpoint is created, you can control the access to your endpoint through endpoint policies. However, for the access to a PRIVATE API, an API Gateway resource policy is also needed in addition to the endpoint policy.</p><p>In the correct answer, the API endpoint type is maintained as PRIVATE, and a resource policy is attached to the REST API to allow access from vpc-beta. This resource policy specifically allows the necessary access for the resources in vpc-beta to use the REST API, aligning with the principle of least privilege.</p><p>The resource policy needs to specify the VPC endpoint in vpc-beta as an allowed source for making API calls. This combination of endpoint and resource policy provides fine-grained access control over your API.</p><p><strong>CORRECT: </strong>\"Maintain the API endpoint type as PRIVATE. Attach a resource policy to the REST API permitting access from vpc-beta\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Implement a VPC peering connection between vpc-alpha and vpc-beta. Attach an IAM policy to the resources in vpc-beta to authorize access to the REST API\" is incorrect.</p><p>VPC peering alone does not grant the required access to the REST API from vpc-beta. Therefore, this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Add a VPC endpoint for vpc-beta in vpc-alpha. Attach an IAM policy to the resources in vpc-beta granting access to the REST API\" is incorrect.</p><p>It's not possible to add a VPC endpoint for one VPC inside another VPC. Therefore, this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Switch the API endpoint type to REGIONAL. Add a resource policy to the REST API permitting access from vpc-beta\" is incorrect.</p><p>Switching the endpoint type to REGIONAL would make the REST API publicly accessible, which contradicts the company's requirement of keeping the API PRIVATE. Hence, this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</a></p>", "answers": ["<p>Implement a VPC peering connection between vpc-alpha and vpc-beta. Attach an IAM policy to the resources in vpc-beta to authorize access to the REST API.</p>", "<p>Add a VPC endpoint for vpc-beta in vpc-alpha. Attach an IAM policy to the resources in vpc-beta granting access to the REST API.</p>", "<p>Switch the API endpoint type to REGIONAL. Add a resource policy to the REST API permitting access from vpc-beta.</p>", "<p>Maintain the API endpoint type as PRIVATE. Attach a resource policy to the REST API permitting access from vpc-beta.</p>"]}, "correct_response": ["d"], "section": "Domain 6: Management and Security Governance", "question_plain": "An enterprise has two VPCs in the ap-south-1 Region: vpc-alpha and vpc-beta. The enterprise has recently created an Amazon API Gateway REST API with the endpoint type set to PRIVATE. It also created a VPC endpoint for the REST API in vpc-alpha, providing the resources in vpc-alpha successful access to the REST API.The enterprise wants to allow resources in vpc-beta to access the REST API. A VPC endpoint for the REST API was created in vpc-beta, but the resources in vpc-beta still can't access the REST API.A security engineer must enable access to the REST API for resources in vpc-beta while adhering to the principle of least privilege.What solution will achieve these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655214, "assessment_type": "multi-select", "prompt": {"question": "<p>A multinational company is operating a global web application on AWS behind a CloudFront distribution. As part of their security enhancement, the company has enabled AWS WAF on the CloudFront distribution with a Web ACL.</p><p>For compliance purposes, the company requires comprehensive logging of all requests hitting the web ACL. They have already prepared an Amazon S3 bucket for storing these logs.</p><p>Which combination of steps should the company take to meet this requirement? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To meet the requirement, you need to enable logging in AWS WAF settings. You would also need to associate the web ACL with a Kinesis Data Firehose delivery stream which can deliver logs to an S3 bucket.</p><p>The creation of a Kinesis Data Firehose delivery stream in the same region as the web ACL would ensure the logs can be successfully delivered. The stream would then be directed to the designated S3 bucket for storing logs.</p><p><strong>CORRECT: </strong>\"Enable logging in AWS WAF settings, associate the web ACL with an Amazon Kinesis Data Firehose delivery stream\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an Amazon Kinesis Data Firehose delivery stream in the same AWS Region as the web ACL. Specify the S3 bucket as the destination for the delivery stream\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS CloudTrail for the web ACL and specify the S3 bucket to store the logs\" is incorrect.</p><p>AWS CloudTrail is used to log and monitor account activity related to actions across AWS infrastructure, not the specific logging of requests hitting an AWS WAF web ACL.</p><p><strong>INCORRECT:</strong> \"Update the web ACL with an AWS Lambda function that logs all request details and stores them in the S3 bucket\" is incorrect.</p><p>While AWS Lambda can be used for a multitude of tasks in AWS, it is not the standard or scalable approach to log all request details coming to an AWS WAF web ACL.</p><p><strong>INCORRECT:</strong> \"Set up VPC Flow Logs for the CloudFront distribution and specify the S3 bucket to store the logs\" is incorrect.</p><p>VPC Flow Logs capture information about IP traffic going to and from network interfaces in your VPC, but it's not meant for logging requests hitting an AWS WAF web ACL.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging.html</a></p>", "answers": ["<p>Enable AWS CloudTrail for the web ACL and specify the S3 bucket to store the logs.</p>", "<p>Enable logging in AWS WAF settings, associate the web ACL with an Amazon Kinesis Data Firehose delivery stream.</p>", "<p>Update the web ACL with an AWS Lambda function that logs all request details and stores them in the S3 bucket.</p>", "<p>Create an Amazon Kinesis Data Firehose delivery stream in the same AWS Region as the web ACL. Specify the S3 bucket as the destination for the delivery stream.</p>", "<p>Set up VPC Flow Logs for the CloudFront distribution and specify the S3 bucket to store the logs.</p>"]}, "correct_response": ["b", "d"], "section": "Domain 6: Management and Security Governance", "question_plain": "A multinational company is operating a global web application on AWS behind a CloudFront distribution. As part of their security enhancement, the company has enabled AWS WAF on the CloudFront distribution with a Web ACL.For compliance purposes, the company requires comprehensive logging of all requests hitting the web ACL. They have already prepared an Amazon S3 bucket for storing these logs.Which combination of steps should the company take to meet this requirement? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 63655216, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A large enterprise has an AWS setup which contains multiple accounts managed through AWS Organizations. The accounts are categorized into several OUs based on the company's departmental structure. The security team now wants to enforce a policy to prevent any accidental deletion of S3 buckets across all accounts.</p><p>Which solution should the security team implement to meet this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>SCPs (Service Control Policies) in AWS Organizations allow you to whitelist or blacklist actions that can be performed in the AWS accounts that the policies apply to.</p><p>Creating an SCP that denies the s3:DeleteBucket action and applying it to the OUs will effectively prevent any deletion of S3 buckets.</p><p><strong>CORRECT: </strong>\"Create an SCP that includes a Deny rule for the s3:DeleteBucket action. Apply this SCP to all the OUs in the organization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to apply a policy that restricts deletion of S3 buckets across all AWS accounts\" is incorrect.</p><p>AWS Systems Manager is a management service that helps you to automatically apply policies, patches, updates, and configurations across your AWS resources. However, it doesn't provide capabilities to directly restrict S3 bucket deletions.</p><p><strong>INCORRECT:</strong> \"Use IAM policy that includes a Deny rule for the s3:DeleteBucket action. Attach this IAM policy to all the users across all accounts\" is incorrect.</p><p>While IAM policies can restrict actions performed by IAM users, they are applied on a per-user or per-role basis, and managing these for many users across multiple accounts can be operationally complex.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to continuously monitor and prevent the deletion of S3 buckets across all accounts\" is incorrect,</p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It can provide details of changes to AWS resources but can't directly prevent actions like deleting an S3 bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>", "answers": ["<p>Use AWS Systems Manager to apply a policy that restricts deletion of S3 buckets across all AWS accounts.</p>", "<p>Create an SCP that includes a Deny rule for the s3:DeleteBucket action. Apply this SCP to all the OUs in the organization.</p>", "<p>Use IAM policy that includes a Deny rule for the s3:DeleteBucket action. Attach this IAM policy to all the users across all accounts.</p>", "<p>Use AWS Config to continuously monitor and prevent the deletion of S3 buckets across all accounts.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A large enterprise has an AWS setup which contains multiple accounts managed through AWS Organizations. The accounts are categorized into several OUs based on the company's departmental structure. The security team now wants to enforce a policy to prevent any accidental deletion of S3 buckets across all accounts.Which solution should the security team implement to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 63655218, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is running a batch data processing application in an Amazon EC2 instance, which requires frequent access to an Amazon DynamoDB table. The company's security policies mandate that all connections to DynamoDB should be private and secure.</p><p>The company has set up a Gateway VPC Endpoint for DynamoDB in the VPC where the EC2 instance resides. Even though the EC2 instance is configured to be within a private subnet with a NAT gateway for internet access, the traffic from the EC2 to DynamoDB goes through the NAT gateway instead of the Gateway VPC endpoint.</p><p>What action can a security engineer take to ensure the EC2 instance uses the Gateway VPC Endpoint for DynamoDB?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The correct answer is to associate the Gateway VPC Endpoint for DynamoDB with the route table of the private subnet. A VPC endpoint enables instances in the VPC to use AWS services (in this case, DynamoDB) via a private network path. This private connection between your VPC and DynamoDB does not require access over the internet, VPN, AWS Direct Connect, or a NAT device, and does not require the traffic to traverse multiple VPCs or accounts.</p><p>The reason the EC2 instance is not using the endpoint could be that the route tables associated with the subnet do not have a route pointing traffic to the endpoint. By default, if no specific route is matched, traffic follows the main route which is generally directed to the NAT Gateway or Internet Gateway.</p><p>When you associate the Gateway VPC Endpoint with the route table of the private subnet, you are essentially adding a route for the IP range that DynamoDB uses and pointing it to the VPC Endpoint. This means that when an EC2 instance in the subnet tries to reach DynamoDB, it matches this specific route and is directed through the VPC Endpoint. This setup ensures that all traffic to DynamoDB from your EC2 instance will stay within the Amazon network, providing secure and efficient connectivity to DynamoDB.</p><p><strong>CORRECT: </strong>\"Associate the Gateway VPC Endpoint with the route table of the private subnet, where the EC2 instance resides\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the route table of the private subnet, where the EC2 instance resides, to remove the route to the NAT gateway\" is incorrect.</p><p>Removing the route to the NAT gateway would isolate the EC2 instance from the internet which may cause other connectivity issues, not a best practice.</p><p><strong>INCORRECT:</strong> \"Modify the policy of the Gateway VPC Endpoint to permit access from the EC2 instance's private IP\" is incorrect.</p><p>The Gateway VPC Endpoint policy does not require specific mention of the EC2 instance's private IP. If the DynamoDB actions are allowed, traffic should flow through the endpoint.</p><p><strong>INCORRECT:</strong> \"Alter the security group of the EC2 instance to permit connections to the DynamoDB network address space\" is incorrect.</p><p>Changing the security group rules of the EC2 instance doesn't influence the route taken by the traffic to DynamoDB. DynamoDB's access is controlled by IAM, not security group rules.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html</a></p>", "answers": ["<p>Update the route table of the private subnet, where the EC2 instance resides, to remove the route to the NAT gateway.</p>", "<p>Associate the Gateway VPC Endpoint with the route table of the private subnet, where the EC2 instance resides.</p>", "<p>Modify the policy of the Gateway VPC Endpoint to permit access from the EC2 instance's private IP.</p>", "<p>Alter the security group of the EC2 instance to permit connections to the DynamoDB network address space.</p>"]}, "correct_response": ["b"], "section": "Domain 6: Management and Security Governance", "question_plain": "A company is running a batch data processing application in an Amazon EC2 instance, which requires frequent access to an Amazon DynamoDB table. The company's security policies mandate that all connections to DynamoDB should be private and secure.The company has set up a Gateway VPC Endpoint for DynamoDB in the VPC where the EC2 instance resides. Even though the EC2 instance is configured to be within a private subnet with a NAT gateway for internet access, the traffic from the EC2 to DynamoDB goes through the NAT gateway instead of the Gateway VPC endpoint.What action can a security engineer take to ensure the EC2 instance uses the Gateway VPC Endpoint for DynamoDB?", "related_lectures": []}, {"_class": "assessment", "id": 63655220, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company manages an application that runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The NLB has access logs enabled which are being stored in an Amazon S3 bucket. A security engineer requires a solution to run ad hoc queries against the access logs to identify application access patterns.</p><p>How should the security engineer accomplish this task with the least amount of administrative overhead?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Athena is a serverless service you can use to run SQL queries against data in Amazon S3. You just need to point Athena to your data in Amazon S3, define the schema, and start querying using the built-in query editor. This is ideal for running ad-hoc queries on access logs stored in an S3 bucket.</p><p><strong>CORRECT: </strong>\"Create an Amazon Athena table that uses the S3 bucket containing the access logs. Run SQL queries using Athena\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the S3 copy command to copy logs to a separate bucket. Enable S3 analytics to analyze access patterns\" is incorrect.</p><p>There\u2019s no need to copy the data and S3 analytics is used to identify object access patterns for requests to S3 objects. It is used for storage class analytics. It does not help with identifying access patterns for your application by reading the file and looking at source IP addresses (for example).</p><p><strong>INCORRECT:</strong> \"Write an AWS Lambda function to query the access logs. Use event notifications to trigger the Lambda functions when log entries are added\" is incorrect.</p><p>This will be more complex and is less useful for running ad hoc queries as it is something that will run every time a file is added.</p><p><strong>INCORRECT:</strong> \"Import the access logs into Amazon CloudWatch Logs. Use CloudWatch Logs Insights to analyze the log data\" is incorrect.</p><p>You cannot natively import logs into CloudWatch Logs from Amazon S3. You may be able to achieve this with a custom Lambda function, but it will be more work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p><p><a href=\"https://aws.amazon.com/athena/features/\">https://aws.amazon.com/athena/features/</a></p>", "answers": ["<p>Create an Amazon Athena table that uses the S3 bucket containing the access logs. Run SQL queries using Athena.</p>", "<p>Write an AWS Lambda function to query the access logs. Use event notifications to trigger the Lambda functions when log entries are added.</p>", "<p>Import the access logs into Amazon CloudWatch Logs. Use CloudWatch Logs Insights to analyze the log data.</p>", "<p>Use the S3 copy command to copy logs to a separate bucket. Enable S3 analytics to analyze access patterns.</p>"]}, "correct_response": ["a"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company manages an application that runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The NLB has access logs enabled which are being stored in an Amazon S3 bucket. A security engineer requires a solution to run ad hoc queries against the access logs to identify application access patterns.How should the security engineer accomplish this task with the least amount of administrative overhead?", "related_lectures": []}, {"_class": "assessment", "id": 63655222, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company created an AWS KMS key in the AWS Key Management Service (KMS) with imported key materials. The company policy requires that all encryption keys must be rotated every 365 days.</p><p>What must be done to implement policy requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you enable automatic key rotation for a customer managed key, AWS KMS generates new cryptographic material for the KMS key every year. However, in some cases it is necessary to create a new KMS key and use it in place of the original KMS key. This has the same effect as rotating the key material in an existing KMS key, so it's often thought of as manually rotating the key.</p><p>Manual rotation is a good choice when you want to control the key rotation schedule. It also provides a way to rotate KMS keys that are not eligible for automatic key rotation, including asymmetric KMS keys, KMS keys in custom key stores, and KMS keys with imported key material.</p><p>In this case manual rotation is required as the key material has been imported so automatic rotation is not possible. Because the new KMS key is a different resource from the current KMS key, it has a different key ID and ARN. When you change KMS keys, you need to update references to the KMS key ID or ARN in your applications. Aliases, which associate a friendly name with a KMS key, make this process easier.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-32-18-7ca3c820d28a7ceed87726316e80728f.jpg\"><p><strong>CORRECT: </strong>\"Create a new KMS key, import new key material, and point the key alias to the new KMS key \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable automatic key rotation for the KMS key every 365 days\" is incorrect.</p><p>This would not be possible as key material was imported so automatic rotation is not allowed.</p><p><strong>INCORRECT:</strong> \"Write an AWS Lambda function that rotates the key material yearly\" is incorrect.</p><p>Though you can use Lambda to automate this kind of activity a new key is needed.</p><p><strong>INCORRECT:</strong> \"Import new key material to the existing KMS key and manually rotate the KMS key\" is incorrect.</p><p>The process requires that a new key is created with new key material as described above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html</a></p>", "answers": ["<p>Write an AWS Lambda function that rotates the key material yearly.</p>", "<p>Import new key material to the existing KMS key and manually rotate the KMS key.</p>", "<p>Create a new KMS key, import new key material, and point the key alias to the new KMS key.</p>", "<p>Enable automatic key rotation for the KMS key every 365 days.</p>"]}, "correct_response": ["c"], "section": "Domain 5 - Data Protection", "question_plain": "A company created an AWS KMS key in the AWS Key Management Service (KMS) with imported key materials. The company policy requires that all encryption keys must be rotated every 365 days.What must be done to implement policy requirements?", "related_lectures": []}, {"_class": "assessment", "id": 63655224, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has created an AWS account structure with a centralized management account and several child accounts. An AWS Organization has been created to manage this configuration. The security team require API auditing using AWS CloudTrail for all accounts. Administrators in child accounts should not have privileges to modify the CloudTrail trail configuration.</p><p>How should AWS CloudTrail be configured with the LEAST operational overhead?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>If you have created an organization in AWS Organizations, you can create a trail that will log all events for all AWS accounts in that organization. This is sometimes referred to as an <em>organization trail</em>.</p><p>When you create an organization trail, a trail with the name that you give it will be created in every AWS account that belongs to your organization. Users with CloudTrail permissions in member accounts will be able to see this trail when they log into the AWS CloudTrail console from their AWS accounts, or when they run AWS CLI commands such as describe-trail.</p><p>However, users in member accounts will not have sufficient permissions to delete the organization trail, turn logging on or off, change what types of events are logged, or otherwise alter the organization trail in any way.</p><p>The diagram below depicts an AWS organization with multiple accounts and an organization trail:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-07_17-34-25-aca2e0db4c9f9141e563c12dfa4cb7dc.jpg\"><p><strong>CORRECT: </strong>\"Create an Amazon S3 bucket in the management account and create an Organization trail in the management account that logs to the S3 bucket\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon S3 bucket in each AWS account and create an Organization trail in the management account that logs to the S3 bucket\" is incorrect.</p><p>The organization trail should log to a central S3 bucket, not a bucket in each AWS account. This is a more secure configuration and is required.</p><p><strong>INCORRECT:</strong> \"Create a CloudTrail trail in each AWS account that logs to separate folders within a central S3 bucket in the management account. Create an SCP to limit permissions\" is incorrect.</p><p>When using AWS organizations it will be much easier to create an organization trail which will achieve the same result with lower overhead and permissions are automatically configured.</p><p><strong>INCORRECT:</strong> \"Create a separate logging account and create a CloudTrail trail within each AWS account that logs to the logging account. Create an SCP to limit permissions\" is incorrect.</p><p>This would be more complex than using an organization trail so it not the best answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>", "answers": ["<p>Create a separate logging account and create a CloudTrail trail within each AWS account that logs to the logging account. Create an SCP to limit permissions.</p>", "<p>Create an Amazon S3 bucket in each AWS account and create an Organization trail in the management account that logs to the S3 bucket.</p>", "<p>Create an Amazon S3 bucket in the management account and create an Organization trail in the management account that logs to the S3 bucket.</p>", "<p>Create a CloudTrail trail in each AWS account that logs to separate folders within a central S3 bucket in the management account. Create an SCP to limit permissions.</p>"]}, "correct_response": ["c"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company has created an AWS account structure with a centralized management account and several child accounts. An AWS Organization has been created to manage this configuration. The security team require API auditing using AWS CloudTrail for all accounts. Administrators in child accounts should not have privileges to modify the CloudTrail trail configuration.How should AWS CloudTrail be configured with the LEAST operational overhead?", "related_lectures": []}, {"_class": "assessment", "id": 63655226, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application running on Amazon EC2 instances generates log files in a folder on the Linux file system. The security team requires that the logs are collected and centrally stored using an AWS managed service. Automatic monitoring should be possible, and an interface must be available for analyzing the log files.</p><p>Which approach meets the stated requirements with the minimum effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The unified CloudWatch agent enables you to do the following:</p><p>\u00b7 Collect internal system-level metrics from Amazon EC2 instances across operating systems.</p><p>\u00b7 Collect system-level metrics from on-premises server.</p><p>\u00b7 Retrieve custom metrics from your applications or services using the StatsD and collectd protocols.</p><p>\u00b7 Collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p><p>You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. Application log files can be collected.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs, just like logs collected by the older CloudWatch Logs agent.</p><p><strong>CORRECT: </strong>\"Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to collect the application log files and send them to Amazon CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a script that sends a custom metric to Amazon CloudWatch that includes the data recorded in the application log files\" is incorrect.</p><p>You cannot create metrics from log files. Metrics are time series data recording information about your system. For example a metric shows the percentage of CPU used. Log files can include all sorts of data in text, json or other formats which cannot be represented on a graph like a metric.</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager Agent on the instances. Run a PowerShell script using Run Command that copies the log files to Amazon CloudWatch Logs\" is incorrect.</p><p>Run Command is used for automating individual tasks, automation documents should be used for automation. Creating PowerShell scripts to do this work is incorrect as the operating system is Linux, not Windows, and this would be more work than using the unified CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Create a cron job that runs on a regular schedule. Configure the cron job to copy the application log files to an Amazon S3 bucket and use Amazon Athena for analysis\" is incorrect.</p><p>Writing scripts is more work than using the unified CloudWatch Agent and Amazon Athena does not provide the automation required.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>", "answers": ["<p>Create a script that sends a custom metric to Amazon CloudWatch that includes the data recorded in the application log files.</p>", "<p>Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to collect the application log files and send them to Amazon CloudWatch Logs.</p>", "<p>Install the AWS Systems Manager Agent on the instances. Run a PowerShell script using Run Command that copies the log files to Amazon CloudWatch Logs.</p>", "<p>Create a cron job that runs on a regular schedule. Configure the cron job to copy the application log files to an Amazon S3 bucket and use Amazon Athena for analysis.</p>"]}, "correct_response": ["b"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "An application running on Amazon EC2 instances generates log files in a folder on the Linux file system. The security team requires that the logs are collected and centrally stored using an AWS managed service. Automatic monitoring should be possible, and an interface must be available for analyzing the log files.Which approach meets the stated requirements with the minimum effort?", "related_lectures": []}, {"_class": "assessment", "id": 63655228, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company stores highly confidential information in an Amazon S3 bucket. The security team requires that any changes to the bucket policy are automatically remediated, and alerts of these changes are sent to their team members.</p><p>Which actions should a security engineer take to meet these requirements with the LEAST effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention.</p><p>An AWS Config rule can be applied to identify and remediate any unauthorized changes to the policy associated with the S3 bucket. Amazon SNS can be integrated as a destination for alerts.</p><p><strong>CORRECT: </strong>\"Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts\" is incorrect.</p><p>Macie is not used for identifying changes to S3 bucket policies or for alerting. Macie is used for identifying personally identifiable information in data sets.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect.</p><p>EventBridge can alert on API events relating to bucket changes. However, this would require creating a custom function and is therefore more effort compared to the correct answer.</p><p><strong>INCORRECT:</strong> \"Use an Amazon CloudWatch alarm with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS\" is incorrect.</p><p>CloudWatch alarms cannot be configured to trigger based on changes to S3 buckets</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>", "answers": ["<p>Use an Amazon CloudWatch alarm with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>", "<p>Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts.</p>", "<p>Use Amazon EventBridge rules with AWS Lambda to automatically remediate S3 bucket policy changes. Configure alerting with Amazon SNS.</p>", "<p>Use AWS Config with Auto Remediation to remediate any changes to S3 bucket policies. Configure alerting with AWS Config and Amazon SNS.</p>"]}, "correct_response": ["d"], "section": "Domain 2 - Logging and Monitoring", "question_plain": "A company stores highly confidential information in an Amazon S3 bucket. The security team requires that any changes to the bucket policy are automatically remediated, and alerts of these changes are sent to their team members.Which actions should a security engineer take to meet these requirements with the LEAST effort?", "related_lectures": []}]}
