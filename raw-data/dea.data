5596958
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 53428392, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You were asked to create a table that can store the below data, orderTime is a timestamp but the finance team when they query this data normally prefer the orderTime in date format, you would like to create a calculated column that can convert the orderTime column timestamp datatype to date and store it, fill in the blank to complete the DDL.</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-10-11_15-07-58-3b1dfd42c7b67dd9f4fcc3dd3483f64f.jpg"><p><br></p><pre class="prettyprint linenums">CREATE TABLE orders (\n    orderId int,\n    orderTime timestamp,\n    orderdate date _____________________________________________ ,\n    units int)</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, <code>GENERATED ALWAYS AS (CAST(orderTime as DATE))</code></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/delta/delta-batch#--use-generated-columns">https://docs.microsoft.com/en-us/azure/databricks/delta/delta-batch#--use-generated-columns</a></p><p>Delta Lake supports generated columns which are a special type of columns whose values are automatically generated based on a user-specified function over other columns in the Delta table. When you write to a table with generated columns and you do not explicitly provide values for them, Delta Lake automatically computes the values.</p><p><strong>Note: Databricks also supports partitioning using generated column</strong></p>', 'answers': ['<p><code> AS DEFAULT (CAST(orderTime as DATE))</code> </p>', '<p><code>GENERATED ALWAYS AS (CAST(orderTime as DATE))</code> </p>', '<p><code>GENERATED DEFAULT AS (CAST(orderTime as DATE))</code> </p>', '<p><code>AS (CAST(orderTime as DATE))</code> </p>', '<p>Delta lake does not support calculated columns, value should be inserted into the table as part of the ingestion process</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You were asked to create a table that can store the below data, orderTime is a timestamp but the finance team when they query this data normally prefer the orderTime in date format, you would like to create a calculated column that can convert the orderTime column timestamp datatype to date and store it, fill in the blank to complete the DDL.CREATE TABLE orders (\n    orderId int,\n    orderTime timestamp,\n    orderdate date _____________________________________________ ,\n    units int)', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428428, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The data engineering team noticed that one of the job fails randomly as a result of using spot instances, what feature in Jobs/Tasks can be used to address this issue so the job is more stable when using spot instances?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; Add a retry policy to the task</p><p><br></p><p>Tasks in Jobs support Retry Policy, which can be used to retry a failed tasks,&nbsp; especially when using spot instance it is common to have failed executors or driver. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_20-20-23-51ce859c2ef0631a12da01bad7da986e.jpg">', 'answers': ['<p>Use Databrick REST API to monitor and restart the job</p>', '<p>Use Jobs runs, active runs UI section to monitor and restart the job</p>', '<p>Add second task and add a check condition to rerun the first task if it fails</p>', '<p>Restart the job cluster, job automatically restarts</p>', '<p>Add a retry policy to the task</p>']}, 'correct_response': ['e'], 'section': 'Production Pipelines', 'question_plain': 'The data engineering team noticed that one of the job fails randomly as a result of using spot instances, what feature in Jobs/Tasks can be used to address this issue so the job is more stable when using spot instances?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428412, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> What is the main difference between AUTO&nbsp;LOADER&nbsp; and COPY&nbsp;INTO?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Auto loader supports both directory listing and file notification but COPY&nbsp;INTO only supports directory listing. </p><p><br></p><p>Auto loader file notification will automatically set up a notification service and queue service that subscribe to file events from the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant and scalable for large input directories or a high volume of files.</p><p><br></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-14_17-29-05-eb36006e7812ce0245401ad5908f66af.jpeg"></p><p>Auto Loader and Cloud Storage Integration</p><p><br></p><p>Auto Loader supports a couple of ways to ingest data incrementally</p><p><br></p><ol><li><p>Directory listing - List Directory and maintain the state in RocksDB, supports incremental file listing </p></li><li><p>File notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike Directory listing File notification can scale up to millions of files per day. </p></li></ol><p><br></p><p><br></p><p><strong>[OPTIONAL] </strong></p><p><strong>Auto Loader vs COPY&nbsp;INTO?</strong></p><p><br></p><p><strong>Auto Loader</strong></p><p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup. Auto Loader provides a new Structured Streaming source called <code>cloudFiles</code>. Given an input directory path on the cloud file storage, the <code>cloudFiles</code> source automatically processes new files as they arrive, with the option of also processing existing files in that directory.</p><p>When to use Auto Loader instead of the COPY&nbsp;INTO?</p><p><br></p><ul><li><p>You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover files more efficiently than the <code>COPY INTO</code> SQL command and can split file processing into multiple batches.</p></li><li><p>You do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess subsets of files. However, you can use the <code>COPY INTO</code> SQL command to reload subsets of files while an Auto Loader stream is simultaneously running.</p></li></ul><p><br></p><p><br></p><p>Auto loader file notification will automatically set up a notification service and queue service that subscribe to file events from the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant and scalable for large input directories or a high volume of files.</p><p><br></p><p>Here are some additional notes on when to use COPY&nbsp;INTO&nbsp;vs Auto Loader </p><p><br></p><p>When to use COPY INTO</p><p><a href="https://docs.databricks.com/delta/delta-ingest.html#copy-into-sql-command">https://docs.databricks.com/delta/delta-ingest.html#copy-into-sql-command</a></p><p>When to use Auto Loader</p><p><a href="https://docs.databricks.com/delta/delta-ingest.html#auto-loader">https://docs.databricks.com/delta/delta-ingest.html#auto-loader</a></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>COPY INTO supports schema evolution.</p>', '<p>AUTO LOADER supports schema evolution.</p>', '<p>COPY INTO supports file notification when performing incremental loads. </p>', '<p>AUTO LOADER supports reading data from Apache Kafka</p>', '<p>AUTO&nbsp;LOADER&nbsp;Supports file notification when performing incremental loads. </p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the main difference between AUTO&nbsp;LOADER&nbsp; and COPY&nbsp;INTO?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428414, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Why does AUTO LOADER require schema location?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Schema location is used to store schema inferred by AUTO LOADER, so the next time AUTO&nbsp;LOADER runs faster as does not need to infer the schema every single time by trying to use the last known schema. </p><p><br></p><p>Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. To avoid incurring this inference cost at every stream start up, and to be able to provide a stable schema across stream restarts, you must set the option <code>cloudFiles.schemaLocation</code>. Auto Loader creates a hidden directory <code>_schemas</code> at this location to track schema changes to the input data over time.</p><p><br></p><p>The below link contains detailed documentation on different options</p><p><br></p><p><a href="https://docs.databricks.com/ingestion/auto-loader/options.html">Auto Loader options | Databricks on AWS</a></p>', 'answers': ['<p>Schema location is used to store user provided schema</p><p><br></p>', '<p>Schema location is used to identify the schema of target table</p>', '<p>AUTO LOADER does not require schema location, because its supports Schema evolution</p>', '<p>Schema location is used to store schema inferred by AUTO LOADER</p>', '<p>Schema location is used to identify the schema of target table and source table</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data Processing', 'question_plain': 'Why does AUTO LOADER require schema location?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428360, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statements are incorrect about the lakehouse</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; Storage is coupled with Compute. </p><p><br></p><p>The question was asking what is the incorrect option, in Lakehouse Storage is decoupled with compute so both can scale independently. </p><p><br></p><p><a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What Is a Lakehouse? - The Databricks Blog</a></p><p><br></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_04-36-01-03449cc0981213275d4bfc1d06844435.jpg"></p>', 'answers': ['<p>Support end-to-end streaming and batch workloads</p>', '<p>Supports ACID</p>', '<p>Support for diverse data types that can store both structured and unstructured</p>', '<p>Supports BI and Machine learning</p>', '<p>Storage is coupled with Compute</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following statements are incorrect about the lakehouse', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428362, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are designing a data model that works for both machine learning using images and Batch ETL/ELT workloads. Which of the following features of data lakehouse can help you meet the needs of both workloads?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is A data lakehouse stores unstructured data and is ACID-compliant, </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_04-39-58-269bb910f048553b611ffbdfdf47d0a5.jpg">', 'answers': ['<p>Data lakehouse requires very little data modeling.</p>', '<p>Data lakehouse combines compute and storage for simple governance.</p>', '<p>Data lakehouse provides autoscaling for compute clusters.</p>', '<p>Data lakehouse can store unstructured data and support ACID transactions. </p>', '<p>Data lakehouse fully exists in the cloud.</p>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are designing a data model that works for both machine learning using images and Batch ETL/ELT workloads. Which of the following features of data lakehouse can help you meet the needs of both workloads?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428364, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following locations in Databricks product architecture hosts jobs/pipelines and queries?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Control Plane, </p><p><br></p><p>Databricks operates most of its services out of a control plane and a data plane, <em>please note serverless features like SQL&nbsp;Endpoint and DLT&nbsp;compute use shared compute in Control pane. </em></p><p><br></p><p><strong>Control Plane:&nbsp;</strong>Stored in Databricks Cloud Account</p><p><br></p><ul><li><p>The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.</p></li></ul><p><strong>Data Plane:</strong>&nbsp; Stored in Customer Cloud Account</p><p><br></p><ul><li><p>The data plane is managed by your Azure account and is where your data resides. This is also where data is processed. You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure account to ingest data or for storage. </p></li></ul><p><br></p><p>Here is the product architecture diagram highlighted where </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-25_03-48-39-5703b4db7bd84e81f9c8ec4f92de9559.jpg">', 'answers': ['<p>Data plane</p>', '<p>Control plane</p>', '<p>Databricks Filesystem</p>', '<p>JDBC data source</p>', '<p>Databricks web application</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following locations in Databricks product architecture hosts jobs/pipelines and queries?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428366, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working on a notebook that will populate a reporting table for downstream process consumption, this process needs to run on a schedule every hour. what type of cluster are you going to use to set up this job?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': "<p>The answer is, The Job cluster is best suited for this purpose. </p><p><br></p><p>Since you don't need to interact with the notebook during the execution especially when it's a scheduled job, job cluster makes sense. Using an all-purpose cluster can be twice as expensive as a job cluster. </p><p>FYI, </p><p>When you run a job scheduler with option of creating a new cluster when the job is complete it terminates the cluster. You cannot restart a job cluster. </p>", 'answers': ['<p> Since it’s just a single job and we need to run every hour, we can use an all-purpose cluster</p>', '<p>The job cluster is best suited for this purpose.</p>', '<p>Use Azure VM to read and write delta tables in Python</p>', '<p>Use delta live table pipeline to run in continuous mode</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are currently working on a notebook that will populate a reporting table for downstream process consumption, this process needs to run on a schedule every hour. what type of cluster are you going to use to set up this job?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428368, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><p>See the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow.</p><p>All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git provider like Github or Azure DevOps</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_16-25-30-225d78a00099d488fdf2be825675f5ad.jpg"><p><br></p><p><br></p>', 'answers': ['<p>Merge when code is committed</p>', '<p>Pull request and review process</p>', '<p>Trigger Databricks Repos API to pull the latest version of code into production folder</p>', '<p>Resolve merge conflicts </p>', '<p>Delete a branch</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428370, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working with the second team and both teams are looking to modify the same notebook, you noticed that the second member is copying the notebooks to the personal folder to edit and replace the collaboration notebook, which notebook feature do you recommend to make the process easier to collaborate.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is Databricks Notebooks support real-time coauthoring on a single notebook</p><p><br></p><p>Every change is saved, and a notebook can be changed my multiple users. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_16-28-45-579fe4e416e0f7028b91f45a72d1e821.jpg">', 'answers': ['<p>Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks</p>', '<p>Databricks notebooks support automatic change tracking and versioning</p>', '<p>Databricks Notebooks support real-time coauthoring on a single notebook</p>', '<p>Databricks notebooks can be exported into dbc archive files and stored in data lake</p>', '<p>Databricks notebook can be exported as HTML and imported at a later time</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are currently working with the second team and both teams are looking to modify the same notebook, you noticed that the second member is copying the notebooks to the personal folder to edit and replace the collaboration notebook, which notebook feature do you recommend to make the process easier to collaborate.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428372, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working on a project that requires the use of SQL and Python in a given notebook, what would be your approach</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is, A single notebook can support multiple languages, use the magic command to switch between the two.</p><p><br></p><p>Use %sql and %python magic commands within the same notebook. </p><p><br></p>', 'answers': ['<p>Create two separate notebooks, one for SQL and the second for Python</p>', '<p>A single notebook can support multiple languages, use the magic command to switch between the two.</p>', '<p>Use an All-purpose cluster for python, SQL endpoint for SQL</p>', '<p>Use job cluster to run python and SQL Endpoint for SQL</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are currently working on a project that requires the use of SQL and Python in a given notebook, what would be your approach', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428374, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statements are correct on how Delta Lake implements a lake house?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><p>Delta lake is</p><p>· Open source</p><p>· Builds up on standard data format</p><p>· Optimized for cloud object storage</p><p>· Built for scalable metadata handling</p><p>Delta lake is not</p><p>· Proprietary technology</p><p>· Storage format</p><p>· Storage medium</p><p>· Database service or data warehouse</p>', 'answers': ['<p>Delta lake uses a proprietary format to write data, optimized for cloud storage</p>', '<p>Using Apache Hadoop on cloud object storage</p>', '<p>Delta lake always stores meta data in memory vs storage</p>', '<p>Delta lake uses open source, open format, optimized cloud storage and scalable meta data</p>', '<p>Delta lake stores data and meta data in computes memory</p>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following statements are correct on how Delta Lake implements a lake house?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428376, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You were asked to create or overwrite an existing delta table to store the below transaction data.</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-06_16-41-20-9e89d31650240d1b2e8d70a6bcae78b0.jpg"></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is </p><pre class="prettyprint linenums">CREATE OR REPLACE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre><p><br></p><p>When creating a table in Databricks by default the table is stored in DELTA&nbsp;format. </p>', 'answers': ['<pre class="prettyprint linenums">CREATE OR REPLACE DELTA TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE TABLE IF EXISTS transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)\nFORMAT DELTA</pre>', '<pre class="prettyprint linenums">CREATE IF EXSITS REPLACE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You were asked to create or overwrite an existing delta table to store the below transaction data.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428378, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>if you run the command <code>VACUUM transactions retain 0 hours</code>? What is the outcome of this command?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p>Command will fail, you cannot run the command with retentionDurationcheck enabled. </p><p><br></p><pre class="prettyprint linenums">VACUUM [ [db_name.]table_name | path] [RETAIN num HOURS] [DRY RUN]\n</pre><ul><li><p>Recursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Default is 7 Days. </p></li></ul><ul><li><p>The reason this check is enabled is because, DELTA is trying to prevent unintentional deletion of history, and also one important thing to point out is with 0 hours of retention there is a possibility of data loss(see below kb)</p></li></ul><p>Documentation in VACUUM https://docs.delta.io/latest/delta-utility.html</p><p>https://kb.databricks.com/delta/data-missing-vacuum-parallel-write.html</p><p><br></p><p><br></p>', 'answers': ['<p>Command will be successful, but no data is removed</p>', '<p>Command will fail if you have an active transaction running</p>', '<p>Command will fail, you cannot run the command with retentionDurationcheck enabled</p>', '<p>Command will be successful, but historical data will be removed</p>', '<p>Command runs successful and compacts all of the data in the table</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'if you run the command VACUUM transactions retain 0 hours? What is the outcome of this command?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428380, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You noticed a colleague is manually copying the data to the backup folder prior to running an update command, incase if the update command did not provide the expected outcome so he can use the backup copy to replace table, which Delta Lake feature would you recommend simplifying the process?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Use time travel feature to refer old data instead of manually copying. </p><p><br></p><p><a href="https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html">https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html</a></p><p><br></p><pre class="prettyprint linenums">SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01"\nSELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)\nSELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01 01:30:00.000"</pre><p><br></p>', 'answers': ['<p>Use time travel feature to refer old data instead of manually copying</p>', '<p>Use DEEP CLONE to clone the table prior to update to make a backup copy</p>', '<p>Use SHADOW copy of the table as preferred backup choice</p>', '<p>Cloud object storage retains previous version of the file</p>', '<p>Cloud object storage automatically backups the data</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You noticed a colleague is manually copying the data to the backup folder prior to running an update command, incase if the update command did not provide the expected outcome so he can use the backup copy to replace table, which Delta Lake feature would you recommend simplifying the process?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428382, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which one of the following is not a Databricks lakehouse object?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>The answer is, Stored Procedures.</p><p>Databricks lakehouse does not support stored procedures. </p><p><br></p>', 'answers': ['<p>Tables</p>', '<p>Views</p>', '<p>Database/Schemas</p>', '<p>Catalog</p>', '<p>Functions</p>', '<p>Stored Procedures</p>']}, 'correct_response': ['f'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which one of the following is not a Databricks lakehouse object?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428384, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What type of table is created when you create delta table with below command?</p><p><code>CREATE TABLE transactions USING DELTA LOCATION "DBFS:/mnt/bronze/transactions"</code> </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Anytime a table is created using the <code>LOCATION</code> keyword it is considered an external table, below is the current syntax.</p><p>Syntax</p><p><code>CREATE TABLE table_name ( column column_data_type…) USING format LOCATION "dbfs:/"</code> </p><p>format -&gt; DELTA, JSON, CSV, PARQUET, TEXT</p><p><br></p><p><br></p><p><strong>I created the table command based on the above question, you can see it created an external table, </strong></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-10_21-31-36-420706296fe2450135d54ed1da44c5a3.jpg"></p><p><br></p><p><br></p><p><strong>Let\'s remove the location keyword and run again, same syntax except for the LOCATION&nbsp;keyword is removed. </strong></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-10_21-31-36-f4fb906874a704ff1f4473735706cd2a.jpg"></p><p><br></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>Managed delta table</p>', '<p>External table</p>', '<p>Managed table</p>', '<p>Temp table</p>', '<p> Delta Lake table</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What type of table is created when you create delta table with below command?CREATE TABLE transactions USING DELTA LOCATION "DBFS:/mnt/bronze/transactions"', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428386, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following command can be used to drop a managed delta table and the underlying files in the storage?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is DROP TABLE table_name, </p><p><br></p><p>When a managed table is dropped, the table definition is dropped from metastore and everything including data, metadata, and history are also dropped from storage. </p><p><br></p>', 'answers': ['<p><code>DROP TABLE table_name CASCADE</code> </p>', '<p><code>DROP TABLE table_name</code> </p>', '<p>Use <code>DROP TABLE table_name</code> command and manually delete files using command <code>dbutils.fs.rm("/path",True)</code> </p>', '<p><code>DROP TABLE table_name INCLUDE_FILES</code> </p>', '<p><code>DROP TABLE table</code> and run <code>VACUUM</code> command</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following command can be used to drop a managed delta table and the underlying files in the storage?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428388, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following is the correct statement for a session scoped temporary view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Temporary views are lost once the notebook is detached and attached</p><p><br></p><p>There are two types of temporary views that can be created, Session scoped and Global</p><p><br></p><ul><li><p>A local/session scoped temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if a notebook is detached and reattached local temporary view is lost.</p></li></ul><ul><li><p>A global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost.</p></li></ul>', 'answers': ['<p>Temporary views are lost once the notebook is detached and re-attached</p>', '<p>Temporary views stored in memory</p>', '<p>Temporary views can be still accessed even if the notebook is detached and attached</p>', '<p>Temporary views can be still accessed even if cluster is restarted</p>', '<p>Temporary views are created in local_temp database</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following is the correct statement for a session scoped temporary view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428390, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following is correct for the global temporary view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is global temporary views can be still accessed even if the notebook is detached and attached</p><p><br></p><p>There are two types of temporary views that can be created Local and Global</p><p>· A local temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if a notebook is detached and reattached local temporary view is lost.</p><p>· A global temporary view is available to all the notebooks in the cluster, even if the notebook is detached and reattached it can still be accessible but if a cluster is restarted the global temporary view is lost.</p>', 'answers': ['<p>global temporary views <strong>cannot </strong>be accessed once the notebook is detached and attached</p>', '<p>global temporary views can be accessed across many clusters</p>', '<p>global temporary views <strong>can be</strong> still accessed even if the notebook is detached and attached</p>', '<p>global temporary views can be still accessed even if the cluster is restarted</p>', '<p>global temporary views are created in a database called <code>temp </code>database</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following is correct for the global temporary view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428394, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working on reloading customer_sales tables using the below query </p><p><br></p><pre class="prettyprint linenums">INSERT&nbsp;OVERWRITE&nbsp;customer_sales\nSELECT&nbsp;* FROM&nbsp;customers c\nINNER&nbsp;JOIN sales_monthly s on s.customer_id = c.customer_id</pre><p><br></p><p>After you ran the above command, the Marketing team quickly wanted to review the old data that was in the table. How does INSERT&nbsp;OVERWRITE&nbsp;impact the data in the <code>customer_sales</code> table if you want to see the previous version of the data prior to running the above statement?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; <code>INSERT&nbsp;OVERWRITE</code>&nbsp; Overwrites the current version of the data but preserves all historical versions of the data, you can time travel to previous versions. </p><p><br></p><pre class="prettyprint linenums">INSERT&nbsp;OVERWRITE&nbsp;customer_sales\nSELECT&nbsp;* FROM&nbsp;customers c\nINNER&nbsp;JOIN sales s on s.customer_id = c.customer_id</pre><p><br></p><p>Let\'s just assume that this is the second time you are running the above statement, you can still query the prior version of the data using time travel, and any DML/DDL&nbsp;except DROP&nbsp;TABLE creates new PARQUET&nbsp;files so you can still access the previous versions of data. </p><p><br></p><p>SQL&nbsp;Syntax for Time travel </p><p><code>SELECT&nbsp;*&nbsp;FROM&nbsp;table_name as of [version number]</code></p><p>with customer_sales example</p><p><code>SELECT&nbsp;*&nbsp;FROM&nbsp;customer_sales as of 1 -- previous version</code></p><p><code>SELECT&nbsp;*&nbsp;FROM&nbsp;customer_sales as of 2 -- current version</code></p><p><br></p><p>You see all historical changes on the table using DESCRIBE&nbsp;HISTORY&nbsp;table_name</p><p><br></p><p>Note: the main difference between <code>INSERT&nbsp;OVERWRITE </code> and <code>CREATE&nbsp;OR&nbsp;REPLACE&nbsp;TABLE(CRAS)</code>&nbsp;is that&nbsp; CRAS can modify the schema of the table, i.e it can add new columns or change data types of existing columns.&nbsp; By default&nbsp; <code>INSERT&nbsp;OVERWRITE</code>&nbsp; only overwrites the data. </p><p><br></p><p><code>INSERT&nbsp;OVERWRITE</code>&nbsp; can also be used to update the schema when <code>spark.databricks.delta.schema.autoMerge.enabled</code> is set <code>true </code>if this option is not enabled and if there is a schema mismatch command&nbsp; <code>INSERT&nbsp;OVERWRITE</code>will fail. </p><p><br></p><p>Any DML/DDL&nbsp;operation(except DROP TABLE) on the Delta table preserves the historical version of the data. </p><p><br></p>', 'answers': ['<p>Overwrites the data in the table, all historical versions of the data, you can not time travel to previous versions</p>', '<p>Overwrites the data in the table but preserves all historical versions of the data, you can time travel to previous versions</p>', '<p>Overwrites the current version of the data but clears all historical versions of the data, so you can not time travel to previous versions. </p>', '<p>Appends the data to the current version, you can time travel to previous versions</p>', '<p>By default, overwrites the data and schema, you cannot perform time travel</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are currently working on reloading customer_sales tables using the below query INSERT&nbsp;OVERWRITE&nbsp;customer_sales\nSELECT&nbsp;* FROM&nbsp;customers c\nINNER&nbsp;JOIN sales_monthly s on s.customer_id = c.customer_idAfter you ran the above command, the Marketing team quickly wanted to review the old data that was in the table. How does INSERT&nbsp;OVERWRITE&nbsp;impact the data in the customer_sales table if you want to see the previous version of the data prior to running the above statement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428396, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL statement can be used to query a table by eliminating duplicate rows from the query results?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The&nbsp; answer is&nbsp; SELECT DISTINCT * FROM table_name</p>', 'answers': ['<p><code>SELECT DISTINCT * FROM table_name</code> </p>', '<p><code>SELECT DISTINCT * FROM table_name HAVING COUNT(*) &gt; 1</code> </p>', '<p><code>SELECT DISTINCT_ROWS (*) FROM table_name</code> </p>', '<p><code>SELECT * FROM table_name GROUP BY * HAVING COUNT(*) &lt; 1</code> </p>', '<p><code>SELECT * FROM table_name GROUP BY * HAVING COUNT(*) &gt; 1</code> </p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following SQL statement can be used to query a table by eliminating duplicate rows from the query results?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428398, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>Which of the below SQL Statements can be used to create a SQL UDF to convert Celsius to Fahrenheit and vice versa, you need to pass two parameters to this function one, actual temperature, and the second that identifies if its needs to be converted to Fahrenheit or Celcius with a one-word letter F or C?</p><p><code>select udf_convert(60,'C')</code>&nbsp; will result in 15.5 </p><p><code>select udf_convert(10,'F')&nbsp; </code>will result in 50 </p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><p><br></p><pre class="prettyprint linenums">CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)\nRETURNS DOUBLE\nRETURN CASE WHEN measure == ‘F’ then (temp * 9/5) + 32\n        ELSE (temp – 33 ) * 5/9\n       END\n</pre><p><br></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums"> CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING)\n    RETURNS DOUBLE\n    RETURN CASE WHEN measure == \'F\' then (temp * 9/5) + 32\n            ELSE (temp – 33 ) * 5/9\n           END</pre>', '<pre class="prettyprint linenums">CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING)\n    RETURN CASE WHEN measure == \'F\' then (temp * 9/5) + 32\n            ELSE (temp – 33 ) * 5/9\n           END</pre>', '<pre class="prettyprint linenums">CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)\nRETURN CASE WHEN measure == \'F\' then (temp * 9/5) + 32\n          ELSE (temp – 33 ) * 5/9\n       END</pre>', '<pre class="prettyprint linenums">CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)\nRETURNS DOUBLE\nRETURN CASE WHEN measure == \'F\' then (temp * 9/5) + 32\n        ELSE (temp – 33 ) * 5/9\n       END</pre>', '<pre class="prettyprint linenums">CREATE USER FUNCTION udf_convert(temp DOUBLE, measure STRING)\nRETURNS DOUBLE\nRETURN CASE WHEN measure == \'F\' then (temp * 9/5) + 32\n          ELSE (temp – 33 ) * 5/9 \n\tEND\n</pre>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': "Which of the below SQL Statements can be used to create a SQL UDF to convert Celsius to Fahrenheit and vice versa, you need to pass two parameters to this function one, actual temperature, and the second that identifies if its needs to be converted to Fahrenheit or Celcius with a one-word letter F or C?select udf_convert(60,'C')&nbsp; will result in 15.5 select udf_convert(10,'F')&nbsp; will result in 50", 'related_lectures': []}, {'_class': 'assessment', 'id': 53428400, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are trying to calculate total sales made by all the employees by parsing a complex struct data type that stores employee and sales data, how would you approach this in SQL</p><p><br></p><p>Table definition, </p><p><code>batchId INT, performance ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;, insertDate TIMESTAMP</code></p><p><br></p><p>Sample data of <code>performance </code>column</p><p><br></p><pre class="prettyprint linenums">[\n{ "employeeId":1234\n"sales" : 10000},\n\n{ "employeeId":3232\n"sales" : 30000}\n]</pre><p><br></p><p>Calculate total sales made by all the employees?</p><p><br></p><p>Sample data with create table syntax for the data:&nbsp;</p><p><br></p><pre class="prettyprint linenums">create or replace table sales as \nselect 1 as batchId ,\n\tfrom_json(\'[{ "employeeId":1234,"sales" : 10000 },{ "employeeId":3232,"sales" : 30000 }]\',\n         \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n  current_timestamp() as insertDate\nunion all \nselect 2 as batchId ,\n  from_json(\'[{ "employeeId":1235,"sales" : 10500 },{ "employeeId":3233,"sales" : 32000 }]\',\n                \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n                current_timestamp() as insertDate</pre><p> \t&nbsp; &nbsp; &nbsp; &nbsp; </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><pre class="prettyprint linenums">select aggregate(flatten(collect_list(performance.sales)), 0, (x, y) -&gt; x + y) \nas  total_sales from sales </pre><p>Nested Struct can be queried using the <strong>.</strong> notation&nbsp;<code>performance.sales</code> will give you access to all the sales values in the performance column.&nbsp; </p><p><br></p><p>Note:&nbsp;option D&nbsp;is wrong because it uses performance<strong>:</strong>sales not performance.sales. ":" this is only used when referring to JSON&nbsp;data but here we are dealing with a struct data type.&nbsp; for the exam please make sure to understand if you are dealing with JSON data or Struct data. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-26_07-01-50-52f86de4301fa03bdfc925ac9a06a2ad.jpg"><p>Here are some additional examples</p><p><a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/functions/dotsign.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/functions/dotsign.html</a></p><p><br></p><p><strong>Other solutions:</strong></p><p>we can also use reduce instead of aggregate </p><p>\n<code>select reduce(flatten(collect_list(performance.sales)), 0, (x, y) -&gt; x + y) as&nbsp; total_sales from sales \t</code></p><p> </p><p>we can also use explode and sum instead of using any higher-order funtions. </p><p><br></p><pre class="prettyprint linenums">with cte as (\n  select\n    explode(flatten(collect_list(performance.sales))) sales from sales\n)\nselect\n  sum(sales) from cte</pre><p>&nbsp; </p><p>Sample data with create table syntax for the data:&nbsp;</p><p><br></p><pre class="prettyprint linenums">create or replace table sales as \nselect 1 as batchId ,\n\tfrom_json(\'[{ "employeeId":1234,"sales" : 10000 },{ "employeeId":3232,"sales" : 30000 }]\',\n         \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n  current_timestamp() as insertDate\nunion all \nselect 2 as batchId ,\n  from_json(\'[{ "employeeId":1235,"sales" : 10500 },{ "employeeId":3233,"sales" : 32000 }]\',\n                \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n                current_timestamp() as insertDate</pre><p><br></p>', 'answers': ['<pre class="prettyprint linenums">WITH CTE as (SELECT EXPLODE (performance) FROM table_name)\nSELECT SUM (performance.sales) FROM CTE</pre>', '<pre class="prettyprint linenums">WITH CTE as (SELECT FLATTEN (performance) FROM table_name)\nSELECT SUM (sales) FROM CTE</pre>', '<pre class="prettyprint linenums">select aggregate(flatten(collect_list(performance.sales)), 0, (x, y) -&gt; x + y) \nas  total_sales from sales  </pre>', '<p><code>SELECT SUM(SLICE (performance, sales)) FROM employee</code> </p>', '<pre class="prettyprint linenums">select reduce(flatten(collect_list(performance:sales)), 0, (x, y) -&gt; x + y) \nas  total_sales from sales  </pre>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are trying to calculate total sales made by all the employees by parsing a complex struct data type that stores employee and sales data, how would you approach this in SQLTable definition, batchId INT, performance ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;, insertDate TIMESTAMPSample data of performance column[\n{ "employeeId":1234\n"sales" : 10000},\n\n{ "employeeId":3232\n"sales" : 30000}\n]Calculate total sales made by all the employees?Sample data with create table syntax for the data:&nbsp;create or replace table sales as \nselect 1 as batchId ,\n\tfrom_json(\'[{ "employeeId":1234,"sales" : 10000 },{ "employeeId":3232,"sales" : 30000 }]\',\n         \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n  current_timestamp() as insertDate\nunion all \nselect 2 as batchId ,\n  from_json(\'[{ "employeeId":1235,"sales" : 10500 },{ "employeeId":3233,"sales" : 32000 }]\',\n                \'ARRAY&lt;STRUCT&lt;employeeId: BIGINT, sales: INT&gt;&gt;\') as performance,\n                current_timestamp() as insertDate \t&nbsp; &nbsp; &nbsp; &nbsp;', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428402, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statements can be used to test the functionality of code to test number of rows in the table equal to 10 in python?</p><p><br></p><p><code>row_count = spark.sql("select count(*) from table").collect()[0][0]</code> </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>assert row_count == 10, "Row count did not match"</code> </p><p><br></p><p>Review below documentation</p><p><br></p><p><a href="https://www.w3schools.com/python/ref_keyword_assert.asp">Assert Python</a></p>', 'answers': ['<p><code>assert (row_count = 10, "Row count did not match")</code> </p>', '<p><code>assert if (row_count = 10, "Row count did not match")</code> </p>', '<p><code>assert row_count == 10, "Row count did not match"</code> </p>', '<p><code>assert if row_count == 10, "Row count did not match"</code> </p>', '<p><code>assert row_count = 10, "Row count did not match"</code> </p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following statements can be used to test the functionality of code to test number of rows in the table equal to 10 in python?row_count = spark.sql("select count(*) from table").collect()[0][0]', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428404, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How do you handle failures gracefully when writing code in Pyspark,&nbsp; fill in the blanks to complete the below statement</p><p><br></p><p><br></p><pre class="prettyprint linenums">_____\n\n    Spark.read.table("table_name").select("column").write.mode("append").SaveAsTable("new_table_name")\n\n_____\n\n    print(f"query failed")</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is try: and except:</p>', 'answers': ['<p> try: failure:</p><p><br></p>', '<p>try: catch:</p>', '<p>try: except:</p>', '<p>try: fail:</p><p><br></p>', '<p>try: error:</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'How do you handle failures gracefully when writing code in Pyspark,&nbsp; fill in the blanks to complete the below statement_____\n\n    Spark.read.table("table_name").select("column").write.mode("append").SaveAsTable("new_table_name")\n\n_____\n\n    print(f"query failed")', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428406, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on a process to query the table based on batch date, and batch date is an input parameter and expected to change every time the program runs, what is the best way to we can parameterize the query to run without manually changing the batch date?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to filter the data based on the python variable</p><p><br></p>', 'answers': ['<p>Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to filter the data based on the python variable</p>', '<p>Create a dynamic view that can calculate the batch date automatically and use the view to query the data</p>', '<p>There is no way we can combine python variable and spark code</p>', '<p>Manually edit code every time to change the batch date</p>', '<p>Store the batch date in the spark configuration and use a spark data frame to filter the data based on the spark configuration.</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are working on a process to query the table based on batch date, and batch date is an input parameter and expected to change every time the program runs, what is the best way to we can parameterize the query to run without manually changing the batch date?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428408, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following commands results in the successful creation of a view on top of the delta stream(stream on delta table)?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>The answer is </p><p><code>Spark.readStream.table("sales").createOrReplaceTempView("streaming_vw")</code> </p><p><br></p><p>When you load a Delta table as a stream source and use it in a streaming query, the query processes all of the data present in the table as well as any new data that arrives after the stream is started.</p><p>You can load both paths and tables as a stream, you also have the ability to ignore deletes and changes(updates, Merge, overwrites) on the delta table. </p><p>Here is more information, </p><p><a href="https://docs.databricks.com/delta/delta-streaming.html#delta-table-as-a-source">https://docs.databricks.com/delta/delta-streaming.html#delta-table-as-a-source</a></p><p><br></p>', 'answers': ['<p><code> Spark.read.format("delta").table("sales").createOrReplaceTempView("streaming_vw")</code> </p>', '<p><code>Spark.readStream.format("delta").table("sales").createOrReplaceTempView("streaming_vw")</code> </p>', '<p><code>Spark.read.format("delta").table("sales").mode("stream").createOrReplaceTempView("streaming_vw")</code> </p>', '<p><code>Spark.read.format("delta").table("sales").trigger("stream").createOrReplaceTempView("streaming_vw")</code> </p>', '<p><code>Spark.read.format("delta").stream("sales").createOrReplaceTempView("streaming_vw")</code> </p>', '<p>You can not create a view on streaming data source. </p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following commands results in the successful creation of a view on top of the delta stream(stream on delta table)?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428410, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following techniques structured streaming uses to create an end-to-end fault tolerance?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Checkpointing and idempotent sinks</p><p><br></p><p>How does structured streaming achieves end to end fault tolerance:</p><ul><li><p>First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.</p></li><li><p>Next, the streaming sinks are designed to be _idempotent_—that is, multiple writes of the same data (as identified by the offset) do <em>not</em> result in duplicates being written to the sink.</p></li></ul><p>Taken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure <strong>end-to-end, exactly-once semantics</strong> under any failure condition.</p>', 'answers': ['<p>Checkpointing and Water marking</p>', '<p>Write ahead logging and water marking</p>', '<p>Checkpointing and idempotent sinks</p>', '<p>Write ahead logging and idempotent sinks</p>', '<p>Stream will failover to available nodes in the cluste</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following techniques structured streaming uses to create an end-to-end fault tolerance?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428416, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following two options are supported in identifying the arrival of new files, and incremental data from Cloud object storage using Auto Loader?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is A, Directory listing, File notifications</p><p><br></p><p><strong>Directory listing</strong>: Auto Loader identifies new files by listing the input directory.</p><p><strong>File notification</strong>: Auto Loader can automatically set up a notification service and queue service that subscribe to file events from the input directory.</p><p><br></p><p><a href="https://docs.databricks.com/ingestion/auto-loader/file-detection-modes.html">Choosing between file notification and directory listing modes | Databricks on AWS</a></p><p><br></p>', 'answers': ['<p>Directory listing, File notification</p>', '<p>Checking pointing, watermarking</p>', '<p>Writing ahead logging, read head logging</p>', '<p>File hashing, Dynamic file lookup</p>', '<p>Checkpointing and Write ahead logging</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following two options are supported in identifying the arrival of new files, and incremental data from Cloud object storage using Auto Loader?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428418, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following data workloads will utilize a Bronze table as its destination?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is A job that ingests raw data from a streaming source into the Lakehouse. </p><p><br></p><p>The ingested data from the raw streaming data source like Kafka is first stored in the Bronze layer as first destination before it is further optimized and stored in Silver. </p><p><br></p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p><br></p><p>Bronze Layer:</p><p>1. Raw copy of ingested data</p><p>2. Replaces traditional data lake</p><p>3. Provides efficient storage and querying of full, unprocessed history of data</p><p>4. No schema is applied at this layer</p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><br></p><p>Purpose of each layer in medallion architecture</p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p>', 'answers': ['<p>A job that aggregates cleaned data to create standard summary statistics</p>', '<p>A job that queries aggregated data to publish key insights into a dashboard</p>', '<p>A job that ingests raw data from a streaming source into the Lakehouse</p>', '<p>A job that develops a feature set for a machine learning application</p>', '<p>A job that enriches data by parsing its timestamps into a human-readable format</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following data workloads will utilize a Bronze table as its destination?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428420, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following data workloads will utilize a silver table as its source?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; A job that aggregates cleaned data to create standard summary statistics</p><p><br></p><p>Silver zone maintains the grain of the original data, in this scenario a job is taking data from the silver zone as the <strong>source </strong>and aggregating and storing them in the gold zone. </p><p><br></p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Silver Layer:</p><p><br></p><p>1. Reduces data storage complexity, latency, and redundency</p><p>2. Optimizes ETL throughput and analytic query performance</p><p>3. Preserves grain of original data (without aggregation)</p><p>4. Eliminates duplicate records</p><p>5. production schema enforced</p><p>6. Data quality checks, quarantine corrupt data</p><p><br></p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><br></p><p>Purpose of each layer in medallion architecture</p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p>', 'answers': ['<p>A job that enriches data by parsing its timestamps into a human-readable format</p>', '<p>A job that queries aggregated data that already feeds into a dashboard</p>', '<p>A job that ingests raw data from a streaming source into the Lakehouse</p>', '<p>A job that aggregates cleaned data to create standard summary statistics</p>', '<p>A job that cleans data by removing malformatted records</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following data workloads will utilize a silver table as its source?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428422, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following data workloads will utilize a gold table as its source?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; A job that queries aggregated data that already feeds into a dashboard</p><p><br></p><p>The gold layer is used to store aggregated data, which are typically used for dashboards and reporting. </p><p><br></p><p><br></p><p>Review the below link for more info,</p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Gold Layer:</p><p><br></p><p>1. Powers Ml applications, reporting, dashboards, ad hoc analytics</p><p>2. Refined views of data, typically with aggregations</p><p>3. Reduces strain on production systems</p><p>4. Optimizes query performance for business-critical data</p><p><br></p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><br></p><p>Purpose of each layer in medallion architecture</p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p>', 'answers': ['<p>A job that enriches data by parsing its timestamps into a human-readable format</p>', '<p>A job that queries aggregated data that already feeds into a dashboard</p>', '<p>A job that ingests raw data from a streaming source into the Lakehouse</p>', '<p>A job that aggregates cleaned data to create standard summary statistics</p>', '<p>A job that cleans data by removing malformatted records</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following data workloads will utilize a gold table as its source?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428424, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently asked to work on building a data pipeline, you have noticed that you are currently working with a data source that has a lot of data quality issues and you need to monitor data quality and enforce it as part of the data ingestion process, which of the following tools can be used to address this problem?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, DELTA LIVE TABLES</p><p><br></p><p>Delta live tables expectations can be used to identify and quarantine bad data, all of the data quality metrics are stored in the event logs which can be used to later analyze and monitor. </p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-expectations">DELTA LIVE Tables expectations</a></p><p><br></p><p>Below are three types of expectations, make sure to pay attention differences between these three.&nbsp; </p><p><br></p><p><strong>Retain invalid records:</strong></p><p>Use the <code>expect</code> operator when you want to keep records that violate the expectation. Records that violate the expectation are added to the target dataset along with valid records:</p><p><br></p><p>Python</p><pre class="prettyprint linenums">@dlt.expect("valid timestamp", "col(“timestamp”) &gt; \'2012-01-01\'")\n</pre><p>SQL</p><pre class="prettyprint linenums">CONSTRAINT valid_timestamp EXPECT (timestamp &gt; \'2012-01-01\')\n</pre><p><strong>Drop invalid records:</strong></p><p>Use the <code>expect or drop</code> operator to prevent the processing of invalid records. Records that violate the expectation are dropped from the target dataset:</p><p><br></p><p>Python</p><pre class="prettyprint linenums">@dlt.expect_or_drop("valid_current_page", "current_page_id IS NOT NULL AND current_page_title IS NOT NULL")\n</pre><p>SQL</p><pre class="prettyprint linenums">CONSTRAINT valid_current_page EXPECT (current_page_id IS NOT NULL and current_page_title IS NOT NULL) ON VIOLATION DROP ROW\n</pre><p><strong>Fail on invalid records:</strong></p><p>When invalid records are unacceptable, use the <code>expect or fail</code> operator to halt execution immediately when a record fails validation. If the operation is a table update, the system atomically rolls back the transaction:</p><p><br></p><p>Python</p><pre class="prettyprint linenums">@dlt.expect_or_fail("valid_count", "count &gt; 0")\n</pre><p>SQL</p><pre class="prettyprint linenums">CONSTRAINT valid_count EXPECT (count &gt; 0) ON VIOLATION FAIL UPDATE</pre><p><br></p>', 'answers': ['<p>AUTO LOADER</p>', '<p>DELTA LIVE TABLES</p>', '<p> JOBS and TASKS</p>', '<p>UNITY Catalog and Data Governance</p>', '<p>STRUCTURED STREAMING with MULTI HOP</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'You are currently asked to work on building a data pipeline, you have noticed that you are currently working with a data source that has a lot of data quality issues and you need to monitor data quality and enforce it as part of the data ingestion process, which of the following tools can be used to address this problem?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428426, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When building a DLT s pipeline you have two options to create a live tables, what is the main difference between <code>CREATE STREAMING LIVE TABLE</code> vs <code>CREATE LIVE TABLE</code>?</p><p><br></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data</p>', 'answers': ['<p><code>CREATE STREAMING LIVE</code> table is used in MULTI HOP Architecture</p>', '<p><code>CREATE LIVE TABLE </code>is used when working with Streaming data sources and Incremental data</p>', '<p> <code>CREATE STREAMING LIVE TABLE</code> is used when working with Streaming data sources and Incremental data</p>', '<p>There is no difference both are the same, <code>CREATE STRAMING LIVE </code>will be deprecated soon</p>', '<p><code>CREATE LIVE TABLE</code> is used in DELTA LIVE TABLES, CREATE STREAMING LIVE can only used in Structured Streaming applications</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'When building a DLT s pipeline you have two options to create a live tables, what is the main difference between CREATE STREAMING LIVE TABLE vs CREATE LIVE TABLE?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428430, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A particular job seems to be performing slower and slower over time, the team thinks this started to happen when a recent production change was implemented, you were asked to take look at the job history and see if we can identify trends and root cause, where in the workspace UI can you perform this analysis?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; </p><p>Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical run</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_20-22-34-4aa4eb26682b88fbe39a4f4b65056563.JPG">', 'answers': ['<p>Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical run</p>', '<p>Under jobs UI select the job cluster, under spark UI select the application job logs, then you can access last 60 day historical runs</p>', '<p>Under Workspace logs, select job logs and select the job you want to monitor to view the last 60 day historical runs</p>', '<p>Under Compute UI, select Job cluster and select the job cluster to see last 60 day historical runs</p>', '<p>Historical job runs can only be accessed by REST API</p>']}, 'correct_response': ['a'], 'section': 'Production Pipelines', 'question_plain': 'A particular job seems to be performing slower and slower over time, the team thinks this started to happen when a recent production change was implemented, you were asked to take look at the job history and see if we can identify trends and root cause, where in the workspace UI can you perform this analysis?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428432, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What are the different ways you can schedule a job in Databricks workspace?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Cron, On-Demand runs</p><p><br></p><p>Supports running job immediately or using can be scheduled using CRON&nbsp;syntax</p><p><br></p><p><a href="https://docs.databricks.com/data-engineering/jobs/jobs.html#run-a-job">Jobs in Databricks</a></p><p><br></p>', 'answers': ['<p>Continuous, Incremental</p>', '<p>On-Demand runs, File notification from Cloud object storage</p>', '<p>Cron, On Demand runs</p>', '<p>Cron, File notification from Cloud object storage</p>', '<p>Once, Continuous</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'What are the different ways you can schedule a job in Databricks workspace?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428434, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have noticed that Databricks SQL queries are running slow, you are asked to look reason why queries are running slow and identify steps to improve the performance, when you looked at the issue you noticed all the queries are running in parallel and using a SQL endpoint(SQL Warehouse) with a single cluster. Which of the following steps can be taken to improve the performance/response times of the queries?</p><p>*Please note Databricks recently renamed SQL&nbsp;endpoint to SQL&nbsp;warehouse. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, They can increase the maximum bound of the SQL endpoint’s scaling range when you increase the max scaling range more clusters are added so queries instead of waiting in the queue can start running using available clusters, see below for more explanation. </p><p><br></p><p><strong>The question is looking to test your ability to know how to scale a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) and you have to look for cue words or need to understand if the queries are running sequentially or concurrently. if the queries are running sequentially then scale up(Size of the cluster from 2X-Small to 4X-Large) if the queries are running concurrently or with more users then scale out(add more clusters). </strong></p><p><br></p><p><strong>SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) Overview:&nbsp;(Please read all of the below points and the below diagram to understand )</strong></p><p><br></p><ol><li><p>A SQL&nbsp;Warehouse should have at least one cluster</p></li><li><p>A cluster comprises one driver node and one or many worker nodes </p></li><li><p>No of worker nodes in a cluster is determined by the size of the cluster (2X -Small -&gt;1 worker, X-Small -&gt;2 workers.... up to 4X-Large -&gt;&nbsp;128 workers) this is called <strong>Scale up</strong></p></li><li><p>A single cluster irrespective of cluster size(2X-Smal.. to ...4XLarge)&nbsp;can only run 10 queries at any given time if a user submits 20 queries all at once to a warehouse with 3X-Large cluster size and cluster scaling (min 1, max1) while 10 queries will start running the remaining 10 queries wait in a queue for these 10 to finish. </p></li><li><p>Increasing the Warehouse cluster size can improve the performance of a query, for example, if a query runs for 1 minute in a 2X-Small warehouse size it may run in 30 Seconds if we change the warehouse size to X-Small. this is due to 2X-Small having 1 worker node and X-Small having 2 worker nodes so the query has more tasks and runs faster (note: this is an ideal case example, the scalability of a query performance depends on many factors, it can not always be linear)</p></li><li><p>A warehouse can have more than one cluster this is called <strong>Scale out</strong>. If a warehouse is configured with X-Small cluster size with cluster scaling(Min1, Max 2) Databricks spins up an additional cluster if it detects queries are waiting in the queue, If a warehouse is configured to run 2 clusters(Min1, Max 2), and let\'s say a user submits 20 queries, 10 queriers will start running and holds the remaining in the queue and databricks will automatically start the second cluster and starts redirecting the 10 queries waiting in the queue to the second cluster. </p></li><li><p>A single query will not span more than one cluster, once a query is submitted to a cluster it will remain in that cluster until the query execution finishes irrespective of how many clusters are available to scale. </p></li></ol><p><br></p><p><br></p><p>Please review the below diagram to understand the above concepts:&nbsp;</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-12_20-08-11-f1e6f0a38c1e03819054685c241fafb5.jpeg"></p><p><br></p><p><br></p><p><br></p><p>SQL endpoint(SQL&nbsp;Warehouse) scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.</p><p><strong>Scale-out</strong> -&gt; to add more clusters for a SQL endpoint, change max number of clusters</p><p>If you are trying to improve the throughput, being able to run as many queries as possible then having an additional cluster(s) will improve the performance.</p><p><br></p><p>Databricks SQL&nbsp;automatically scales as soon as it detects queries are in queuing state, in this example scaling is set for min 1 and max 3 which means the warehouse can add three clusters if it detects queries are waiting.&nbsp; </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-04_16-05-29-810fdd9c0793c9e68e4f1ba6087d441b.jpeg"></p><p><br></p><p>During the warehouse creation or after you have the ability to change the warehouse size (2X-Small....to ...4XLarge) to improve query performance and the maximize scaling range to add more clusters on a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) scale-out, if you are changing an existing warehouse you may have to restart the warehouse to make the changes effective. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-27_21-21-10-b0d0bff72dffb8e7a4706d84d7559cf6.jpg"><p><strong>How do you know how many clusters you need(How to set Max cluster size)?</strong></p><p>When you click on an existing warehouse and select the monitoring tab, you can see warehouse utilization information(see below), there are two graphs that provide important information on how the warehouse is being utilized, if you see queries are being queued that means your warehouse can benefit from additional clusters. Please review the additional DBU&nbsp;cost associated with adding clusters so you can take a well balanced decision between cost and performance. </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-27_21-21-10-6f448bcfea12b8d1b1938a9b180d9458.jpeg">', 'answers': ['<p>They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;warehouse).</p>', '<p> They can increase the maximum bound of the SQL endpoint(SQL&nbsp;warehouse)’s scaling range</p>', '<p>They can increase the warehouse size from 2X-Smal to 4XLarge of the SQL endpoint(SQL&nbsp;warehouse).</p>', '<p> They can turn on the Auto Stop feature for the SQL endpoint(SQL&nbsp;warehouse).</p>', '<p>They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;warehouse) and change the Spot Instance Policy to “Reliability Optimized.”</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'You have noticed that Databricks SQL queries are running slow, you are asked to look reason why queries are running slow and identify steps to improve the performance, when you looked at the issue you noticed all the queries are running in parallel and using a SQL endpoint(SQL Warehouse) with a single cluster. Which of the following steps can be taken to improve the performance/response times of the queries?*Please note Databricks recently renamed SQL&nbsp;endpoint to SQL&nbsp;warehouse.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428436, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You currently working with the marketing team to setup a dashboard for ad campaign analysis, since the team is not sure how often the dashboard should be refreshed they have decided to do a manual refresh on an as needed basis. Which of the following steps can be taken to reduce the overall cost of the compute when the team is not using the compute?</p><p><br>*Please note that Databricks recently change the name of SQL&nbsp;Endpoint to SQL&nbsp;Warehouses. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, They can turn on the Auto Stop feature for the SQL endpoint(SQL&nbsp;Warehouse).</p><p><br></p><p>Use auto stop to automatically terminate the cluster when you are not using it. </p><p><br></p>', 'answers': ['<p>They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;Warehouse).</p>', '<p>They can decrease the maximum bound of the SQL endpoint(SQL&nbsp;Warehouse) scaling range.</p>', '<p>They can decrease the cluster size of the SQL endpoint(SQL&nbsp;Warehouse).</p>', '<p>They can turn on the Auto Stop feature for the SQL endpoint(SQL&nbsp;Warehouse).</p>', '<p>They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;Warehouse) and change the Spot Instance Policy from “Reliability Optimized” to&nbsp; “Cost optimized” </p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'You currently working with the marketing team to setup a dashboard for ad campaign analysis, since the team is not sure how often the dashboard should be refreshed they have decided to do a manual refresh on an as needed basis. Which of the following steps can be taken to reduce the overall cost of the compute when the team is not using the compute?*Please note that Databricks recently change the name of SQL&nbsp;Endpoint to SQL&nbsp;Warehouses.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428438, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You had worked with the Data analysts team to set up a SQL Endpoint(SQL&nbsp;warehouse) point so they can easily query and analyze data in the gold layer, but once they started consuming the SQL Endpoint(SQL&nbsp;warehouse)&nbsp; you noticed that during the peak hours as the number of users increase you are seeing queries taking longer to finish, which of the following steps can be taken to resolve the issue?</p><p>*Please note Databricks recently renamed SQL&nbsp;endpoint to SQL&nbsp;warehouse. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>the answer is,&nbsp; </p><p>They can increase the maximum bound of the SQL endpoint’s scaling range, when you increase the maximum bound you can add more clusters to the warehouse which can then run additional queries that are waiting in the queue to run, focus on the below explanation that talks about Scale-out. </p><p><br></p><p><strong>The question is looking to test your ability to know how to scale a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) and you have to look for cue words or need to understand if the queries are running sequentially or concurrently. if the queries are running sequentially then scale up(Size of the cluster from 2X-Small to 4X-Large) if the queries are running concurrently or with more users then scale out(add more clusters). </strong></p><p><br></p><p><strong>SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) Overview:&nbsp;(Please read all of the below points and the below diagram to understand )</strong></p><p><br></p><ol><li><p>A SQL&nbsp;Warehouse should have at least one cluster</p></li><li><p>A cluster comprises one driver node and one or many worker nodes </p></li><li><p>No of worker nodes in a cluster is determined by the size of the cluster (2X -Small -&gt;1 worker, X-Small -&gt;2 workers.... up to 4X-Large -&gt;&nbsp;128 workers) this is called <strong>Scale up</strong></p></li><li><p>A single cluster irrespective of cluster size(2X-Smal.. to ...4XLarge)&nbsp;can only run 10 queries at any given time if a user submits 20 queries all at once to a warehouse with 3X-Large cluster size and cluster scaling (min 1, max1) while 10 queries will start running the remaining 10 queries wait in a queue for these 10 to finish. </p></li><li><p>Increasing the Warehouse cluster size can improve the performance of a query, example if a query runs for 1 minute in a 2X-Small warehouse size, it may run in 30 Seconds if we change the warehouse size to X-Small. this is due to 2X-Small has 1 worker node and X-Small has 2 worker nodes so the query has more tasks and runs faster (note: this is an ideal case example, the scalability of a query performance depends on many factors, it can not always be linear)</p></li><li><p>A warehouse can have more than one cluster this is called <strong>Scale out</strong>. If a warehouse is configured with X-Small cluster size with cluster scaling(Min1, Max 2) Databricks spins up an additional cluster if it detects queries are waiting in the queue, If a warehouse is configured to run 2 clusters(Min1, Max 2), and let\'s say a user submits 20 queries, 10 queriers will start running and holds the remaining in the queue and databricks will automatically start the second cluster and starts redirecting the 10 queries waiting in the queue to the second cluster. </p></li><li><p>A single query will not span more than one cluster, once a query is submitted to a cluster it will remain in that cluster until the query execution finishes irrespective of how many clusters are available to scale. </p></li></ol><p><br></p><p>Please review the below diagram to understand the above concepts:&nbsp;</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-12_20-08-11-f1e6f0a38c1e03819054685c241fafb5.jpeg"></p><p><br></p><p><br></p><p><br></p><p>SQL endpoint(SQL&nbsp;Warehouse) scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.</p><p><strong>Scale-out</strong> -&gt; to add more clusters for a SQL endpoint, change max number of clusters</p><p>If you are trying to improve the throughput, being able to run as many queries as possible then having an additional cluster(s) will improve the performance.</p><p><br></p><p>Databricks SQL&nbsp;automatically scales as soon as it detects queries are in queuing state, in this example scaling is set for min 1 and max 3 which means the warehouse can add three clusters if it detects queries are waiting.&nbsp; </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-04_16-05-29-810fdd9c0793c9e68e4f1ba6087d441b.jpeg"></p><p><br></p><p>During the warehouse creation or after you have the ability to change the warehouse size (2X-Small....to ...4XLarge) to improve query performance and the maximize scaling range to add more clusters on a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) scale-out, if you are changing an existing warehouse you may have to restart the warehouse to make the changes effective. </p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-27_21-21-10-b0d0bff72dffb8e7a4706d84d7559cf6.jpg"></p><p><strong>How do you know how many clusters you need(How to set Max cluster size)?</strong></p><p>When you click on an existing warehouse and select the monitoring tab, you can see warehouse utilization information(see below), there are two graphs that provide important information on how the warehouse is being utilized, if you see queries are being queued that means your warehouse can benefit from additional clusters. Please review the additional DBU&nbsp;cost associated with adding clusters so you can take a well balanced decision between cost and performance. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-27_21-21-10-6f448bcfea12b8d1b1938a9b180d9458.jpeg"></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p> They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;warehouse).</p>', '<p>They can increase the maximum bound of the SQL endpoint(SQL&nbsp;warehouse) ’s scaling range.</p>', '<p>They can increase the cluster size from 2X-Small to 4X-Large of the SQL endpoint(SQL&nbsp;warehouse) .</p>', '<p>They can turn on the Auto Stop feature for the SQL endpoint(SQL&nbsp;warehouse) .</p>', '<p>They can turn on the Serverless feature for the SQL endpoint(SQL&nbsp;warehouse)&nbsp; and change the Spot Instance Policy from “Cost optimized” to “Reliability Optimized.”</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'You had worked with the Data analysts team to set up a SQL Endpoint(SQL&nbsp;warehouse) point so they can easily query and analyze data in the gold layer, but once they started consuming the SQL Endpoint(SQL&nbsp;warehouse)&nbsp; you noticed that during the peak hours as the number of users increase you are seeing queries taking longer to finish, which of the following steps can be taken to resolve the issue?*Please note Databricks recently renamed SQL&nbsp;endpoint to SQL&nbsp;warehouse.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428440, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The research team has put together a funnel analysis query to monitor the customer traffic on the e-commerce platform, the query takes about 30 mins to run on a small SQL endpoint cluster with max scaling set to 1 cluster. What steps can be taken to improve the performance of the query?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is,&nbsp; They can increase the cluster size anywhere from 2X-Small to 4XL(Scale Up) to review the performance and select the size that meets your SLA. If you are trying to improve the performance of a single query at a time having additional memory, additional worker nodes mean that more tasks can run in a cluster which will improve the performance of that query.</p><p><br></p><p><strong>The question is looking to test your ability to know how to scale a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) and you have to look for cue words or need to understand if the queries are running sequentially or concurrently. if the queries are running sequentially then scale up(Size of the cluster from 2X-Small to 4X-Large) if the queries are running concurrently or with more users then scale out(add more clusters). </strong></p><p><br></p><p><strong>SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) Overview:&nbsp;(Please read all of the below points and the below diagram to understand )</strong></p><p><br></p><ol><li><p>A SQL&nbsp;Warehouse should have at least one cluster</p></li><li><p>A cluster comprises one driver node and one or many worker nodes </p></li><li><p>No of worker nodes in a cluster is determined by the size of the cluster (2X -Small -&gt;1 worker, X-Small -&gt;2 workers.... up to 4X-Large -&gt;&nbsp;128 workers) this is called <strong>Scale Up</strong></p></li><li><p>A single cluster irrespective of cluster size(2X-Smal.. to ...4XLarge)&nbsp;can only run 10 queries at any given time if a user submits 20 queries all at once to a warehouse with 3X-Large cluster size and cluster scaling (min 1, max1) while 10 queries will start running the remaining 10 queries wait in a queue for these 10 to finish. </p></li><li><p>Increasing the Warehouse cluster size can improve the performance of a query, example if a query runs for 1 minute in a 2X-Small warehouse size, it may run in 30 Seconds if we change the warehouse size to X-Small. this is due to 2X-Small has 1 worker node and X-Small has 2 worker nodes so the query has more tasks and runs faster (note: this is an ideal case example, the scalability of a query performance depends on many factors, it can not always be linear)</p></li><li><p>A warehouse can have more than one cluster this is called <strong>Scale Out</strong>. If a warehouse is configured with X-Small cluster size with cluster scaling(Min1, Max 2) Databricks spins up an additional cluster if it detects queries are waiting in the queue, If a warehouse is configured to run 2 clusters(Min1, Max 2), and let\'s say a user submits 20 queries, 10 queriers will start running and holds the remaining in the queue and databricks will automatically start the second cluster and starts redirecting the 10 queries waiting in the queue to the second cluster. </p></li><li><p>A single query will not span more than one cluster, once a query is submitted to a cluster it will remain in that cluster until the query execution finishes irrespective of how many clusters are available to scale. </p></li></ol><p><br></p><p>Please review the below diagram to understand the above concepts:&nbsp;</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-12_19-57-57-3abac2c190a8b1947c1c9be08c7c1ece.jpeg"><p><br></p><p><strong>Scale-up</strong>-&gt; Increase the size of the SQL endpoint, change cluster size from 2X-Small to up to 4X-Large</p><p>If you are trying to improve the performance of a single query having additional memory, additional worker nodes and cores will result in more tasks running in the cluster will ultimately improve the performance.</p><p><br></p><p>During the warehouse creation or after, you have the ability to change the warehouse size (2X-Small....to ...4XLarge) to improve query performance and the maximize scaling range to add more clusters on a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) scale-out if you are changing an existing warehouse you may have to restart the warehouse to make the changes effective. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-04_16-05-29-f35a13ac5da3cffec97e5332f8115bbc.jpg"></p>', 'answers': ['<p> They can turn on the Serverless feature for the SQL endpoint.</p>', '<p>They can increase the maximum bound of the SQL endpoint’s scaling range anywhere from between 1 to 100 to review the performance and select the size that meets the required SLA.</p>', '<p>They can increase the cluster size anywhere from X small to 3XL to review the performance and select the size that meets the required SLA.</p>', '<p>They can turn off the Auto Stop feature for the SQL endpoint to more than 30 mins.</p>', '<p>They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Cost optimized” to “Reliability Optimized.”</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'The research team has put together a funnel analysis query to monitor the customer traffic on the e-commerce platform, the query takes about 30 mins to run on a small SQL endpoint cluster with max scaling set to 1 cluster. What steps can be taken to improve the performance of the query?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428442, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Unity catalog simplifies managing multiple workspaces, by storing and managing permissions and ACL at _______ level</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Account Level </p><p>The classic access control list (tables, workspace, cluster) is at the workspace level, Unity catalog is at the account level and can manage all the workspaces in an Account. </p>', 'answers': ['<p>Workspace</p>', '<p>Account</p>', '<p>Storage</p>', '<p>Data pane</p>', '<p>Control pane</p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'Unity catalog simplifies managing multiple workspaces, by storing and managing permissions and ACL at _______ level', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428444, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following section in the UI can be used to manage permissions and grants to tables?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Data Explorer</p><p><br></p><p><a href="https://docs.databricks.com/sql/user/data/index.html">Data explorer</a></p><p><br></p>', 'answers': ['<p>User Settings</p>', '<p>Admin UI</p>', '<p>Workspace admin settings</p>', '<p>User access control lists </p>', '<p>Data Explorer</p>']}, 'correct_response': ['e'], 'section': 'Data Governance', 'question_plain': 'Which of the following section in the UI can be used to manage permissions and grants to tables?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428446, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following is not a privilege in the Unity catalog?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The Answer is DELETE and UPDATE&nbsp;permissions do not exit, you have to use MODIFY&nbsp;which provides both Update and Delete permissions. </p><p><br></p><p><strong>Please note:&nbsp;TABLE&nbsp;ACL&nbsp;privilege types are different from Unity Catalog privilege types, please read the question carefully. </strong></p><p><br></p><p>Here is the list of all privileges in <strong>Unity Catalog:</strong></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-11_12-15-16-d2690dde51388f4930e7659c5694d5d9.jpg"></p><p><br></p><p><br></p><p>Unity Catalog Privileges </p><p><a href="https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-privileges#privilege-types">https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-privileges#privilege-types</a></p><p><br></p><p>Table ACL&nbsp;privileges</p><p><a href="https://learn.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges">https://learn.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges</a></p><p><br></p>', 'answers': ['<p>SELECT</p>', '<p>MODIFY</p>', '<p>DELETE</p>', '<p>CREATE TABLE</p>', '<p>EXECUTE</p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': 'Which of the following is not a privilege in the Unity catalog?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53428448, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> A team member is leaving the team and he/she is currently the owner of the few tables, instead of transfering the ownership to a user you have decided to transfer the ownership to a group so in the future anyone in the group can manage the permissions rather than a single individual, which of the following commands help you accomplish this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is ALTER TABLE table_name OWNER to ‘group’</p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#assign-owner-to-object">Assign owner to object </a></p>', 'answers': ["<p><code>ALTER TABLE table_name OWNER to 'group'</code> </p>", "<p><code>TRANSFER OWNER table_name to 'group'</code> </p>", "<p><code>GRANT OWNER table_name to 'group'</code> </p>", "<p><code>ALTER OWNER ON table_name to 'group'</code> </p>", "<p><code>GRANT OWNER On table_name to 'group'</code> </p>"]}, 'correct_response': ['a'], 'section': 'Data Governance', 'question_plain': 'A team member is leaving the team and he/she is currently the owner of the few tables, instead of transfering the ownership to a user you have decided to transfer the ownership to a group so in the future anyone in the group can manage the permissions rather than a single individual, which of the following commands help you accomplish this?', 'related_lectures': []}]}
5606606
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 52292560, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the best way to describe a data lakehouse compared to a data warehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Anser is A data lakehouse enables both batch and streaming analytics.</p><p><br></p><p><strong>A lakehouse has the following key features:</strong></p><ul><li><p><strong>Transaction support:</strong> In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently. Support for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using SQL.</p></li><li><p><strong>Schema enforcement and governance:</strong> The Lakehouse should have a way to support schema enforcement and evolution, supporting DW schema architectures such as star/snowflake-schemas. The system should be able to <a href="https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html">reason about data integrity</a>, and it should have robust governance and auditing mechanisms.</p></li><li><p><strong>BI support:</strong> Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency, reduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a warehouse.</p></li><li><p><strong>Storage is decoupled from compute:</strong> In practice this means storage and compute use separate clusters, thus these systems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also have this property.</p></li><li><p><strong>Openness:</strong> The storage formats they use are open and standardized, such as Parquet, and they provide an API so a variety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data <strong>directly</strong>.</p></li><li><p><strong>Support for diverse data types ranging from unstructured to structured data</strong>: The lakehouse can be used to store, refine, analyze, and access data types needed for many new data applications, including images, video, audio, semi-structured data, and text.</p></li><li><p><strong>Support for diverse workloads: </strong>including data science, machine learning, and SQL and analytics. Multiple tools might be needed to support all these workloads but they all rely on the same data repository.</p></li><li><p><strong>End-to-end streaming:</strong> <code><strong><em>Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for separate systems dedicated to serving real-time data applications.</em></strong></code></p></li></ul>', 'answers': ['<p>A data lakehouse provides a relational system of data management</p>', '<p>A data lakehouse captures snapshots of data for version control purposes.</p>', '<p>A data lakehouse couples storage and compute for complete control.</p>', '<p>A data lakehouse utilizes proprietary storage formats for data.</p>', '<p>A data lakehouse enables both batch and streaming analytics.</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'What is the best way to describe a data lakehouse compared to a data warehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292562, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are designing an analytical to store structured data from your e-commerce platform and unstructured data from website traffic and app store, how would you approach where you store this data?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p> The answer is, Data lakehouse can store structured and unstructured data and can enforce schema</p><p><a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What Is a Lakehouse? - The Databricks Blog</a></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-05_20-47-34-4bf1511e15339d358d36a0f90c90c101.JPG"></p><p><br></p><p><br></p>', 'answers': ['<p>Use traditional data warehouse for structured data and use data lakehouse for unstructured data.</p>', '<p>Data lakehouse can only store unstructured data but cannot enforce a schema</p>', '<p> Data lakehouse can store structured and unstructured data and can enforce schema</p>', '<p>Traditional data warehouses are good for storing structured data and enforcing schema</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are designing an analytical to store structured data from your e-commerce platform and unstructured data from website traffic and app store, how would you approach where you store this data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292564, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working on a production job failure with a job set up in job clusters due to a data issue, what cluster do you need to start to investigate and analyze the data?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Answer is&nbsp; All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.</p><p><br></p><p>A job cluster can not provide a way for a user to interact with a notebook once the job is submitted, but an Interactive cluster allows to you display data, view visualizations write or edit quries, which makes it a perfect fit to investigate and analyze the data. </p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>A Job cluster can be used to analyze the problem</p>', '<p>All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.</p>', '<p>Existing job cluster can be used to investigate the issue</p>', '<p>Databricks SQL Endpoint can be used to investigate the issue</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are currently working on a production job failure with a job set up in job clusters due to a data issue, what cluster do you need to start to investigate and analyze the data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292566, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following describes how Databricks Repos can help facilitate CI/CD workflows on the Databricks Lakehouse Platform?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is Databricks Repos can commit or push code changes to trigger a CI/CD process</p><p><br></p><p>See below diagram to understand the role Databricks Repos&nbsp; and Git provider plays when building a CI/CD workdlow. </p><p>All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git provider like Github or Azure Devops. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-05_21-06-38-85467974c65529ccdcd133fd3acd2075.jpg"><p><br></p>', 'answers': ['<p>Databricks Repos can facilitate the pull request, review, and approval process before merging branches</p>', '<p>Databricks Repos can merge changes from a secondary Git branch into a main Git branch</p>', '<p>Databricks Repos can be used to design, develop, and trigger Git automation pipelines</p>', '<p>Databricks Repos can store the single-source-of-truth Git repository</p>', '<p>Databricks Repos can commit or push code changes to trigger a CI/CD process</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following describes how Databricks Repos can help facilitate CI/CD workflows on the Databricks Lakehouse Platform?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292568, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You noticed that colleague is manually copying the notebook with _bkp to store the previous versions, which of the following feature would you recommend instead.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Answer is Databricks notebooks support automatic change tracking and versioning. </p><p><br></p><p>When you are editing the notebook on the right side check version history to view all the changes, every change you are making is captured and saved. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-05_22-41-53-84c71a20a3e839fb2ab6f18531fd7569.jpg"><p><br></p>', 'answers': ['<p>Databricks notebooks support change tracking and versioning</p>', '<p>Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks</p>', '<p>Databricks notebooks can be exported into dbc archive files and stored in data lake</p>', '<p>Databricks notebook can be exported as HTML and imported at a later time</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You noticed that colleague is manually copying the notebook with _bkp to store the previous versions, which of the following feature would you recommend instead.', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292570, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Newly joined data analyst requested read-only access to tables, assuming you are owner/admin which section of Databricks platform is going to facilitate granting select access to the user</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Anser is Data Explorer</p><p><br></p><p>https://docs.databricks.com/sql/user/data/index.html</p><p><br></p><p>Data explorer lets you easily explore and manage permissions on databases and tables. Users can view schema details, preview sample data, and see table details and properties. Administrators can <a href="https://docs.databricks.com/sql/user/security/access-control/data-acl.html#manage-data-object-ownership">view and change owners</a>, and admins and data object owners can <a href="https://docs.databricks.com/sql/user/security/access-control/data-acl.html#manage-data-object-permissions">grant and revoke permissions</a>.</p><p><br></p><p>To open data explorer, click <img src="https://docs.databricks.com/_images/data-icon.png"> <strong>Data</strong> in the sidebar.</p><p><br></p>', 'answers': ['<p>Admin console</p>', '<p>User settings</p>', '<p>Data explorer</p>', '<p>Azure Databricks control pane IAM</p>', '<p>Azure RBAC</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Newly joined data analyst requested read-only access to tables, assuming you are owner/admin which section of Databricks platform is going to facilitate granting select access to the user', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292572, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> How does a Delta Lake differ from a traditional data lake?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is, Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and performance</p><p><br></p><p>Delta lake is</p><p>· Open source</p><p>· Builds up on standard data format</p><p>· Optimized for cloud object storage</p><p>· Built for scalable metadata handling</p><p>Delta lake is not</p><p>· Proprietary technology</p><p>· Storage format</p><p>· Storage medium</p><p>· Database service or data warehouse</p>', 'answers': ['<p>Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance</p>', '<p>Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance</p>', '<p>Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and performance</p>', '<p>Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide reliability, security, and performance</p>', '<p>Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'How does a Delta Lake differ from a traditional data lake?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292574, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>As a Data Engineer, you were asked to create a delta table to store below transaction data?</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-05_22-58-04-7c3c514120a64f6bffc1f784fe6002a9.jpg">', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is </p><p><br></p><pre class="prettyprint linenums">CREATE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre><p><br></p><p>When creating a table in Databricks by default the table is stored in DELTA&nbsp;format. </p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">CREATE DELTA TABLE transactions (\n                  transactionId int,\n                  transactionDate timestamp,\n                  unitsSold int)</pre>', '<pre class="prettyprint linenums">CREATE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)\nFORMAT DELTA</pre>', '<pre class="prettyprint linenums">CREATE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)</pre>', '<pre class="prettyprint linenums">CREATE TABLE USING DELTA transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int) </pre>', '<pre class="prettyprint linenums">CREATE TABLE transactions (\ntransactionId int,\ntransactionDate timestamp,\nunitsSold int)\nLOCATION DELTA</pre>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'As a Data Engineer, you were asked to create a delta table to store below transaction data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292576, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following is a correct statement on how the data is organized in the storage when when managing a DELTA&nbsp;table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is </p><p>All of the data is broken down into one or many parquet files, log files are broken down into one or many json files, and each transaction creates a new data file(s) and log file.</p><p><br></p><p>here is sample layout of how DELTA&nbsp;table might look, </p><p><br></p><p><img src="https://miro.medium.com/max/1400/1*L1yM-xDaIUNMSIoanVr8BA.png"></p><p><br></p>', 'answers': ['<p>All of the data is broken down into one or many parquet files, log files are broken down into one or many JSON files, and each transaction creates a new data file(s) and log file. </p>', '<p>All of the data and log are stored in a single parquet file</p>', '<p>All of the data is broken down into one or many parquet files, but the log file is stored as a single json file, and every transaction creates a new data file(s) and log file gets appended.</p>', '<p>All of the data is broken down into one or many parquet files, log file is removed once the transaction is committed.</p>', '<p>All of the data is stored into one parquet file, log files are broken down into one or many json files.</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following is a correct statement on how the data is organized in the storage when when managing a DELTA&nbsp;table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292608, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> What is the underlying technology that makes the Auto Loader work?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Structured Streaming</p><p><br></p><p>Auto Loader is built on top of Structured Streaming, Auto Loader provides a Structured Streaming source called <code>cloudFiles</code>. Given an input directory path on the cloud file storage, the <code>cloudFiles</code> source automatically processes new files as they arrive, with the option of also processing existing files in that directory</p><p><br></p>', 'answers': ['<p>Loader</p>', '<p>Delta Live Tables</p>', '<p>Structured Streaming</p>', '<p> DataFrames</p>', '<p>Live DataFames</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the underlying technology that makes the Auto Loader work?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292610, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working to ingest millions of files that get uploaded to the cloud object storage for consumption, and you are asked to build a process to ingest this data, the schema of the file is expected to change over time, and the ingestion process should be able to handle these changes automatically. Which of the following method can be used to ingest the data incrementally?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is AUTO LOADER, </p><p>Use Auto Loader instead of the <a href="https://docs.microsoft.com/en-us/azure/databricks/delta/delta-ingest#copy-into-sql-command">COPY INTO SQL command</a> when:</p><p><br></p><ul><li><p>You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover files more efficiently than the <code>COPY INTO</code> SQL command and can split file processing into multiple batches.</p></li><li><p>COPY&nbsp;INTO&nbsp;only directory listing but AUTO LOADER supports File notification method where the Auto Loader continues to ingest files as they arrive in cloud object storage leveraging cloud provider(Queues and triggers)&nbsp;and Spark\'s structured streaming.&nbsp; </p></li><li><p>Your data schema evolves frequently. Auto Loader provides better support for schema inference and evolution. See <a href="https://docs.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/schema">Configuring schema inference and evolution in Auto Loader</a>.</p></li></ul><p><br></p><p><br></p>', 'answers': ['<p> AUTO APPEND</p>', '<p>AUTO LOADER</p>', '<p>COPY INTO</p>', '<p>Structured Streaming</p>', '<p>Checkpoint</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'You are currently working to ingest millions of files that get uploaded to the cloud object storage for consumption, and you are asked to build a process to ingest this data, the schema of the file is expected to change over time, and the ingestion process should be able to handle these changes automatically. Which of the following method can be used to ingest the data incrementally?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292612, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>At the end of the inventory process, a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to command to load the data, fill in the blanks for successful execution of below code.&nbsp;</p><p><br></p><pre class="prettyprint linenums">spark.readStream\n.format("cloudfiles")\n.option("_______",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("_______", "true")\n.table(table_name))</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema. </p><p><br></p><p>Here is the end to end syntax of streaming ELT, below link contains complete options <a href="https://docs.databricks.com/ingestion/auto-loader/options.html">Auto Loader options | Databricks on AWS</a></p><p><br></p><pre class="prettyprint linenums">spark.readStream\n.format("cloudfiles") # Returns a stream data source, reads data as it arrives based on the trigger.\n.option("cloudfiles.format","csv") # Format of the incoming files\n.option("cloudfiles.schemalocation", "dbfs:/location/checkpoint/")  The location to store the inferred schema and subsequent changes\n.load(data_source)\n.writeStream\n.option("checkpointlocation","dbfs:/location/checkpoint/") # The location of the stream’s checkpoint\n.option("mergeSchema", "true") # Infer the schema across multiple files and to merge the schema of each file. Enabled by default for Auto Loader when inferring the schema.\n.table(table_name)) # target table</pre><p><br></p>', 'answers': ['<p> format, checkpointlocation, schemalocation, overwrite</p>', '<p> cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'At the end of the inventory process, a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to command to load the data, fill in the blanks for successful execution of below code.&nbsp;spark.readStream\n.format("cloudfiles")\n.option("_______",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("_______", "true")\n.table(table_name))', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292614, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the purpose of the bronze layer in a Multi-hop architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Provides efficient storage and querying of full unprocessed history of data</p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p><br></p><p>Bronze Layer:&nbsp;</p><ol><li><p>Raw copy of ingested data</p></li><li><p>Replaces traditional data lake</p></li><li><p>Provides efficient storage and querying of full, unprocessed history of data</p></li><li><p>No schema is applied at this layer</p></li></ol><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p><p><br></p>', 'answers': ['<p>Can be used to eliminate duplicate records</p>', '<p> Used as a data source for Machine learning applications. </p>', '<p>Perform data quality checks, corrupt data quarantined</p>', '<p>Contains aggregated data that is to be consumed into Silver</p>', '<p>Provides efficient storage and querying of full unprocessed history of data</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of the bronze layer in a Multi-hop architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292616, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the purpose of a silver layer in Multi hop architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, A schema is enforced, with data quality checks.</p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Silver Layer:&nbsp;</p><p><br></p><ol><li><p>Reduces data storage complexity, latency, and redundency </p></li><li><p>Optimizes ETL throughput and analytic query performance</p></li><li><p>Preserves grain of original data (without aggregation)</p></li><li><p>Eliminates duplicate records</p></li><li><p>production schema enforced</p></li><li><p>Data quality checks, quarantine corrupt data</p></li></ol><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p>', 'answers': ['<p>Replaces a traditional data lake</p>', '<p> Efficient storage and querying of full and unprocessed history of data</p>', '<p>A schema is enforced, with data quality checks.</p>', '<p>Refined views with aggregated data</p>', '<p>Optimized query performance for business-critical data</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of a silver layer in Multi hop architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292618, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the purpose of a gold layer in Multi-hop architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Powers ML applications, reporting, dashboards and adhoc reports.</p><p>Review the below link for more info, </p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Gold Layer:</p><p><br></p><ol><li><p>Powers Ml applications, reporting, dashboards, ad hoc analytics </p></li><li><p>Refined views of data, typically with aggregations</p></li><li><p>Reduces strain on production systems</p></li><li><p>Optimizes query performance for business-critical data</p></li></ol><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p>', 'answers': ['<p>Optimizes ETL throughput and analytic query performance</p>', '<p>Eliminate duplicate records</p>', '<p>Preserves grain of original data, without any aggregations</p>', '<p>Data quality checks and schema enforcement</p>', '<p>Powers ML applications, reporting, dashboards and adhoc reports.</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of a gold layer in Multi-hop architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292620, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently asked to work on building a data pipeline, you have noticed that you are currently working on a very large scale ETL many data dependencies, which of the following tools can be used to address this problem?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, DELTA&nbsp;LIVE&nbsp;TABLES</p><p><br></p><p>DLT&nbsp;simplifies data dependencies by building DAG-based joins between live tables. Here is a view of how the dag looks with data dependencies without additional meta data, </p><p><br></p><pre class="prettyprint linenums">create or replace live view customers \nselect *&nbsp;from customers;\n\ncreate or replace live view sales_orders_raw\nselect *&nbsp;from sales_orders;\n\ncreate or replace live view sales_orders_cleaned \nas\nselect sales.*&nbsp;from \nlive.sales_orders_raw s\n join live.customers c \non c.customer_id = s.customer_id\nwhere c.city = \'LA\';\n\ncreate or replace live table sales_orders_in_la\nselect *&nbsp;from sales_orders_cleaned;\n</pre><p>Above code creates below dag</p><p><img src="https://docs.databricks.com/_images/dlt-sales-graph.png"></p><p><br></p><p>Documentation on DELTA&nbsp;LIVE&nbsp;TABLES, </p><p>https://databricks.com/product/delta-live-tables</p><p>https://databricks.com/blog/2022/04/05/announcing-generally-availability-of-databricks-delta-live-tables-dlt.html</p><p><br></p><p>DELTA&nbsp;LIVE&nbsp;TABLES, addresses below challenges when building ETL&nbsp;processes</p><p><br></p><ol><li><p>Complexities of large scale ETL</p><ol><li><p>Hard to build and maintain dependencies</p></li><li><p>Difficult to switch between batch and stream</p></li></ol></li><li><p>Data quality and governance</p><ol><li><p>Difficult to monitor and enforce data quality</p></li><li><p>Impossible to trace data lineage</p></li></ol></li><li><p>Difficult pipeline operations</p><ol><li><p>Poor observability at granular data level</p></li><li><p>Error handling and recovery is laborious</p></li></ol></li></ol><p><br></p>', 'answers': ['<p> AUTO LOADER</p>', '<p> JOBS and TASKS</p>', '<p>SQL Endpoints</p>', '<p>DELTA LIVE TABLES</p>', '<p>STRUCTURED STREAMING with MULTI HOP</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data Processing', 'question_plain': 'You are currently asked to work on building a data pipeline, you have noticed that you are currently working on a very large scale ETL many data dependencies, which of the following tools can be used to address this problem?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292622, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How do you create a delta live tables pipeline and deploy using DLT&nbsp;UI?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the notebook with DLT code. </p><p>https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-quickstart.html</p><p><br></p><p><br></p>', 'answers': ['<p>Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the notebook with DLT code. </p>', '<p>Under Cluster UI, select SPARK UI and select Structured Streaming and click create pipeline and select the notebook with DLT code. </p>', '<p>There is no UI, you can only setup DELTA LIVE TABLES using Python and SQL API and select the notebook with DLT code. </p>', '<p>Use VS Code and download DBX plugin, once the plugin is loaded you can build DLT pipelines and select the notebook with DLT code. </p>', '<p>Within the Workspace UI, click on SQL Endpoint, select Delta Live tables and create pipelinea and select the notebook with DLT code. </p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'How do you create a delta live tables pipeline and deploy using DLT&nbsp;UI?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292624, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are noticing job cluster is taking 6 to 8 mins to start which is delaying your job to finish on time, what steps you can take to reduce the amount of time cluster startup time</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': "<p>The answer is, Use cluster pools to reduce the startup time of the jobs. </p><p>Cluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM&nbsp;are grabbed from the pool. Note: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only billed once VM&nbsp;is allocated to a cluster. </p><p><br></p><p>Here is a demo of how to setup and follow some best practices, </p><p>https://www.youtube.com/watch?v=FVtITxOabxg&amp;ab_channel=DatabricksAcademy</p><p><br></p><p><br></p>", 'answers': ['<p>Setup a second job ahead of first job to start the cluster, so the cluster is ready with resources when the job starts</p>', '<p>Use All purpose cluster instead to reduce cluster start up time</p>', '<p>Reduce the size of the cluster, smaller the cluster size shorter it takes to start the cluster</p>', '<p>Use cluster pools to reduce the startup time of the jobs</p>', '<p> Use SQL endpoints to reduce the startup time</p>']}, 'correct_response': ['d'], 'section': 'Building Production Pipelines', 'question_plain': 'You are noticing job cluster is taking 6 to 8 mins to start which is delaying your job to finish on time, what steps you can take to reduce the amount of time cluster startup time', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292626, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Data engineering team has a job currently setup to run a task load data into a reporting table every day at 8: 00 AM takes about 20 mins, Operations teams are planning to use that data to run a second job, so they access latest complete set of data. What is the best to way to orchestrate this job setup?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Add Operation reporting task in the same job and set the operations reporting task to depend on Data Engineering task. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_03-44-18-a81719b1f13e005ad04fc8e7862fecf4.jpg"><p>Job View </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_03-44-18-b338e369557faa0c0566743e7bf3c3bd.jpg"><p><br></p>', 'answers': ['<p>Add Operation reporting task in the same job and set the Data Engineering task to depend on Operations reporting task</p>', '<p>Setup a second job to run at 8:20 AM in the same workspace</p>', '<p>Add Operation reporting task in the same job and set the operations reporting task to depend on Data Engineering task</p>', '<p>Use Auto Loader to run every 20 mins to read the initial table and set the trigger to once and create a second job</p>', '<p>Setup a Delta live to table based on the first table, set the job to run in continuous mode</p>']}, 'correct_response': ['c'], 'section': 'Building Production Pipelines', 'question_plain': 'Data engineering team has a job currently setup to run a task load data into a reporting table every day at 8: 00 AM takes about 20 mins, Operations teams are planning to use that data to run a second job, so they access latest complete set of data. What is the best to way to orchestrate this job setup?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292628, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The data engineering team noticed that one of the job normally finishes in 15 mins but gets stuck randomly when reading remote databases due to a network packet drop, which of the following steps can be used to improve the stability of the job?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Modify the task, to include time out to kill the job if it runs more than 15 mins.</p><p>https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs#timeout</p>', 'answers': ['<p> Use Databrick REST API to monitor long running jobs and issue a kill command</p>', '<p>Use Jobs runs, active runs UI section to monitor and kill long running job</p>', '<p>Modify the task, to include a timeout to kill the job if it runs more than 15 mins.</p>', '<p>Use Spark job time out setting in the Spark UI</p>', '<p>Use Cluster timeout setting in the Job cluster UI</p>']}, 'correct_response': ['c'], 'section': 'Building Production Pipelines', 'question_plain': 'The data engineering team noticed that one of the job normally finishes in 15 mins but gets stuck randomly when reading remote databases due to a network packet drop, which of the following steps can be used to improve the stability of the job?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292630, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following programming languages can be used to build a Databricks SQL dashboard?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is SQL</p>', 'answers': ['<p>Python</p>', '<p>Scala</p>', '<p>SQL</p>', '<p>R</p>', '<p>All of the above</p>']}, 'correct_response': ['c'], 'section': 'Building Production Pipelines', 'question_plain': 'Which of the following programming languages can be used to build a Databricks SQL dashboard?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292632, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The data analyst team had put together queries that identify items that are out of stock based on orders and replenishment but when they run all together for final output the team noticed it takes a really long time, you were asked to look at the reason why queries are running slow and identify steps to improve the performance and when you looked at it you noticed all the code queries are running sequentially and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?</p><p>Here is the example query</p><pre class="prettyprint linenums">--- Get order summary \ncreate or replace table orders_summary\nas \nselect product_id, sum(order_count) order_count\nfrom \n (\n  select product_id,order_count&nbsp;from orders_instore\n  union all \n  select product_id,order_count&nbsp;from orders_online\n )\ngroup by product_id\n-- get supply summary \ncreate or repalce tabe supply_summary\nas \nselect product_id, sum(supply_count) supply_count\nfrom supply\ngroup by product_id\n\n-- get on hand based on orders summary and supply summary\n\nwith stock_cte\nas (\nselect nvl(s.product_id,o.product_id) as product_id,\n\t nvl(supply_count,0) -  nvl(order_count,0) as on_hand\nfrom supply_summary s \nfull outer join orders_summary o\n        on s.product_id = o.product_id\n)\nselect *\nfrom \nstock_cte\nwhere on_hand = 0 </pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is to increase the cluster size of the SQL&nbsp;Endpoint,&nbsp; here queries are running sequentially and since the single query can not span more than one cluster adding more clusters won\'t improve the query but rather increasing the cluster size will improve performance so it can use additional compute in a warehouse.</p><p><br></p><p><strong><em>In the exam please note that additional context will not be given instead you have to look for cue words or need to understand if the queries are running sequentially or concurrently. if the queries are running sequentially then scale up(more nodes) if the queries are running concurrently (more users)&nbsp;then scale out(more clusters).</em></strong></p><p><br></p><p>Below is the snippet from&nbsp;Azure, as you can see by increasing the cluster size you are able to add more worker nodes. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-26_17-50-16-75c6575d7a31fcf71e3268b28be10864.jpg"></p><p><br></p><p>SQL endpoint scales horizontally(scale-out) and vertically (scale-up), you have to understand when to use what. </p><p><br></p><p><strong>Scale-up</strong>-&gt; Increase the size of the cluster from x-small to small, to medium, X Large.... </p><p>If you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the cluster will improve the performance. </p><p><br></p><p><strong>Scale-out</strong> -&gt;&nbsp;Add more clusters, change max number of clusters </p><p>If you are trying to improve the throughput, being able to run as many queries as possible then having an additional cluster(s) will improve the performance. </p><p><br></p><p>SQL&nbsp;endpoint</p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_04-06-52-44d9cc981d7bb2c2f97aaa85a861cded.jpg"></p><p><br></p><p><br></p>', 'answers': ['<p>Turn on the Serverless feature for the SQL endpoint.</p>', '<p>Increase the maximum bound of the SQL endpoint’s scaling range.</p>', '<p>Increase the cluster size of the SQL endpoint.</p>', '<p>Turn on the Auto Stop feature for the SQL endpoint.</p>', '<p>Turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to “Reliability Optimized.”</p>']}, 'correct_response': ['c'], 'section': 'Building Production Pipelines', 'question_plain': 'The data analyst team had put together queries that identify items that are out of stock based on orders and replenishment but when they run all together for final output the team noticed it takes a really long time, you were asked to look at the reason why queries are running slow and identify steps to improve the performance and when you looked at it you noticed all the code queries are running sequentially and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?Here is the example query--- Get order summary \ncreate or replace table orders_summary\nas \nselect product_id, sum(order_count) order_count\nfrom \n (\n  select product_id,order_count&nbsp;from orders_instore\n  union all \n  select product_id,order_count&nbsp;from orders_online\n )\ngroup by product_id\n-- get supply summary \ncreate or repalce tabe supply_summary\nas \nselect product_id, sum(supply_count) supply_count\nfrom supply\ngroup by product_id\n\n-- get on hand based on orders summary and supply summary\n\nwith stock_cte\nas (\nselect nvl(s.product_id,o.product_id) as product_id,\n\t nvl(supply_count,0) -  nvl(order_count,0) as on_hand\nfrom supply_summary s \nfull outer join orders_summary o\n        on s.product_id = o.product_id\n)\nselect *\nfrom \nstock_cte\nwhere on_hand = 0', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292634, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The operations team is interested in monitoring the recently launched product, team wants to set up an email alert when the number of units sold increases by more than 10,000 units. They want to monitor this every 5 mins.</p><p><br></p><p>Fill in the below blanks to finish the steps we need to take</p><p><br></p><p>· Create ___ query that calculates total units sold</p><p>· Setup ____ with query on trigger condition Units Sold &gt; 10,000</p><p>· Setup ____ to run every 5 mins</p><p>· Add destination ______</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <strong>SQL, Alert, Refresh, email address</strong></p><p><br></p><p>Here the steps from Databricks documentation, </p><p><br></p><p><a href="https://docs.databricks.com/sql/user/alerts/index.html#id2"><strong>Create an alert</strong></a></p><p>Follow these steps to create an alert on a single column of a query.</p><ol><li><p>Do one of the following:</p><ul><li><p>Click <img src="https://docs.databricks.com/_images/create-icon.png"> <strong>Create</strong> in the sidebar and select <strong>Alert</strong>.</p></li><li><p>Click <img src="https://docs.databricks.com/_images/alerts-icon.png"> <strong>Alerts</strong> in the sidebar and click the <strong>+ New Alert</strong> button.</p></li></ul></li><li><p>Search for a target query.</p><p><img src="https://docs.databricks.com/_images/new-alert-query-search.png"></p><p>To alert on multiple columns, you need to modify your query. See <a href="https://docs.databricks.com/sql/user/alerts/index.html#alert-multiple-columns">Alert on multiple columns</a>.</p></li><li><p>In the <strong>Trigger when</strong> field, configure the alert.</p><ul><li><p>The <strong>Value column</strong> drop-down controls which field of your query result is evaluated.</p></li><li><p>The <strong>Condition</strong> drop-down controls the logical operation to be applied.</p></li><li><p>The <strong>Threshold</strong> text input is compared against the Value column using the Condition you specify.</p></li></ul><p><img src="https://docs.databricks.com/_images/alert_settings.png"></p><p><strong>Note</strong></p><p>If a target query returns multiple records, Databricks SQL alerts act on the first one. As you change the Value column setting, the current value of that field in the top row is shown beneath it.</p></li><li><p>In the <strong>When triggered, send notification</strong> field, select how many notifications are sent when your alert is triggered:</p><ul><li><p><strong>Just once</strong>: Send a notification when the <a href="https://docs.databricks.com/sql/user/alerts/index.html#view-alerts">alert status</a> changes from <code>OK</code> to <code>TRIGGERED</code>.</p></li><li><p><strong>Each time alert is evaluated</strong>: Send a notification whenever the alert status is <code>TRIGGERED</code> regardless of its status at the previous evaluation.</p></li><li><p><strong>At most every</strong>: Send a notification whenever the alert status is <code>TRIGGERED</code> at a specific interval. This choice lets you avoid notification spam for alerts that trigger often.</p></li></ul><p>Regardless of which notification setting you choose, you receive a notification whenever the status goes from <code>OK</code> to <code>TRIGGERED</code> or from <code>TRIGGERED</code> to <code>OK</code>. The schedule settings affect how many notifications you will receive if the status remains <code>TRIGGERED</code> from one execution to the next. For details, see <a href="https://docs.databricks.com/sql/user/alerts/index.html#notification-frequency">Notification frequency</a>.</p></li><li><p>In the <strong>Template</strong> drop-down, choose a template:</p><ul><li><p><strong>Use default template</strong>: Alert notification is a message with links to the Alert configuration screen and the Query screen.</p></li><li><p><strong>Use custom template</strong>: Alert notification includes more specific information about the alert.</p><ol><li><p>A box displays, consisting of input fields for subject and body. Any static content is valid, and you can incorporate built-in template variables:</p><ul><li><p><code>ALERT_STATUS</code>: The evaluated alert status (string).</p></li><li><p><code>ALERT_CONDITION</code>: The alert condition operator (string).</p></li><li><p><code>ALERT_THRESHOLD</code>: The alert threshold (string or number).</p></li><li><p><code>ALERT_NAME</code>: The alert name (string).</p></li><li><p><code>ALERT_URL</code>: The alert page URL (string).</p></li><li><p><code>QUERY_NAME</code>: The associated query name (string).</p></li><li><p><code>QUERY_URL</code>: The associated query page URL (string).</p></li><li><p><code>QUERY_RESULT_VALUE</code>: The query result value (string or number).</p></li><li><p><code>QUERY_RESULT_ROWS</code>: The query result rows (value array).</p></li><li><p><code>QUERY_RESULT_COLS</code>: The query result columns (string array).</p></li></ul><p>An example subject, for instance, could be: <code>Alert "{{ALERT_NAME}}" changed status to {{ALERT_STATUS}}</code>.</p></li><li><p>Click the <strong>Preview</strong> toggle button to preview the rendered result.</p><p><strong>Important</strong></p><p>The preview is useful for verifying that template variables are rendered correctly. It is not an accurate representation of the eventual notification content, as each alert destination can display notifications differently.</p></li><li><p>Click the <strong>Save Changes</strong> button.</p></li></ol></li></ul></li><li><p>In <strong>Refresh</strong>, set a refresh schedule. An alert’s refresh schedule is independent of the query’s refresh schedule.</p><ul><li><p>If the query is a <strong>Run as owner</strong> query, the query runs using the query owner’s credential on the alert’s refresh schedule.</p></li><li><p>If the query is a <strong>Run as viewer</strong> query, the query runs using the alert creator’s credential on the alert’s refresh schedule.</p></li></ul></li><li><p>Click <strong>Create Alert</strong>.</p></li><li><p>Choose an <a href="https://docs.databricks.com/sql/admin/alert-destinations.html">alert destination</a>.</p><p><strong>Important</strong></p><p>If you skip this step you <em>will not</em> be notified when the alert is triggered.</p><p><img src="https://docs.databricks.com/_images/alert_destination.png"></p></li></ol>', 'answers': ['<p>Python, Job, SQL Cluster, email address</p>', '<p>SQL, Alert, Refresh, email address</p>', '<p>SQL, Job, SQL Cluster, email address</p>', '<p>SQL, Job, Refresh, email address</p>', '<p>Python, Job, Refresh, email address</p>']}, 'correct_response': ['b'], 'section': 'Building Production Pipelines', 'question_plain': 'The operations team is interested in monitoring the recently launched product, team wants to set up an email alert when the number of units sold increases by more than 10,000 units. They want to monitor this every 5 mins.Fill in the below blanks to finish the steps we need to take· Create ___ query that calculates total units sold· Setup ____ with query on trigger condition Units Sold &gt; 10,000· Setup ____ to run every 5 mins· Add destination ______', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292636, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The marketing team is launching a new campaign to monitor the performance of the new campaign for the first two weeks, they would like to set up a dashboard with a refresh schedule to run every 5 minutes, which of the below steps can be taken to reduce of the cost of this refresh over time?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Setup the dashboard refresh schedule to end in two weeks</p>', 'answers': ['<p>Reduce the size of the SQL Cluster size</p>', '<p>Reduce the max size of auto scaling from 10 to 5</p>', '<p>Setup the dashboard refresh schedule to end in two weeks</p>', '<p>Change the spot instance policy from reliability optimized to cost optimized</p>', '<p>Always use X-small cluster</p>']}, 'correct_response': ['c'], 'section': 'Building Production Pipelines', 'question_plain': 'The marketing team is launching a new campaign to monitor the performance of the new campaign for the first two weeks, they would like to set up a dashboard with a refresh schedule to run every 5 minutes, which of the below steps can be taken to reduce of the cost of this refresh over time?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292638, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following tool provides Data Access control, Access Audit, Data Lineage, and Data discovery?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Unity Catalog</p>', 'answers': ['<p>DELTA LIVE Pipelines</p>', '<p>Unity Catalog</p>', '<p>Data Governance</p>', '<p> DELTA lake</p>', '<p>Lakehouse</p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'Which of the following tool provides Data Access control, Access Audit, Data Lineage, and Data discovery?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292640, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Data engineering team is required to share the data with Data science team and both the teams are using different workspaces in the same organizationwhich of the following techniques can be used to simplify sharing data across?</p><p>*Please note the question is asking how data is shared within an organization across multiple workspaces. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is the Unity catalog. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_04-16-41-7a1a2cb776765ab4ad2c664e870db8fa.JPG"></p><p><br></p><p>Unity Catalog works at the Account level, it has the ability to create a meta store and attach that meta store to many workspaces </p><p>see the below diagram to understand how Unity Catalog Works, as you can see a metastore can now be shared with both workspaces using Unity Catalog, prior to Unity Catalog the options was to use single cloud object storage manually mount in the second databricks workspace, and you can see here Unity Catalog really simplifies that. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-05_12-46-33-bb9681fb915f99ae98e6d76b1b2aa7a9.jpeg"><p><strong><em>sorry for the inconvenience watermark was added because other people on Udemy are copying my questions and images. </em></strong></p><p><br></p><p>Review product features</p><p>https://databricks.com/product/unity-catalog</p>', 'answers': ['<p>Data Sharing</p>', '<p>Unity Catalog</p>', '<p>DELTA lake</p><p><br></p>', '<p>Use a single storage location</p>', '<p>DELTA LIVE Pipelines</p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'Data engineering team is required to share the data with Data science team and both the teams are using different workspaces in the same organizationwhich of the following techniques can be used to simplify sharing data across?*Please note the question is asking how data is shared within an organization across multiple workspaces.', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292642, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A newly joined team member John Smith in the Marketing team who currently does not have any access to the data requires read access to customers table, which of the following statements can be used to grant access.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com</p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges">Data object privileges - Azure Databricks | Microsoft Docs</a></p>', 'answers': ['<p><code>GRANT SELECT, USAGE TO john.smith@marketing.com ON TABLE customers</code> </p>', '<p><code>GRANT READ, USAGE TO john.smith@marketing.com ON TABLE customers</code> </p>', '<p><code>GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com</code> </p>', '<p><code>GRANT READ, USAGE ON TABLE customers TO john.smith@marketing.com</code> </p>', '<p><code>GRANT READ, USAGE ON customers TO john.smith@marketing.com</code> </p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': 'A newly joined team member John Smith in the Marketing team who currently does not have any access to the data requires read access to customers table, which of the following statements can be used to grant access.', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292644, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Grant full privileges to new marketing user Kevin Smith to table sales</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is<strong> GRANT ALL&nbsp;PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com</strong></p><p><br></p><p>GRANT &lt;privilege&gt; ON &lt;securable_type&gt; &lt;securable_name&gt; TO &lt;principal&gt;</p><p><br></p><p>Here are the available privileges and ALL&nbsp;Privileges gives full access to an object. </p><p><br></p><p><strong>Privileges</strong></p><ul><li><p><code>SELECT</code>: gives read access to an object.</p></li><li><p><code>CREATE</code>: gives ability to create an object (for example, a table in a schema).</p></li><li><p><code>MODIFY</code>: gives ability to add, delete, and modify data to or from an object.</p></li><li><p><code>USAGE</code>: does not give any abilities, but is an additional requirement to perform any action on a schema object.</p></li><li><p><code>READ_METADATA</code>: gives ability to view an object and its metadata.</p></li><li><p><code>CREATE_NAMED_FUNCTION</code>: gives ability to create a named UDF in an existing catalog or schema.</p></li><li><p><code>MODIFY_CLASSPATH</code>: gives ability to add files to the Spark class path.</p></li><li><p><strong>ALL PRIVILEGES</strong>: gives all privileges (is translated into all the above privileges).</p></li></ul><p><br></p>', 'answers': ['<p><code>GRANT FULL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales</code> </p>', '<p><code>GRANT ALL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales</code> </p>', '<p><code>GRANT FULL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com</code> </p>', '<p><code>GRANT ALL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com</code> </p>', '<p><code>GRANT ANY PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com</code> </p>']}, 'correct_response': ['d'], 'section': 'Data Governance', 'question_plain': 'Grant full privileges to new marketing user Kevin Smith to table sales', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292646, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following locations in the Databricks product architecture hosts the notebooks and jobs?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Control Pane,</p><p><br></p><p>Databricks operates most of its services out of a control plane and a data plane, <em>please note serverless features like SQL Endpoint and DLT compute use shared compute in Control pane.</em></p><p><br></p><p><strong>Control Plane: </strong>Stored in Databricks Cloud Account</p><p><br></p><ul><li><p>The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.</p></li></ul><p><strong>Data Plane:</strong> Stored in Customer Cloud Account</p><p><br></p><ul><li><p>The data plane is managed by your Azure account and is where your data resides. This is also where data is processed. You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure account to ingest data or for storage.</p></li></ul><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-06_04-30-42-1860e718dc3e7dfa5bb0f0d0afc01eac.jpg"></p><p><br></p><p><br></p>', 'answers': ['<p>Data plane</p>', '<p>Control plane</p>', '<p>Databricks Filesystem</p>', '<p>JDBC data source</p>', '<p>Databricks web application</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following locations in the Databricks product architecture hosts the notebooks and jobs?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292648, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION FAIL</p><p>What is the expected behavior when a batch of data containing data that violates these constraints is processed?</p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is&nbsp; Records that violate the expectation cause the job to fail. </p><p>Delta live tables support three types of expectations to fix bad data in DLT&nbsp;pipelines </p><p>Review below example code to examine these expectations,&nbsp; </p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_04-52-20-291618c7a5e4d2e8820d6fc07f4acc7d.jpg"></p><p><br></p><p><br></p><p><strong>Invalid records:</strong></p><p>Use the expect operator when you want to keep records that violate the expectation. Records that violate the expectation are added to the target dataset along with valid records:</p><p><br></p><p>SQL</p><p>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; \'2020-01-01\') </p><p><br></p><p><strong>Drop invalid records:</strong></p><p>Use the expect or drop operator to prevent the processing of invalid records. Records that violate the expectation are dropped from the target dataset:</p><p><br></p><p>SQL</p><p>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; \'2020-01-01\')&nbsp; ON VIOLATION DROP ROW</p><p><br></p><p><strong><em>Fail on invalid records:</em></strong></p><p>When invalid records are unacceptable, use the expect or fail operator to halt execution immediately when a record fails validation. If the operation is a table update, the system atomically rolls back the transaction:</p><p><br></p><p>SQL</p><p><em>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; \'2020-01-01\') ON VIOLATION FAIL UPDATE</em></p>', 'answers': ['<p>Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p><p><br></p>', '<p>Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.</p>', '<p> Records that violate the expectation cause the job to fail</p>', '<p>Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.</p>', '<p>Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': "A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION FAILWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 52292578, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are still noticing slowness in query after performing optimize which helped you to resolve the small files problem, the column(transactionId) you are using to filter the data has high cardinality and auto incrementing number. Which delta optimization can you enable to filter data effectively based on this column?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><p>The answer is, <strong>perform Optimize with Z-order by transactionid</strong></p><p><br></p><p>Here is a simple explanation of how Z-order works, once the data is naturally ordered, when a flle is scanned it only brings the data it needs into spark\'s memory</p><p><br></p><p>Based on the column min and max it knows which data files needs to be scanned. </p><p><img src="https://databricks.com/wp-content/uploads/2018/07/image7.gif"></p><p><br></p><p><img src="https://databricks.com/wp-content/uploads/2018/07/Screen-Shot-2018-07-30-at-2.03.55-PM.png"></p>', 'answers': ['<p>Create BLOOM FLTER index on the transactionId</p>', '<p>Perform Optimize with Zorder on transactionId</p>', '<p>transactionId has high cardinality, you cannot enable any optimization.</p>', '<p> Increase the cluster size and enable delta optimization</p>', '<p> Increase the driver size and enable delta optimization</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are still noticing slowness in query after performing optimize which helped you to resolve the small files problem, the column(transactionId) you are using to filter the data has high cardinality and auto incrementing number. Which delta optimization can you enable to filter data effectively based on this column?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292580, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>If you create a database sample_db with the statement <code>CREATE DATABASE sample_db</code> what will be the <strong>default </strong>location of the database in DBFS?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The Answer is dbfs:/user/hive/warehouse this is the default location where spark stores user databases, the default can be changed using <code>spark.sql.warehouse.dir</code> a parameter.&nbsp; You can also provide a custom location using the LOCATION&nbsp;keyword. </p><p><br></p><p>Here is how this works, </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-01_05-10-54-5156b2d9bded0492d3a4f233601f99f3.jpg"></p><p>Default location</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-01_05-10-54-3a14802cd406fc80733244a9e33f0919.jpg"></p><p><br></p><p><br></p><p>FYI,&nbsp; This can be changed used using cluster spark config or session config. </p><p><br></p><p>Modify spark.sql.warehouse.dir location to change the default location</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-01_05-10-55-b75de6d48e9b03d498670bcf3702eed2.jpg"></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-09-01_05-10-55-0459ecfed7686c5b30c1abf400240f65.jpg"></p><p><br></p><p><br></p>', 'answers': ['<p>Default location, DBFS:/user/</p>', '<p>Default location, /user/db/</p>', '<p>Default Storage account</p>', '<p>Statement fails “Unable to create database without location”</p>', '<p>Default Location, dbfs:/user/hive/warehouse</p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'If you create a database sample_db with the statement CREATE DATABASE sample_db what will be the default location of the database in DBFS?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292582, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following results in the creation of an external table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION ‘/mnt/delta/transactions’</p><p><br></p><p>Anytime a table is created using Location it is considered an external table, below is the current syntax.</p><p>Syntax</p><p>CREATE TABLE table_name ( column column_data_type…) USING format LOCATION “dbfs:/”</p>', 'answers': ['<p><code>CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION EXTERNAL</code> </p>', '<p><code>CREATE TABLE transactions (id int, desc string)</code> </p>', '<p><code>CREATE EXTERNAL TABLE transactions (id int, desc string)</code> </p>', '<p><code>CREATE TABLE transactions (id int, desc string) TYPE EXTERNAL</code> </p>', "<p><code>CREATE TABLE transactions (id int, desc string) LOCATION '/mnt/delta/transactions'</code> </p>"]}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following results in the creation of an external table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292584, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When you drop an external DELTA&nbsp;table using the SQL&nbsp;Command <code>DROP TABLE table_name</code>, how does it impact metadata(delta log, history), and data stored in the storage?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is&nbsp; Drops table from metastore, but keeps metadata and data in storage.</p><p><br></p><p>When an external table is dropped, only the table definition is dropped from metastore everything including data and metadata(Delta transaction log, time travel history) remains in the storage. Delta log is considered as part of metadata because if you drop a column in a delta table(managed or external) the column is not physically removed from the parquet files rather it is recorded in the delta log. The delta log becomes a key metadata layer for a Delta table to work. </p><p><br></p><p>Please see the below image to compare the external delta table and managed delta table and how they differ in how they are created and what happens if you drop the table.</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-21_20-27-55-57480da7209b6c3ae64a5193f869b3e4.jpeg"></p><p><br></p><p><br></p>', 'answers': ['<p>Drops table from metastore, metadata(delta log, history)and data in storage</p>', '<p>Drops table from metastore, data but keeps metadata(delta log, history) in storage</p>', '<p>Drops table from metastore, metadata(delta log, history)but keeps the data in storage</p>', '<p> Drops table from metastore, but keeps metadata(delta log, history)and data in storage</p>', '<p>Drops table from metastore and data in storage but keeps metadata(delta log, history)</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'When you drop an external DELTA&nbsp;table using the SQL&nbsp;Command DROP TABLE table_name, how does it impact metadata(delta log, history), and data stored in the storage?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292586, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following is a true statement about the global temporary view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, A global temporary view is available only on the cluster it was created. </p><p><br></p><p>Two types of temporary views can be created Session scoped and Global </p><ul><li><p>A session scoped temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if a notebook is detached and re attached the temporary view is lost. </p></li><li><p>A global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost. </p></li></ul><p><br></p>', 'answers': ['<p> A global temporary view is available only on the cluster it was created, when the cluster restarts global temporary view is automatically dropped. </p>', '<p>A global temporary view is available on all clusters for a given workspace</p>', '<p>A global temporary view persists even if the cluster is restarted</p>', '<p>A global temporary view is stored in a user database</p>', '<p>A global temporary view is automatically dropped after 7 days</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following is a true statement about the global temporary view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292588, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are trying to create an object by joining two tables that and it is accessible to data scientist’s team, so it does not get dropped if the cluster restarts or if the notebook is detached. What type of object are you trying to create?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is View, A view can be used to join multiple tables but also persist into meta stores so others can accesses it </p>', 'answers': ['<p> Temporary view</p>', '<p>Global Temporary view</p>', '<p>Global Temporary view with cache option</p>', '<p>External view</p>', '<p>View</p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are trying to create an object by joining two tables that and it is accessible to data scientist’s team, so it does not get dropped if the cluster restarts or if the notebook is detached. What type of object are you trying to create?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292590, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the best way to query external csv files located on DBFS Storage to inspect the data using SQL?&nbsp; </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is,&nbsp; SELECT * FROM CSV. ‘dbfs:/location/csv_files/’</p><p>you can query external files stored on the storage using below syntax</p><p>SELECT&nbsp;*&nbsp;FROM&nbsp;format.`/Location`</p><p>format -&nbsp;CSV,&nbsp; JSON, PARQUET, TEXT</p><p><br></p>', 'answers': ["<p><code>SELECT * FROM 'dbfs:/location/csv_files/' FORMAT = 'CSV'</code> </p>", "<p><code>SELECT CSV. * from 'dbfs:/location/csv_files/'</code> </p>", "<p><code>SELECT * FROM CSV. 'dbfs:/location/csv_files/'</code> </p>", '<p>You can not query external files directly, us COPY INTO to load the data into a table first</p>', "<p><code>SELECT * FROM 'dbfs:/location/csv_files/' USING CSV</code> </p>"]}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What is the best way to query external csv files located on DBFS Storage to inspect the data using SQL?&nbsp;', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292592, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Direct query on external files limited options, create external tables for CSV files with header and pipe delimited CSV files, fill in the blanks to complete the create table statement</p><p><br></p><p>CREATE TABLE sales (id int, unitsSold int, price FLOAT, items STRING)</p><p>&nbsp; &nbsp; ________</p><p>&nbsp; &nbsp; ________</p><p><br></p><p>LOCATION “dbfs:/mnt/sales/*.csv”</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is </p><p><br></p><p>USING&nbsp;CSV</p><p>OPTIONS ( header =“true”, delimiter = ”|”)</p><p><br></p><p>Here is the syntax to create an external table with additional options</p><p><br></p><p>CREATE TABLE table_name (col_name1 col_typ1,..)</p><p>USING data_source</p><p>OPTIONS (key=’value’, key2=vla2)</p><p>LOCATION = “/location“</p>', 'answers': ['<p>FORMAT CSV</p><p>OPTIONS ( “true”,”|”)</p>', '<p>USING CSV</p><p>TYPE ( “true”,”|”)</p>', '<p>USING&nbsp;CSV</p><p>OPTIONS ( header =“true”, delimiter = ”|”)</p>', '<p>FORMAT CSV</p><p>FORMAT TYPE ( header =“true”, delimiter = ”|”)</p>', '<p>FORMAT CSV</p><p>TYPE ( header =“true”, delimiter = ”|”)</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Direct query on external files limited options, create external tables for CSV files with header and pipe delimited CSV files, fill in the blanks to complete the create table statementCREATE TABLE sales (id int, unitsSold int, price FLOAT, items STRING)&nbsp; &nbsp; ________&nbsp; &nbsp; ________LOCATION “dbfs:/mnt/sales/*.csv”', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292594, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What could be the expected output of query SELECT COUNT (DISTINCT *) FROM user on this table</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-06_00-16-46-6a3796f2f4d54cf5eefd99e58e52160a.JPG">', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is 2, </p><p><br></p><p>Count(DISTINCT&nbsp;*)&nbsp;removes rows with any column with a NULL&nbsp;value</p><p><br></p>', 'answers': ['<p>3</p>', '<p>2</p>', '<p>1</p>', '<p>0</p>', '<p>NULL</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What could be the expected output of query SELECT COUNT (DISTINCT *) FROM user on this table', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292596, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on a table called <strong>orders </strong>which contains data for 2021 and you have the second table called <strong>orders_archive </strong>which contains data for 2020, you need to combine the data from two tables and there could be a possibility of the same rows between both the tables, you are looking to combine the results from both the tables and eliminate the duplicate rows, which of the following SQL&nbsp;statements helps you accomplish this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is <code>SELECT * FROM orders UNION SELECT * FROM orders_archive</code></p><p><br></p><p>UNION and UNION&nbsp;ALL are set operators, </p><p>UNION&nbsp;combines the output from both queries but also eliminates the duplicates. </p><p>UNION ALL combines the output from both queries.</p>', 'answers': ['<p><code>SELECT * FROM orders UNION SELECT * FROM orders_archive</code> </p>', '<p><code>SELECT * FROM orders INTERSECT SELECT * FROM orders_archive</code></p>', '<p><code>SELECT * FROM orders UNION ALL SELECT * FROM orders_archive</code> </p>', '<p><code>SELECT * FROM orders_archive MINUS SELECT * FROM orders</code> </p>', '<p><code>SELECT distinct * FROM orders JOIN orders_archive on order.id = orders_archive.id </code></p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are working on a table called orders which contains data for 2021 and you have the second table called orders_archive which contains data for 2020, you need to combine the data from two tables and there could be a possibility of the same rows between both the tables, you are looking to combine the results from both the tables and eliminate the duplicate rows, which of the following SQL&nbsp;statements helps you accomplish this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292598, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following python statement can be used to replace the schema name and table name in the query statement?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Answer is</p><p><br></p><p>table_name = “sales”</p><p>query = f”select * from {schema_name}.{table_name}”</p><p><br></p><p>f strings can be used to format a string.&nbsp; f"&nbsp;This is string {python variable}"</p><p>https://realpython.com/python-f-strings/</p>', 'answers': ['<pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"\nquery = f”select * from schema_name.table_name”</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"\nquery = "select * from {schema_name}.{table_name}"</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"\nquery = f"select * from { schema_name}.{table_name}"</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"\nquery = f"select * from + schema_name +"."+table_name"</pre>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following python statement can be used to replace the schema name and table name in the query statement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292600, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL statements can replace python variables in Databricks SQL&nbsp;code, when the notebook is set in SQL mode?</p><p><br></p><p><br></p><pre class="prettyprint linenums">%python \ntable_name = "sales"\nschema_name = "bronze"\n\n%sql\nSELECT&nbsp;*&nbsp;FROM&nbsp;____________________</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is, SELECT * FROM ${schema_name}.${table_name}</p><p><br></p><p>%python </p><p>table_name = "sales"</p><p>schema_name = "bronze"</p><p>%sql </p><p>SELECT * FROM ${schema_name}.${table_name}</p><p><br></p><p><br></p><p>${python variable} -&gt;&nbsp;Python variables in Databricks SQL code</p><p><br></p>', 'answers': ['<p><code>SELECT * FROM f{schema_name.table_name}</code> </p>', '<p><code> SELECT * FROM {schem_name.table_name}</code> </p>', '<p><code>SELECT * FROM ${schema_name}.${table_name}</code> </p>', '<p><code>SELECT * FROM schema_name.table_name</code> </p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following SQL statements can replace python variables in Databricks SQL&nbsp;code, when the notebook is set in SQL mode?%python \ntable_name = "sales"\nschema_name = "bronze"\n\n%sql\nSELECT&nbsp;*&nbsp;FROM&nbsp;____________________', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292602, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A notebook accepts an input parameter that is assigned to a python variable called <code>department </code>and this is an optional parameter to the notebook, you are looking to control the flow of the code using this parameter. you have to check department variable is present then execute the code and if no department value is passed then skip the code execution. How do you achieve this using python? </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p><br></p><pre class="prettyprint linenums">if department is not None:\n  #Execute code\nelse:\n  pass</pre>', 'answers': ['<pre class="prettyprint linenums">if department is not None:\n  #Execute code\nelse:\n  pass</pre>', '<pre class="prettyprint linenums">if (department is not None)\n   #Execute code\nelse\n  pass</pre>', '<pre class="prettyprint linenums">if department is not None:\n #Execute code\nend:\n pass\n</pre>', '<pre class="prettyprint linenums">if department is not None:\n #Execute code\nthen:\n pass</pre>', '<pre class="prettyprint linenums">if department is None:\n  #Execute code\nelse:\n  pass</pre>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'A notebook accepts an input parameter that is assigned to a python variable called department and this is an optional parameter to the notebook, you are looking to control the flow of the code using this parameter. you have to check department variable is present then execute the code and if no department value is passed then skip the code execution. How do you achieve this using python?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292604, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following operations are not supported on a streaming dataset view?</p><p><code>spark.readStream.format("delta").table("sales").createOrReplaceTempView("streaming_view")</code></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is<code>SELECT * FROM streadming_view order by id&nbsp; </code><strong>Please Note: Sorting with Group by will work without any issues</strong></p><p><br></p><p>see below explanation for each option of the options, </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-33-621860216822ec21ec1edce8a088b935.jpg"><p><br></p><p>Certain operations are not allowed on streaming data, please see highlighted in bold.</p><p><br></p><p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations</a></p><p><br></p><ul><li><p>Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.</p></li><li><p>Limit and take the first N rows are not supported on streaming Datasets.</p></li><li><p>Distinct operations on streaming Datasets are not supported.</p></li><li><p>Deduplication operation is not supported after aggregation on a streaming Datasets.</p></li><li><p><strong>Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.</strong></p></li></ul><p><strong>&nbsp; &nbsp; &nbsp; &nbsp; Note: Sorting without aggregation function is not supported. </strong></p><p><br></p><p><br></p><p>Here is the sample code to prove this, </p><p><br></p><p>Setup test stream</p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-33-27841e4abb96473411637e0b2d47036e.jpg"><p><br></p><p><strong>Sum aggregation function has no issues on stream </strong></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-33-98fd25e99f24e4e25b018d4e18af8ff7.jpg"><p><br></p><p><strong>Max aggregation function has no issues on stream </strong></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-34-07a731e835fad3103ccdfd87136b8f48.jpg"><p><strong>Group by with Order by&nbsp; has no issues on stream </strong></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-34-27baab54c03b4f9d4f6eee5f2ab9a6d9.jpg"><p><strong>Group by has no issues on stream</strong></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-34-b4f50188359aaa07fd1c012255ac9815.jpg"><p><br></p><p>Order by without group by fails. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-03_19-57-34-aaa5e9ad687407eb89fe5bb7a3815628.jpg"><p><br></p><p><br></p>', 'answers': ['<p><code>SELECT sum(unitssold) FROM streaming_view</code> </p>', '<p><code> SELECT max(unitssold) FROM streaming_view</code> </p>', '<p><code>SELECT id, sum(unitssold) FROM streaming_view GROUP&nbsp;BY id ORDER&nbsp;BY&nbsp;id</code> </p>', '<p><code>SELECT id, count(*) FROM streaming_view GROUP&nbsp;BY id</code> </p>', '<p><code>SELECT * FROM streadming_view ORDER&nbsp;BY id</code> </p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following operations are not supported on a streaming dataset view?spark.readStream.format("delta").table("sales").createOrReplaceTempView("streaming_view")', 'related_lectures': []}, {'_class': 'assessment', 'id': 52292606, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following techniques structured streaming uses to ensure recovery of failures during stream processing?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>The answer is Checkpointing and write-ahead logging. </p><p><br></p><p>Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval. </p><p><br></p><p><br></p>', 'answers': ['<p>Checkpointing and Watermarking</p>', '<p>Write ahead logging and watermarking</p>', '<p>Checkpointing and write-ahead logging</p>', '<p>Delta time travel</p>', '<p>The stream will failover to available nodes in the cluster</p>', '<p>Checkpointing and Idempotent sinks</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following techniques structured streaming uses to ensure recovery of failures during stream processing?', 'related_lectures': []}]}
5608500
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 52243300, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the statements are <strong>incorrect</strong> when choosing between lakehouse and Datawarehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads. </p><p><br></p><p>Lakehouse can replace traditional warehouses by leveraging storage and compute optimizations like caching to serve them with low query latency with high reliability. </p><p><br></p><p>Focus on comparisons between Spark Cache vs Delta Cache.</p><p><br></p><p><a href="https://docs.databricks.com/delta/optimizations/delta-cache.html&nbsp;">https://docs.databricks.com/delta/optimizations/delta-cache.html&nbsp;</a></p><p><br></p><p><a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What Is a Lakehouse? - The Databricks Blog</a></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_21-11-31-dac604a713bff8cee944529ab3842d00.jpg"></p>', 'answers': ['<p>Lakehouse can have special indexes and caching which are optimized for Machine learning</p>', '<p>Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads. </p>', '<p>Lakehouse can be accessed through various API’s including but not limited to Python/R/SQL</p>', '<p>Traditional Data warehouses have storage and compute are coupled.</p>', '<p>Lakehouse uses standard data formats like Parquet.</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Which of the statements are incorrect when choosing between lakehouse and Datawarehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243302, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the statements are correct about lakehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is&nbsp; Lakehouse supports schema enforcement and evolution, </p><p><br></p><p>Lakehouse using Delta lake can not only enforce a schema on write which is contrary to traditional big data systems that can only enforce a schema on read, it also supports evolving schema over time with the ability to control the evolution. </p><p>For example below is the Dataframe writer API&nbsp;and it supports three modes of enforcement and evolution, </p><p><br></p><p><strong><em>Default:</em>&nbsp; </strong>Only enforcement, no changes are allowed and any schema drift/evolution will result in failure.</p><p><br></p><p><strong><em>Merge:</em></strong> Flexible, supports enforcement and evolution</p><p><br></p><ul><li><p>New columns are added</p></li></ul><ul><li><p>Evolves nested columns</p></li></ul><ul><li><p>Supports evolving data types, like Byte to Short to Integer to Bigint</p></li></ul><p>How to enable:</p><pre class="prettyprint linenums">DF.write.format("delta").option("mergeSchema", "true").saveAsTable("table_name")\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                  or \nspark.databricks.delta.schema.autoMerge&nbsp;=&nbsp;True&nbsp; ## Spark session</pre><p><br></p><p><strong><em>Overwrite:</em> </strong>No enforcement</p><ul><li><p>Dropping columns</p></li></ul><ul><li><p>Change string to integer</p></li></ul><ul><li><p>Rename columns</p></li></ul><p>How to enable:</p><pre class="prettyprint linenums">DF.write.format("delta").option("overwriteSchema", "True").saveAsTable("table_name")</pre><p><br></p><p><br></p><p> <a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What Is a Lakehouse? - The Databricks Blog</a></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_23-37-51-733a766de7a3aa78935901b452e3f398.JPG"></p><p><br></p><p><br></p>', 'answers': ['<p>Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads</p>', '<p>Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads</p>', '<p> Lakehouse does not support ACID</p>', '<p>In Lakehouse Storage and compute are coupled</p>', '<p>Lakehouse supports schema enforcement and evolution</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Which of the statements are correct about lakehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243304, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following are stored in the control pane of Databricks Architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Databricks Web Application</p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/getting-started/overview">Azure Databricks architecture overview - Azure Databricks | Microsoft Docs</a></p><p><br></p><p>Databricks operates most of its services out of a control plane and a data plane, <em>please note serverless features like SQL Endpoint and DLT compute use shared compute in Control pane.</em></p><p><br></p><p><strong>Control Plane: </strong>Stored in Databricks Cloud Account</p><p><br></p><ul><li><p>The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.</p></li></ul><p><strong>Data Plane:</strong> Stored in Customer Cloud Account</p><p><br></p><ul><li><p>The data plane is managed by your Azure account and is where your data resides. This is also where data is processed. You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure account to ingest data or for storage.</p></li></ul><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_23-40-40-bf94c22fefbf27e03604a299fcfab48f.jpg"></p>', 'answers': ['<p>Job Clusters</p>', '<p>All Purpose Clusters</p>', '<p>Databricks Filesystem</p>', '<p>Databricks Web Application</p>', '<p>Delta tables</p>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Which of the following are stored in the control pane of Databricks Architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243306, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job cluster, but you realized it takes 8 minutes to start the cluster, what feature can be used to start the cluster in a timely fashion so your job can run immediatley? </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Cluster pools allow us to reserve VM\'s ahead of time, when a new job cluster is created VM are grabbed from the pool. Note: when the VM\'s are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only billed once VM is allocated to a cluster.</p><p>Here is a demo of how to setup a pool and follow some best practices,</p><p><br></p><p><a href="https://www.youtube.com/watch?v=FVtITxOabxg&amp;ab_channel=DatabricksAcademy"><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_23-46-22-360839a6425f1edf3c040c0501031fac.JPG"></a></p><p><br></p><p><br></p>', 'answers': ['<p>Setup an additional job to run ahead of the actual job so the cluster is running second job starts</p>', '<p>Use the Databricks cluster pools feature to reduce the startup time</p>', '<p>Use Databricks Premium edition instead of Databricks standard edition</p>', '<p>Pin the cluster in the cluster UI page so it is always available to the jobs</p>', '<p>Disable auto termination so the cluster is always running</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job cluster, but you realized it takes 8 minutes to start the cluster, what feature can be used to start the cluster in a timely fashion so your job can run immediatley?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243308, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following developer operations in the CI/CD can only be implemented through a GIT provider when using Databricks Repos.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Pull request and review process, please note: the question is asking for steps that are being implemented in GIT&nbsp;provider not Databricks Repos. </p><p><br></p><p>See below diagram to understand the role of Databricks Repos and Git provider plays when building a CI/CD workdlow.</p><p>All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a <strong>git </strong>provider like Github or Azure Devops. </p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-08_23-50-15-872f6168b16e2128bd6fbab99f91b0e2.jpg"></p><p><br></p>', 'answers': ['<p>Trigger Databricks Repos pull API to update the latest version</p>', '<p>Commit and push code</p>', '<p>Create and edit code</p>', '<p>Create a new branch</p>', '<p>Pull request and review process</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Which of the following developer operations in the CI/CD can only be implemented through a GIT provider when using Databricks Repos.', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243310, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have noticed the Data scientist team is using the notebook versioning feature with git integration, you have recommended them to switch to using Databricks Repos, which of the below reasons could be the reason the why the team needs to switch to Databricks Repos.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is&nbsp; Databricks Repos allow you to add comments and select the changes you want to commit.</p><p><br></p>', 'answers': ['<p>Databricks Repos allows multiple users to make changes</p>', '<p>Databricks Repos allows merge and conflict resolution</p>', '<p>Databricks Repos has a built-in version control system</p>', '<p>Databricks Repos automatically saves changes</p>', '<p> Databricks Repos allow you to add comments and select the changes you want to commit. </p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'You have noticed the Data scientist team is using the notebook versioning feature with git integration, you have recommended them to switch to using Databricks Repos, which of the below reasons could be the reason the why the team needs to switch to Databricks Repos.', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243312, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Data science team members are using a single cluster to perform data analysis, although cluster size was chosen to handle multiple users and auto-scaling was enabled, the team realized queries are still running slow, what would be the suggested fix for this?</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is Use High concurrency mode instead of the standard mode, </p><p><br></p><p><a href="https://docs.databricks.com/clusters/cluster-config-best-practices.html#cluster-mode">https://docs.databricks.com/clusters/cluster-config-best-practices.html#cluster-mode</a></p><p><br></p><p>High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs.&nbsp; Databricks recommends enabling autoscaling for High Concurrency clusters.</p><p><br></p>', 'answers': ['<p>Setup multiple clusters so each team member has their own cluster</p>', '<p>Disable the auto-scaling feature</p>', '<p>Use High concurrency mode instead of the standard mode</p>', '<p>Increase the size of the driver node</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Data science team members are using a single cluster to perform data analysis, although cluster size was chosen to handle multiple users and auto-scaling was enabled, the team realized queries are still running slow, what would be the suggested fix for this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243314, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL commands are used to append rows to an existing delta table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is INSERT INTO table_name</p><p><br></p><p>Insert adds rows to an existing table, this is very similar to add rows a traditional Database or Datawarehouse. </p><p><br></p>', 'answers': ['<p>APPEND INTO DELTA table_name</p>', '<p>APPEND INTO table_name</p>', '<p>COPY DELTA INTO table_name</p>', '<p>INSERT INTO table_name</p>', '<p>UPDATE table_name</p>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'Which of the following SQL commands are used to append rows to an existing delta table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243316, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How are Delt tables stored?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the transaction log is stored as JSON files.</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-09_00-05-03-3dffa9d5715fee1b721d1654f344fe4a.png">', 'answers': ['<p>A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the transaction log is stored as JSON files.</p>', '<p>A Directory where parquet data files are stored, all of the meta data is stored in memory</p>', '<p>A Directory where parquet data files are stored in Data plane, a sub directory _delta_log where meta data, history and log is stored in control pane.</p>', '<p>A Directory where parquet data files are stored, all of the metadata is stored in parquet files</p>', '<p>Data is stored in Data plane and Metadata and delta log are stored in control pane</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'How are Delt tables stored?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243318, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>While investigating a data issue in a Delta table, you wanted to review logs to see when and who updated the table, what is the best way to review this data?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Run SQL command DESCRIBE HISTORY table_name. </p><p><br></p><p>here is the sample data of how DESCRIBE HISTORY table_name looks </p><p><br></p><pre class="prettyprint linenums">+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|  null|    null|   DELETE|[predicate -&gt; ["(...|null|    null|     null|          4|  Serializable|        false|[numTotalRows -&gt; ...|\n|      4|2019-07-29 14:07:41|  null|    null|   UPDATE|[predicate -&gt; (id...|null|    null|     null|          3|  Serializable|        false|[numTotalRows -&gt; ...|\n|      3|2019-07-29 14:07:29|  null|    null|   DELETE|[predicate -&gt; ["(...|null|    null|     null|          2|  Serializable|        false|[numTotalRows -&gt; ...|\n|      2|2019-07-29 14:06:56|  null|    null|   UPDATE|[predicate -&gt; (id...|null|    null|     null|          1|  Serializable|        false|[numTotalRows -&gt; ...|\n|      1|2019-07-29 14:04:31|  null|    null|   DELETE|[predicate -&gt; ["(...|null|    null|     null|          0|  Serializable|        false|[numTotalRows -&gt; ...|\n|      0|2019-07-29 14:01:40|  null|    null|    WRITE|[mode -&gt; ErrorIfE...|null|    null|     null|       null|  Serializable|         true|[numFiles -&gt; 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+</pre>', 'answers': ['<p>Review event logs in the Workspace</p>', '<p>Run SQL SHOW HISTORY table_name</p>', '<p>Check Databricks SQL Audit logs</p>', '<p>Run SQL command DESCRIBE HISTORY table_name</p>', '<p> Review workspace audit logs</p>']}, 'correct_response': ['d'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'While investigating a data issue in a Delta table, you wanted to review logs to see when and who updated the table, what is the best way to review this data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243320, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>While investigating a performance issue, you realized that you have too many small files for a given table, which command are you going to run to fix this issue</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is OPTIMIZE table_name,</p><p><br></p><p>Optimize compacts small parquet files into a bigger file, by default the size of the files are determined based on the table size at the time of OPTIMIZE, the file size can also be set manually or adjusted based on the workload. </p><p><br></p><p><a href="https://docs.databricks.com/delta/optimizations/file-mgmt.html">https://docs.databricks.com/delta/optimizations/file-mgmt.html</a></p><p><br></p><p>Tune file size based on Table size</p><p><br></p><p>To minimize the need for manual tuning, Databricks automatically tunes the file size of Delta tables based on the size of the table. Databricks will use smaller file sizes for smaller tables and larger file sizes for larger tables so that the number of files in the table does not grow too large.</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-09_00-16-37-6785349b3f610fe3eae7220469067193.JPG"></p>', 'answers': ['<p>COMPACT table_name</p>', '<p>VACUUM table_name</p>', '<p>MERGE table_name</p>', '<p>SHRINK table_name</p>', '<p>OPTIMIZE table_name</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse platform', 'question_plain': 'While investigating a performance issue, you realized that you have too many small files for a given table, which command are you going to run to fix this issue', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243322, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>Create a sales database using the DBFS location <code>'dbfs:/mnt/delta/databases/sales.db/'</code> </p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': "<p>The answer is </p><p><code>CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'</code> </p><p><br></p><p>Note:&nbsp;with the introduction of the Unity catalog and three-layer namespace usage of <code>SCHEMA</code> and <code>DATABASE</code> is interchangeable</p>", 'answers': ["<p><code>CREATE DATABASE sales FORMAT DELTA LOCATION 'dbfs:/mnt/delta/databases/sales.db/'’</code> </p>", "<p><code>CREATE DATABASE sales USING LOCATION 'dbfs:/mnt/delta/databases/sales.db/'</code> </p>", "<p><code>CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'</code> </p>", '<p>The sales database can only be created in Delta lake</p>', "<p><code>CREATE DELTA DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'</code> </p>"]}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': "Create a sales database using the DBFS location 'dbfs:/mnt/delta/databases/sales.db/'", 'related_lectures': []}, {'_class': 'assessment', 'id': 52243324, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the type of table created when you issue SQL DDL command <code>CREATE TABLE sales (id int, units int)</code> </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is Managed Delta table</p><p>Anytime a table is created without the Location keyword it is considered a managed table, by default all managed tables DELTA&nbsp;tables</p><p>Syntax</p><p><code>CREATE TABLE table_name ( column column_data_type…)</code> </p>', 'answers': ['<p>Query fails due to missing location</p>', '<p>Query fails due to missing format</p>', '<p>Managed Delta table</p>', '<p>External Table</p>', '<p>Managed Parquet table</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What is the type of table created when you issue SQL DDL command CREATE TABLE sales (id int, units int)', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243326, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How to determine if a table is a managed table vs external table?&nbsp; </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Run SQL command<code> DESCRIBE EXTENDED table_name</code> and check type</p><p><br></p><p>Example of External table </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-09_00-30-56-f09db3d3e74fbcdb6136e0b005666694.jpg"><p>Example of managed table</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-09_00-30-56-6e416e9b1305e8b8380bff6df5db6164.jpg">', 'answers': ['<p>Run <code>IS_MANAGED(‘table_name’)</code> function</p>', '<p>All external tables are stored in data lake, managed tables are stored in DELTA lake</p>', '<p>All managed tables are stored in unity catalog</p>', '<p>Run SQL command <code>DESCRIBE EXTENDED table_name</code> and check type</p>', '<p>A. Run SQL command <code>SHOW TABLES</code> to see the type of the table</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'How to determine if a table is a managed table vs external table?&nbsp;', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243328, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the below SQL commands creates a session scoped temporary view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><pre class="prettyprint linenums">CREATE OR REPLACE TEMPORARY VIEW view_name\nAS SELECT * FROM table_name</pre><p>The default temporary view is session scoped, as soon as the session ends or if a notebook is detached session scoped temporary view is dropped. </p>', 'answers': ['<pre class="prettyprint linenums">CREATE OR REPLACE TEMPORARY VIEW view_name\nAS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE LOCAL TEMPORARY VIEW view_name\nAS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name\nAS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE VIEW view_name\nAS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE LOCAL VIEW view_name\nAS SELECT * FROM table_name</pre>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the below SQL commands creates a session scoped temporary view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243330, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Drop the customers database and associated tables and data, all of the tables inside the database are managed tables. Which of the following SQL&nbsp;commands will help you accomplish this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>DROP DATABASE customers CASCADE</code></p><p>Drop database with cascade option drops all the tables, since all of the tables inside the database are managed tables we do not need to perform any additional steps to clean the data in the storage. </p>', 'answers': ['<p><code>DROP DATABASE customers FORCE</code> </p>', '<p><code>DROP DATABASE customers CASCADE</code> </p>', '<p><code>DROP DATABASE customers&nbsp; INCLUDE</code> </p>', '<p>All the tables must be dropped first before dropping database</p>', '<p><code>DROP DELTA DATABSE customers</code> </p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Drop the customers database and associated tables and data, all of the tables inside the database are managed tables. Which of the following SQL&nbsp;commands will help you accomplish this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243332, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Define an external SQL table by connecting to a local instance of an SQLite database using JDBC</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p><br></p><pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n    url = "jdbc:sqlite:/sqmple_db",\n    dbtable = "users"\n)</pre><p><br></p><p>Databricks runtime currently supports connecting to a few flavors of SQL Database including SQL Server, My SQL, SQL Lite and Snowflake using JDBC.</p><p><br></p><pre class="prettyprint linenums">CREATE TABLE &lt;jdbcTable&gt;\nUSING org.apache.spark.sql.jdbc or JDBC\nOPTIONS (\n    url = "jdbc:&lt;databaseServerType&gt;://&lt;jdbcHostname&gt;:&lt;jdbcPort&gt;",\n    dbtable " = &lt;jdbcDatabase&gt;.atable",\n    user = "&lt;jdbcUsername&gt;",\n    password = "&lt;jdbcPassword&gt;"\n)</pre><p><br></p><p>For more detailed documentation </p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases">SQL databases using JDBC - Azure Databricks | Microsoft Docs</a></p><p><br></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING SQLITE\nOPTIONS (\n    url = "jdbc:/sqmple_db",\n    dbtable = "users"\n)</pre>', '<pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING SQL\nURL = {server:"jdbc:/sqmple_db",dbtable: “users”}</pre>', '<pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING SQL\nOPTIONS (\n    url = "jdbc:sqlite:/sqmple_db",\n    dbtable = "users"\n)</pre>', '<pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING org.apache.spark.sql.jdbc.sqlite\nOPTIONS (\n    url = "jdbc:/sqmple_db",\n    dbtable = "users"\n)</pre>', '<pre class="prettyprint linenums">CREATE TABLE users_jdbc\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n    url = "jdbc:sqlite:/sqmple_db",\n    dbtable = "users"\n)</pre>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Define an external SQL table by connecting to a local instance of an SQLite database using JDBC', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243334, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When defining external tables using formats CSV, JSON, TEXT, BINARY any query on the external tables caches the data and location for performance reasons, so within a given spark session any new files that may have arrived will not be available after the initial query. How can we address this limitation?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>REFRESH TABLE table_name</code> </p><p><code>REFRESH TABLE table_name</code>&nbsp; will force Spark to refresh the availability of external files and any changes. </p><p>When spark queries an external table it caches the files associated with it, so that way if the table is queried again it can use the cached files so it does not have to retrieve them again from cloud object storage, but the drawback here is that if new files are available Spark does not know until the Refresh command is ran. </p>', 'answers': ['<p><code>UNCACHE TABLE table_name</code> </p>', '<p><code>CACHE TABLE table_name</code> </p>', '<p><code>REFRESH TABLE table_name</code> </p>', '<p><code>BROADCAST TABLE table_name</code> </p>', '<p><code>CLEAR CACH table_name</code> </p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'When defining external tables using formats CSV, JSON, TEXT, BINARY any query on the external tables caches the data and location for performance reasons, so within a given spark session any new files that may have arrived will not be available after the initial query. How can we address this limitation?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243336, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following table constraints that can be enforced on Delta lake tables are supported?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Not Null, Check Constraints </p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/delta/delta-constraints">https://docs.microsoft.com/en-us/azure/databricks/delta/delta-constraints</a></p><p><br></p><pre class="prettyprint linenums">CREATE TABLE events( id LONG,\n                     date STRING, \n                    location STRING,\n                    description STRING \n                    ) USING DELTA;\n</pre><p><code>ALTER TABLE events CHANGE COLUMN id SET NOT NULL;</code></p><p><code>ALTER TABLE events ADD CONSTRAINT dateWithinRange CHECK (date &gt; \'1900-01-01\');</code> </p><p><br></p><p><strong>Note:&nbsp;</strong> Databricks as of DBR 11.1 added support for Primary Key and Foreign Key when Unity Catalog is enabled but this is for information purposes only these are not actually enforced. You may ask then why are we defining these if they are not enforced, so especially these information constraints are very helpful if you have a BI tool that can benefit from knowing the relationship between the tables, so it will be easy when creating reports/dashboards or understanding the data model when using any Data modeling tool. </p><p><a href="https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint">Primary and Foreign Key </a></p><p><br></p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-20_12-28-12-dd4b7112fa7e25cc5cf9de64e924e055.jpg"><p><br></p><p><br></p>', 'answers': ['<p>Primary key, foreign key, Not Null, Check Constraints</p>', '<p>Primary key, Not Null, Check Constraints</p>', '<p>Default, Not Null, Check Constraints</p>', '<p>Not Null, Check Constraints</p>', '<p>Unique, Not Null, Check Constraints</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following table constraints that can be enforced on Delta lake tables are supported?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243338, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The data engineering team is looking to add a new column to the table, but the QA team would like to test the change before implementing in production, which of the below options allow you to quickly copy the table from Prod to the QA environment, modify and run the tests?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is SHALLOW CLONE</p><p><br></p><p><code><strong>SHALLOW CLONE </strong></code>If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, <code><strong>SHALLOW CLONE</strong></code> can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn\'t move so it can be very quick. </p><p><br></p><pre class="prettyprint linenums">CREATE OR REPLACE TABLE {new_table_name} SHALLOW CLONE {source_table_name}|[LOCATION path]</pre><p><br></p><p><code><strong>DEEP CLONE</strong></code> fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location. It copies all of the data and transaction logs this can take a long time based on the size of the table. </p><p><br></p><pre class="prettyprint linenums">CREATE OR REPLACE TABLE {new_table_name} DEEP&nbsp;CLONE {source_table_name}|[LOCATION path]</pre>', 'answers': ['<p>DEEP CLONE</p>', '<p>SHADOW CLONE</p>', '<p>ZERO COPY CLONE</p>', '<p>SHALLOW CLONE</p>', '<p>METADATA CLONE</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'The data engineering team is looking to add a new column to the table, but the QA team would like to test the change before implementing in production, which of the below options allow you to quickly copy the table from Prod to the QA environment, modify and run the tests?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243340, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Sales team is looking to get a report on a measure number of units sold by date, below is the schema. Fill in the blank with the appropriate array function.</p><p><br></p><p>Table <strong>orders</strong>: <code>orderDate DATE, orderIds ARRAY&lt;INT&gt;</code></p><p><br></p><p>Table <strong>orderDetail</strong>: <code>orderId INT, unitsSold INT, salesAmt DOUBLE</code></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-09_06-00-22-e0aa536f9fd4ae52228c78e3cb8a0270.JPG"><p><br></p><pre class="prettyprint linenums">SELECT orderDate, SUM(unitsSold)\n      FROM orderDetail od\nJOIN (select orderDate, ___________(orderIds) as orderId FROM orders) o\n    ON o.orderId = od.orderId\nGROUP BY orderDate</pre><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is EXPLODE, </p><p><br></p><p>explode is table-valued function, takes an array or map and returns a row for each element in the array. </p><p><br></p><p>For below table</p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-09_06-00-22-49633fbf235f12bb11825d826774cbb6.JPG"><p><br></p><p><code>select explode(orderIds) orderId, orderdate from orders</code></p><p><br></p><p>This above would return a result something like below, creates a row for each element in the array</p><p><br></p><p>OrderDate&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; orderId</p><p>10-10-2021&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 100</p><p>10-10-2021&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 101</p><p>10-10-2021&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 102</p><p>...</p><p>..</p>', 'answers': ['<p>FLATTEN</p>', '<p>EXTEND</p>', '<p>EXPLODE</p>', '<p>EXTRACT</p>', '<p>ARRAY_FLATTEN</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Sales team is looking to get a report on a measure number of units sold by date, below is the schema. Fill in the blank with the appropriate array function.Table orders: orderDate DATE, orderIds ARRAY&lt;INT&gt;Table orderDetail: orderId INT, unitsSold INT, salesAmt DOUBLESELECT orderDate, SUM(unitsSold)\n      FROM orderDetail od\nJOIN (select orderDate, ___________(orderIds) as orderId FROM orders) o\n    ON o.orderId = od.orderId\nGROUP BY orderDate', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243342, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are asked to write a python function that can read data from a delta table and return the DataFrame, which of the following is correct?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Python function can return a DataFrame</p><p><br></p><p>The function would something like this, </p><p><br></p><pre class="prettyprint linenums">get_source_dataframe(tablename):\n df = spark.read.table(tablename)\nreturn df</pre><p><br></p><p>df = get_source_dataframe(\'test_table\')</p><p>since there is no action spark returns a Dataframe and assigns to df python variable </p><p><br></p>', 'answers': ['<p>Python function cannot return a DataFrame</p>', '<p>Write SQL UDF to return a DataFrame</p>', '<p>Write SQL UDF that can return tabular data</p>', '<p>Python function will result in out of memory error due to data volume</p>', '<p>Python function can return a DataFrame</p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are asked to write a python function that can read data from a delta table and return the DataFrame, which of the following is correct?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243344, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the output of the below function when executed with input parameters 1, 3 &nbsp;:</p><p><br></p><pre class="prettyprint linenums">def check_input(x,y):\n    if x &lt; y:\n        x= x+1\n        if x&lt;y:\n            x= x+1\n            if x &lt;y:\n                x = x+1\n     return x</pre><p><br></p><p> check_input(1,3)</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is 3</p>', 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>4</p>', '<p>5</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What is the output of the below function when executed with input parameters 1, 3 &nbsp;:def check_input(x,y):\n    if x &lt; y:\n        x= x+1\n        if x&lt;y:\n            x= x+1\n            if x &lt;y:\n                x = x+1\n     return x check_input(1,3)', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243346, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL statements can replace a python variable, when the notebook is set in SQL mode</p><p><br></p><pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code> spark.sql(f"SELECT * FROM {schema_name}.{table_name}")</code> </p>', 'answers': ['<p><code>spark.sql(f"SELECT * FROM f{schema_name.table_name}")</code> </p>', '<p><code>spark.sql(f"SELECT * FROM {schem_name.table_name}")</code> </p>', '<p><code>spark.sql(f"SELECT * FROM ${schema_name}.${table_name}")</code> </p>', '<p><code>spark.sql(f"SELECT * FROM {schema_name}.{table_name}")</code> </p>', '<p><code>spark.sql("SELECT * FROM schema_name.table_name")</code> </p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following SQL statements can replace a python variable, when the notebook is set in SQL modetable_name = "sales"\nschema_name = "bronze"', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243348, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When writing streaming data, Spark’s structured stream supports the below write modes</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Append, Complete, Update</p><p><br></p><ul><li><p><strong>Append mode (default)</strong> - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only <code>select</code>, <code>where</code>, <code>map</code>, <code>flatMap</code>, <code>filter</code>, <code>join</code>, etc. will support Append mode.</p></li><li><p><strong>Complete mode</strong> - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.</p></li><li><p><strong>Update mode</strong> - (<em>Available since Spark 2.1.1</em>) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.</p></li></ul><p><br></p>', 'answers': ['<p>Append, Delta, Complete</p>', '<p>Delta, Complete, Continuous</p>', '<p>Append, Complete, Update</p>', '<p>Complete, Incremental, Update</p>', '<p>Append, overwrite, Continuous</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data processing', 'question_plain': 'When writing streaming data, Spark’s structured stream supports the below write modes', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243350, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When using the complete mode to write stream data, how does it impact the target table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Target table is overwritten for each batch</p><p><br></p><p><strong>Complete mode</strong> - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries</p>', 'answers': ['<p>Entire stream waits for complete data to write</p>', '<p>Stream must complete to write the data</p>', '<p>Target table cannot be updated while stream is pending</p>', '<p>Target table is overwritten for each batch</p>', '<p>Delta commits transaction once the stream is stopped</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data processing', 'question_plain': 'When using the complete mode to write stream data, how does it impact the target table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243352, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, the schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader command to load the data, fill in the blanks for successful execution of the below code.</p><p><br></p><pre class="prettyprint linenums">spark.readStream\n.format("cloudfiles")\n.option("cloudfiles.format",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("mergeSchema", "true")\n.table(table_name))</pre><p><br></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>cloudfiles.schemalocation, checkpointlocation</code></p><p>When reading the data <code>cloudfiles.schemalocation</code> is used to store the inferred schema of the incoming data. </p><p>When writing a stream to recover from failures <code>checkpointlocation </code>is used to store the offset of the byte that was most recently processed. </p>', 'answers': ['<p>checkpointlocation, schemalocation</p>', '<p> checkpointlocation, cloudfiles.schemalocation</p>', '<p>schemalocation, checkpointlocation</p>', '<p>cloudfiles.schemalocation, checkpointlocation</p>', '<p>cloudfiles.schemalocation, cloudfiles.checkpointlocation</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data processing', 'question_plain': 'At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, the schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader command to load the data, fill in the blanks for successful execution of the below code.spark.readStream\n.format("cloudfiles")\n.option("cloudfiles.format",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("mergeSchema", "true")\n.table(table_name))', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243380, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When working with AUTO LOADER you noticed that most of the columns that were inferred as part of loading are string data types including columns that were supposed to be integers, how can we fix this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Provide schema hints. </p><p><br></p><pre class="prettyprint linenums">spark.readStream \\\n  .format("cloudFiles") \\\n  .option("cloudFiles.format", "csv") \\\n  .option("header", "true") \\\n  .option("cloudFiles.schemaLocation", schema_location) \\\n  .option("cloudFiles.schemaHints", "id int, description string")\n  .load(raw_data_location)\n  .writeStream \\\n  .option("checkpointLocation", checkpoint_location) \\\n  .start(target_delta_table_location)</pre><p><br></p><p><br></p><p><strong>.option("cloudFiles.schemaHints", "id int, description string")</strong></p><p><strong>#&nbsp;Here we are providing a hint that id column is int and the description is a string</strong></p><p>When cloudfiles.schemalocation is used to store the output of the schema inference during the load process,&nbsp; with schema hints you can enforce data types for known columns ahead of time.&nbsp; </p><p><br></p>', 'answers': ['<p>Provide the schema of the source table in the cloudfiles.schemalocation</p>', '<p>Provide the schema of the target table in the cloudfiles.schemalocation</p>', '<p>Provide schema hints</p>', '<p>Update the checkpoint location</p>', '<p>Correct the incoming data by explicitly casting the data types</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data processing', 'question_plain': 'When working with AUTO LOADER you noticed that most of the columns that were inferred as part of loading are string data types including columns that were supposed to be integers, how can we fix this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243354, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have configured AUTO LOADER to process incoming IOT data from cloud object storage every 15 mins, recently a change was made to the notebook code to update the processing logic but the team later realized that the notebook was failing for the last 24 hours, what steps team needs to take to reprocess the data that was not loaded after the notebook was corrected?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p>Autoloader automatically re-processes data that was not loaded using the checkpoint. </p>', 'answers': ['<p>Move the files that were not processed to another location and manually copy the files into the ingestion path to reprocess them</p>', '<p> Enable back_fill = TRUE to reprocess the data</p>', '<p>Delete the checkpoint folder and run the autoloader again</p>', '<p>Autoloader automatically re-processes data that was not loaded</p>', '<p>Manually re-load the data</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data processing', 'question_plain': 'You have configured AUTO LOADER to process incoming IOT data from cloud object storage every 15 mins, recently a change was made to the notebook code to update the processing logic but the team later realized that the notebook was failing for the last 24 hours, what steps team needs to take to reprocess the data that was not loaded after the notebook was corrected?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243356, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following Structured Streaming queries is performing a hop from a bronze table to a Silver table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The question is asking to identify a structured streaming command that is moving data from bronze to silver. </p><p><br></p><p>The answer is</p><pre class="prettyprint linenums">(spark.table("sales")\n.withColumn("avgPrice", col("sales") / col("units"))\n.writeStream\n.option("checkpointLocation", checkpointPath)\n.outputMode("append") \n.table("cleanedSales"))</pre><p>We are preserving the grain of incoming data and enriching the data by adding avg price, the other options listed use aggregations which are mostly performed on top of the silver to move data to Gold. </p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">(spark.table("sales").groupBy("store")\n.agg(sum("sales")).writeStream\n.option("checkpointLocation",checkpointPath)\n.outputMode("complete")\n.table("aggregatedSales")) </pre>', '<pre class="prettyprint linenums">(spark.table("sales").agg(sum("sales"),sum("units"))\n.writeStream\n.option("checkpointLocation",checkpointPath)\n.outputMode("complete")\n.table("aggregatedSales"))</pre>', '<pre class="prettyprint linenums">(spark.table("sales")\n.withColumn("avgPrice", col("sales") / col("units"))\n.writeStream\n.option("checkpointLocation", checkpointPath)\n.outputMode("append") \n.table("cleanedSales"))</pre>', '<pre class="prettyprint linenums">(spark.readStream.load(rawSalesLocation)\n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("append") \n.table("uncleanedSales") )</pre>', '<pre class="prettyprint linenums">(spark.read.load(rawSalesLocation) \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("append") \n.table("uncleanedSales") ) </pre>']}, 'correct_response': ['c'], 'section': 'Incremental Data processing', 'question_plain': 'Which of the following Structured Streaming queries is performing a hop from a bronze table to a Silver table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243358, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following Structured Streaming queries successfully performs a hop from a Silver to Gold table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><pre class="prettyprint linenums">(spark.table("sales") \n.groupBy("store") \n.agg(sum("sales")) \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("complete") \n.table("aggregatedSales") ) </pre><p><br></p><p>The gold layer is normally used to store aggregated data</p><p><br></p><p>Review the below link for more info,</p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Gold Layer:</p><p><br></p><p>1. Powers Ml applications, reporting, dashboards, ad hoc analytics</p><p>2. <strong>Refined views of data, typically with aggregations</strong></p><p>3. Reduces strain on production systems</p><p>4. Optimizes query performance for business-critical data</p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">(spark.table("sales") \n.groupBy("store") \n.agg(sum("sales")) \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("complete") \n.table("aggregatedSales") ) </pre>', '<pre class="prettyprint linenums">(spark.table("sales") \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("complete") \n.table("sales") ) </pre>', '<pre class="prettyprint linenums">(spark.table("sales") \n.withColumn("avgPrice", col("sales") / col("units")) \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("append") \n.table("cleanedSales") ) </pre>', '<pre class="prettyprint linenums">(spark.readStream.load(rawSalesLocation) \n.writeStream \n.option("checkpointLocation", checkpointPath) \n.outputMode("append") \n.table("uncleanedSales") ) </pre>', '<pre class="prettyprint linenums">(spark.read.load(rawSalesLocation) \n    .writeStream\n    .option("checkpointLocation", checkpointPath) \n    .outputMode("append") \n    .table("uncleanedSales") )</pre>']}, 'correct_response': ['a'], 'section': 'Incremental Data processing', 'question_plain': 'Which of the following Structured Streaming queries successfully performs a hop from a Silver to Gold table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243360, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following Auto loader structured streaming commands successfully performs a hop from the landing area into Bronze?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><pre class="prettyprint linenums">spark\\\n.readStream\\\n.format("cloudFiles") \\# use Auto loader\n.option("cloudFiles.format","csv") \\ # csv format files\n.option("cloudFiles.schemaLocation", checkpoint_directory)\\\n.load(\'landing\')\\\n.writeStream.option("checkpointLocation", checkpoint_directory)\\\n.table(raw)</pre><p><br></p><p>Note:&nbsp;if you chose the below option which is incorrect because it does not have readStream </p><pre class="prettyprint linenums">spark.read.format("cloudFiles")\n.option("cloudFiles.format",”csv”)\n...\n..\n..</pre><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">spark\\\n.readStream\\\n.format("csv")\\\n.option("cloudFiles.schemaLocation", checkpoint_directory)\\\n.load("landing")\\\n.writeStream.option("checkpointLocation", checkpoint_directory)\\\n.table(raw)</pre>', '<pre class="prettyprint linenums">spark\\\n.readStream\\\n.format("cloudFiles")\\\n.option("cloudFiles.format","csv")\\\n.option("cloudFiles.schemaLocation", checkpoint_directory)\\\n.load("landing")\\\n.writeStream.option("checkpointLocation", checkpoint_directory)\\\n.table(raw)</pre>', '<pre class="prettyprint linenums">spark\\\n.read\\\n.format("cloudFiles")\\\n.option("cloudFiles.format",”csv”)\\\n.option("cloudFiles.schemaLocation", checkpoint_directory)\\\n.load("landing")\\\n.writeStream.option("checkpointLocation", checkpoint_directory)\\\n.table(raw)</pre>', '<pre class="prettyprint linenums">spark\\\n.readStream\\\n.load(rawSalesLocation)\\\n.writeStream \\\n.option("checkpointLocation", checkpointPath).outputMode("append")\\\n.table("uncleanedSales")</pre>', '<pre class="prettyprint linenums">spark\\\n.read\\\n.load(rawSalesLocation) \\\n.writeStream\\\n.option("checkpointLocation", checkpointPath) \\\n.outputMode("append")\\\n.table("uncleanedSales")</pre>']}, 'correct_response': ['b'], 'section': 'Incremental Data processing', 'question_plain': 'Which of the following Auto loader structured streaming commands successfully performs a hop from the landing area into Bronze?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243362, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A DELTA LIVE TABLE pipelines can be scheduled to run in two different modes, what are these two different modes?</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Triggered, Continuous</p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--continuous-and-triggered-pipelines">https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--continuous-and-triggered-pipelines</a></p><p><br></p><ul><li><p>Triggered pipelines update each table with whatever data is currently available and then stop the cluster running the pipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those that read from external sources. Tables within the pipeline are updated after their dependent data sources have been updated.</p></li><li><p>Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run until manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers have the most up-to-date data.</p></li></ul>', 'answers': ['<p>Triggered, Incremental</p>', '<p>Once, Continuous</p>', '<p>Triggered, Continuous</p>', '<p>Once, Incremental</p>', '<p>Continuous, Incremental</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data processing', 'question_plain': 'A DELTA LIVE TABLE pipelines can be scheduled to run in two different modes, what are these two different modes?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243364, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Your team member is trying to set up a delta pipeline and build a second gold table to the same pipeline with aggregated metrics based on an existing Delta Live table called sales_orders_cleaned but he is facing a problem in starting the pipeline, the pipeline is failing to state it cannot find the table sales_orders_cleaned, you are asked to identify and fix the problem.</p><p><br></p><pre class="prettyprint linenums">CREATE LIVE TABLE sales_order_in_chicago\nAS\nSELECT order_date, city, sum(price) as sales,\nFROM sales_orders_cleaned\nWHERE city = \'Chicago\')\nGROUP BY order_date, city</pre><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Sales_orders_cleaned table is missing schema name LIVE </p><p>Every Delta live table should have schema LIVE</p><p>Here is the correct syntax, </p><p><br></p><pre class="prettyprint linenums">CREATE LIVE TABLE sales_order_in_chicago\nAS\nSELECT order_date, city, sum(price) as sales,\nFROM LIVE.sales_orders_cleaned\nWHERE city = \'Chicago\')\nGROUP BY order_date, city\n</pre>', 'answers': ['<p>Use STREAMING LIVE instead of LIVE table</p>', '<p>Delta live table can be used in a group by clause</p>', '<p>Delta live tables pipeline can only have one table</p>', '<p>Sales_orders_cleaned table is missing schema name LIVE </p>', '<p>The pipeline needs to be deployed so the first table is created before we add a second table</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data processing', 'question_plain': "Your team member is trying to set up a delta pipeline and build a second gold table to the same pipeline with aggregated metrics based on an existing Delta Live table called sales_orders_cleaned but he is facing a problem in starting the pipeline, the pipeline is failing to state it cannot find the table sales_orders_cleaned, you are asked to identify and fix the problem.CREATE LIVE TABLE sales_order_in_chicago\nAS\nSELECT order_date, city, sum(price) as sales,\nFROM sales_orders_cleaned\nWHERE city = 'Chicago')\nGROUP BY order_date, city", 'related_lectures': []}, {'_class': 'assessment', 'id': 52243366, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following type of tasks cannot setup through a job?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Notebook</p>', '<p>DELTA LIVE PIPELINE</p>', '<p>Spark Submit</p>', '<p>Python</p>', '<p>Databricks SQL Dashboard refresh</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data processing', 'question_plain': 'Which of the following type of tasks cannot setup through a job?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243368, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following approaches can the data engineer use to obtain a version-controllable configuration of the Job’s schedule and configuration?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>They can link the Job to notebooks that are a part of a Databricks Repo.</p>', '<p>They can submit the Job once on a Job cluster.</p>', '<p>They can download the JSON equivalent of the job from the Job’s page.</p>', '<p>They can submit the Job once on an all-purpose cluster.</p>', '<p>They can download the XML description of the Job from the Job’s page</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'Which of the following approaches can the data engineer use to obtain a version-controllable configuration of the Job’s schedule and configuration?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243370, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What steps need to be taken to set up a DELTA LIVE PIPELINE as a job using the workspace UI?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is, </p><p>Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook. </p><p><br></p><p><strong>Create a pipeline</strong></p><p>To create a new pipeline using the Delta Live Tables notebook:</p><ol><li><p>Click <img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/icons/jobs-icon.png"> <strong>Workflows</strong> in the sidebar, click the <strong>Delta Live Tables</strong> tab, and click <strong>Create Pipeline</strong>.</p></li><li><p>Give the pipeline a name and click <img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/icons/file-picker.png"> to select a notebook.</p></li><li><p>Optionally enter a storage location for output data from the pipeline. The system uses a default location if you leave <strong>Storage Location</strong> empty.</p></li><li><p>Select <strong>Triggered</strong> for <strong>Pipeline Mode</strong>.</p></li><li><p>Click <strong>Create</strong>.</p></li></ol><p>The system displays the <strong>Pipeline Details</strong> page after you click <strong>Create</strong>. You can also access your pipeline by clicking the pipeline name in the <strong>Delta Live Tables</strong> tab.</p>', 'answers': ['<p> DELTA LIVE TABLES do not support job cluster</p>', '<p>Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook</p>', '<p>Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the pipeline JSON file</p>', '<p>Use Pipeline creation UI, select a new pipeline and job cluster</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'What steps need to be taken to set up a DELTA LIVE PIPELINE as a job using the workspace UI?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243372, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Data engineering team has provided 10 queries and asked Data Analyst team to build a dashboard and refresh the data every day at 8 AM, identify the best approach to set up data refresh for this dashaboard?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p>The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.</p><p><br></p><p><strong>Automatically refresh a dashboard</strong></p><p>A dashboard’s owner and users with the <strong>Can Edit</strong> permission can configure a dashboard to automatically refresh on a schedule. To automatically refresh a dashboard:</p><ol><li><p>Click the <strong>Schedule</strong> button at the top right of the dashboard. The scheduling dialog appears.</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/dashboard-schedule-azure.png"></p></li><li><p>In the <strong>Refresh every</strong> drop-down, select a period.</p></li><li><p>In the <strong>SQL Warehouse</strong> drop-down, optionally select a SQL warehouse to use for all the queries. If you don’t select a warehouse, the queries execute on the last used SQL warehouse.</p></li><li><p>Next to <strong>Subscribers</strong>, optionally enter a list of email addresses to notify when the dashboard is automatically updated.</p><p>Each email address you enter must be associated with a Azure Databricks account or configured as an <a href="https://docs.microsoft.com/en-us/azure/databricks/sql/admin/alert-destinations#create-an-alert-destination">alert destination</a>.</p></li><li><p>Click <strong>Save</strong>. The <strong>Schedule</strong> button label changes to <strong>Scheduled</strong>.</p></li></ol>', 'answers': ['<p>Each query requires a separate task and setup 10 tasks under a single job to run at 8 AM to refresh the dashboard</p>', '<p>The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.</p>', '<p>Setup JOB with linear dependency to all load all 10 queries into a table so the dashboard can be refreshed at once.</p>', '<p>A dashboard can only refresh one query at a time, 10 schedules to set up the refresh.</p>', '<p>Use Incremental refresh to run at 8 AM every day.</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'Data engineering team has provided 10 queries and asked Data Analyst team to build a dashboard and refresh the data every day at 8 AM, identify the best approach to set up data refresh for this dashaboard?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243374, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> The data engineering team is using a SQL query to review data completeness every day to monitor the ETL job, and query output is being used in multiple dashboards which of the following approaches can be used to set up a schedule and automate this process?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL, </p><p><br></p><p>The query pane view in Databricks SQL&nbsp;workspace provides the ability to add or edit and schedule individual queries to run. </p><p><br></p><p><br></p><p>You can use scheduled query executions to keep your dashboards updated or to enable routine alerts. By default, your queries do not have a schedule.</p><p>Note</p><p>If your query is used by an alert, the alert runs on its own refresh schedule and does not use the query schedule.</p><p>To set the schedule:</p><ol><li><p>Click the query info tab.</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/query-info-tab.png"></p></li><li><p>Click the link to the right of <strong>Refresh Schedule</strong> to open a picker with schedule intervals.</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/schedule-modal.png"></p></li><li><p>Set the schedule.</p><p>The picker scrolls and allows you to choose:</p><ul><li><p>An interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks</p></li><li><p>A time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is greater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer’s timezone and converts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For example, if you want a query to execute at <code>00:00</code> UTC each day, but your current timezone is PDT (UTC-7), you should select <code>17:00</code> in the picker:</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/weekly-schedule.png"></p></li></ul></li></ol><p><br></p>', 'answers': ['<p>They can schedule the query to run every day from the Jobs UI.</p>', '<p>They can schedule the query to refresh every day from the query’s page in Databricks SQL</p>', '<p>They can schedule the query to run every 12 hours from the Jobs UI.</p>', '<p>They can schedule the query to refresh every day from the SQL endpoint’s page in Databricks SQL.</p>', '<p>They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'The data engineering team is using a SQL query to review data completeness every day to monitor the ETL job, and query output is being used in multiple dashboards which of the following approaches can be used to set up a schedule and automate this process?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243376, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A data engineer is using a Databricks SQL query to monitor the performance of an ELT job. The ELT job is triggered by a specific number of input records being ready to process. The Databricks SQL query returns the number of minutes since the job’s most recent runtime. Which of the following approaches can enable the data engineering team to be notified if the ELT job has not been run in an hour?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, They can set up an Alert for the query to notify them if the returned value is greater than 60.</p><p><br></p><p>The important thing to note here is that alert can only be setup on query not on the dashboard, query can&nbsp; return a value, which is used if alert can be triggered. </p><p><br></p>', 'answers': ['<p>They can set up an Alert for the accompanying dashboard to notify them if the returned value is greater than 60.</p>', '<p>They can set up an Alert for the query to notify when the ELT job fails.</p>', '<p>They can set up an Alert for the accompanying dashboard to notify when it has not refreshed in 60 minutes.</p>', '<p>They can set up an Alert for the query to notify them if the returned value is greater than 60.</p>', '<p> This type of alert is not possible in Databricks</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'A data engineer is using a Databricks SQL query to monitor the performance of an ELT job. The ELT job is triggered by a specific number of input records being ready to process. The Databricks SQL query returns the number of minutes since the job’s most recent runtime. Which of the following approaches can enable the data engineering team to be notified if the ELT job has not been run in an hour?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243378, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following is true, when building a Databricks SQL dashboard?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>the answer is, More than one visualization can be developed using a single query result. </p><p><br></p><p>In the query editor pane + Add visualization tab can be used for many visualizations for a single query result. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_04-15-31-dcf0a8275cf070fafbdb6e671338c286.JPG"><p><br></p>', 'answers': ['<p>A dashboard can only use results from one query</p>', '<p>Only one visualization can be developed with one query result</p>', '<p>A dashboard can only connect to one schema/Database</p>', '<p>More than one visualization can be developed using a single query result</p>', '<p>A dashboard can only have one refresh schedule</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'Which of the following is true, when building a Databricks SQL dashboard?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243382, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A newly joined team member John Smith in the Marketing team currently has access read access to sales tables but does not have access to update the table, which of the following commands help you accomplish this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com</p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges">https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges</a></p>', 'answers': ['<p><code>GRANT UPDATE ON TABLE table_name TO john.smith@marketing.com</code> </p>', '<p><code>GRANT USAGE ON TABLE table_name TO john.smith@marketing.com</code> </p>', '<p><code>GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com</code> </p>', '<p><code>GRANT UPDATE TO TABLE table_name ON john.smith@marketing.com</code> </p>', '<p><code> GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com</code> </p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': 'A newly joined team member John Smith in the Marketing team currently has access read access to sales tables but does not have access to update the table, which of the following commands help you accomplish this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243384, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A new user who currently does not have access to the catalog or schema is requesting access to the customer table in sales schema, but the customer table contains sensitive information, so you have decided to create view on the table excluding columns that are sensitive and granted access to the view GRANT SELECT ON view_name to user@company.com but when the user tries to query the view, gets the error view does not exist. What is the issue preventing user to access the view and how to fix it?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is User requires USAGE privilege on Sales schema, </p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges">Data object privileges - Azure Databricks | Microsoft Docs</a></p><p><br></p><p><code>GRANT USAGE ON SCHEMA sales TO user@company.com;</code></p><p><br></p><ul><li><p><code>USAGE</code>: does not give any abilities, but is an additional requirement to perform any action on a schema object.</p><p><br></p></li></ul>', 'answers': ['<p>User requires SELECT on the underlying table</p>', '<p>User requires to be put in a special group that has access to PII data</p>', '<p>User has to be the owner of the view</p>', '<p>User requires USAGE privilege on Sales schema</p>', '<p>User needs ADMIN privilege on the view</p>']}, 'correct_response': ['d'], 'section': 'Data Governance', 'question_plain': 'A new user who currently does not have access to the catalog or schema is requesting access to the customer table in sales schema, but the customer table contains sensitive information, so you have decided to create view on the table excluding columns that are sensitive and granted access to the view GRANT SELECT ON view_name to user@company.com but when the user tries to query the view, gets the error view does not exist. What is the issue preventing user to access the view and how to fix it?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243386, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How do you access or use tables in the unity catalog? </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is<strong> catalog_name.schema_name.table_name</strong></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_04-44-53-5d3c9f7f284c76cadf7102c02071009c.jpg"></p><p><br></p><p><strong>note:&nbsp;Database and Schema are analogous they are interchangeably used in the Unity catalog.&nbsp; </strong></p><p><br></p><p>FYI, A catalog is registered under a metastore, by default every workspace has a default metastore called hive_metastore, with a unity catalog you have the ability to create meatstores and share that across multiple workspaces. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-05_21-30-24-2aa912520a59ec9cf1ad14dbe1eebb40.jpg"><p><br></p><p><br></p><p><br></p>', 'answers': ['<p><code>schema_name.table_name</code> </p>', '<p><code>schema_name.catalog_name.table_name</code> </p>', '<p><code>catalog_name.table_name</code> </p>', '<p><code>catalog_name.database_name.schema_name.table_name</code> </p>', '<p><code>catalog_name.schema_name.table_name</code> </p>']}, 'correct_response': ['e'], 'section': 'Data Governance', 'question_plain': 'How do you access or use tables in the unity catalog?', 'related_lectures': []}, {'_class': 'assessment', 'id': 52243388, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How do you upgrade an existing workspace managed table to a unity catalog table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>Create table catalog_name.schema_name.table_name as select * from hive_metastore.old_schema.old_table</code></p><p><br></p><p>Basically, we are moving the data from an internal hive metastore to a metastore and catalog that is registered in the Unity catalog. </p><p>note:&nbsp;if it is a managed table the data is copied to a different storage account, for a large tables this can take a lot of time.&nbsp; For an external table the process is different. </p><p>Managed table:<a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate#--upgrade-a-table-to-unity-catalog"> Upgrade a managed to Unity Catalog</a></p><p>External table:&nbsp; <a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate#--upgrade-an-external-table-to-unity-catalog">Upgrade an external table to Unity Catalog </a></p><p><br></p>', 'answers': ['<p><code>ALTER TABLE table_name SET UNITY_CATALOG = TRUE</code> </p>', '<p><code>Create table catalog_name.schema_name.table_name </code></p><p><code>as select * from hive_metastore.old_schema.old_table</code> </p>', '<p><code>Create table table_name as select * from hive_metastore.old_schema.old_table</code> </p>', '<p><code>Create table table_name format = UNITY as select * from old_table_name</code> </p>', '<p><code>Create or replace table_name format = UNITY using deep clone old_table_name</code> </p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'How do you upgrade an existing workspace managed table to a unity catalog table?', 'related_lectures': []}]}
5609188
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 53426932, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the statements is correct when choosing between lakehouse and Datawarehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that desire:</p><p>· Open, direct access to data stored in standard data formats.</p><p>· Indexing protocols optimized for machine learning and data science.</p><p>· Low query latency and high reliability for BI and advanced analytics.</p>', 'answers': ['<p>Traditional Data warehouses have special indexes which are optimized for Machine learning</p>', '<p>Traditional Data warehouses can serve low query latency with high reliability for BI workloads</p>', '<p>SQL support is only available for Traditional Datawarehouse’s, Lakehouses support Python and Scala</p>', '<p>Traditional Data warehouses are the preferred choice if we need to support ACID, Lakehouse does not support ACID. </p>', '<p>Lakehouse replaces the current dependency on data lakes and data warehouses uses an open standard storage format and supports low latency BI workloads.</p>']}, 'correct_response': ['e'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Which of the statements is correct when choosing between lakehouse and Datawarehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426934, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Where are Interactive notebook results stored in Databricks product architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Data and Control plane, </p><p>Only Job results are stored in Data Plane(your storage), Interactive notebook results are stored in a combination of the control plane (partial results for presentation in the UI) and customer storage. </p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/getting-started/overview#--high-level-architecture">https://docs.microsoft.com/en-us/azure/databricks/getting-started/overview#--high-level-architecture</a></p><p><br></p><p>Snippet from the above documentation, </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-24_18-12-10-7a2c015cebdd01ca29fd2ba5c0a9c2af.jpg"><p><br></p><p><strong>How to change this behavior?</strong></p><p><br></p><p>You can change this behavior using Workspace/Admin Console settings for that workspace, once enabled all of the interactive results are stored in the customer account(data plane)&nbsp;except the new notebook visualization feature Databricks has recently introduced, this still stores some metadata in the control pane irrespective of the below settings. please refer to the documentation for more details. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-24_18-12-11-348c1b7a5b8a7d91380e4ece710e03b4.jpg"><p><br></p><p><strong>Why is this important to know?</strong></p><p>I recently worked on a project where we had to deal with sensitive information of customers and we had a security requirement that all of the data need to be stored in the data plane including notebook results. </p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>Data plane</p>', '<p>Control plane</p>', '<p>Data and Control plane</p>', '<p>JDBC data source</p>', '<p>Databricks web application</p>']}, 'correct_response': ['c'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Where are Interactive notebook results stored in Databricks product architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426936, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statements are true about a lakehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><p><a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What Is a Lakehouse? - The Databricks Blog</a></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_05-17-29-033eff57ffa28756f6ded964144a5248.jpg">', 'answers': ['<p>Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads</p>', '<p>Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads</p>', '<p>Lakehouse does not support ACID</p>', '<p>Lakehouse do not support SQL</p>', '<p>Lakehouse supports Transactions</p>']}, 'correct_response': ['e'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Which of the following statements are true about a lakehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426944, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL command can be used to insert or update or delete rows based on a condition to check if a row(s) exists?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>here is the additional documentation for your review. </p><p><a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html</a></p><p><br></p><pre class="prettyprint linenums">MERGE INTO target_table_name [target_alias]\n   USING source_table_reference [source_alias]\n   ON merge_condition\n   [ WHEN MATCHED [ AND condition ] THEN matched_action ] [...]\n   [ WHEN NOT MATCHED [ AND condition ]  THEN not_matched_action ] [...]\n\nmatched_action\n { DELETE |\n   UPDATE SET * |\n   UPDATE SET { column1 = value1 } [, ...] }\n\nnot_matched_action\n { INSERT * |\n   INSERT (column1 [, ...] ) VALUES (value1 [, ...])</pre>', 'answers': ['<p><code> MERGE INTO table_name</code> </p>', '<p><code>COPY INTO table_name</code> </p>', '<p><code>UPDATE table_name</code> </p>', '<p><code>INSERT INTO OVERWRITE table_name</code> </p>', '<p><code>INSERT IF EXISTS table_name</code> </p>']}, 'correct_response': ['a'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Which of the following SQL command can be used to insert or update or delete rows based on a condition to check if a row(s) exists?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426946, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>When investigating a data issue you realized that a process accidentally updated the table,&nbsp; you want to query the same table with yesterday's version of the data so you can review what the prior version looks like, what is the best way to query historical data so you can do your analysis?</p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>&nbsp; SELECT * FROM table_name TIMESTAMP&nbsp;as of date_sub(current_date(), 1)</code>&nbsp; </p><p><br></p><p>FYI, Time travel supports two ways one is using timestamp and the second way is using version number, </p><p><br></p><p><strong>Timestamp:</strong></p><p><br></p><pre class="prettyprint linenums">SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01"\nSELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)\nSELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01 01:30:00.000"</pre><p><br></p><p><strong>Version Number:&nbsp;</strong></p><p><br></p><pre class="prettyprint linenums">SELECT count(*) FROM my_table VERSION AS OF 5238\nSELECT count(*) FROM my_table@v5238\nSELECT count(*) FROM delta.`/path/to/my/table@v5238`</pre><p><br></p><p><a href="https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html">https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html</a></p><p><br></p>', 'answers': ["<p><code>SELECT * FROM TIME_TRAVEL(table_name) WHERE time_stamp = 'timestamp'</code> </p>", '<p><code>TIME_TRAVEL FROM table_name WHERE&nbsp;time_stamp = date_sub(current_date(), 1)</code></p><p> </p>', '<p><code> SELECT * FROM table_name TIMESTAMP&nbsp;AS&nbsp;OF date_sub(current_date(), 1)</code> </p>', '<p><code>DISCRIBE HISTORY table_name AS&nbsp;OF&nbsp;date_sub(current_date(), 1)</code> </p>', '<p><code>SHOW HISTORY table_name AS&nbsp;OF&nbsp;date_sub(current_date(), 1)</code></p>']}, 'correct_response': ['c'], 'section': 'Data Lakehouse Platform', 'question_plain': "When investigating a data issue you realized that a process accidentally updated the table,&nbsp; you want to query the same table with yesterday's version of the data so you can review what the prior version looks like, what is the best way to query historical data so you can do your analysis?", 'related_lectures': []}, {'_class': 'assessment', 'id': 53426948, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>While investigating a data issue, you wanted to review yesterday's version of the table using below command, while querying the previous version of the table using time travel you realized that you are no longer able to view the historical data in the table and you could see it the table was updated yesterday based on the table history(DESCRIBE&nbsp;HISTORY table_name) command what could be the reason why you can not access this data?</p><p><br></p><p><code>SELECT * FROM table_name TIMESTAMP AS OF date_sub(current_date(), 1)</code> </p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, VACUUM table_name RETAIN 0 was ran</p><p><br></p><p>The VACUUM&nbsp;command recursively vacuums directories associated with the Delta table and removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. The default is 7 Days.</p><p>When VACUUM table_name RETAIN 0 is ran all of the historical versions of data are lost time travel can only provide the current state. </p>', 'answers': ['<p>You currently do not have access to view historical data</p>', '<p>By default, historical data is cleaned every 180 days in DELTA</p>', '<p>A command <code>VACUUM table_name RETAIN 0</code> was ran on the table</p>', '<p>Time travel is disabled</p>', '<p>Time travel must be enabled before you query previous data</p>']}, 'correct_response': ['c'], 'section': 'Data Lakehouse Platform', 'question_plain': "While investigating a data issue, you wanted to review yesterday's version of the table using below command, while querying the previous version of the table using time travel you realized that you are no longer able to view the historical data in the table and you could see it the table was updated yesterday based on the table history(DESCRIBE&nbsp;HISTORY table_name) command what could be the reason why you can not access this data?SELECT * FROM table_name TIMESTAMP AS OF date_sub(current_date(), 1)", 'related_lectures': []}, {'_class': 'assessment', 'id': 53426950, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have accidentally deleted records from a table called transactions, what is the easiest way to restore the records deleted or the previous state of the table? Prior to deleting the version of the table is 3 and after delete the version of the table is 4. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><p><a href="https://docs.databricks.com/sql/language-manual/delta-restore.html">RESTORE (Databricks SQL) | Databricks on AWS</a></p><p><br></p><pre class="prettyprint linenums">RESTORE [TABLE] table_name [TO] time_travel_version</pre><p><br></p><p>Time travel supports using timestamp or version number</p><p><br></p><pre class="prettyprint linenums">time_travel_version\n { TIMESTAMP AS OF timestamp_expression |\n   VERSION AS OF version }</pre><ul><li><p><code>timestamp_expression</code> can be any one of:</p><ul><li><p><code>\'2018-10-18T22:15:12.013Z\'</code>, that is, a string that can be cast to a timestamp</p></li><li><p><code>cast(\'2018-10-18 13:36:32 CEST\' as timestamp)</code></p></li><li><p><code>\'2018-10-18\'</code>, that is, a date string</p></li><li><p><code>current_timestamp() - interval 12 hours</code></p></li><li><p><code>date_sub(current_date(), 1)</code></p></li><li><p>Any other expression that is or can be cast to a timestamp</p></li></ul></li></ul><p><br></p><p><br></p>', 'answers': ['<p><code> RESTORE TABLE transactions FROM VERSION as of 4</code></p>', '<p><code>RESTORE TABLE transactions TO VERSION as of 3</code></p>', '<pre class="prettyprint linenums">INSERT INTO OVERWRITE&nbsp;transactions\nSELECT * FROM transactions VERSION&nbsp;AS&nbsp;OF 3\nMINUS\nSELECT * FROM transactions</pre>', '<pre class="prettyprint linenums">INSERT INTO OVERWRITE transactions\nSELECT * FROM transactions VERSION&nbsp;AS&nbsp;OF&nbsp;4\nINTERSECT\nSELECT * FROM transactions</pre>', '<p><code>COPY OVERWRITE transactions from VERSION as of 3</code></p>']}, 'correct_response': ['b'], 'section': 'Data Lakehouse Platform', 'question_plain': 'You have accidentally deleted records from a table called transactions, what is the easiest way to restore the records deleted or the previous state of the table? Prior to deleting the version of the table is 3 and after delete the version of the table is 4.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426952, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Create a schema called bronze using location ‘/mnt/delta/bronze’, and check if the schema exists before creating.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html</a></p><p><br></p><pre class="prettyprint linenums">CREATE SCHEMA [ IF NOT EXISTS ] schema_name [ LOCATION schema_directory ]</pre><p><br></p>', 'answers': ["<p><code>CREATE SCHEMA IF NOT EXISTS bronze LOCATION '/mnt/delta/bronze'</code> </p>", "<p><code>CREATE SCHEMA bronze IF NOT EXISTS LOCATION '/mnt/delta/bronze'</code> </p>", "<p><code>if IS_SCHEMA('bronze'): CREATE SCHEMA bronze LOCATION '/mnt/delta/bronze'</code> </p>", '<p>Schema creation is not available in metastore, it can only be done in Unity catalog UI</p>', '<p>Cannot create schema without a database</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Create a schema called bronze using location ‘/mnt/delta/bronze’, and check if the schema exists before creating.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426954, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How do you check the location of an existing schema in Delta Lake?&nbsp; </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Here is an example of how it looks</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_07-02-45-f20575c0da694aaf43911a15dd47be22.JPG">', 'answers': ['<p> Run SQL command <code>SHOW LOCATION schema_name</code> </p>', '<p>Check unity catalog UI</p>', '<p>Use Data explorer</p>', '<p>Run SQL&nbsp;command <code>DESCRIBE SCHEMA EXTENDED schema_name</code> </p>', '<p>Schemas are internally in-store external hive meta stores like MySQL or SQL Server</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'How do you check the location of an existing schema in Delta Lake?&nbsp;', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426956, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the below SQL commands create a Global temporary view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><br></p><pre class="prettyprint linenums"> CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name\n    AS SELECT * FROM table_name</pre><p><br></p><p>There are two types of temporary views that can be created Local and Global</p><ul><li><p>A session-scoped temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if a notebook is detached and reattached local temporary view is lost.</p></li></ul><ul><li><p>A global temporary view is available to all the notebooks in the cluster but if a cluster restarts a global temporary view is lost.</p></li></ul>', 'answers': ['<pre class="prettyprint linenums">CREATE OR REPLACE TEMPORARY VIEW view_name\n    AS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums"> CREATE OR REPLACE LOCAL TEMPORARY VIEW view_name\n    AS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums"> CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name\n    AS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE VIEW view_name\n    AS SELECT * FROM table_name</pre>', '<pre class="prettyprint linenums"> CREATE OR REPLACE LOCAL VIEW view_name\n    AS SELECT * FROM table_name</pre>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the below SQL commands create a Global temporary view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426958, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When you drop a managed table using SQL&nbsp;syntax<code> DROP TABLE table_name</code> how does it impact metadata, history, and data stored in the table?</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>For a managed table, a drop command will drop everything from metastore and storage. </p><p>See the below image to understand the differences between dropping an external table. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-16_21-57-03-ed10de74f43515b96cfe510e6494966a.jpeg">', 'answers': ['<p>Drops table from meta store, drops metadata, history, and data in storage. </p>', '<p>Drops table from meta store and data from storage but keeps metadata and history in storage</p>', '<p>Drops table from meta store, meta data and history but keeps the data in storage</p>', '<p>Drops table but keeps meta data, history and data in storage</p>', '<p>Drops table and history but keeps meta data and data in storage</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'When you drop a managed table using SQL&nbsp;syntax DROP TABLE table_name how does it impact metadata, history, and data stored in the table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426960, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The team has decided to take advantage of table properties to identify a business owner for each table, which of the following table DDL syntax allows you to populate a table property identifying the business owner of a table</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><code>CREATE TABLE inventory (id INT, units FLOAT) TBLPROPERTIES (business_owner = ‘supply chain’)</code> </p><p><br></p><p><a href="https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-tblproperties.html#tblproperties">Table properties and table options (Databricks SQL) | Databricks on AWS</a></p><p><br></p><p>Alter table command can used to update the TBLPROPERTIES</p><p><br></p><p><code>ALTER TABLE inventory SET TBLPROPERTIES(business_owner , \'operations\')</code> </p>', 'answers': ["<p><code>CREATE TABLE inventory (id INT, units FLOAT) </code></p><p><code>SET TBLPROPERTIES business_owner = 'supply chain'</code> </p>", "<p><code>CREATE TABLE inventory (id INT, units FLOAT) </code></p><p><code>TBLPROPERTIES (business_owner = 'supply chain')</code> </p>", '<p><code>CREATE TABLE inventory (id INT, units FLOAT) </code></p><p><code>SET (business_owner = ‘supply chain’)</code> </p>', '<p><code>CREATE TABLE inventory (id INT, units FLOAT) </code></p><p><code>SET PROPERTY (business_owner = ‘supply chain’)</code> </p>', '<p><code>CREATE TABLE inventory (id INT, units FLOAT) </code></p><p><code>SET TAG (business_owner = ‘supply chain’)</code> </p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'The team has decided to take advantage of table properties to identify a business owner for each table, which of the following table DDL syntax allows you to populate a table property identifying the business owner of a table', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426962, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Data science team has requested they are missing a column in the table called average price, this can be calculated using units sold and sales amt, which of the following SQL statements allow you to reload the data with additional column</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<pre class="prettyprint linenums">CREATE OR REPLACE TABLE sales\nAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales</pre><p><br></p><p>The main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify the schema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT OVERWRITE only overwrites the data.</p><p>INSERT OVERWRITE can also be used to overwrite schema, only when <code>spark.databricks.delta.schema.autoMerge.enabled is set true</code> if this option is not enabled and if there is a schema mismatch command will fail.</p>', 'answers': ['<pre class="prettyprint linenums">INSERT OVERWRITE sales\nSELECT *, salesAmt/unitsSold as avgPrice FROM sales</pre>', '<pre class="prettyprint linenums">CREATE OR REPLACE TABLE sales\nAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales</pre>', '<p><code> MERGE INTO sales USING (SELECT *, salesAmt/unitsSold as avgPrice FROM sales)</code> </p>', '<p><code>OVERWRITE sales AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales</code> </p>', '<p><code>COPY INTO SALES AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales</code> </p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Data science team has requested they are missing a column in the table called average price, this can be calculated using units sold and sales amt, which of the following SQL statements allow you to reload the data with additional column', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426964, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on a process to load external CSV files into a delta table by leveraging the COPY INTO command, but after running the command for the second time no data was loaded into the table name, why is that?</p><p><br></p><pre class="prettyprint linenums">COPY INTO table_name\nFROM \'dbfs:/mnt/raw/*.csv\'\nFILEFORMAT = CSV</pre><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is COPY INTO did not detect new files after the last load, </p><p>COPY&nbsp;INTO&nbsp;keeps track of files that were successfully loaded into the table, the next time when the COPY&nbsp;INTO&nbsp;runs it skips them. </p><p><br></p><p>FYI, you can change this behavior by using COPY_OPTIONS&nbsp;\'force\'= \'true\', when this option is enabled all files in the path/pattern are loaded. </p><p><br></p><pre class="prettyprint linenums">COPY INTO table_identifier\n  FROM [ file_location | (SELECT identifier_list FROM file_location) ]\n  FILEFORMAT = data_source\n  [FILES = [file_name, ... | PATTERN = \'regex_pattern\']\n  [FORMAT_OPTIONS (\'data_source_reader_option\' = \'value\', ...)]\n  [COPY_OPTIONS \'force\' = (\'false\'|\'true\')]</pre><p><br></p>', 'answers': ['<p>COPY INTO only works one time data load</p>', '<p>Run REFRESH TABLE sales before running COPY INTO</p>', '<p>COPY INTO did not detect new files after the last load</p>', '<p>Use incremental = TRUE option to load new files</p>', '<p>COPY INTO does not support incremental load, use AUTO LOADER</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': "You are working on a process to load external CSV files into a delta table by leveraging the COPY INTO command, but after running the command for the second time no data was loaded into the table name, why is that?COPY INTO table_name\nFROM 'dbfs:/mnt/raw/*.csv'\nFILEFORMAT = CSV", 'related_lectures': []}, {'_class': 'assessment', 'id': 53426966, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the main difference between the below two commands?</p><p><br></p><pre class="prettyprint linenums">INSERT OVERWRITE table_name\nSELECT * FROM table\n</pre><pre class="prettyprint linenums">CREATE OR REPLACE TABLE table_name\nAS SELECT * FROM table</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, <code>INSERT OVERWRITE</code> replaces data, CRAS replaces data and Schema</p><p><br></p><p>The main difference between <code>INSERT OVERWRITE</code> and <code>CREATE OR REPLACE TABLE(CRAS)</code> is that CRAS can modify the schema of the table, i.e it can add new columns or change data types of existing columns. By default <code>INSERT OVERWRITE</code> only overwrites the data.</p><p><code>INSERT OVERWRITE</code> can also be used to overwrite schema, only when <code>spark.databricks.delta.schema.autoMerge.enabled</code> is set <code>true</code> if this option is not enabled and if there is a schema mismatch command will fail.</p>', 'answers': ['<p><code>INSERT OVERWRITE </code>replaces data by default, <code>CREATE&nbsp;OR&nbsp;REPLACE</code> replaces data and Schema by default</p>', '<p><code>INSERT OVERWRITE</code> replaces data and schema by default, <code>CREATE&nbsp;OR&nbsp;REPLACE</code>replaces data by default</p>', '<p><code>INSERT OVERWRITE</code> maintains historical data versions by default, <code>CREATE&nbsp;OR&nbsp;REPLACE</code>clears the historical data versions by default</p>', '<p><code>INSERT OVERWRITE</code> clears historical data versions by default, <code>CREATE&nbsp;OR&nbsp;REPLACE</code> maintains the historical data versions by default</p>', '<p>Both are same and results in identical outcomes</p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What is the main difference between the below two commands?INSERT OVERWRITE table_name\nSELECT * FROM table\nCREATE OR REPLACE TABLE table_name\nAS SELECT * FROM table', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426968, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following functions can be used to convert JSON string to Struct data type?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><strong>Syntax</strong></p><p>Copy</p><pre class="prettyprint linenums">from_json(jsonStr, schema [, options])\n</pre><p><strong>Arguments</strong></p><ul><li><p><code>jsonStr</code>: A STRING expression specifying a row of CSV data.</p></li><li><p><code>schema</code>: A STRING literal or invocation of <a href="https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/schema_of_json">schema_of_json function (Databricks SQL)</a>.</p></li><li><p><code>options</code>: An optional MAP&lt;STRING,STRING&gt; literal specifying directives.</p></li></ul><p>Refer documentation for more details, </p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/from_json">https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/from_json</a></p>', 'answers': ['<p>TO_STRUCT (json value)</p>', '<p> FROM_JSON (json value)</p>', '<p>FROM_JSON (json value, schema of json)</p>', '<p>CONVERT (json value, schema of json)</p>', '<p>CAST&nbsp;(json value as STRUCT)</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following functions can be used to convert JSON string to Struct data type?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426970, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on a marketing team request to identify customers with the same information between two tables CUSTOMERS_2021 and CUSTOMERS_2020 each table contains 25 columns with the same schema, You are looking to identify rows that match between two tables across all columns, which of the following can be used to perform in SQL</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Answer is, </p><p><br></p><pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 \n INTERSECT\nSELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2020</pre><p><br></p><p>To compare all the rows between both the tables across <strong>all the columns</strong> using intersect will help us achieve that, an inner join is only going to check if the same column value exists across both the tables on a single column. </p><p><br></p><p><strong>INTERSECT [ALL | DISTINCT]</strong></p><p><br></p><ul><li><p>Returns the set of rows which are in both subqueries.</p><p>If <code>ALL</code> is specified a row that appears multiple times in the <code>subquery1</code> as well as in <code>subquery</code> will be returned multiple times.</p><p>If <code>DISTINCT</code> is specified the result does not contain duplicate rows. This is the default.</p></li></ul>', 'answers': ['<pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 \n UNION\nSELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2020</pre>', '<pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 \n UNION ALL\nSELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2020</pre>', '<pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 C1\nINNER&nbsp;JOIN&nbsp;CUSTOMERS_2020 C2\nON&nbsp;C1.CUSTOMER_ID&nbsp;= C2.CUSTOMER_ID</pre>', '<pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 \n INTERSECT\nSELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2020</pre>', '<pre class="prettyprint linenums">SELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2021 \nEXCEPT\nSELECT&nbsp;*&nbsp;FROM&nbsp;CUSTOMERS_2020</pre>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are working on a marketing team request to identify customers with the same information between two tables CUSTOMERS_2021 and CUSTOMERS_2020 each table contains 25 columns with the same schema, You are looking to identify rows that match between two tables across all columns, which of the following can be used to perform in SQL', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426972, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are looking to process the data based on two variables, one to check if the department is supply chain and second to check if process flag is set to True</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p><code>if department = “supply chain” &amp; process:</code> </p>', '<p><code> if department == “supply chain” &amp;&amp; process: </code> </p>', '<p><code>if department == “supply chain” &amp; process == TRUE:</code> </p>', '<p><code>if department == “supply chain” &amp; if process == TRUE:</code> </p>', '<p><code>if department == "supply chain" and process:</code> </p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are looking to process the data based on two variables, one to check if the department is supply chain and second to check if process flag is set to True', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426974, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You were asked to create a notebook that can take department as a parameter and process the data accordingly, which is the following statements result in storing the notebook parameter into a python variable </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>department = dbutils.widget.get("department")</code> </p><p>Refer to additional documentation here </p><p><a href="https://docs.databricks.com/notebooks/widgets.html">https://docs.databricks.com/notebooks/widgets.html</a></p>', 'answers': ['<p><code>SET department = dbutils.widget.get("department")</code> </p>', '<p><code>ASSIGN department == dbutils.widget.get("department")</code> </p>', '<p><code>department = dbutils.widget.get("department")</code> </p>', '<p><code>department = notebook.widget.get("department")</code> </p>', '<p><code>department = notebook.param.get("department")</code> </p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You were asked to create a notebook that can take department as a parameter and process the data accordingly, which is the following statements result in storing the notebook parameter into a python variable', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426976, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statements can successfully read the notebook widget and pass the python variable to a SQL statement in a Python notebook cell?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<pre class="prettyprint linenums">order_date  = dbutils.widgets.get("widget_order_date")\n\nspark.sql(f"SELECT * FROM sales WHERE orderDate = \'f{order_date }\'") \n</pre>', '<pre class="prettyprint linenums">order_date  = dbutils.widgets.get("widget_order_date")\n \nspark.sql(f"SELECT * FROM sales WHERE orderDate = \'order_date\' ") </pre>', '<pre class="prettyprint linenums">order_date  = dbutils.widgets.get("widget_order_date")\n\nspark.sql(f”SELECT * FROM sales WHERE orderDate = \'${order_date }\' ") </pre>', '<pre class="prettyprint linenums">order_date  = dbutils.widgets.get("widget_order_date")\n\nspark.sql(f"SELECT * FROM sales WHERE orderDate = \'{order_date}\' ") </pre>', '<pre class="prettyprint linenums">order_date  = dbutils.widgets.get("widget_order_date")\n\nspark.sql("SELECT * FROM sales WHERE orderDate = order_date") </pre>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following statements can successfully read the notebook widget and pass the python variable to a SQL statement in a Python notebook cell?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426978, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The below spark command is looking to create a summary table based customerId and the number of times the customerId is present in the event_log delta table and write a one-time micro-batch to a summary table, fill in the blanks to complete the query.</p><p><br></p><pre class="prettyprint linenums">spark._________\n  .format("delta")\n  .table("events_log")\n  .groupBy("customerId")\n  .count()\n  ._______\n  .format("delta")\n  .outputMode("complete")\n  .option("checkpointLocation", "/tmp/delta/eventsByCustomer/_checkpoints/")\n  .trigger(______)\n  .table("target_table")</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is readStream, writeStream, once = True. </p><p><br></p><p>spark.<strong>readStream</strong></p><p>&nbsp; .format("delta")</p><p>&nbsp; .table("events_log")</p><p>&nbsp; .groupBy("customerId")</p><p>&nbsp; .count()</p><p>&nbsp; .<strong>writeStream</strong></p><p>&nbsp; .format("delta")</p><p>&nbsp; .outputMode("complete")</p><p>&nbsp; .option("checkpointLocation", "/tmp/delta/eventsByCustomer/_checkpoints/")</p><p>&nbsp; .trigger(<strong>once = True</strong>)</p><p>&nbsp; .table("target_table")</p>', 'answers': ['<p> writeStream, readStream, once</p>', '<p>readStream, writeStream, once</p>', '<p>writeStream, processingTime = once</p>', '<p>writeStream, readStream, once = True</p>', '<p>readStream, writeStream, once = True</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'The below spark command is looking to create a summary table based customerId and the number of times the customerId is present in the event_log delta table and write a one-time micro-batch to a summary table, fill in the blanks to complete the query.spark._________\n  .format("delta")\n  .table("events_log")\n  .groupBy("customerId")\n  .count()\n  ._______\n  .format("delta")\n  .outputMode("complete")\n  .option("checkpointLocation", "/tmp/delta/eventsByCustomer/_checkpoints/")\n  .trigger(______)\n  .table("target_table")', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426980, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You would like to build a spark streaming process to read from a Kafka queue and write to a Delta table every 15 minutes, what is the correct trigger option</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code> trigger(processingTime = "15 Minutes")</code> </p><p><br></p><p>Triggers:</p><p><br></p><ul><li><p><strong>Unspecified</strong></p><p> This is the default. This is equivalent to using processingTime="500ms"</p></li><li><p><strong><em>Fixed interval micro-batches&nbsp; &nbsp;.trigger(processingTime="2 minutes")</em></strong></p><p>The query will be executed in micro-batches and kicked off at the user-specified intervals</p></li><li><p><strong>One-time micro-batch .trigger(once=True)</strong></p><p>The query will execute a single micro-batch to process all the available data and then stop on its own</p></li><li><p><strong>One-time micro-batch.trigger .trigger(availableNow=True)&nbsp; --&nbsp;New feature a better version of (once=True)</strong></p><p><br></p></li></ul><p>Databricks supports <code>trigger(availableNow=True)</code> in Databricks Runtime 10.2 and above for Delta Lake and Auto Loader sources. This functionality combines the batch processing approach of trigger once with the ability to configure batch size, resulting in multiple parallelized batches that give greater control for right-sizing batches and the resultant files.</p><p><br></p>', 'answers': ['<p><code> trigger("15 minutes")</code> </p>', '<p><code>trigger(process "15 minutes")</code> </p>', '<p><code> trigger(processingTime = 15)</code> </p>', '<p><code> trigger(processingTime = "15 Minutes")</code> </p>', '<p><code>trigger(15)</code> </p>']}, 'correct_response': ['d'], 'section': 'Incremental Data Processing', 'question_plain': 'You would like to build a spark streaming process to read from a Kafka queue and write to a Delta table every 15 minutes, what is the correct trigger option', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426982, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following scenarios is the best fit for the AUTO LOADER solution?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Efficiently process new data incrementally from cloud object storage. </p><p>Please note:&nbsp; AUTO&nbsp;LOADER&nbsp;only works on data/files located in cloud object storage like S3 or Azure Blob Storage it does not have the ability to read other data sources, although AUTO&nbsp;LOADER&nbsp;is built on top of structured streaming it only supports files in the cloud object storage. If you want to use Apache Kafka then you can just use structured streaming. </p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-14_17-29-05-eb36006e7812ce0245401ad5908f66af.jpeg"></p><p>Auto Loader and Cloud Storage Integration</p><p><br></p><p>Auto Loader supports a couple of ways to ingest data incrementally</p><p><br></p><ol><li><p>Directory listing - List Directory and maintain the state in RocksDB, supports incremental file listing </p></li><li><p>File notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike Directory listing File notification can scale up to millions of files per day. </p></li></ol><p><br></p><p><br></p><p><strong>[OPTIONAL] </strong></p><p><strong>Auto Loader vs COPY&nbsp;INTO?</strong></p><p><br></p><p><strong>Auto Loader</strong></p><p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup. Auto Loader provides a new Structured Streaming source called <code>cloudFiles</code>. Given an input directory path on the cloud file storage, the <code>cloudFiles</code> source automatically processes new files as they arrive, with the option of also processing existing files in that directory.</p><p>When to use Auto Loader instead of the COPY&nbsp;INTO?</p><p><br></p><ul><li><p>You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover files more efficiently than the <code>COPY INTO</code> SQL command and can split file processing into multiple batches.</p></li><li><p>You do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess subsets of files. However, you can use the <code>COPY INTO</code> SQL command to reload subsets of files while an Auto Loader stream is simultaneously running.</p></li></ul><p><br></p><p>Refer to more documentation here, </p><p>https://docs.microsoft.com/en-us/azure/databricks/ingestion/auto-loader</p>', 'answers': ['<p>Efficiently process new data incrementally from cloud object storage</p>', '<p>Incrementally process new streaming data from Apache Kafa into delta lake</p>', '<p>Incrementally process new data from relational databases like MySQL</p>', '<p>Efficiently copy data from data lake location to another data lake location</p>', '<p>Efficiently move data incrementally from one delta table to another delta table</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following scenarios is the best fit for the AUTO LOADER solution?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426984, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You had AUTO LOADER to process millions of files a day and noticed slowness in load process, so you scaled up the Databricks cluster but realized the performance of the Auto loader is still not improving, what is the best way to resolve this.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The default value of maxFilesPerTrigger is 1000 it can be increased to a much higher number but will require a much larger compute to process. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-20_05-27-46-d3ce85ff124147429f54b9d279f818ed.JPG"><p><br></p><p><a href="https://docs.databricks.com/ingestion/auto-loader/options.html">https://docs.databricks.com/ingestion/auto-loader/options.html</a></p><p><br></p><p><br></p>', 'answers': ['<p>AUTO LOADER is not suitable to process millions of files a day</p>', '<p>Setup a second AUTO LOADER process to process the data</p>', '<p>Increase the <strong>maxFilesPerTrigger </strong>option to a sufficiently high number</p>', '<p>Copy the data from cloud storage to local disk on the cluster for faster access</p>', '<p>Merge files to one large file</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'You had AUTO LOADER to process millions of files a day and noticed slowness in load process, so you scaled up the Databricks cluster but realized the performance of the Auto loader is still not improving, what is the best way to resolve this.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426986, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The current ELT&nbsp;pipeline is receiving data from the operations team once a day so you had setup an AUTO LOADER process to run once a day using trigger (Once = True) and scheduled a job to run once a day, operations team recently rolled out a new feature that allows them to send data every&nbsp;1 min, what changes do you need to make to AUTO LOADER to process the data every 1 min.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Convert AUTO LOADER to structured streaming</p>', '<p> Change AUTO LOADER trigger to .trigger(ProcessingTime = "1 minute")</p>', '<p>Setup a job cluster run the notebook once a minute</p>', '<p>Enable stream processing</p>', '<p>Change AUTO LOADER trigger to ("1 minute")</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'The current ELT&nbsp;pipeline is receiving data from the operations team once a day so you had setup an AUTO LOADER process to run once a day using trigger (Once = True) and scheduled a job to run once a day, operations team recently rolled out a new feature that allows them to send data every&nbsp;1 min, what changes do you need to make to AUTO LOADER to process the data every 1 min.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426988, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> What is the purpose of the bronze layer in a Multi-hop Medallion architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, copy of raw data, easy to query and ingest data for downstream processes, </p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p><br></p><p>Here are the typical role of Bronze Layer in a medallion architecture. </p><p>Bronze Layer:</p><p>1. Raw copy of ingested data</p><p>2. Replaces traditional data lake</p><p>3. Provides efficient storage and querying of full, unprocessed history of data</p><p>4. No schema is applied at this layer</p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p><em>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content.</em> </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-45-43-8b2fadf107c07b5921ebb1f90eba2337.jpeg"><p><br></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>Copy of raw data, easy to query and ingest data for downstream processes.</p>', '<p>Powers ML applications</p>', '<p>Data quality checks, corrupt data quarantined</p>', '<p>Contain aggregated data that is to be consumed into Silver</p>', '<p>Reduces data storage by compressing the data</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of the bronze layer in a Multi-hop Medallion architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426990, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the purpose of the silver layer in a Multi hop architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Silver Layer:</p><p>1. Reduces data storage complexity, latency, and redundency</p><p>2. Optimizes ETL throughput and analytic query performance</p><p>3. Preserves grain of original data (without aggregation)</p><p>4. Eliminates duplicate records</p><p>5. production schema enforced</p><p>6. Data quality checks, quarantine corrupt data</p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content. </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-46-01-c1b1f255aa4f7b827cc2e8c25316f3ae.jpeg">', 'answers': ['<p>Replaces a traditional data lake</p>', '<p>Efficient storage and querying of full, unprocessed history of data</p>', '<p>Eliminates duplicate data, quarantines bad data</p>', '<p>Refined views with aggregated data</p>', '<p>Optimized query performance for business-critical data</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of the silver layer in a Multi hop architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426992, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the purpose of gold layer in Multi hop architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Gold Layer:</p><p><br></p><p>1. Powers Ml applications, reporting, dashboards, ad hoc analytics</p><p>2. Refined views of data, typically with aggregations</p><p>3. Reduces strain on production systems</p><p>4. Optimizes query performance for business-critical data</p><p><br></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content. </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-46-08-4faaf86540ab39a2dd0bf59f33d76537.jpeg">', 'answers': ['<p>Optimizes ETL throughput and analytic query performance</p>', '<p>Eliminate duplicate records</p>', '<p>Preserves grain of original data, without any aggregations</p>', '<p>Data quality checks and schema enforcement</p>', '<p>Optimized query performance for business-critical data</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the purpose of gold layer in Multi hop architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426994, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The Delta Live Tables Pipeline is configured to run in Development mode using the Triggered Pipeline Mode. what is the expected outcome after clicking Start to update the pipeline?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing. </p><p><br></p><p>DLT&nbsp;pipeline supports two modes Development and Production, you can switch between the two based on the stage of your development and deployment lifecycle. </p><p><br></p><p><strong>Development and production modes</strong></p><p>When you run your pipeline in development mode, the Delta Live Tables system:</p><ul><li><p>Reuses a cluster to avoid the overhead of restarts.</p></li><li><p>Disables pipeline retries so you can immediately detect and fix errors.</p></li></ul><p>In production mode, the Delta Live Tables system:</p><ul><li><p>Restarts the cluster for specific recoverable errors, including memory leaks and stale credentials.</p></li><li><p>Retries execution in the event of specific errors, for example, a failure to start a cluster.</p></li></ul><p>Use the <img src="https://docs.databricks.com/_images/dlt-env-toggle.png"> buttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in development mode.</p><p>Switching between development and production modes only controls cluster and pipeline execution behavior. Storage locations must be configured as part of pipeline settings and are not affected when switching between modes.</p><p><br></p><p>Please review additional DLT&nbsp;concepts using below link </p><p><br></p><p>https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts</p><p><br></p>', 'answers': ['<p>All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated</p>', '<p>All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped</p>', '<p> All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after the pipeline is stopped to allow for additional development and testing</p>', '<p>All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional development and testing</p>', '<p>All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with the pipeline</p>']}, 'correct_response': ['d'], 'section': 'Incremental Data Processing', 'question_plain': 'The Delta Live Tables Pipeline is configured to run in Development mode using the Triggered Pipeline Mode. what is the expected outcome after clicking Start to update the pipeline?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426996, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The Delta Live Table Pipeline is configured to run in Production mode using the continuous Pipeline Mode. what is the expected outcome after clicking Start to update the pipeline?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, </p><p> All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with the pipeline until it is shut down since the execution mode is chosen to be continuous. It does not matter if the pipeline mode is development or production, pipeline mode only matters during the pipeline initialization. </p><p><br></p><p>DLT&nbsp;pipeline supports two modes Development and Production, you can switch between the two based on the stage of your development and deployment lifecycle. </p><p><br></p><p><strong>Development and production modes</strong></p><p><strong>Development:&nbsp;</strong></p><p>When you run your pipeline in development mode, the Delta Live Tables system:</p><ul><li><p>Reuses a cluster to avoid the overhead of restarts.</p></li><li><p>Disables pipeline retries so you can immediately detect and fix errors.</p></li></ul><p><strong>Production:&nbsp;</strong></p><p><em>In production mode, the Delta Live Tables system:</em></p><ul><li><p><em>Restarts the cluster for specific recoverable errors, including memory leaks and stale credentials.</em></p></li><li><p><em>Retries execution in the event of specific errors, for example, a failure to start a cluster.</em></p></li></ul><p>Use the <img src="https://docs.databricks.com/_images/dlt-env-toggle.png"> buttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in development mode.</p><p>Switching between development and production modes only controls cluster and pipeline execution behavior. Storage locations must be configured as part of pipeline settings and are not affected when switching between modes.</p><p><br></p><p><strong>Delta Live Tables supports two different modes of execution:</strong></p><p>Triggered pipelines update each table with whatever data is currently available and then stop the cluster running the pipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those that read from external sources. Tables within the pipeline are updated after their dependent data sources have been updated.</p><p><em>Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run until manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers have the most up-to-date data</em></p><p><br></p><p>Please review additional DLT&nbsp;concepts using the below link </p><p><br></p><p>https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts</p><p><br></p>', 'answers': ['<p>All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated</p>', '<p>All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped</p>', '<p> All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after the pipeline is stopped to allow for additional testing</p>', '<p>All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing</p>', '<p> All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with the pipeline</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'The Delta Live Table Pipeline is configured to run in Production mode using the continuous Pipeline Mode. what is the expected outcome after clicking Start to update the pipeline?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426998, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working to set up two notebooks to run on a schedule, the second notebook is dependent on the first notebook but both notebooks need different types of compute to run in an optimal fashion, what is the best way to set up these notebooks as jobs?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Tasks in Jobs support different clusters for each task in the same job. </p>', 'answers': ['<p>Use DELTA LIVE PIPELINES instead of notebook tasks</p>', '<p>A Job can only use single cluster, setup job for each notebook and use job dependency to link both jobs together</p>', '<p>Each task can use different cluster, add these two notebooks as two tasks in a single job with linear dependency and modify the cluster as needed for each of the tasks </p>', '<p>Use a single job to setup both notebooks as individual tasks, but use the cluster API to setup the second cluster before the start of second task</p>', '<p>Use a very large cluster to run both the tasks in a single job</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'You are working to set up two notebooks to run on a schedule, the second notebook is dependent on the first notebook but both notebooks need different types of compute to run in an optimal fashion, what is the best way to set up these notebooks as jobs?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427000, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are tasked to set up a set notebook as a job for six departments and each department can run the task parallelly, the notebook takes an input parameter dept number to process the data by department, how do you go about to setup this up in job?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Here is how you setup </p><p><br></p><p>Create a single job and six tasks with the same notebook and assign a different parameter for each task , </p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_06-15-02-179ce342961e4a5f556ee6db26f37b82.JPG"><p><br></p><p>All tasks are added in a single job and can run parallel either using single shared cluster or with individual clusters. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_06-15-03-a86e816accf269bcd11af70d49ce0aa5.JPG"><p><br></p>', 'answers': ['<p>Use a single notebook as task in the job and use dbutils.notebook.run to run each notebook with parameter in a different cell</p>', '<p>A task in the job cannot take an input parameter, create six notebooks with hardcoded dept number and setup six tasks with linear dependency in the job</p>', '<p>A task accepts key-value pair parameters, creates six tasks pass department number as parameter foreach task with no dependency in the job as they can all run in parallel. </p>', '<p>A parameter can only be passed at the job level, create six jobs pass department number to each job with linear job dependency</p>', '<p>A parameter can only be passed at the job level, create six jobs pass department number to each job with no job dependency</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'You are tasked to set up a set notebook as a job for six departments and each department can run the task parallelly, the notebook takes an input parameter dept number to process the data by department, how do you go about to setup this up in job?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427002, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are asked to setup two tasks in a databricks job, the first task runs a notebook to download the data from a remote system, and the second task is a DLT pipeline that can process this data, how do you plan to configure this in Jobs UI</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Single job can be used to set up both notebook and DLT pipeline, use two different tasks with linear dependency, </p><p><br></p><p>Here is the JOB UI </p><ol><li><p>Create a notebook task </p></li><li><p>Create DLT&nbsp;task </p><ol><li><p>add notebook task as dependency </p></li></ol></li><li><p>Final view </p></li></ol><p><br></p><p>Create the notebook task </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_06-27-34-ef0b04b0e35016814a7fdd85e6ea9b51.jpg"><p><br></p><p>DLT task </p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_06-27-35-69123d69cddd04f2060e22f71afa0953.jpg"><p><br></p><p>Final view </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-24_06-27-35-b783dd767f8de5b19ee99437d7f16424.JPG">', 'answers': ['<p>Single job cannot have a notebook task and DLT Pipeline task, use two different jobs with linear dependency.</p>', '<p>Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in continuous mode.</p>', '<p>Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in trigger mode.</p>', '<p> Single job can be used to setup both notebook and DLT pipeline, use two different tasks with linear dependency.</p>', '<p>Add first step in the DLT pipeline and run the DLT pipeline as triggered mode in JOBS UI</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'You are asked to setup two tasks in a databricks job, the first task runs a notebook to download the data from a remote system, and the second task is a DLT pipeline that can process this data, how do you plan to configure this in Jobs UI', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427004, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are asked to set up an alert to notify in an email every time a KPI&nbsp;indicater increases beyond a threshold value, team also asked you to include the actual value in the alert email notification.</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Alerts support custom template supports using variables to customize the default message, set up the query to compare the KPI&nbsp;current value to the threshold and use the variable QUERY_RESULT_VALUE&nbsp;to display the value in the email notification. </p><p><br></p><p>here is a simple alert, that uses variables in the custom template to present these values in the email notification message, when the alert is fired these variables get replaced with actual values. </p><p><br></p><p>Alert with custom template </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-16_21-37-14-3514735f6aec0bd5ab879dbb3205d246.jpg"><p><br></p><p>When you enable preview you can see how the alert looks when you substitute the variables. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-16_21-37-14-bbbef74a7cb0eb45adba25bafef8e1ea.jpg"><p><br></p><p><br></p><p>Below are additional template variables available to you with the custom template. </p><p><br></p><p><a href="https://docs.databricks.com/sql/user/alerts/index.html#id2">Alerts | Databricks on AWS</a></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_06-41-06-5434df69f90135dc08c83e3c721c15a4.JPG"></p><p><br></p>', 'answers': ['<p>Use notebook and python code to run every minute, using python variables to capture send the information in an email</p>', '<p>Setup an alert but use the default template to notify the message in email’s subject</p>', '<p>Setup an alert but use the custom template to notify the message in email’s subject</p>', '<p>Use the webhook destination instead so alert message can be customized</p>', '<p>Use custom email hook to customize the message</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'You are asked to set up an alert to notify in an email every time a KPI&nbsp;indicater increases beyond a threshold value, team also asked you to include the actual value in the alert email notification.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427006, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Operations team is using a centralized data quality monitoring system, a user can publish data quality metrics through a webhook, you were asked to develop a process to send messages using a webhook if there is atleast one duplicate record, which of the following approaches can be taken to integrate an alert with current data quality monitoring system</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Alerts supports multiple destinations, email is the default destination.&nbsp; </p><p><br></p><p><a href="https://docs.databricks.com/sql/admin/alert-destinations.html">Alert destinations | Databricks on AWS</a></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_06-43-56-f628b58f18868e1eb01235dc06d15538.JPG"></p><p><br></p>', 'answers': ['<p>Use notebook and Jobs to use python to publish DQ metrics</p>', '<p>Setup an alert to send an email, use python to parse email, and publish a webhook message</p>', '<p>Setup an alert with custom template</p>', '<p>Setup an alert with custom Webhook destination</p>', '<p>Setup an alert with dynamic template</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'Operations team is using a centralized data quality monitoring system, a user can publish data quality metrics through a webhook, you were asked to develop a process to send messages using a webhook if there is atleast one duplicate record, which of the following approaches can be taken to integrate an alert with current data quality monitoring system', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427008, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working with the application team to setup a SQL Endpoint point, once the team started consuming the SQL Endpoint you noticed that during peak hours as the number of concurrent users increases you are seeing degradation in the query performance and the same queries are taking longer to run, which of the following steps can be taken to resolve the issue?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, They can increase the maximum bound of the SQL endpoint’s scaling range, when you increase the max scaling range more clusters are added so queries instead of waiting in the queue can start running using available clusters, see below for more explanation. </p><p><br></p><p><strong>The question is looking to test your ability to know how to scale a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) and you have to look for cue words or need to understand if the queries are running sequentially or concurrently. if the queries are running sequentially then scale up(Size of the cluster from 2X-Small to 4X-Large) if the queries are running concurrently or with more users then scale out(add more clusters). </strong></p><p><br></p><p><strong>SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) Overview:&nbsp;(Please read all of the below points and the below diagram to understand )</strong></p><p><br></p><ol><li><p>A SQL&nbsp;Warehouse should have at least one cluster</p></li><li><p>A cluster comprises one driver node and one or many worker nodes </p></li><li><p>No of worker nodes in a cluster is determined by the size of the cluster (2X -Small -&gt;1 worker, X-Small -&gt;2 workers.... up to 4X-Large -&gt;&nbsp;128 workers) this is called <strong>Scale up</strong></p></li><li><p>A single cluster irrespective of cluster size(2X-Smal.. to ...4XLarge)&nbsp;can only run 10 queries at any given time if a user submits 20 queries all at once to a warehouse with 3X-Large cluster size and cluster scaling (min 1, max1) while 10 queries will start running the remaining 10 queries wait in a queue for these 10 to finish. </p></li><li><p>Increasing the Warehouse cluster size can improve the performance of a query, example if a query runs for 1 minute in a 2X-Small warehouse size, it may run in 30 Seconds if we change the warehouse size to X-Small. this is due to 2X-Small has 1 worker node and X-Small has 2 worker nodes so the query has more tasks and runs faster (note: this is an ideal case example, the scalability of a query performance depends on many factors, it can not always be linear)</p></li><li><p>A warehouse can have more than one cluster this is called <strong>Scale out</strong>. If a warehouse is configured with X-Small cluster size with cluster scaling(Min1, Max 2) Databricks spins up an additional cluster if it detects queries are waiting in the queue, If a warehouse is configured to run 2 clusters(Min1, Max 2), and let\'s say a user submits 20 queries, 10 queriers will start running and holds the remaining in the queue and databricks will automatically start the second cluster and starts redirecting the 10 queries waiting in the queue to the second cluster. </p></li><li><p>A single query will not span more than one cluster, once a query is submitted to a cluster it will remain in that cluster until the query execution finishes irrespective of how many clusters are available to scale. </p></li></ol><p><br></p><p><br></p><p>Please review the below diagram to understand the above concepts:&nbsp;</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-12_20-08-11-f1e6f0a38c1e03819054685c241fafb5.jpeg"></p><p><br></p><p><br></p><p><br></p><p>SQL endpoint(SQL&nbsp;Warehouse) scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.</p><p><strong>Scale-out</strong> -&gt; to add more clusters for a SQL endpoint, change max number of clusters</p><p>If you are trying to improve the throughput, being able to run as many queries as possible then having an additional cluster(s) will improve the performance.</p><p><br></p><p>Databricks SQL&nbsp;automatically scales as soon as it detects queries are in queuing state, in this example scaling is set for min 1 and max 3 which means the warehouse can add three clusters if it detects queries are waiting.&nbsp; </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-04_16-05-29-810fdd9c0793c9e68e4f1ba6087d441b.jpeg"></p><p><br></p><p>During the warehouse creation or after you have the ability to change the warehouse size (2X-Small....to ...4XLarge) to improve query performance and the maximize scaling range to add more clusters on a SQL&nbsp;Endpoint(SQL&nbsp;Warehouse) scale-out, if you are changing an existing warehouse you may have to restart the warehouse to make the changes effective. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-04_16-05-29-f35a13ac5da3cffec97e5332f8115bbc.jpg"></p>', 'answers': ['<p>They can turn on the Serverless feature for the SQL endpoint.</p>', '<p>They can increase the maximum bound of the SQL endpoint’s scaling range.</p>', '<p>They can increase the cluster size(2X-Small to 4X-Large) of the SQL endpoint.</p>', '<p>They can turn on the Auto Stop feature for the SQL endpoint.</p>', '<p>They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Cost optimized” to “Reliability Optimized.”</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'You are currently working with the application team to setup a SQL Endpoint point, once the team started consuming the SQL Endpoint you noticed that during peak hours as the number of concurrent users increases you are seeing degradation in the query performance and the same queries are taking longer to run, which of the following steps can be taken to resolve the issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427010, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The data engineering team is using a bunch of SQL queries to review data quality and monitor the ETL job every day, which of the following approaches can be used to set up a schedule and automate this process?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Individual queries can be refreshed on a schedule basis,</p><p><br></p><p>To set the schedule:</p><ol><li><p>Click the query info tab.</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/query-info-tab.png"></p></li><li><p>Click the link to the right of <strong>Refresh Schedule</strong> to open a picker with schedule intervals.</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/schedule-modal.png"></p></li><li><p>Set the schedule.</p><p>The picker scrolls and allows you to choose:</p><ul><li><p>An interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks</p></li><li><p>A time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is greater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer’s timezone and converts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For example, if you want a query to execute at <code>00:00</code> UTC each day, but your current timezone is PDT (UTC-7), you should select <code>17:00</code> in the picker:</p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/weekly-schedule.png"></p></li></ul></li><li><p>Click <strong>OK</strong>.</p><p>Your query will run automatically.</p><p>If you experience a scheduled query not executing according to its schedule, you should manually trigger the query to make sure it doesn’t fail. However, you should be aware of the following:</p><ul><li><p>If you schedule an interval—for example, “every 15 minutes”—the interval is calculated from the <em>last successful execution</em>. If you manually execute a query, the scheduled query will not be executed until the interval has passed.</p></li><li><p>If you schedule a time, Databricks SQL waits for the results to be “outdated”. For example, if you have a query set to refresh every Thursday and you manually execute it on Wednesday, by Thursday the results will still be considered “valid”, so the query wouldn’t be scheduled for a new execution. Thus, for example, when setting a weekly schedule, check the last query execution time and expect the scheduled query to be executed on the selected day after that execution is a week old. Make sure not to manually execute the query during this time.</p></li></ul><p>If a query execution fails, Databricks SQL retries with a back-off algorithm. The more failures the further away the next retry will be (and it might be beyond the refresh interval).</p></li></ol><p><br></p><p>Refer documentation for additional info, </p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/schedule-query">https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/schedule-query</a></p><p><br></p><p><br></p>', 'answers': ['<p>They can schedule the query to run every 1 day from the Jobs UI</p>', '<p>They can schedule the query to refresh every 1 day from the query’s page in Databricks SQL.</p>', '<p>They can schedule the query to run every 12 hours from the Jobs UI.</p>', '<p>They can schedule the query to refresh every 1 day from the SQL endpoint’s page in Databricks SQL.</p>', '<p>They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'The data engineering team is using a bunch of SQL queries to review data quality and monitor the ETL job every day, which of the following approaches can be used to set up a schedule and automate this process?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427012, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> In order to use Unity catalog features, which of the following steps needs to be taken on managed/external tables in the Databricks workspace?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate">Upgrade tables and views to Unity Catalog - Azure Databricks | Microsoft Docs</a></p><p><br></p><p>Managed table:<a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate#--upgrade-a-table-to-unity-catalog"> Upgrade a managed to Unity Catalog</a></p><p>External table: <a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/migrate#--upgrade-an-external-table-to-unity-catalog">Upgrade an external table to Unity Catalog</a></p><p><br></p>', 'answers': ['<p>Enable unity catalog feature in workspace settings</p>', '<p>Migrate/upgrade objects in workspace managed/external tables/view to unity catalog</p>', '<p>Upgrade to DBR version 15.0</p>', '<p>Copy data from workspace to unity catalog</p>', '<p>Upgrade workspace to Unity catalog</p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'In order to use Unity catalog features, which of the following steps needs to be taken on managed/external tables in the Databricks workspace?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427014, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the top-level object in unity catalog?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/key-concepts">Key concepts - Azure Databricks | Microsoft Docs</a></p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_06-59-41-c66dd1af4c4eb7eeb31a7f49fe3661cb.jpg"><p><br></p>', 'answers': ['<p>Catalog</p>', '<p>Table</p>', '<p>Workspace</p>', '<p>Database</p>', '<p>Metastore</p>']}, 'correct_response': ['e'], 'section': 'Data Governance', 'question_plain': 'What is the top-level object in unity catalog?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427016, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> One of the team members Steve who has the ability to create views, created a new view called regional_sales_vw on the existing table called sales which is owned by John, and the second team member Kevin who works with regional sales managers wanted to query the data in regional_sales_vw, so Steve granted the permission to Kevin using command</p><p><code>GRANT VIEW, USAGE ON regional_sales_vw to kevin@company.com</code> but Kevin is still unable to access the view?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Ownership determines whether or not you can grant privileges on derived objects to other users, since Steve is not the owner of the underlying sales table, he can not grant access to the table or data in the table indirectly.</p><p><br></p><p>Only owner(user or group) can grant access to a object</p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#a-user-has-select-privileges-on-a-view-of-table-t-but-when-that-user-tries-to-select-from-that-view-they-get-the-error-user-does-not-have-privilege-select-on-table">https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#a-user-has-select-privileges-on-a-view-of-table-t-but-when-that-user-tries-to-select-from-that-view-they-get-the-error-user-does-not-have-privilege-select-on-table</a></p><p><br></p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#a-user-has-select-privileges-on-a-view-of-table-t-but-when-that-user-tries-to-select-from-that-view-they-get-the-error-user-does-not-have-privilege-select-on-table">Data object privileges - Azure Databricks | Microsoft Doc</a></p><p><br></p>', 'answers': ['<p> Kevin needs select access on the table sales</p>', '<p>Kevin needs owner access on the view regional_sales_vw</p>', '<p>Steve is not the owner of the sales table</p>', '<p>Kevin is not the owner of the sales table</p>', '<p>Table access control is not enabled on the table and view</p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': 'One of the team members Steve who has the ability to create views, created a new view called regional_sales_vw on the existing table called sales which is owned by John, and the second team member Kevin who works with regional sales managers wanted to query the data in regional_sales_vw, so Steve granted the permission to Kevin using commandGRANT VIEW, USAGE ON regional_sales_vw to kevin@company.com but Kevin is still unable to access the view?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53427018, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Kevin is the owner of the schema sales, Steve wanted to create new table in sales schema called regional_sales so Kevin grants the create table permissions to Steve. Steve creates the new table called regional_sales in sales schema, who is the owner of the table regional_sales</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>A user who creates the object becomes its owner, does not matter who is the owner of the parent object. </p>', 'answers': ['<p>Kevin is the owner of sales schema, all the tables in the schema will be owned by Kevin</p>', '<p>Steve is the owner of the table</p>', '<p>By default ownership is assigned DBO</p>', '<p> By default ownership is assigned to DEFAULT_OWNER</p>', '<p>Kevin and Smith both are owners of table</p>']}, 'correct_response': ['b'], 'section': 'Data Governance', 'question_plain': 'Kevin is the owner of the schema sales, Steve wanted to create new table in sales schema called regional_sales so Kevin grants the create table permissions to Steve. Steve creates the new table called regional_sales in sales schema, who is the owner of the table regional_sales', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426930, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You were asked to setup a new all-purpose cluster, but the cluster is unable to start which of the following steps do you need to take to identify the root cause of the issue and the reason why the cluster was unable to start?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Cluster event logs are very useful, to identify issues pertaining to cluster availability.&nbsp; Cluster may not start due to resource limitations or issues with the cloud providers.</p><p>Some of the common issues include a subnet for compute VM&nbsp;reaching its limits or exceeding the subscription or cloud account CPU&nbsp;quota limit. </p><p><br></p><p>Here is an example where the cluster did not start due to subscription reaching the quota limit on a certain type of cpu cores for a VM&nbsp;type. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_16-33-32-ced871f5c775883a5432107529d38593.jpg"><p>Click on event logs</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_16-33-32-63bf362213f6263e35ecfcd78e4fc17a.jpg"><p><br></p><p><br></p><p>Click on the message to see the detailed error message on why the cluster did not start. </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_16-33-33-a462c0fedae643c204ad96f1a93fce9d.jpg"><p><br></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p> Check the cluster driver logs</p>', '<p>Check the cluster event logs</p>', '<p>Workspace logs</p>', '<p>Storage account</p>', '<p>Data plane</p>']}, 'correct_response': ['b'], 'section': 'Data Lakehouse Platform', 'question_plain': 'You were asked to setup a new all-purpose cluster, but the cluster is unable to start which of the following steps do you need to take to identify the root cause of the issue and the reason why the cluster was unable to start?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426938, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Commit and push code. </p><p><br></p><p>See the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow.</p><p><br></p><p>All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git provider like Github or Azure Devops. </p><p><br></p><p><strong><em>Exam focus: Please study the below image carefully to understand all of the steps in the CI/CD flow to understand the tasks that are implemented in Databricks Repo vs Git Provider, exam may ask a different type of questions based on this flow. </em></strong></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_05-29-14-bae91a02b8021d71bb2bc97ef30e2e96.jpg"></p><p><br></p>', 'answers': ['<p>Delete branch</p>', '<p>Trigger Databricks CICD pipeline</p>', '<p>Commit and push code</p>', '<p>Create a pull request</p>', '<p>Approve the pull request</p>']}, 'correct_response': ['c'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426940, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You noticed that a team member started using an all-purpose cluster to develop a notebook and used the same all-purpose cluster to set up a job that can run every 30 mins so they can update underlying tables which are used in a dashboard. What would you recommend for reducing the overall cost of this approach?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>While using an all-purpose cluster is ok during development but anytime you don\'t need to interact with a notebook, especially for a scheduled job it is less expensive to use a job cluster. Using an all-purpose cluster can be twice as expensive as a job cluster.</p><p>Please note:&nbsp;The compute cost you pay the cloud provider for the same cluster type and size between an all-purpose cluster and job cluster is the same the only difference is the DBU&nbsp;cost. </p><p><br></p><p>The total cost of cluster = Total cost of VM&nbsp;compute(Azure or AWS or GCP)&nbsp;+&nbsp;Cost per DBU</p><p><br></p><p>The per DBU&nbsp;cost varies between all-purpose and Job Cluster</p><p><br></p><p>Here is the recent cost estimate from AWS&nbsp;between Jobs Cluster and all-purpose Cluster, for jobs compute its $0.15 cents per DBU&nbsp;v$0.55 cents per DBU&nbsp;for all-purpose</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_17-47-51-c505d5f1fe97a20cadb8b499cb73c358.jpg"><p><br></p><p>How do I&nbsp;check how much the DBU&nbsp;cost for my cluster?</p><p><br></p><p>When you click on an exister cluster or when you look at the cluster details you will see this in the top right corner</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_17-47-51-a26b854def787b44c11e1e58737851c1.jpg"><p><br></p>', 'answers': ['<p>Reduce the size of the cluster</p>', '<p>Reduce the number of nodes and enable auto scale</p>', '<p>Enable auto termination after 30 mins</p>', '<p>Change the cluster all-purpose to job cluster when scheduling the job</p>', '<p> Change the cluster mode from all-purpose to single-mode</p>']}, 'correct_response': ['d'], 'section': 'Data Lakehouse Platform', 'question_plain': 'You noticed that a team member started using an all-purpose cluster to develop a notebook and used the same all-purpose cluster to set up a job that can run every 30 mins so they can update underlying tables which are used in a dashboard. What would you recommend for reducing the overall cost of this approach?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53426942, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following commands can be used to run one notebook from another notebook?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>dbutils.notebook.run(" full notebook path ")</code> </p><p><br></p><p>Here is the full command with additional options. </p><p><br></p><p><code><strong>run(path: String, timeout_seconds: int, arguments: Map): String</strong></code></p><p><br></p><pre class="prettyprint linenums">dbutils.notebook.run("ful-notebook-name", 60, {"argument": "data", "argument2": "data2", ...})\n</pre>', 'answers': ['<p><code> notebook.utils.run("full notebook path")</code> </p>', '<p><code>execute.utils.run("full notebook path")</code> </p>', '<p><code>dbutils.notebook.run("full notebook path")</code> </p>', '<p>only job clusters can run notebook</p>', '<p><code>spark.notebook.run("full notebook path")</code> </p>']}, 'correct_response': ['c'], 'section': 'Data Lakehouse Platform', 'question_plain': 'Which of the following commands can be used to run one notebook from another notebook?', 'related_lectures': []}]}
5609190
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 53082410, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How does Lakehouse replace the dependency on using Data lakes and Data warehouses in a Data and Analytics solution?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Lakehouse combines the benefits of a data warehouse and data lakes, </p><p><br></p><p><strong>Lakehouse = Data Lake + DataWarehouse</strong></p><p><br></p><p>Here are some of the major benefits of a lakehouse </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-13_16-47-48-184b1db5cb71f2cd2c566b406e9b1bf8.JPG"><p><br></p><p><br></p><p><strong>Lakehouse = Data Lake + DataWarehouse</strong></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-13_16-47-48-eeec9ba02202f99cd46e1faf3dc4279c.png"><p><br></p>', 'answers': ['<p>Open, direct access to data stored in standard data formats.</p>', '<p>Supports ACID transactions.</p>', '<p>Supports BI and Machine learning workloads</p>', '<p>Support for end-to-end streaming and batch workloads</p>', '<p>All the above</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'How does Lakehouse replace the dependency on using Data lakes and Data warehouses in a Data and Analytics solution?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082412, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently working on storing data you received from different customer surveys, this data is highly unstructured and changes over time,&nbsp; why Lakehouse is a better choice compared to a Data warehouse?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Lakehouse supports schema enforcement and evolution, traditional data warehouses lack schema evolution.</p>', '<p>Lakehouse supports SQL</p>', '<p>Lakehouse supports ACID</p>', '<p>Lakehouse enforces data integrity</p>', '<p>Lakehouse supports primary and foreign keys like a data warehouse</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You are currently working on storing data you received from different customer surveys, this data is highly unstructured and changes over time,&nbsp; why Lakehouse is a better choice compared to a Data warehouse?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082414, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following locations hosts the driver and worker nodes of a Databricks-managed cluster?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Data Plane, which is where compute(all-purpose, Job Cluster, DLT) are stored this is generally a customer cloud account,&nbsp; there is one exception SQL&nbsp;Warehouses, currently there are 3 types of SQL&nbsp;Warehouse compute available(classic, pro, serverless), in classic and pro compute is located in customer cloud account but serverless computed is located in Databricks cloud account. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_07-13-33-966784533b163b669ddd3fbff77f5af4.jpg"></p>', 'answers': ['<p>Data plane</p>', '<p>Control plane</p>', '<p>Databricks Filesystem</p>', '<p>JDBC data source</p>', '<p>Databricks web application</p>']}, 'correct_response': ['a'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following locations hosts the driver and worker nodes of a Databricks-managed cluster?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082416, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job cluster, but you realized it takes an average of 8 minutes to start the cluster, what feature can be used to start the cluster in a timely fashion?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Cluster pools allow us to reserve VM\'s ahead of time, when a new job cluster is created VM are grabbed from the pool. Note: when the VM\'s are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only billed once VM is allocated to a cluster.</p><p>Here is a demo of how to setup and follow some best practices,</p><p><a href="https://www.youtube.com/watch?v=FVtITxOabxg&amp;ab_channel=DatabricksAcademy">https://www.youtube.com/watch?v=FVtITxOabxg&amp;ab_channel=DatabricksAcademy</a></p>', 'answers': ['<p>Setup an additional job to run ahead of the actual job so the cluster is running second job starts</p>', '<p>Use the Databricks cluster pools feature to reduce the startup time</p>', '<p>Use Databricks Premium edition instead of Databricks standard edition</p>', '<p>Pin the cluster in the cluster UI page so it is always available to the jobs</p>', '<p>Disable auto termination so the cluster is always running</p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job cluster, but you realized it takes an average of 8 minutes to start the cluster, what feature can be used to start the cluster in a timely fashion?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082418, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following statement is true about Databricks repos?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>See below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workdlow.</p><p>All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git provider like Github or Azure Devops</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_07-19-42-c8c69d16d8dc7f96c813ed795fc7e21e.jpg">', 'answers': ['<p>You can approve the pull request if you are the owner of Databricks repos</p>', '<p>A workspace can only have one instance of git integration</p>', '<p>Databricks Repos and Notebook versioning are the same features</p>', '<p>You cannot create a new branch in Databricks repos</p>', '<p>Databricks repos allow you to comment and commit code changes and push them to a remote branch</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the following statement is true about Databricks repos?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082420, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the statement is correct about the cluster pools?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Cluster pools allow you to perform load balancing</p>', '<p>Cluster pools allow you to create a cluster</p>', '<p>Cluster pools allow you to save time when starting a new cluster</p>', '<p>Cluster pools are used to share resources among multiple teams</p>', '<p>Cluster pools allow you to have all the nodes in the cluster from single physical server rack</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the statement is correct about the cluster pools?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082422, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Once a cluster is deleted, below additional actions need to performed by the administrator</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Remove virtual machines but storage and networking are automatically dropped</p>', '<p>Drop storage disks but Virtual machines and networking are automatically dropped</p>', '<p>Remove networking but Virtual machines and storage disks are automatically dropped</p>', '<p>Remove logs</p>', '<p>No action needs to be performed. All resources are automatically removed.</p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Once a cluster is deleted, below additional actions need to performed by the administrator', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082424, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How does a Delta Lake differ from a traditional data lake?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>What is Delta?</p><p><br></p><p>Delta lake is</p><p>· Open source</p><p>· Builds up on standard data format</p><p>· Optimized for cloud object storage</p><p>· Built for scalable metadata handling</p><p>Delta lake is not</p><p>· Proprietary technology</p><p>· Storage format</p><p>· Storage medium</p><p>· Database service or data warehouse</p>', 'answers': ['<p>Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance</p>', '<p>Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance</p>', '<p>Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and performance</p>', '<p>Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide reliability, security, and performance</p>', '<p>Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance</p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'How does a Delta Lake differ from a traditional data lake?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082426, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How VACCUM&nbsp;and OPTIMIZE&nbsp;commands can be used to manage the DELTA&nbsp;lake?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><strong>VACCUM:&nbsp;</strong></p><p>You can remove files no longer referenced by a Delta table and are older than the retention threshold by running the <code>vacuum</code> command on the table. <code>vacuum</code> is not triggered automatically. The default retention threshold for the files is 7 days. To change this behavior, see <a href="https://docs.databricks.com/delta/history.html#data-retention">Configure data retention for time travel</a>.</p><p><br></p><p><strong>OPTIMIZE:&nbsp;</strong></p><p>Using OPTIMIZE&nbsp;you can compact data files on Delta Lake, this can improve the speed of read queries on the table. Too many small files can significantly degrade the performance of the query. </p>', 'answers': ['<p>VACCUM&nbsp;command can be used to compact small parquet files, and the OPTIMZE&nbsp;command can be used to delete parquet files that are marked for deletion/unused. </p>', '<p>VACCUM&nbsp;command can be used to delete empty/blank parquet files in a delta table. OPTIMIZE&nbsp;command can be used to update stale statistics on a delta table. </p>', '<p>VACCUM command can be used to compress the parquet files to reduce the size of the table, OPTIMIZE command can be used to cache frequently delta tables for better performance. </p>', '<p>VACCUM command can be used to delete empty/blank parquet files in a delta table, OPTIMIZE command can be used to cache frequently delta tables for better performance. </p>', '<p>OPTIMIZE&nbsp;command can be used to compact small parquet files, and the VACCUM&nbsp;command can be used to delete parquet files that are marked for deletion/unused. </p>']}, 'correct_response': ['e'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'How VACCUM&nbsp;and OPTIMIZE&nbsp;commands can be used to manage the DELTA&nbsp;lake?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082428, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the below commands can be used to drop a DELTA table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p><code>DROP DELTA table_name</code> </p>', '<p><code>DROP TABLE table_name</code> </p>', '<p><code>DROP TABLE table_name FORMAT DELTA</code> </p>', '<p><code>DROP&nbsp; table_name</code> </p>']}, 'correct_response': ['b'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Which of the below commands can be used to drop a DELTA table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082430, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Delete records from the transactions Delta table where transactionDate is greater than current timestamp?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p><code>DELETE FROM transactions FORMAT DELTA where transactionDate &gt; currenct_timestmap()</code> </p>', '<p><code>DELETE FROM transactions if transctionDate &gt; current_timestamp()</code> </p>', '<p><code>DELETE FROM transactions where transactionDate &gt; current_timestamp()</code> </p>', '<p><code>DELETE FROM transactions where transactionDate &gt; current_timestamp() KEEP_HISTORY</code> </p>', '<p><code> DELET FROM transactions where transactionDate GE current_timestamp()</code> </p>']}, 'correct_response': ['c'], 'section': 'Databricks Lakehouse Platform', 'question_plain': 'Delete records from the transactions Delta table where transactionDate is greater than current timestamp?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082432, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Identify one of the below statements that can query a delta table in PySpark Dataframe API</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p><code>Spark.read.mode("delta").table("table_name")</code> </p>', '<p><code>Spark.read.table.delta("table_name")</code> </p>', '<p><code>Spark.read.table("table_name")</code> </p>', '<p><code> Spark.read.format("delta").LoadTableAs("table_name")</code> </p>', '<p><code>Spark.read.format("delta").TableAs("table_name")</code> </p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Identify one of the below statements that can query a delta table in PySpark Dataframe API', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082434, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The default threshold of VACUUM is 7 days, internal audit team asked to certain tables to maintain at least 365 days as part of compliance requirement, which of the below setting is needed to implement. </p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<pre class="prettyprint linenums">ALTER&nbsp;TABLE&nbsp;table_name SET TBLPROPERTIES ( property_key [ = ] property_val [, ...] )</pre><p><br></p><p>TBLPROPERTIES&nbsp;allow you to set key-value pairs</p><p><br></p><p><a href="https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-tblproperties.html#common-tblproperties-and-options-keys">Table properties and table options (Databricks SQL) | Databricks on AWS</a></p>', 'answers': ['<p><code>ALTER TABLE table_name set TBLPROPERTIES (delta.deletedFileRetentionDuration= ‘interval 365 days’)</code> </p>', '<p><code>MODIFY TABLE table_name set TBLPROPERTY (delta.maxRetentionDays = ‘interval 365 days’)</code> </p>', '<p><code>ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.deletedFileRetentionDuration= ‘interval 365 days’)</code> </p>', '<p><code>ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.vaccum.duration= ‘interval 365 days’)</code> </p>']}, 'correct_response': ['a'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'The default threshold of VACUUM is 7 days, internal audit team asked to certain tables to maintain at least 365 days as part of compliance requirement, which of the below setting is needed to implement.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082436, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following commands can be used to query a delta table?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is both options A&nbsp;and B</p><p>Options C and D are incorrect because there is no command in Spark called execute.sql or delta.sql</p>', 'answers': ['<pre class="prettyprint linenums">%python\nspark.sql("select * from table_name")</pre>', '<pre class="prettyprint linenums">%sql \nSelect * from table_name </pre>', '<p>Both A &amp; B</p>', '<pre class="prettyprint linenums">%python \nexecute.sql("select * from table") </pre>', '<pre class="prettyprint linenums">%python \ndelta.sql("select * from table") </pre>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following commands can be used to query a delta table?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082438, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Below table <strong>temp_data </strong>has one column called <strong>raw</strong> contains JSON data that records temperature for every four hours in the day for the city of <strong>Chicago</strong>, you are asked to calculate the <strong>maximum </strong>temperature that was ever recorded for <strong>12:00 PM</strong> hour across all the days.&nbsp; Parse the JSON&nbsp;data and use the necessary array function to calculate the max temp. </p><p><br></p><p>Table:&nbsp;temp_date</p><p>Column:&nbsp;raw</p><p>Datatype:&nbsp;string</p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-11-11_21-25-36-d1ed281192335adb6b4f92bf39a1d0cd.jpg"><p><strong>Expected output:&nbsp;58 </strong></p><p><br></p><p><br></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Note: This is a difficult question, more likely you may see easier questions similar to this but the more you are prepared for the exam easier it is to pass the exam. </p><p><br></p><p>Use this below link to look for more examples, this will definitely help you, </p><p><a href="https://docs.databricks.com/optimizations/semi-structured.html">https://docs.databricks.com/optimizations/semi-structured.html</a></p><p><br></p><p><br></p><p>Here is the solution, step by step</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-11-11_21-25-36-d9bb4c16031d48f7b922b8280f1b624f.jpg"><p><br></p><p>Use this below link to look for more examples, this will definitely help you, </p><p><br></p><p><a href="https://docs.databricks.com/optimizations/semi-structured.html">https://docs.databricks.com/optimizations/semi-structured.html</a></p><p><br></p><p><br></p><p>If you want to try this solution use below DDL, </p><p><br></p><pre class="prettyprint linenums">create or replace table temp_data \n  as select \' {\n\t\t "chicago":[\n\t\t\t{"date":"01-01-2021",\n\t\t\t"temp":[25,28,45,56,39,25]\n\t\t\t},\n\t\t{"date":"01-02-2021",\n\t\t"temp":[25,28,49,54,38,25]\n\t\t},\n\t\t{"date":"01-03-2021",\n\t\t"temp":[25,28,49,58,38,25]\n\t      }]\n              } \n             \' as raw\n\nselect array_max(from_json(raw:chicago[*].temp[3],\'array&lt;int&gt;\')) from temp_data\n\n</pre><p><br></p><p><br></p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">select max(raw.chicago.temp[3]) from temp_data</pre>', '<pre class="prettyprint linenums">select array_max(raw.chicago[*].temp[3]) from temp_data</pre>', '<pre class="prettyprint linenums">select array_max(from_json(raw[\'chicago\'].temp[3],\'array&lt;int&gt;\')) from temp_data</pre>', '<pre class="prettyprint linenums">select array_max(from_json(raw:chicago[*].temp[3],\'array&lt;int&gt;\')) from temp_data</pre>', '<pre class="prettyprint linenums">select max(from_json(raw:chicago[3].temp[3],\'array&lt;int&gt;\')) from temp_data</pre>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Below table temp_data has one column called raw contains JSON data that records temperature for every four hours in the day for the city of Chicago, you are asked to calculate the maximum temperature that was ever recorded for 12:00 PM hour across all the days.&nbsp; Parse the JSON&nbsp;data and use the necessary array function to calculate the max temp. Table:&nbsp;temp_dateColumn:&nbsp;rawDatatype:&nbsp;stringExpected output:&nbsp;58', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082440, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following SQL statements can be used to update a transactions table, to set a flag on the table from Y to N</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': "<p>The answer is </p><p><code>UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'</code> </p><p><br></p><p>Delta Lake supports UPDATE&nbsp;statements on the delta table, all of the changes as part of the update are ACID compliant. </p><p><br></p>", 'answers': ["<p><code>MODIFY transactions SET active_flag = 'N' WHERE active_flag = 'Y'</code> </p>", "<p><code>MERGE transactions SET active_flag = 'N' WHERE active_flag = 'Y'</code> </p>", "<p><code>UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'</code> </p>", "<p><code>REPLACE transactions SET active_flag = 'N' WHERE active_flag = 'Y'</code> </p>"]}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following SQL statements can be used to update a transactions table, to set a flag on the table from Y to N', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082442, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Below sample input data contains two columns, one cartId also known as session id, and the second column is called items, every time a customer makes a change to the cart this is stored as an array in the table, the Marketing team asked you to create a unique list of item’s that were ever added to the cart by each customer, fill in blanks by choosing the appropriate array function so the query produces below <strong>expected</strong> result as shown below. </p><p><br></p><p>Schema: cartId INT, items Array&lt;INT&gt;</p><p><br></p><p>Sample Data</p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-10_07-46-04-a0bf6529b1094477c6c11775071292ee.JPG"></p><p><br></p><pre class="prettyprint linenums">SELECT cartId, ___ (___(items)) as items\nFROM carts GROUP BY cartId</pre><p><br></p><p><strong>Expected result:</strong></p><p>cartId&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; items</p><p> 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[1,100,200,300,250]</p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>COLLECT&nbsp;SET&nbsp;is a kind of aggregate function that combines a column value from all rows into a unique list</p><p><br></p><p>ARRAY_UNION&nbsp;combines and removes any duplicates, </p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-22_15-11-23-47260f10608222a7b0a16b7dbddeaf6c.jpg">', 'answers': ['<p>FLATTEN, COLLECT_UNION</p>', '<p> ARRAY_UNION, FLATTEN</p>', '<p>ARRAY_UNION, ARRAY_DISTINT</p>', '<p>ARRAY_UNION, COLLECT_SET</p>', '<p>ARRAY_DISTINCT, ARRAY_UNION</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Below sample input data contains two columns, one cartId also known as session id, and the second column is called items, every time a customer makes a change to the cart this is stored as an array in the table, the Marketing team asked you to create a unique list of item’s that were ever added to the cart by each customer, fill in blanks by choosing the appropriate array function so the query produces below expected result as shown below. Schema: cartId INT, items Array&lt;INT&gt;Sample DataSELECT cartId, ___ (___(items)) as items\nFROM carts GROUP BY cartIdExpected result:cartId&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; items 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[1,100,200,300,250]', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082444, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> You were asked to identify number of times a temperature sensor exceed threshold temperature (100.00) by each device, each row contains 5 readings collected every 5 minutes, fill in the blank with the appropriate functions.</p><p><br></p><p><code>Schema: deviceId INT, deviceTemp ARRAY&lt;double&gt;, dateTimeCollected TIMESTAMP</code></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-10_07-48-44-ce161b7b323528430beda1ec35cfd36e.JPG"></p><p><br></p><p><code>SELECT deviceId, __ (__ (__(deviceTemp], i -&gt; i &gt; 100.00)))</code></p><p><code> FROM devices </code></p><p><code>GROUP BY deviceId</code></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>FILER&nbsp;function can be used to filter an array based on an expression</p><p>SIZE&nbsp;function can be used to get size of an array </p><p>SUM&nbsp;is used to calculate to total by device</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-22_15-30-29-f8be5396fe23f99555de00319cba427c.jpg">', 'answers': ['<p>SUM, COUNT, SIZE</p>', '<p>SUM, SIZE, SLICE</p>', '<p>SUM, SIZE, ARRAY_CONTAINS</p>', '<p>SUM, SIZE, ARRAY_FILTER</p>', '<p>SUM, SIZE, FILTER</p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You were asked to identify number of times a temperature sensor exceed threshold temperature (100.00) by each device, each row contains 5 readings collected every 5 minutes, fill in the blank with the appropriate functions.Schema: deviceId INT, deviceTemp ARRAY&lt;double&gt;, dateTimeCollected TIMESTAMPSELECT deviceId, __ (__ (__(deviceTemp], i -&gt; i &gt; 100.00))) FROM devices GROUP BY deviceId', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082446, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are currently looking at a table that contains data from an e-commerce platform, each row contains a list of items(Item number) that were present in the cart, when the customer makes a change to the cart the entire information is saved as a separate list and appended to an existing list for the duration of the customer session, to identify all the items customer bought you have to make a unique list of items, you were asked to create a unique item’s list that was added to the cart by the user, <strong>fill in the blanks</strong> of below query by choosing the appropriate higher-order function?</p><p>Note:&nbsp;See below sample data and expected output. </p><p><br></p><p><code>Schema: cartId INT, items Array&lt;INT&gt;</code></p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-10_07-51-51-7e500d049895f0740b51430bd488c5da.JPG"></p><p><br></p><p><strong>Fill in the blanks:&nbsp;</strong></p><p><br></p><p><code>SELECT cartId, _(_(items)) FROM carts</code> </p><p><br></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>FLATTEN&nbsp;-&gt; Transforms an array of arrays into a single array.</p><p>ARRAY_DISTINCT -&gt;&nbsp;The function returns an array of the same type as the input argument where all duplicate values have been removed.</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-22_15-50-33-169e19ac5e3ebf3e8585c89267814abf.jpg">', 'answers': ['<p>ARRAY_UNION, ARRAY_DISCINT</p>', '<p> ARRAY_DISTINCT, ARRAY_UNION</p>', '<p> ARRAY_DISTINCT, FLATTEN</p>', '<p> FLATTEN, ARRAY_DISTINCT</p>', '<p>ARRAY_DISTINCT, ARRAY_FLATTEN</p>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are currently looking at a table that contains data from an e-commerce platform, each row contains a list of items(Item number) that were present in the cart, when the customer makes a change to the cart the entire information is saved as a separate list and appended to an existing list for the duration of the customer session, to identify all the items customer bought you have to make a unique list of items, you were asked to create a unique item’s list that was added to the cart by the user, fill in the blanks of below query by choosing the appropriate higher-order function?Note:&nbsp;See below sample data and expected output. Schema: cartId INT, items Array&lt;INT&gt;Fill in the blanks:&nbsp;SELECT cartId, _(_(items)) FROM carts', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082448, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on IOT data where each device has 5 reading in an array collected in Celsius, you were asked to covert each individual reading from Celsius to Fahrenheit, fill in the blank with an appropriate function that can be used in this scenario.</p><p><br></p><p><code>Schema: deviceId INT, deviceTemp ARRAY&lt;double&gt;</code></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question/2022-07-10_07-53-50-c2a0819f981fc87dc8230857f77bf702.JPG"><p><br></p><p><code>SELECT deviceId, __(deviceTempC,i-&gt; (i * 9/5) + 32) as deviceTempF </code></p><p><code>FROM sensors</code> </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>TRANSFORM -&gt; Transforms elements in an array in <code>expr</code> using the function <code>func</code>.</p><p><br></p><pre class="prettyprint linenums">transform(expr, func)</pre>', 'answers': ['<p>APPLY</p>', '<p>MULTIPLY</p>', '<p>ARRAYEXPR</p>', '<p>TRANSFORM</p>', '<p>FORALL</p>']}, 'correct_response': ['d'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are working on IOT data where each device has 5 reading in an array collected in Celsius, you were asked to covert each individual reading from Celsius to Fahrenheit, fill in the blank with an appropriate function that can be used in this scenario.Schema: deviceId INT, deviceTemp ARRAY&lt;double&gt;SELECT deviceId, __(deviceTempC,i-&gt; (i * 9/5) + 32) as deviceTempF FROM sensors', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082450, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following array functions takes input column return unique list of values in an array?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-07-10_07-57-16-1d218f853aeffb20ad4fc1a0d0753606.JPG"><p><br></p><p><br></p>', 'answers': ['<p>COLLECT_LIST</p>', '<p>COLLECT_SET</p>', '<p>COLLECT_UNION</p>', '<p>ARRAY_INTERSECT</p>', '<p>ARRAY_UNION</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following array functions takes input column return unique list of values in an array?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082452, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> You are looking to process the data based on two variables, one to check if the department is supply chain or check if process flag is set to True</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>if department = “supply chain” | process:</p>', '<p>if department == “supply chain” or process = TRUE:</p>', '<p>if department == “supply chain” | process == TRUE:</p>', '<p>if department == “supply chain” | if process == TRUE:</p>', '<p>if department == “supply chain” or process:</p>']}, 'correct_response': ['e'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'You are looking to process the data based on two variables, one to check if the department is supply chain or check if process flag is set to True', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082454, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the output of below function when executed with input parameters 1, 3 :</p><pre class="prettyprint linenums">def check_input(x,y):\n    if x &lt; y:\n        x= x+1\n        if x&gt;y:\n            x= x+1\n            if x &lt;y:\n            x = x+1\n    return x</pre><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is 2 </p>', 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>4</p>', '<p>5</p>']}, 'correct_response': ['b'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'What is the output of below function when executed with input parameters 1, 3 :def check_input(x,y):\n    if x &lt; y:\n        x= x+1\n        if x&gt;y:\n            x= x+1\n            if x &lt;y:\n            x = x+1\n    return x', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082456, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which of the following python statements can be used to replace the schema name and table name in the query?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>The answer is </p><p><br></p><pre class="prettyprint linenums">table_name = "sales"\nquery = f"select * from {schema_name}.{table_name}"</pre><p><br></p><p>It is always best to use f strings to replace python variables, rather than using string concatenation. </p><p><br></p>', 'answers': ['<pre class="prettyprint linenums">table_name = "sales"\nschema_name = "bronze"\nquery = f"select * from schema_name.table_name"</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nquery = "select * from {schema_name}.{table_name}"</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nquery = f"select * from {schema_name}.{table_name}"</pre>', '<pre class="prettyprint linenums">table_name = "sales"\nquery = f"select * from + schema_name +"."+table_name"</pre>']}, 'correct_response': ['c'], 'section': 'ELT with Spark SQL and Python', 'question_plain': 'Which of the following python statements can be used to replace the schema name and table name in the query?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082458, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>you are currently working on creating a spark stream process to read and write in for a one-time micro batch, and also rewrite the existing target table, fill in the blanks to complete the below command sucesfully. </p><p><br></p><p><br></p><pre class="prettyprint linenums">spark.table("source_table")\n.writeStream\n.option("____", “dbfs:/location/silver")\n.outputMode("____")\n.trigger(Once=____)\n.table("target_table")</pre><p><br></p><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>checkpointlocation, complete, True</p>', '<p>targetlocation, overwrite, True</p>', '<p>checkpointlocation, True, overwrite</p>', '<p>checkpointlocation, True, complete</p>', '<p>checkpointlocation, overwrite, True</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'you are currently working on creating a spark stream process to read and write in for a one-time micro batch, and also rewrite the existing target table, fill in the blanks to complete the below command sucesfully. spark.table("source_table")\n.writeStream\n.option("____", “dbfs:/location/silver")\n.outputMode("____")\n.trigger(Once=____)\n.table("target_table")', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082460, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You were asked to write python code to stop all running streams, which of the following command can be used to get a list of all active streams currently running so we can stop them, fill in the blank. </p><p><br></p><pre class="prettyprint linenums">for s in _______________:\n  s.stop()</pre><p><br></p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Spark.getActiveStreams()</p>', '<p>spark.streams.active</p>', '<p>activeStreams()</p>', '<p>getActiveStreams()</p>', '<p>spark.streams.getActive</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'You were asked to write python code to stop all running streams, which of the following command can be used to get a list of all active streams currently running so we can stop them, fill in the blank. for s in _______________:\n  s.stop()', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082462, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to command to load the data, fill in the blanks for successful execution of below code.</p><p><br></p><pre class="prettyprint linenums">spark.readStream\n.format("cloudfiles")\n.option("_______",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("_______", "true")\n.table(table_name))</pre>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>format, checkpointlocation, schemalocation, overwrite</p>', '<p>cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append</p>', '<p>cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to command to load the data, fill in the blanks for successful execution of below code.spark.readStream\n.format("cloudfiles")\n.option("_______",”csv)\n.option("_______", ‘dbfs:/location/checkpoint/’)\n.load(data_source)\n.writeStream\n.option("_______",’ dbfs:/location/checkpoint/’)\n.option("_______", "true")\n.table(table_name))', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082464, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p> Which of the following scenarios is the best fit for AUTO LOADER?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Efficiently process new data incrementally from cloud object storage, AUTO&nbsp;LOADER only supports ingesting files stored in a cloud object storage. Auto Loader cannot process streaming data sources like Kafka or Delta streams, use Structured streaming for these data sources. </p><p><br></p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-14_17-29-05-eb36006e7812ce0245401ad5908f66af.jpeg"><p>Auto Loader and Cloud Storage Integration</p><p><br></p><p>Auto Loader supports a couple of ways to ingest data incrementally</p><p><br></p><ol><li><p>Directory listing - List Directory and maintain the state in RocksDB, supports incremental file listing </p></li><li><p>File notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike Directory listing File notification can scale up to millions of files per day. </p></li></ol><p><br></p><p><br></p><p><strong>[OPTIONAL] </strong></p><p><strong>Auto Loader vs COPY&nbsp;INTO?</strong></p><p><br></p><p><strong>Auto Loader</strong></p><p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup. Auto Loader provides a new Structured Streaming source called <code>cloudFiles</code>. Given an input directory path on the cloud file storage, the <code>cloudFiles</code> source automatically processes new files as they arrive, with the option of also processing existing files in that directory.</p><p>When to use Auto Loader instead of the COPY&nbsp;INTO?</p><p><br></p><ul><li><p>You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover files more efficiently than the <code>COPY INTO</code> SQL command and can split file processing into multiple batches.</p></li><li><p>You do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess subsets of files. However, you can use the <code>COPY INTO</code> SQL command to reload subsets of files while an Auto Loader stream is simultaneously running.</p></li></ul><p><br></p><p><br></p><p><br></p><p><br></p>', 'answers': ['<p>Efficiently process new data incrementally from cloud object storage</p>', '<p>Efficiently move data incrementally from one delta table to another delta table</p>', '<p>Incrementally process new data from streaming data sources like Kafka into delta lake</p>', '<p>Incrementally process new data from relational databases like MySQL</p>', '<p>Efficiently copy data from one data lake location to another data lake location</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'Which of the following scenarios is the best fit for AUTO LOADER?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082466, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are asked to setup an AUTO LOADER to process the incoming data, this data arrives in JSON format and get dropped into cloud object storage and you are required to process the data as soon as it arrives in cloud storage, which of the following statements is correct</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Auto Loader supports two modes when ingesting new files from cloud object storage</p><p><br></p><p><strong>Directory listing</strong>: Auto Loader identifies new files by listing the input directory, and uses a directory polling approach. </p><p><strong>File notification</strong>: Auto Loader can automatically set up a notification service and queue service that subscribe to file events from the input directory.</p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-14_17-35-55-6eb0868d84a4c42f046be083a0114d72.jpeg"><p><br></p><p>File notification is more efficient and can be used to process the data in real-time as data arrives in cloud object storage. </p><p><br></p><p><a href="https://docs.databricks.com/ingestion/auto-loader/file-detection-modes.html">Choosing between file notification and directory listing modes | Databricks on AWS</a></p>', 'answers': ['<p>AUTO LOADER is native to DELTA lake it cannot support external cloud object storage</p>', '<p>AUTO LOADER has to be triggered from an external process when the file arrives in the cloud storage</p>', '<p>AUTO LOADER needs to be converted to a Structured stream process</p>', '<p>AUTO LOADER can only process continuous data when stored in DELTA lake</p>', '<p>AUTO LOADER can support file notification method so it can process data as it arrives</p>']}, 'correct_response': ['e'], 'section': 'Incremental Data Processing', 'question_plain': 'You are asked to setup an AUTO LOADER to process the incoming data, this data arrives in JSON format and get dropped into cloud object storage and you are required to process the data as soon as it arrives in cloud storage, which of the following statements is correct', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082468, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the main difference between the bronze layer and silver layer in a medallion architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p><br></p><p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content. </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-47-19-45985c46789b94eb08ae6fd17782f4a5.jpeg"><p><br></p>', 'answers': ['<p>Duplicates are removed in bronze, schema is applied in silver</p>', '<p>Silver may contain aggregated data</p>', '<p>Bronze is raw copy of ingested data, silver contains data with production schema and optimized for ELT/ETL&nbsp;throughput </p>', '<p>Bad data is filtered in Bronze, silver is a copy of bronze data</p>']}, 'correct_response': ['c'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the main difference between the bronze layer and silver layer in a medallion architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082470, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the main difference between the silver layer and the gold layer in medalion architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content. </p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-48-17-130508c1a3f2623237be68188b708c74.jpeg"><p><br></p><p><br></p>', 'answers': ['<p>Silver may contain aggregated data</p>', '<p>Gold may contain aggregated data</p>', '<p>Data quality checks are applied in gold</p>', '<p>Silver is a copy of bronze data</p>', '<p>God is a copy of silver data</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the main difference between the silver layer and the gold layer in medalion architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082472, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the main difference between the silver layer and gold layer in medallion architecture?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p><a href="https://databricks.com/glossary/medallion-architecture">Medallion Architecture – Databricks</a></p><p>Gold Layer:</p><p>1. Powers Ml applications, reporting, dashboards, ad hoc analytics</p><p>2. Refined views of data, typically with aggregations</p><p>3. Reduces strain on production systems</p><p>4. Optimizes query performance for business-critical data</p><p><br></p><p><strong><em>Exam focus: Please review the below image and understand the role of each layer(bronze, silver, gold) in medallion architecture, you will see varying questions targeting each layer and its purpose. </em></strong></p><p><br></p><p>Sorry I&nbsp;had to add the watermark some people in Udemy are copying my content. </p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-10-07_18-48-17-130508c1a3f2623237be68188b708c74.jpeg"></p>', 'answers': ['<p>Silver optimized to perform ETL, Gold is optimized query performance</p>', '<p>Gold is optimized go perform ETL, Silver is optimized for query performance</p>', '<p>Silver is copy of Bronze, Gold is a copy of Silver</p>', '<p>Silver is stored in Delta Lake, Gold is stored in memory</p>', '<p>Silver may contain aggregated data, gold may preserve the granularity of original data</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': 'What is the main difference between the silver layer and gold layer in medallion architecture?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082474, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01')</p><p>What is the expected behavior when a batch of data containing data that violates these constraints is processed?</p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p><p><br></p><p>Delta live tables support three types of expectations to fix bad data in DLT&nbsp;pipelines </p><p><br></p><p>Review below example code to examine these expectations,&nbsp; </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_04-52-20-291618c7a5e4d2e8820d6fc07f4acc7d.jpg"><p><br></p>', 'answers': ['<p>Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p>', '<p>Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.</p>', '<p>Records that violate the expectation cause the job to fail.</p>', '<p>Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.</p>', '<p> Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.</p>']}, 'correct_response': ['a'], 'section': 'Incremental Data Processing', 'question_plain': "A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01')What is the expected behavior when a batch of data containing data that violates these constraints is processed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 53082476, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION DROP ROW</p><p>What is the expected behavior when a batch of data containing data that violates these constraints is processed?</p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.</p><p><br></p><p>Delta live tables support three types of expectations to fix bad data in DLT&nbsp;pipelines </p><p><br></p><p>Review below example code to examine these expectations,&nbsp; </p><p><br></p><p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_04-52-48-9d693d757275c2fea537282ffdf70e0a.jpg"></p>', 'answers': ['<p>Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.</p>', '<p>Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.</p>', '<p>Records that violate the expectation cause the job to fail.</p>', '<p>Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.</p>', '<p>Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.</p>']}, 'correct_response': ['b'], 'section': 'Incremental Data Processing', 'question_plain': "A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION DROP ROWWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 53082478, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are asked to debug a databricks job that is taking too long to run on Sunday’s, what are the steps you are going to take to identify the step that is taking longer to run?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be viewed.</p><p><br></p><p>You have the ability to view current active runs or completed runs, once you click the run you can see the </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_05-14-11-c7d004dbce39e6e065f1656e0d2d9c09.jpg"><p>Click on the run to view the notebook output </p><p><br></p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_05-14-12-97775792e14c3c86a106443dd4afd992.jpg"><p><br></p>', 'answers': ['<p>A notebook activity of job run is only visible when using all-purpose cluster.</p>', '<p>Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be viewed.</p>', '<p>Enable debug mode in the Jobs to see the output activity of a job, output should be available to view.</p>', '<p>Once a job is launched, you cannot access the job’s notebook activity.</p>', '<p>Use the compute’s spark UI to monitor the job activity.</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'You are asked to debug a databricks job that is taking too long to run on Sunday’s, what are the steps you are going to take to identify the step that is taking longer to run?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082480, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Your colleague was walking you through how a job was setup, but you noticed a warning message that said, “Jobs running on all-purpose cluster are considered all purpose compute", the colleague was not sure why he was getting the warning message, how do you best explain this warning message?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Warning message:&nbsp;</p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_05-19-49-2389593be00e7a8809a49ede335478cb.jpg"><p><br></p><p>Pricing for All-purpose clusters are more expensive than the job clusters</p><p><br></p><p>AWS&nbsp;pricing(Aug 15th 2022)</p><img src="https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2022-08-15_05-19-49-fcf95d2887543210d78e4b66b63ab124.jpg"><p><br></p>', 'answers': ['<p>All-purpose clusters cannot be used for Job clusters, due to performance issues.</p>', '<p>All-purpose clusters take longer to start the cluster vs a job cluster</p>', '<p>All-purpose clusters are less expensive than the job clusters</p>', '<p>All-purpose clusters are more expensive than the job clusters</p>', '<p>All-purpose cluster provide interactive messages that can not be viewed in a job</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'Your colleague was walking you through how a job was setup, but you noticed a warning message that said, “Jobs running on all-purpose cluster are considered all purpose compute", the colleague was not sure why he was getting the warning message, how do you best explain this warning message?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082482, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Your team has hundreds of jobs running but it is difficult to track cost of each job run, you are asked to provide a recommendation on how to monitor and track cost across various workloads</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is Use Tags, during job creation so cost can be easily tracked</p><p><br></p><p>Review below link for more details <a href="https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html">https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html</a></p><p><br></p><p>Here is a view how tags get propagated from pools to clusters and clusters without pools, </p><p><img src="https://docs.databricks.com/_images/tag-propagation-aws.png"></p>', 'answers': ['<p>Create jobs in different workspaces, so we can track the cost easily</p>', '<p>Use Tags, during job creation so cost can be easily tracked</p>', '<p>Use job logs to monitor and track the costs</p>', '<p>Use workspace admin reporting</p>', '<p>Use a single cluster for all the jobs, so cost can be easily tracked</p>']}, 'correct_response': ['b'], 'section': 'Production Pipelines', 'question_plain': 'Your team has hundreds of jobs running but it is difficult to track cost of each job run, you are asked to provide a recommendation on how to monitor and track cost across various workloads', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082484, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores, but the sales team would like to use the dashboard but would like to select individual store location, which of the following approaches Data Engineering team can use to build this functionality into the dashboard. </p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is </p><p><br></p><p>Databricks supports many types of parameters in the dashboard, a drop-down list can be created based on a query that has a unique list of store locations. </p><p><br></p><p>Here is a simple query that takes a parameter for </p><p><br></p><p><code>SELECT *&nbsp; FROM sales WHERE field IN ( {{ Multi Select Parameter }} )</code></p><p>Or </p><p><code>SELECT *&nbsp; FROM sales WHERE field =&nbsp; {{ Single Select Parameter }} </code></p><p><br></p><p><strong>Query parameter types</strong></p><ul><li><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters#text">Text</a></p></li><li><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters#number">Number</a></p></li><li><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters#dropdown-list">Dropdown List</a></p></li><li><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters#query-based-dropdown-list">Query Based Dropdown List</a></p></li><li><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters#date-and-time">Date and Time</a></p><p><br></p></li></ul>', 'answers': ['<p>Use query Parameters which then allow user to choose any location</p>', '<p>Currently dashboards do not support parameters</p>', '<p>Use Databricks REST API to create a dashboard for each location</p>', '<p>Use SQL UDF function to filter the data based on the location</p>', '<p>Use Dynamic views to filter the data based on the location</p>']}, 'correct_response': ['a'], 'section': 'Production Pipelines', 'question_plain': 'The sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores, but the sales team would like to use the dashboard but would like to select individual store location, which of the following approaches Data Engineering team can use to build this functionality into the dashboard.', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082486, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working on a dashboard that takes a long time to load in the browser, due to the fact that each visualization contains a lot of data to populate, which of the following approaches can be taken to address this issue?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Note*: The question may sound misleading but these are types of questions the exam tries to ask. </p><p><br></p><p>A query filter lets you interactively reduce the amount of data shown in a visualization, similar to <a href="https://docs.databricks.com/sql/user/queries/query-parameters.html">query parameter</a> but with a few key differences. A query filter limits data <em>after</em> it has been loaded into your browser. This makes filters ideal for smaller datasets and environments where query executions are time-consuming, rate-limited, or costly.</p><p>This query filter is different from than filter that needs to be applied at the data level, this filter is at the visualization level so you can toggle how much data you want to see. </p><p><br></p><pre class="prettyprint linenums">SELECT action AS `action::filter`, COUNT(0) AS "actions count"\nFROM events\nGROUP BY action</pre><p><br></p><p>When queries <a href="https://docs.databricks.com/sql/user/queries/query-filters.html">have filters</a> you can also apply filters at the dashboard level. Select the <strong>Use Dashboard Level Filters</strong> checkbox to apply the filter to all queries. </p><p><br></p><p><a href="https://docs.databricks.com/sql/user/dashboards/index.html#dashboard-filters">Dashboard filters</a></p><p><br></p><p><a href="https://docs.databricks.com/sql/user/queries/query-filters.html">Query filters | Databricks on AWS</a></p><p><br></p><p><br></p>', 'answers': ['<p>Increase size of the SQL endpoint cluster</p>', '<p>Increase the scale of maximum range of SQL endpoint cluster</p>', '<p>Use Databricks SQL&nbsp;Query filter to limit the amount of data in each visualization</p>', '<p> Remove data from Delta Lake</p>', '<p>Use Delta cache to store the intermediate results</p>']}, 'correct_response': ['c'], 'section': 'Production Pipelines', 'question_plain': 'You are working on a dashboard that takes a long time to load in the browser, due to the fact that each visualization contains a lot of data to populate, which of the following approaches can be taken to address this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082488, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>One of the queries in the Databricks SQL&nbsp;Dashboard takes a long time to refresh, which of the below steps can be taken to identify the root cause of this issue?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Use Query History, to view queries and select query, and check the query profile to see time spent in each step.</p><p><br></p><p>Here is the view of the query profile, for more info use the link </p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/sql/admin/query-profile">https://docs.microsoft.com/en-us/azure/databricks/sql/admin/query-profile</a></p><p><br></p><p>As you can see here Databricks SQL&nbsp;query profile is much different to Spark&nbsp;UI and provides much more clear information on how time is being spent on different queries and time it spent on each step.</p><p><br></p><p><img src="https://docs.microsoft.com/en-us/azure/databricks/_static/images/sql/query-profile.png"></p><p><br></p><p><br></p>', 'answers': ['<p>Restart the SQL endpoint</p>', '<p>Select the SQL endpoint cluster, spark UI, SQL tab to see the execution plan and time spent in each step</p>', '<p>Run optimize and Z ordering</p>', '<p>Change the Spot Instance Policy from “Cost optimized” to “Reliability Optimized.”</p>', '<p>Use Query History, to view queries and select query, and check query profile to time spent in each step</p>']}, 'correct_response': ['e'], 'section': 'Production Pipelines', 'question_plain': 'One of the queries in the Databricks SQL&nbsp;Dashboard takes a long time to refresh, which of the below steps can be taken to identify the root cause of this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082490, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A SQL Dashboard was built for the supply chain team to monitor the inventory and product orders, but all of the timestamps displayed on the dashboards are showing in UTC format, so they requested to change the time zone to the location of New York. How would you approach resolving this issue?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York</p><p><br></p><p>Here are steps you can take this to configure, so the entire dashboard is changed without changing individual queries</p><p><br></p><p><strong>Configure SQL parameters</strong></p><p>To configure all warehouses with SQL parameters:</p><ol><li><p>Click <img src="https://docs.databricks.com/_images/user-settings-icon.png"> <strong>Settings</strong> at the bottom of the sidebar and select <strong>SQL Admin Console</strong>.</p></li><li><p>Click the <strong>SQL Warehouse Settings</strong> tab.</p></li><li><p>In the <strong>SQL Configuration Parameters</strong> textbox, specify one key-value pair per line. Separate the name of the parameter from its value using a space. For example, to enable <code>ANSI_MODE</code>:</p><p><img src="https://docs.databricks.com/_images/global-sql-config-parameters.png"></p></li></ol><p>Similarly, we can add a line in the SQL&nbsp;Configuration parameters</p><p><strong>timezone America/New_York</strong></p><p><a href="https://docs.databricks.com/sql/admin/sql-configuration-parameters.html">SQL configuration parameters | Databricks on AWS</a></p>', 'answers': ['<p>Move the workspace from Central US zone to East US Zone</p>', '<p>Change the timestamp on the delta tables to America/New_York format</p>', '<p>Change the spark configuration of SQL endpoint to format the timestamp to America/New_York</p>', '<p>Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York</p>', '<p>Add SET Timezone = America/New_York on every of the SQL queries in the dashboard.</p>']}, 'correct_response': ['d'], 'section': 'Production Pipelines', 'question_plain': 'A SQL Dashboard was built for the supply chain team to monitor the inventory and product orders, but all of the timestamps displayed on the dashboards are showing in UTC format, so they requested to change the time zone to the location of New York. How would you approach resolving this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082492, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>Which of the following technique can be used to implement fine-grained access control to rows and columns of the Delta table based on the user's access?</p>", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Use dynamic view functions.</p><p><br></p><p>Here is an example that limits access to rows based on the user being part managers group, in the below view if a user is not a part of the manager\'s group you can only see rows where the total amount is &lt;= 1000000</p><p><br></p><p>Dynamic view function to filter rows</p><p><br></p><pre class="prettyprint linenums">CREATE VIEW sales_redacted AS \nSELECT user_id, country, product, total \nFROM sales_raw \nWHERE CASE WHEN is_member(\'managers\') THEN TRUE ELSE total &lt;= 1000000 END;</pre><p><br></p><p>Dynamic view function to hide a column data based on user\'s access,&nbsp; </p><p><br></p><pre class="prettyprint linenums">CREATE VIEW sales_redacted AS \nSELECT user_id,\n       CASE WHEN is_member(\'auditors\') THEN email ELSE \'REDACTED\' END AS email,\n       country, \n       product, \n       total \nFROM sales_raw\n</pre><p><br></p><p>Please review below for more details</p><p><a href="https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#dynamic-view-functions">https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#dynamic-view-functions </a></p>', 'answers': ['<p>Use Unity catalog to grant access to rows and columns</p>', '<p>Row and column access control lists</p>', '<p>Use dynamic view functions</p>', '<p>Data access control lists</p>', '<p>Dynamic Access control lists with Unity Catalog</p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': "Which of the following technique can be used to implement fine-grained access control to rows and columns of the Delta table based on the user's access?", 'related_lectures': []}, {'_class': 'assessment', 'id': 53082494, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Unity catalog helps you manage the below resources in Databricks at account level</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is all of the above. </p><p>Unity Catalog is a unified governance solution for all data and AI assets including files, tables, machine learning models, and dashboards in your lakehouse on any cloud.</p><p><br></p><p><br></p>', 'answers': ['<p>Tables</p>', '<p>ML Models</p>', '<p>Dashboards</p>', '<p>Meta Stores and Catalogs</p>', '<p>All of the above</p>']}, 'correct_response': ['e'], 'section': 'Data Governance', 'question_plain': 'Unity catalog helps you manage the below resources in Databricks at account level', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082496, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>John Smith is a newly joined team member in the Marketing team who currently has access read access to sales tables but does not have access to delete rows from the table, which of the following commands help you accomplish this?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is <code>GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com</code> , please note INSERT, UPDATE, and DELETE&nbsp;are combined into one role called MODIFY. </p><p><br></p><p>Below are the list of privileges that can be granted to a user or a group, </p><p><br></p><p>SELECT: gives read access to an object.</p><p>CREATE: gives the ability to create an object (for example, a table in a schema).</p><p><strong>MODIFY: gives the ability to add, delete, and modify data to or from an object.</strong></p><p>USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.</p><p>READ_METADATA: gives the ability to view an object and its metadata.</p><p>CREATE_NAMED_FUNCTION: gives the ability to create a named UDF in an existing catalog or schema.</p><p>MODIFY_CLASSPATH: gives the ability to add files to the Spark classpath.</p><p>ALL PRIVILEGES: gives all privileges (is translated into all the above privileges</p><p><br></p>', 'answers': ['<p><code> GRANT USAGE ON TABLE table_name TO john.smith@marketing.com</code> </p>', '<p><code>GRANT DELETE ON TABLE table_name TO john.smith@marketing.com</code> </p>', '<p><code>GRANT DELETE TO TABLE table_name ON john.smith@marketing.com</code> </p>', '<p><code>GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com</code> </p>', '<p><code>GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com</code> </p>']}, 'correct_response': ['e'], 'section': 'Data Governance', 'question_plain': 'John Smith is a newly joined team member in the Marketing team who currently has access read access to sales tables but does not have access to delete rows from the table, which of the following commands help you accomplish this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 53082498, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Kevin is the owner of both the sales table and regional_sales_vw view which uses the sales table as the underlying source for the data, and Kevin is looking to grant select privilege on the view regional_sales_vw to one of newly joined team members Steven. Which of the following is a true statement?</p>', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The answer is, Kevin can grant access to the view, because he is the owner of the view and the underlying table, </p><p>Ownership determines whether or not you can grant privileges on derived objects to other users, a user who creates a schema, table, view, or function becomes its owner. The owner is granted all privileges and can grant privileges to other users</p>', 'answers': ['<p>Kevin can not grant access to Steven since he does not have security admin privilege</p>', '<p>Kevin although is the owner but does not have ALL PRIVILEGES permission</p>', '<p>Kevin can grant access to the view, because he is the owner of the view and the underlying table</p>', '<p>Kevin can not grant access to Steven since he does have workspace admin privilege</p>', '<p>Steve will also require SELECT access on the underlying table</p>']}, 'correct_response': ['c'], 'section': 'Data Governance', 'question_plain': 'Kevin is the owner of both the sales table and regional_sales_vw view which uses the sales table as the underlying source for the data, and Kevin is looking to grant select privilege on the view regional_sales_vw to one of newly joined team members Steven. Which of the following is a true statement?', 'related_lectures': []}]}
