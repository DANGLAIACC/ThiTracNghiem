5888730
~~~
{"count": 60, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70683028, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to pass multiple parameters from a Databricks Job to a notebook. They already configured the key and value of each parameter in the configurations of the job.</p><p><br></p><p>Which of the following utilities can the data engineer use to read the passed parameters inside the notebook ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>dbutils.widgets allow you to add parameters to your notebooks.</p><p><br></p><p>Example: Adding a parameter named \u2018param1\u2019</p><pre class=\"prettyprint linenums\">\u200b\u200bdbutils.widgets.text(\"param1\", \"default\")\nparam1 = dbutils.widgets.get(\"param1\")</pre><p><br></p><p>The defined parameters can be passed from Databricks Jobs by adding them to the Parameters section in the task configuration.</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_17-41-15-25709f2ec4c43f9da80d012a01117246.png\"><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/notebooks/widgets.html\">https://docs.databricks.com/notebooks/widgets.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059620/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>dbutils.secrets</p>", "<p>dbutils.library</p>", "<p>dbutils.fs</p>", "<p>dbutils.notebook</p>", "<p>dbutils.widgets</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer wants to pass multiple parameters from a Databricks Job to a notebook. They already configured the key and value of each parameter in the configurations of the job.Which of the following utilities can the data engineer use to read the passed parameters inside the notebook ?", "related_lectures": []}, {"_class": "assessment", "id": 70683030, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to view the metrics, driver logs, and Spark UI of an existing cluster ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_17-37-06-f5c980e41151eadc51ceaa698c2a7ab1.png\"></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>\u201cCan Attach To\u201d privilege on the cluster</p>", "<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Attach To\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to view the metrics, driver logs, and Spark UI of an existing cluster ?", "related_lectures": []}, {"_class": "assessment", "id": 70683032, "assessment_type": "multiple-choice", "prompt": {"question": "<p>For production Databricks jobs, which of the following cluster types is recommended to use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Job Clusters are dedicated clusters for a job or task run. A job cluster auto terminates once the job is completed, which saves cost compared to all-purpose clusters.</p><p>In addition, Databricks recommends using job clusters in production so that each job runs in a fully isolated environment.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job\">https://docs.databricks.com/workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059620/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>All-purpose clusters</p>", "<p>Production clusters</p>", "<p>Job clusters</p>", "<p>On-premises clusters</p>", "<p>Serverless clusters</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "For production Databricks jobs, which of the following cluster types is recommended to use?", "related_lectures": []}, {"_class": "assessment", "id": 70683034, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta Lake table created with following query:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE target\nAS SELECT * FROM source</pre><p><br></p><p>A data engineer wants to drop the source table with the following query:</p><p><code>DROP TABLE source</code></p><p><br></p><p>Which statement describes the result of running this drop command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>CREATE TABLE AS SELECT</code> statements, or CTAS statements create new Delta tables and populate them using the output of a SELECT query. So, when dropping the source table, the target table will not be affected.</p><p><br></p><p>Reference: (cf. AS query clause)</p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664704/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>An error will occur indicating that other tables are based on this source table</p>", "<p>Both the target and source tables will be dropped</p>", "<p>No table will be dropped until CASCADE keyword is added to the command</p>", "<p>Only the source table will be dropped, but the target table will be no more queryable</p>", "<p>Only the source table will be dropped, while the target table will not be affected</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "The data engineering team has a Delta Lake table created with following query:CREATE TABLE target\nAS SELECT * FROM sourceA data engineer wants to drop the source table with the following query:DROP TABLE sourceWhich statement describes the result of running this drop command ?", "related_lectures": []}, {"_class": "assessment", "id": 70683036, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to start and terminate an existing cluster ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_17-36-47-4f3807e4ccb5babe6eddeaee0c1a0b51.png\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>\u201cCan Attach To\u201d privilege on the cluster</p>", "<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Attach To\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to start and terminate an existing cluster ?", "related_lectures": []}, {"_class": "assessment", "id": 70683038, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta Lake table created with following query:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE customers_clone\nLOCATION 'dbfs:/mnt/backup'\nAS SELECT * FROM customers</pre><p><br></p><p>A data engineer wants to drop the table with the following query:</p><p><code>DROP TABLE customers_clone</code></p><p><br></p><p>Which statement describes the result of running this drop command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>External (unmanaged) tables are tables whose data is stored in an external storage path by using a LOCATION clause.</p><p>When you run DROP TABLE on an external table, only the table's metadata is deleted, while the underlying data files are kept.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table\">https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p><p><br></p>", "answers": ["<p>An error will occur as the table is deep cloned from the customers table</p>", "<p>An error will occur as the table is shallowly cloned from the customers table</p>", "<p>Only the table's metadata will be deleted from the catalog, while the data files will be kept in the storage</p>", "<p>Both the table's metadata and the data files will be deleted</p>", "<p>The table will not be dropped until VACUUM command is run</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a Delta Lake table created with following query:CREATE TABLE customers_clone\nLOCATION 'dbfs:/mnt/backup'\nAS SELECT * FROM customersA data engineer wants to drop the table with the following query:DROP TABLE customers_cloneWhich statement describes the result of running this drop command ?", "related_lectures": []}, {"_class": "assessment", "id": 70683040, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to edit the configurations of an existing cluster ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_17-47-15-1495716eb8f00092ac53372be4277385.png\"></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Manage\u201d privileges on the cluster</p>", "<p>Only administrators can edit the configurations on existing clusters</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to edit the configurations of an existing cluster ?", "related_lectures": []}, {"_class": "assessment", "id": 70683042, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following code block in a notebook</p><p><br></p><pre class=\"prettyprint linenums\">db_password = dbutils.secrets.get(scope=\"dev\", key=\"database_password\")\n\nprint (db_password)</pre><p><br></p><p>Which statement describes what will happen when the above code is executed?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Secrets allows you to securely store your credentials and reference them in notebooks and jobs. To prevent accidentally printing a secret to standard output buffers or displaying the value during variable assignment, Databricks redacts secret values that are read using dbutils.secrets.get(). When displayed in notebook cell output, the secret values are replaced with [REDACTED] string.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/secrets/redaction.html\">https://docs.databricks.com/security/secrets/redaction.html</a></p><p><a href=\"https://docs.databricks.com/security/secrets/index.html\">https://docs.databricks.com/security/secrets/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059672/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>An interactive input box will appear in the notebook</p>", "<p>The string \"REDACTED\" will be printed.</p>", "<p>The error message \u201cSecrets can not be printed\u201d will be shown</p>", "<p>The string value of the password will be printed in plain text.</p>", "<p>If the user has \u201cCan Read\u201d permission, the string value of the password will be printed in plain text. Otherwise, the string \"REDACTED\" will be printed.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following code block in a notebookdb_password = dbutils.secrets.get(scope=\"dev\", key=\"database_password\")\n\nprint (db_password)Which statement describes what will happen when the above code is executed?", "related_lectures": []}, {"_class": "assessment", "id": 70683044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer is using the %sh magic command to run some legacy code. A senior data engineer has recommended refactoring the code instead.</p><p><br></p><p>Which of the following could explain why a data engineer may need to avoid using the %sh magic command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks support the %sh auxiliary magic command to run shell code in notebooks. This command runs only on the Apache Spark driver, and not on the worker nodes.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/notebooks-code.html#mix-languages\">https://docs.databricks.com/notebooks/notebooks-code.html#mix-languages</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p><p><br></p>", "answers": ["<p>%sh restarts the Python interpreter. This clears all the variables declared in the notebook</p>", "<p>%sh executes shell code only on the local driver machine which leads to significant performance overhead.</p>", "<p>%sh can not access storage to persist the output</p>", "<p>All the above reasons explain why %sh may need to be avoided</p>", "<p>None of these reasons correctly describe why %sh may need to be avoided</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A junior data engineer is using the %sh magic command to run some legacy code. A senior data engineer has recommended refactoring the code instead.Which of the following could explain why a data engineer may need to avoid using the %sh magic command ?", "related_lectures": []}, {"_class": "assessment", "id": 70683046, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given a Delta table \u2018products\u2019 with the following schema:</p><p><br></p><p>name STRING, category STRING, expiration_date DATE,&nbsp; price FLOAT</p><p><br></p><p>When executing the below query:</p><p><br></p><pre class=\"prettyprint linenums\">SELECT * FROM products\nWHERE price &gt; 30.5</pre><p><br></p><p>Which of the following will be leaverged by the query optimizer to identify the data files to load?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In the Transaction log, Delta Lake captures statistics for each data file of the table. These statistics indicate per file:</p><ul><li><p>Total number of records</p></li><li><p>Minimum value in each column of the first 32 columns of the table</p></li><li><p>Maximum value in each column of the first 32 columns of the table</p></li><li><p>Null value counts for in each column of the first 32 columns of the table</p></li></ul><p><br></p><p>When a query with a selective filter is executed against the table, the query optimizer uses these statistics to generate the query result. it leverages them to identify data files that may contain records matching the conditional filter.</p><p><br></p><p>For the SELECT query in the question, The transaction log is scanned for min and max statistics for the price column.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Columns statistics in the Hive metastore</p>", "<p>Columns statistics in the metadata of Parquet files</p>", "<p>Files statistics in the Delta transaction log</p>", "<p>Files statistics in the in the Hive metastore</p>", "<p>None of the above. All data files are fully scanned to identify the ones to load</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Given a Delta table \u2018products\u2019 with the following schema:name STRING, category STRING, expiration_date DATE,&nbsp; price FLOATWhen executing the below query:SELECT * FROM products\nWHERE price &gt; 30.5Which of the following will be leaverged by the query optimizer to identify the data files to load?", "related_lectures": []}, {"_class": "assessment", "id": 70683048, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a table \u2018orders_backup\u2019 that was created using Delta Lake\u2019s SHALLOW CLONE functionality from the table \u2018orders\u2019. Recently, the team started getting an error when querying the \u2018orders_backup\u2019 table indicating that some data files are no longer present.</p><p><br></p><p>Which of the following correctly explains this error ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With Shallow Clone, you create a copy of a table by just copying the Delta transaction logs.</p><p>That means that there is no data moving during Shallow Cloning.</p><p>Running the VACUUM command on the source table may purge data files referenced in the transaction log of the clone. In this case, you will get an error when querying the clone indicating that some data files are no longer present.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/clone.html\">https://docs.databricks.com/delta/clone.html</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>The VACUUM command was run on the orders table</p>", "<p>The VACUUM command was run on the orders_backup table</p>", "<p>The OPTIMIZE command was run on the orders table</p>", "<p>The OPTIMIZE command was run on the orders_backup table</p>", "<p>The REFRESH command was run on the orders_backup table</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "The data engineering team has a table \u2018orders_backup\u2019 that was created using Delta Lake\u2019s SHALLOW CLONE functionality from the table \u2018orders\u2019. Recently, the team started getting an error when querying the \u2018orders_backup\u2019 table indicating that some data files are no longer present.Which of the following correctly explains this error ?", "related_lectures": []}, {"_class": "assessment", "id": 70683050, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a Delta Lake table named \u2018orders_archive\u2019 created using the following command:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE orders_archive\nDEEP CLONE orders</pre><p><br></p><p>They want to sync up the new changes in the orders table to the clone.</p><p><br></p><p>Which of the following commands can be run to achieve this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Cloning can occur incrementally. Executing the <code>CREATE OR REPLACE TABLE</code> command can sync changes from the source to the target location.</p><p>Now, If you run <code>DESCRIBE HISTORY orders_archive</code>, you will see a new version of CLONE&nbsp;operation occurred on the table.</p><p><br></p><p>* The last choice in the question is incorrect since dropping the table will lead to removing all the table's history.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/clone.html\">https://docs.databricks.com/delta/clone.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>REFRESH orders_archive</p>", "<p>SYNC orders_archive</p>", "<p>INSERT OVERWRITE orders_archive</p><p>SELECT * FROM orders</p>", "<p>CREATE OR REPLACE TABLE orders_archive</p><p>DEEP CLONE orders</p>", "<p>DROP TABLE orders_archive;</p><p><br></p><p>CREATE TABLE orders_archive</p><p>DEEP CLONE orders</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer has a Delta Lake table named \u2018orders_archive\u2019 created using the following command:CREATE TABLE orders_archive\nDEEP CLONE ordersThey want to sync up the new changes in the orders table to the clone.Which of the following commands can be run to achieve this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70683052, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta Lake table named \u2018daily_activities\u2019 that is completely overwritten each night with new data received from the source system.</p><p><br></p><p>For auditing purposes, the team wants to set up a post-processing task that uses Delta Lake Time Travel functionality to determine the difference between the new version and the previous version of the table. They start by getting the current table version via this code:</p><p><br></p><pre class=\"prettyprint linenums\">current_version = spark.sql(\"SELECT max(version) FROM (DESCRIBE HISTORY daily_activities)\").collect()[0][0]</pre><p><br></p><p>Which of the following queries can be used by the team to complete this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Each operation that modifies a Delta Lake table creates a new table version. You can use history information to audit operations or query a table at a specific point in time using:</p><p><br></p><ul><li><p>Version number</p></li></ul><pre class=\"prettyprint linenums\">SELECT * FROM my_table@v36\nSELECT * FROM my_table VERSION AS OF 36</pre><p><br></p><ul><li><p>Timestamp</p></li></ul><pre class=\"prettyprint linenums\">SELECT * FROM my_table TIMESTAMP AS OF \"2019-01-01\"</pre><p><br></p><p>Using the EXCEPT set operator, you can get the difference between the new version and the previous version of the table</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/history.html\">https://docs.databricks.com/delta/history.html</a></p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-setops.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-setops.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37085216/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136\">Hands-on (Associate course)</a></p><p><br></p>", "answers": ["<p>SELECT * FROM daily_activities</p><p>UNION</p><p>SELECT * FROM daily_activities AS VERSION = {current_version-1}</p>", "<p>SELECT * FROM daily_activities</p><p>UNION ALL</p><p>SELECT * FROM daily_activities@v{current_version-1}</p>", "<p>SELECT * FROM daily_activities</p><p>INTERSECT</p><p>SELECT * FROM daily_activities AS VERSION = {current_version-1}</p>", "<p>SELECT * FROM daily_activities</p><p>EXCEPT</p><p>SELECT * FROM daily_activities@v{current_version-1}</p>", "<p>SELECT * FROM daily_activities</p><p>MINUS</p><p>SELECT * FROM daily_activities AS VERSION = {current_version-1}</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "The data engineering team has a Delta Lake table named \u2018daily_activities\u2019 that is completely overwritten each night with new data received from the source system.For auditing purposes, the team wants to set up a post-processing task that uses Delta Lake Time Travel functionality to determine the difference between the new version and the previous version of the table. They start by getting the current table version via this code:current_version = spark.sql(\"SELECT max(version) FROM (DESCRIBE HISTORY daily_activities)\").collect()[0][0]Which of the following queries can be used by the team to complete this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70683054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team wants to build a pipeline that receives customers data as change data capture (CDC) feed from a source system. The CDC events logged at the source contain the data of the records along with metadata information. This metadata indicates whether the specified record was inserted, updated, or deleted. In addition to a timestamp column identified by the field update_time indicating the order in which the changes happened. Each record has a primary key identified by the field customer_id.</p><p><br></p><p>In the same batch, multiple changes for the same customer could be received with different update_time. The team wants to store only the most recent information for each customer in the target Delta Lake table.</p><p><br></p><p>Which of the following solutions meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>MERGE INTO</code> command allows you to upsert data from a source table, view, or DataFrame into a target Delta table. Delta Lake supports inserts, updates, and deletes in merge operations.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/merge.html\">https://docs.databricks.com/delta/merge.html</a></p><p><a href=\"https://docs.databricks.com/sql/language-manual/delta-merge-into.html\">https://docs.databricks.com/sql/language-manual/delta-merge-into.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37177394/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059482/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Enable Delta Lake's Change Data Feed (CDF) on the target table to automatically merge the received CDC feed</p>", "<p>Use MERGE INTO to upsert the most recent entry for each customer_id into the table</p>", "<p>Use MERGE INTO with SEQUENCE BY clause on the update_time for ordering how operations should be applied</p>", "<p>Use dropDuplicates function to remove duplicates by customer_id, then merge the duplicate records into the table.</p>", "<p>Use the option mergeSchema when writing the CDC data into the table to automatically merge the changed data with its most recent schema.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team wants to build a pipeline that receives customers data as change data capture (CDC) feed from a source system. The CDC events logged at the source contain the data of the records along with metadata information. This metadata indicates whether the specified record was inserted, updated, or deleted. In addition to a timestamp column identified by the field update_time indicating the order in which the changes happened. Each record has a primary key identified by the field customer_id.In the same batch, multiple changes for the same customer could be received with different update_time. The team wants to store only the most recent information for each customer in the target Delta Lake table.Which of the following solutions meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70683056, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is using a foreachBatch logic to upsert data in a target Delta table.</p><p><br></p><p>The function to be called at each new microbatch processing is displayed below with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">def upsert_data(microBatchDF, batch_id):\n    microBatchDF.createOrReplaceTempView(\"sales_microbatch\")\n\n    sql_query = \"\"\"\n                MERGE INTO sales_silver a\n                USING sales_microbatch b\n                ON a.item_id=b.item_id\n                    AND a.item_timestamp=b.item_timestamp\n                WHEN NOT MATCHED THEN INSERT *\n                \"\"\"\n\n    ________________\n</pre><p><br></p><p>Which option correctly fills in the blank to execute the sql query in the function on a cluster with Databricks Runtime below 10.5 ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Usually, we use spark.sq() function to run SQL queries. However, in this particular case, the spark session can not be accessed from within the microbatch process. Instead, we can access the local spark session from the microbatch dataframe.</p><p><br></p><p>For clusters with Databricks Runtime version below 10.5, the syntax to access the local spark session is:</p><p><code>microBatchDF._jdf.sparkSession().sql(sql_query)</code></p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.gcp.databricks.com/structured-streaming/delta-lake.html#language-python\">https://docs.gcp.databricks.com/structured-streaming/delta-lake.html#language-python</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>spark.sql(sql_query)</p>", "<p>batch_id.sql(sql_query)</p>", "<p>microBatchDF.sql(sql_query)</p>", "<p>microBatchDF.sparkSession.sql(sql_query)</p>", "<p>microBatchDF._jdf.sparkSession().sql(sql_query)</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer is using a foreachBatch logic to upsert data in a target Delta table.The function to be called at each new microbatch processing is displayed below with a blank:def upsert_data(microBatchDF, batch_id):\n    microBatchDF.createOrReplaceTempView(\"sales_microbatch\")\n\n    sql_query = \"\"\"\n                MERGE INTO sales_silver a\n                USING sales_microbatch b\n                ON a.item_id=b.item_id\n                    AND a.item_timestamp=b.item_timestamp\n                WHEN NOT MATCHED THEN INSERT *\n                \"\"\"\n\n    ________________\nWhich option correctly fills in the blank to execute the sql query in the function on a cluster with Databricks Runtime below 10.5 ?", "related_lectures": []}, {"_class": "assessment", "id": 70683058, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a singleplex bronze table called \u2018orders_raw\u2019 where new orders data is appended every night. They created a new Silver table called \u2018orders_cleaned\u2019 in order to provide a more refined view of the orders data.</p><p><br></p><p>The team wants to create a batch processing pipeline to process all new records inserted in the orders_raw table and propagate them to the orders_cleaned table.</p><p><br></p><p>Which solution minimizes the compute costs to propagate this batch of data?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks supports trigger(availableNow=True) for Delta Lake and Auto Loader sources. This functionality consumes all available records in an incremental batch.</p><p><br></p><p>There is also the trigger(once=True) option for incremental batch processing. However, this setting is now deprecated in the newer Databricks Runtime versions.</p><p>NOTE: You may still see this option in the current certification exam version. However, Databricks recommends you use trigger(availableNow=True) for all future incremental batch processing workloads.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing\">https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37054444/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059428/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>Use time travel capabilities in Delta Lake to compare the latest version of orders_raw with one version prior, then write the difference to the orders_cleaned table.</p>", "<p>Use Spark Structured Streaming to process the new records from orders_raw in batch mode using the trigger availableNow option</p>", "<p>Use Spark Structured Streaming's foreachBatch logic to process the new records from orders_raw using trigger(processingTime=\u201d24 hours\")</p>", "<p>Use batch overwrite logic to reprocess all records in orders_raw and overwrite the orders_cleaned table</p>", "<p>Use insert-only merge into the orders_cleaned table using orders_raw data based on a composite key</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team has a singleplex bronze table called \u2018orders_raw\u2019 where new orders data is appended every night. They created a new Silver table called \u2018orders_cleaned\u2019 in order to provide a more refined view of the orders data.The team wants to create a batch processing pipeline to process all new records inserted in the orders_raw table and propagate them to the orders_cleaned table.Which solution minimizes the compute costs to propagate this batch of data?", "related_lectures": []}, {"_class": "assessment", "id": 70683060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Silver table called \u2018sales_cleaned\u2019 where new sales data is appended in near real-time.</p><p><br></p><p>They want to create a new Gold-layer entity against the \u2018sales_cleaned\u2019 table to calculate the year-to-date (YTD) of the sales amount. The new entity will have the following schema:</p><p><br></p><p>country_code STRING, category STRING, ytd_total_sales FLOAT, updated TIMESTAMP</p><p><br></p><p>It\u2019s enough for these metrics to be recalculated once daily. But since they will be queried very frequently by several business teams, the data engineering team wants to cut down the potential costs and latency associated with materializing the results.</p><p><br></p><p>Which of the following solutions meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Data engineers must understand how materializing results is different between views and tables on Databricks, and how to reduce total compute and storage cost associated with each materialization depending on the scenario.</p><p><br></p><p>Consider using a view when:</p><ul><li><p>Your query is not complex. Because views are computed on demand, the view is re-computed every time the view is queried. So, frequently querying complex queries with joins and subqueries increases compute costs</p></li><li><p>You want to reduce storage costs. Views do not require additional storage resources.</p></li></ul><p>Consider using a gold table when:</p><ul><li><p>Multiple downstream queries consume the table, so you want to avoid re-computing complex ad-hoc queries every time.</p></li><li><p>Query results should be computed incrementally from a data source that is continuously or incrementally growing.</p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059586/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Define the new entity as a view to avoid persisting the results each time the metrics are recalculated</p>", "<p>Define the new entity as a global temporary view since it can be shared between notebooks or jobs that share computing resources.</p>", "<p>Configuring a nightly batch job to recalculate the metrics and store them as a table overwritten with each update.</p>", "<p>Create multiple tables, one per business team so the metrics can be queried quickly and efficiently.</p>", "<p>All the above solutions meet the required requirements since Databricks uses the Delta Caching feature</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a Silver table called \u2018sales_cleaned\u2019 where new sales data is appended in near real-time.They want to create a new Gold-layer entity against the \u2018sales_cleaned\u2019 table to calculate the year-to-date (YTD) of the sales amount. The new entity will have the following schema:country_code STRING, category STRING, ytd_total_sales FLOAT, updated TIMESTAMPIt\u2019s enough for these metrics to be recalculated once daily. But since they will be queried very frequently by several business teams, the data engineering team wants to cut down the potential costs and latency associated with materializing the results.Which of the following solutions meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70683062, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to calculate predictions using a MLFlow model logged in a given \u201cmodel_url\u201d. They want to register the model as a Spark UDF in order to apply it to a test dataset.</p><p><br></p><p>Which code block allows the data engineer to register the MLFlow model as a Spark UDF ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>Mlflow.pyfunc.spark_udf</code> function allows to register a MLFlow model as a Apache Spark UDF. It needs at least 2 parameters:</p><ul><li><p><code>spark</code>: A SparkSession object</p></li><li><p><code>model_uri</code>: the location, in URI format, of the MLflow model</p></li></ul><p><br></p><p>Once the Spark UDF created, it can be applied to a dataset to calculate the predictions:</p><pre class=\"prettyprint linenums\">predict_udf = mlflow.pyfunc.spark_udf(spark, \"model_url\")\npred_df = data_df.withColumn(\"prediction\", predict_udf(*column_list))</pre><p><br></p><p>Sample Notebook:</p><p><a href=\"https://docs.databricks.com/en/_extras/notebooks/source/mlflow/mlflow-quick-start-inference.html\">https://docs.databricks.com/en/_extras/notebooks/source/mlflow/mlflow-quick-start-inference.html</a></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/en/mlflow/models.html\">https://docs.databricks.com/en/mlflow/models.html</a></p><p><a href=\"https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf\">https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf</a></p><p><br></p>", "answers": ["<p>predict_udf = mlflow.pyfunc.spark_udf(spark, \"model_url\")</p>", "<p>predict_udf = mlflow.spark_udf(spark, \"model_url\")</p>", "<p>predict_udf = mlflow.udf(spark, \"model_url\")</p>", "<p>predict_udf = pyfunc.spark_udf(spark, \"model_url\")</p>", "<p>predict_udf = mlflow.pyfunc(spark, \"model_url\")</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "A data engineer wants to calculate predictions using a MLFlow model logged in a given \u201cmodel_url\u201d. They want to register the model as a Spark UDF in order to apply it to a test dataset.Which code block allows the data engineer to register the MLFlow model as a Spark UDF ?", "related_lectures": []}, {"_class": "assessment", "id": 70683064, "assessment_type": "multiple-choice", "prompt": {"question": "<p>\u201cA Delta Lake\u2019s functionality that automatically compacts small files during individual writes to a table by performing two complementary operations on the table\u201d</p><p><br></p><p>Which of the following is being described in the above statement? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Optimize is a functionality that allows Delta Lake to automatically compact small data files of Delta tables. This can be achieved during individual writes to the Delta table.</p><p><br></p><p>Auto optimize consists of 2 complementary operations:</p><p>- Optimized writes: with this feature enabled, Databricks attempts to write out 128 MB files for each table partition.</p><p>- Auto compaction: this will check after an individual write, if files can further be compacted. If yes, it runs an OPTIMIZE job with 128 MB file sizes (instead of the 1 GB file size used in the standard OPTIMIZE)</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/tune-file-size.html\">https://docs.databricks.com/delta/tune-file-size.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37123146/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>Optimized writes</p>", "<p>Auto compaction</p>", "<p>Auto Optimize</p>", "<p>OPTIMIZE command</p>", "<p>REORG TABLE command</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "\u201cA Delta Lake\u2019s functionality that automatically compacts small files during individual writes to a table by performing two complementary operations on the table\u201dWhich of the following is being described in the above statement?", "related_lectures": []}, {"_class": "assessment", "id": 70683066, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a large external Delta table where new changes are merged very frequently. They enabled Optimized writes and Auto Compaction on the table in order to automatically compact small data files to target files of size 128 MB. However, when they look at the table directory, they see that most data files are smaller than 128 MB.</p><p><br></p><p>Which of the following likely explains these smaller file sizes ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Having many small files can help minimize rewrites during some operations like merges and deletes. For such operations, Databricks can automatically tune the file size of Delta tables. As a result, it can generate data files smaller than the default 128MB. This helps in reducing the duration of future MERGE operations.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/tune-file-size.html#autotune-file-size-based-on-workload\">https://docs.databricks.com/delta/tune-file-size.html#autotune-file-size-based-on-workload</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37123146/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>Optimized Writes and Auto Compaction have no effect on large Delta tables. The table needs to be partitioned so Auto Compaction can be applied at partition level.</p>", "<p>Optimized Writes and Auto Compaction have no effect on external tables. The table needs to be managed in order to store the information of file sizes in the Hive metastore.</p>", "<p>Optimized Writes and Auto Compaction automatically generate smaller data files to reduce the duration of future MERGE operations.</p>", "<p>Auto compaction supports Auto Z-Ordering which is more expensive than just compaction</p>", "<p>The team needs to look at the table\u2019s _auto_optimize directory, where all new compacted files are written.</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a large external Delta table where new changes are merged very frequently. They enabled Optimized writes and Auto Compaction on the table in order to automatically compact small data files to target files of size 128 MB. However, when they look at the table directory, they see that most data files are smaller than 128 MB.Which of the following likely explains these smaller file sizes ?", "related_lectures": []}, {"_class": "assessment", "id": 70683068, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which statement regarding streaming state in Stream-Stream Joins is correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>When performing stream-stream join, Spark buffers past inputs as a streaming state for both input streams, so that it can match every future input with past inputs. This state can be limited by using watermarks.</p><p><br></p><p>Reference: </p><p><a href=\"https://www.databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\">https://www.databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059524/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Stream-Stream Joins are not stateful. Spark does not buffers past inputs as a streaming state for the input streams</p>", "<p>Spark buffers past inputs as a streaming state only for the left input stream, so that it can match future right inputs with past left inputs.</p>", "<p>Spark buffers past inputs as a streaming state only for the right input stream, so that it can match future left inputs with past right inputs.</p>", "<p>Spark buffers past inputs as a streaming state for both input streams, so that it can match every future input with past inputs.</p>", "<p>Stream-Stream Joins does not support limiting the state information using watermarks.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which statement regarding streaming state in Stream-Stream Joins is correct?", "related_lectures": []}, {"_class": "assessment", "id": 70683070, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which statement regarding static Delta tables in Stream-Static joins is correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Stream-static joins take advantage of Delta Lake guarantee that the latest version of the static delta table is returned each time it is queried in a join operation with a data stream.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/structured-streaming/delta-lake.html#performing-stream-static-joins\">https://docs.databricks.com/structured-streaming/delta-lake.html#performing-stream-static-joins</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059536/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059566/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Static Delta tables must be small enough to be broadcasted to all worker nodes in the cluster.</p>", "<p>Static Delta tables need to be partitioned in order to be used in stream-static join.</p>", "<p>Static Delta tables need to be refreshed with REFRESH TABLE command for each microbatch of a stream-static join</p>", "<p>The latest version of the static Delta table is returned each time it is queried by a microbatch of the stream-static join</p>", "<p>The latest version of the static Delta table is returned only for the first microbatch of the stream-static join. Then, it will be cached to be used by any upcoming microbatch.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which statement regarding static Delta tables in Stream-Static joins is correct?", "related_lectures": []}, {"_class": "assessment", "id": 70683146, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has the following streaming query with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n       .table(\"orders_cleaned\")\n       ____________________________\n       .groupBy(\n           \"order_timestamp\",\n           \"author\")\n       .agg(\n           count(\"order_id\").alias(\"orders_count\"),\n           avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"orders_stats\")</pre><p><br></p><p>For handling late-arriving data, they want to maintain the streaming state information for 30 minutes. </p><p><br></p><p>Which option correctly fills in the blank to meet this requirement ? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>pyspark.sql.DataFrame.withWatermark function allows you to only track state information for a window of time in which we expect records could be delayed.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withWatermark.html\">https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withWatermark.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>.trigger(processingTime=\u201d30 minutes\")</p>", "<p>.awaitTermination(\"order_timestamp\", \"30 minutes\")</p>", "<p>.awaitWatermark(\"order_timestamp\", \"30 minutes\")</p>", "<p>.withWatermark(\"order_timestamp\", \"30 minutes\")</p>", "<p>.window(\"order_timestamp\", \"30 minutes\")</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer has the following streaming query with a blank:spark.readStream\n       .table(\"orders_cleaned\")\n       ____________________________\n       .groupBy(\n           \"order_timestamp\",\n           \"author\")\n       .agg(\n           count(\"order_id\").alias(\"orders_count\"),\n           avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"orders_stats\")For handling late-arriving data, they want to maintain the streaming state information for 30 minutes. Which option correctly fills in the blank to meet this requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70683072, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n        .table(\"orders_cleaned\")\n        .withWatermark(\"order_timestamp\", \"10 minutes\")\n        .groupBy(\n            window(\"order_timestamp\", \"5 minutes\").alias(\"time\"),\n            \"author\")\n        .agg(\n            count(\"order_id\").alias(\"orders_count\"),\n            avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n        .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n        .table(\"orders_stats\")</pre><p><br></p><p>Which of the following statements best describe this query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Pyspark.sql.functions.window function bucketizes rows into one or more time windows given a timestamp specifying column. In this query, we are performing aggregations per order_timestamp for each non-overlapping five minute interval.</p><p><br></p><p>pyspark.sql.DataFrame.withWatermark function allows you to only track state information for a window of time in which we expect records could be delayed. Here we define a watermark of 10 minutes.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.window.html\">https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.window.html</a></p><p><a href=\"https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withWatermark.html\">https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withWatermark.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059586/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>It calculates business-level aggregates for each non-overlapping ten-minute interval. Incremental state information is maintained for 5 minutes for late-arriving data.</p>", "<p>It calculates business-level aggregates for each non-overlapping five-minute interval. Incremental state information is maintained for 10 minutes for late-arriving data.</p>", "<p>It calculates business-level aggregates for each overlapping five-minute interval. Incremental state information is maintained for 10 minutes for late-arriving data.</p>", "<p>It calculates business-level aggregates for each overlapping ten-minute interval. Incremental state information is maintained for 5 minutes for late-arriving data.</p>", "<p>None of the above statements correctly describe this query</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following streaming query:spark.readStream\n        .table(\"orders_cleaned\")\n        .withWatermark(\"order_timestamp\", \"10 minutes\")\n        .groupBy(\n            window(\"order_timestamp\", \"5 minutes\").alias(\"time\"),\n            \"author\")\n        .agg(\n            count(\"order_id\").alias(\"orders_count\"),\n            avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n        .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n        .table(\"orders_stats\")Which of the following statements best describe this query ?", "related_lectures": []}, {"_class": "assessment", "id": 70683074, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which statement regarding checkpointing in Spark Structured Streaming is <strong>Not</strong> correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Checkpoints cannot be shared between separate streams. Each stream needs to have its own checkpoint directory to ensure processing guarantees.</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/blog/2022/12/12/streaming-production-collected-best-practices.html\">https://www.databricks.com/blog/2022/12/12/streaming-production-collected-best-practices.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>Checkpoints stores the current state of a streaming job to cloud storage</p>", "<p>Checkpointing allows the streaming engine to track the progress of a stream processing</p>", "<p>Checkpoints can be shared between separate streams</p>", "<p>To specify the checkpoint in a streaming query, we use the <strong>checkpointLocation</strong> option.</p>", "<p>Checkpointing with write-ahead logs mechanism ensure fault-tolerant stream processing</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Which statement regarding checkpointing in Spark Structured Streaming is Not correct?", "related_lectures": []}, {"_class": "assessment", "id": 70683076, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes Delta Lake Auto Compaction?</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Compaction is part of the Auto Optimize feature in Databricks. it checks after an individual write, if files can further be compacted, if yes, it runs an OPTIMIZE job with 128 MB file sizes instead of the 1 GB file size used in the standard OPTIMIZE</p><p><br></p><p>Auto compaction does not support Z-Ordering as Z-Ordering is significantly more expensive than just compaction.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/tune-file-size.html#auto-compaction-for-delta-lake-on-databricks\">https://docs.databricks.com/delta/tune-file-size.html#auto-compaction-for-delta-lake-on-databricks</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37123146/?referralCode=936CBDC941031CE4D795\">Lecture</a></p>", "answers": ["<p>Auto Compaction occurs after a write to a table has succeeded to check if files can further be compacted; if yes, it runs an OPTIMIZE job with Z-Ordering toward a file size of 128 MB.</p>", "<p>Auto Compaction occurs after a write to a table has succeeded to check if files can further be compacted; if yes, it runs an OPTIMIZE job without Z-Ordering toward a file size of 128 MB.</p>", "<p>Auto Compaction occurs after a write to a table has succeeded to check if files can further be compacted; if yes, it runs an OPTIMIZE job with Z-Ordering toward a file size of 1 GB.</p>", "<p>Auto Compaction occurs after a write to a table has succeeded to check if files can further be compacted; if yes, it runs an OPTIMIZE job without Z-Ordering toward a file size of 1 GB.</p>", "<p>None of the above statements correctly describe Delta Lake Auto Compaction.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following statements best describes Delta Lake Auto Compaction?", "related_lectures": []}, {"_class": "assessment", "id": 70683078, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes Auto Loader ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Loader incrementally and idempotently processes new data files as they arrive in cloud storage and load them into a target Delta Lake table.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html\">https://docs.databricks.com/ingestion/auto-loader/index.html</a></p><p><br></p><p>Study materials from our Associate exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37054468/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>Auto loader allows applying Change Data Capture (CDC) feed to update tables based on changes captured in source data.</p>", "<p>Auto loader monitors a source location, in which files accumulate, to identify and ingest only new arriving files with each command run. While the files that have already been ingested in previous runs are skipped.</p>", "<p>Auto loader allows cloning a source Delta table to a target destination at a specific version.</p>", "<p>Auto loader defines data quality expectations on the contents of a dataset, and reports the records that violate these expectations in metrics.</p>", "<p>Auto loader enables efficient insert, update, deletes, and rollback capabilities by adding a storage layer that provides better data reliability to data lakes.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following statements best describes Auto Loader ?", "related_lectures": []}, {"_class": "assessment", "id": 70683080, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following functions can a data engineer use to return a new DataFrame containing the distinct rows from a given DataFrame based on multiple columns?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>pyspark.sql.DataFrame.dropDuplicates function returns a new DataFrame with duplicate rows removed, optionally only considering certain columns.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html\">https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>pyspark.sql.DataFrame.drop</p>", "<p>pyspark.sql.DataFrame.distinct</p>", "<p>pyspark.sql.DataFrame.dropDuplicates</p>", "<p>pyspark.sql.DataFrame.na.drop</p>", "<p>pyspark.sql.DataFrame.dropna</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Which of the following functions can a data engineer use to return a new DataFrame containing the distinct rows from a given DataFrame based on multiple columns?", "related_lectures": []}, {"_class": "assessment", "id": 70683082, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following approaches allows to correctly perform streaming deduplication ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To perform streaming deduplication, we use dropDuplicates() function to eliminate duplicate records within each new micro batch. In addition, we need to ensure that records to be inserted are not already in the target table. We can achieve this using insert-only merge.</p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html\">https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html</a></p><p><a href=\"https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables\">https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>De-duplicate records within each batch, and then append the result into the target table</p>", "<p>De-duplicate records within each batch, and then merge the result into the target table using insert-only merge</p>", "<p>De-duplicate records within each batch, rank the result, and then insert only records having rank = 1 into the target table</p>", "<p>De-duplicate records in all batches with watermarking, and then overwrite the target table by the result</p>", "<p>None of the above approaches allows to correctly perform streaming deduplication</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following approaches allows to correctly perform streaming deduplication ?", "related_lectures": []}, {"_class": "assessment", "id": 70683084, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer is testing the following code block to get the newest entry for each item added in the \u2018sales\u2019 table since the last table update.</p><p><br></p><pre class=\"prettyprint linenums\">from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"item_id\").orderBy(F.col(\"item_time\").desc())\n\nranked_df = (spark.readStream\n                    .table(\"sales\")\n                    .withColumn(\"rank\", F.rank().over(window))\n                    .filter(\"rank == 1\")\n                    .drop(\"rank\")\n            )\n\ndisplay(ranked_df)</pre><p><br></p><p>However, the command fails when executed.</p><p><br></p><p>Which statement explains the cause of this failure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>If you try to call such a window operation on a streaming DataFrames, this will generate an error indicating that \u201cNon-time-based window operations are not supported on streaming DataFrames\u201d.</p><p>Instead, these window operations need to be implemented inside a foreachBatch logic.</p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059482/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The query output can not be displayed. They should use spark.writeStream to persist the query result.</p>", "<p>Watermarking is missing. It should be added to allow tracking state information for the window of time.</p>", "<p>Non-time-based window operations are not supported on streaming DataFrames. They need to be implemented inside a foreachBatch logic instead.</p>", "<p>The item_id field is not unique. Records must be de-duplicated on the item_id using dropDuplicates function</p>", "<p>The item_id field is not unique. The drop(\"rank\") must be called before applying the rank function in order to drop any duplicate record.</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A junior data engineer is testing the following code block to get the newest entry for each item added in the \u2018sales\u2019 table since the last table update.from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"item_id\").orderBy(F.col(\"item_time\").desc())\n\nranked_df = (spark.readStream\n                    .table(\"sales\")\n                    .withColumn(\"rank\", F.rank().over(window))\n                    .filter(\"rank == 1\")\n                    .drop(\"rank\")\n            )\n\ndisplay(ranked_df)However, the command fails when executed.Which statement explains the cause of this failure?", "related_lectures": []}, {"_class": "assessment", "id": 70683086, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following query on the Delta table \u2018customers\u2019 on which Change Data Feed is enabled:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter (col(\"_change_type\").isin([\"update_postimage\"]))\n    .writeStream\n        .option (\"checkpointLocation\", \"dbfs:/checkpoints\")\n        .trigger (availableNow=True)\n        .table(\"customers_updates\")</pre><p><br></p><p>Which statement describes the results of this query each time it is executed ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks supports reading table\u2019s changes captured by CDF in streaming queries using spark.readStream. This allows you to get only the new changes captured since the last time the streaming query was run.</p><p><br></p><p>The query in the question then appends the data to the target table at each execution since it\u2019s using the default writing mode, which is \u2018append\u2019.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-streaming-queries\">https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-streaming-queries</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Newly updated records will be merged into the target table, modifying previous entries with the same primary keys.</p>", "<p>Newly updated records will be appended to the target table.</p>", "<p>Newly updated records will overwrite the target table.</p>", "<p>The entire history of updated records will be appended to the target table at each execution, which leads to duplicate entries.</p>", "<p>The entire history of updated records will overwrite the target table at each execution.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following query on the Delta table \u2018customers\u2019 on which Change Data Feed is enabled:spark.readStream\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter (col(\"_change_type\").isin([\"update_postimage\"]))\n    .writeStream\n        .option (\"checkpointLocation\", \"dbfs:/checkpoints\")\n        .trigger (availableNow=True)\n        .table(\"customers_updates\")Which statement describes the results of this query each time it is executed ?", "related_lectures": []}, {"_class": "assessment", "id": 70683088, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following query on the Delta table customers on which Change Data Feed is enabled:</p><p><br></p><pre class=\"prettyprint linenums\">spark.read\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter(col(\"_change_type\").isin([\"update_postimage\"]))\n    .write\n        .mode(\u201coverwrite\u201d)\n        .table(\"customers_updates\")</pre><p><br></p><p>Which statement describes the results of this query each time it is executed ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Reading table\u2019s changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table\u2019s changes (starting from the specified startingVersion) will be read.</p><p><br></p><p>The query in the question then writes the data in mode \u201coverwrite\u201d to the target table, which completely overwrites the table at each execution.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-batch-queries\">https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-batch-queries</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Newly updated records will be merged into the target table, modifying previous entries with the same primary keys.</p>", "<p>Newly updated records will be appended to the target table.</p>", "<p>Newly updated records will overwrite the target table.</p>", "<p>The entire history of updated records will be appended to the target table at each execution, which leads to duplicate entries.</p>", "<p>The entire history of updated records will overwrite the target table at each execution.</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "Given the following query on the Delta table customers on which Change Data Feed is enabled:spark.read\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter(col(\"_change_type\").isin([\"update_postimage\"]))\n    .write\n        .mode(\u201coverwrite\u201d)\n        .table(\"customers_updates\")Which statement describes the results of this query each time it is executed ?", "related_lectures": []}, {"_class": "assessment", "id": 70683090, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team maintains a Type 1 table that is overwritten each night with new data received from the source system.</p><p><br></p><p>A junior data engineer has suggested enabling the Change Data Feed (CDF) feature on the table in order to identify those rows that were updated, inserted, or deleted.</p><p><br></p><p>Which response to the junior data engineer's suggestion is correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Generally speaking, we use CDF for sending incremental data changes to downstream tables in a multi-hop architecture. So, use CDF when only small fraction of records updated in each batch. Such updates are usually received from external sources in CDC format. If most of the records in the table are updated, or if the table is overwritten in each batch, like in the question, don\u2019t use CDF.</p><p><br></p><p>Here is some guidance for when to use CDF (from the below reference link)</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_18-33-59-7f713b34bcd4a31e0e20bf53b9ecd02f.png\"><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html\">https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>CDF can not be enabled on existing tables. It can only be enabled on newly created tables.</p>", "<p>Table\u2019s data changes captured by CDF can only be read in streaming mode</p>", "<p>CDF is useful when only a small fraction of records are updated in each batch</p>", "<p>CDF is useful when the table is a Slowly Changing Dimension (SCD) of Type 2</p>", "<p>All the above are correct responses to the data engineer's suggestion</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team maintains a Type 1 table that is overwritten each night with new data received from the source system.A junior data engineer has suggested enabling the Change Data Feed (CDF) feature on the table in order to identify those rows that were updated, inserted, or deleted.Which response to the junior data engineer's suggestion is correct?", "related_lectures": []}, {"_class": "assessment", "id": 70683092, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to ingest input json data into a target Delta table. They want the data ingestion to happen incrementally in near real-time.</p><p><br></p><p>Which option correctly meets the specified requirement ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In order to ingest input json data into a target Delta table, we use Autoloader. Auto Loader is based on Spark Structured Streaming and provides a Structured Streaming source called \u2018cloudFiles\u2019.</p><p>If you want the data ingestion to happen incrementally in near real-time, you can use the default trigger method which is trigger(processingTime=\u201d500ms\"). This allows the processing of data in micro-batches at a fixed interval of half a second.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html\">https://docs.databricks.com/ingestion/auto-loader/index.html</a></p><p><a href=\"https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval\">https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.format(\"autoloader\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option (\"autoloader.format\", \"json\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.load(source_path)</p><p>.writeStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option(\"checkpointLocation\", checkpointPath)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.start(\"target_table\")</p>", "<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.format(\"autoloader\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option (\"autoloader.format\", \"json\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.load(source_path)</p><p>.writeStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option(\"checkpointLocation\", checkpointPath)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.trigger(real-time=True)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.start(\"target_table\")</p>", "<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.format(\"cloudFiles\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option (\"cloudFiles.format\", \"json\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.load(source_path)</p><p>.writeStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option(\"checkpointLocation\", checkpointPath)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.start(\"target_table\")</p>", "<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.format(\"cloudFiles\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option (\"cloudFiles.format\", \"json\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.load(source_path)</p><p>.writeStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.trigger(real-time=True)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.start(\"target_table\")</p>", "<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.format(\"cloudFiles\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.option (\"cloudFiles.format\", \"json\")</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.load(source_path)</p><p>.writeStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.trigger(availableNow=True)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;.start(\"target_table\")</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer wants to ingest input json data into a target Delta table. They want the data ingestion to happen incrementally in near real-time.Which option correctly meets the specified requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70683094, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n        ._____________\n        .table(\"new_orders\")\n)</pre><p><br></p><p>Fill in the blank to make the query executes a micro-batch to process data every 2 minutes</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Spark Structured Streaming, in order to process data in micro-batches at a user-specified intervals, you can use the processingTime trigger method. This allows you to specify a time duration as a string. By default, it\u2019s \u201c500ms\u201d.</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/triggers.html#configure-structured-streaming-trigger-intervals\">https://docs.databricks.com/structured-streaming/triggers.html#configure-structured-streaming-trigger-intervals</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>trigger(once=\u201d2 minutes\u201d)</p>", "<p>trigger(processingTime=\u201d2 minutes\")</p>", "<p>processingTime(\u201d2 minutes\")</p>", "<p>trigger(\u201d2 minutes\")</p>", "<p>trigger()</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following Structured Streaming query:(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n        ._____________\n        .table(\"new_orders\")\n)Fill in the blank to make the query executes a micro-batch to process data every 2 minutes", "related_lectures": []}, {"_class": "assessment", "id": 70683096, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which statement regarding Delta Lake File Statistics is correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake automatically captures statistics in the transaction log for each added data file of the table. By default, Delta Lake collects the statistics on the first 32 columns of each table. These statistics indicate per file:</p><ul><li><p>Total number of records</p></li><li><p>Minimum value in each column of the first 32 columns of the table</p></li><li><p>Maximum value in each column of the first 32 columns of the table</p></li><li><p>Null value counts for in each column of the first 32 columns of the table</p></li></ul><p>These statistics are leveraged for data skipping based on query filters.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>By default, Delta Lake captures statistics in the Hive metastore on the first 16 columns of each table.</p>", "<p>By default, Delta Lake captures statistics in the Hive metastore on the first 32 columns of each table.</p>", "<p>By default, Delta Lake captures statistics in the transaction log on the first 16 columns of each table.</p>", "<p>By default, Delta Lake captures statistics in the transaction log on the first 32 columns of each table.</p>", "<p>By default, Delta Lake captures statistics in both Hive metastore and transaction log for each added data file.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which statement regarding Delta Lake File Statistics is correct?", "related_lectures": []}, {"_class": "assessment", "id": 70683098, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer uses the following SQL query:</p><p><br></p><p><code>GRANT USAGE ON DATABASE sales_db TO finance_team</code></p><p><br></p><p>Which of the following is the benefit of the USAGE privilege ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The USAGE does not give any abilities, but it's an additional requirement to perform any action on a schema (database) object.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>Gives read access on the database</p>", "<p>Gives full permissions on the entire database</p>", "<p>Gives the ability to view database objects and their metadata</p>", "<p>No effect! but it's required to perform any action on the database</p>", "<p>USAGE privilege is not part of the Databricks governance model</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer uses the following SQL query:GRANT USAGE ON DATABASE sales_db TO finance_teamWhich of the following is the benefit of the USAGE privilege ?", "related_lectures": []}, {"_class": "assessment", "id": 70683100, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team is using the LOCATION keyword for every new Delta Lake table created in the Lakehouse.</p><p><br></p><p>Which of the following describes the purpose of using the LOCATION keyword in this case ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>External (unmanaged) tables are tables whose data is stored in an external storage path by using a LOCATION clause.</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table\">https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p><p><br></p>", "answers": ["<p>The LOCATION keyword is used to configure the created Delta Lake tables as managed tables.</p>", "<p>The LOCATION keyword is used to configure the created Delta Lake tables as external tables.</p>", "<p>The LOCATION keyword is used to define the created Delta Lake tables in an external database.</p>", "<p>The LOCATION keyword is used to define the created Delta Lake tables in a database over a JDBC connection.</p>", "<p>The LOCATION keyword is used to set a default schema and checkpoint location for the created Delta Lake tables.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team is using the LOCATION keyword for every new Delta Lake table created in the Lakehouse.Which of the following describes the purpose of using the LOCATION keyword in this case ?", "related_lectures": []}, {"_class": "assessment", "id": 70683102, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to create a Delta Lake table for storing user activities of a website. The table has the following schema:</p><p><br></p><p>user_id LONG, page STRING, activity_type LONG, ip_address STRING, activity_time TIMESTAMP, activity_date DATE</p><p><br></p><p>Based on the above schema, which column is a good candidate for partitioning the Delta Table?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>When choosing partitioning columns, it's good to consider the fact that records with a given value (the activities of a given user) will continue to arrive indefinitely. In such a case, we use a datetime column for partitioning. This allows your partitions to be optimized, and allows you to easily archive those partitions of previous time periods, if necessary.</p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059604/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>user_id</p>", "<p>activity_type</p>", "<p>page</p>", "<p>activity_time</p>", "<p>activity_date</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer wants to create a Delta Lake table for storing user activities of a website. The table has the following schema:user_id LONG, page STRING, activity_type LONG, ip_address STRING, activity_time TIMESTAMP, activity_date DATEBased on the above schema, which column is a good candidate for partitioning the Delta Table?", "related_lectures": []}, {"_class": "assessment", "id": 70683104, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a large Delta table named \u2018users\u2019. A recent query on the table returned some entries with negative values in the \u2018age\u2019 column.</p><p><br></p><p>To avoid this issue and enforce data quality, a junior data engineer decided to add a CHECK constraint to the table with the following command:</p><p><br></p><p><code>ALTER TABLE users ADD CONSTRAINT valid_age CHECK (age&gt; 0);</code></p><p><br></p><p>However, the command fails when executed.</p><p><br></p><p>Which statement explains the cause of this failure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>ADD CONSTRAINT command verifies that all existing rows in the table satisfy the constraint before adding it to the table. Otherwise, the command failed with an error that says some rows in the table violate the new CHECK constraint.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/tables/constraints.html#set-a-check-constraint-in-databricks\">https://docs.databricks.com/tables/constraints.html#set-a-check-constraint-in-databricks</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059434/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The syntax for adding the CHECK constraint is incorrect. Instead, the command should be:</p><p>ALTER TABLE users ADD CONSTRAINT ON COLUMN age (CHECK &gt; 0)</p>", "<p>The users table already exists; CHECK constraints can only be added during table creation using CREATE TABLE command.</p>", "<p>The users table already contains rows that violate the new constraint; all existing rows must satisfy the constraint before adding it to the table.</p>", "<p>The users table is not partitioned on the age column. CHECK constraints can only be added on partitioning columns.</p>", "<p>The users table already contains rows; CHECK constraints can only be added on empty tables</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a large Delta table named \u2018users\u2019. A recent query on the table returned some entries with negative values in the \u2018age\u2019 column.To avoid this issue and enforce data quality, a junior data engineer decided to add a CHECK constraint to the table with the following command:ALTER TABLE users ADD CONSTRAINT valid_age CHECK (age&gt; 0);However, the command fails when executed.Which statement explains the cause of this failure?", "related_lectures": []}, {"_class": "assessment", "id": 70683106, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer added a CHECK constraint to the table using the following command:</p><p><br></p><pre class=\"prettyprint linenums\">ALTER TABLE sales ADD CONSTRAINT valid_date CHECK (item_date &gt;= '2023-01-01');</pre><p><br></p><p>Which of the following commands allows the data engineer to verify that the constraint has been successfully added on the table ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>DESCRIBE TABLE EXTENDED or simply DESCRIBE EXTENDED allows to show the added tables constraints in the \u2018Table Properties\u2019 field. It shows both the name and the actual condition of the check constraints.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-table.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-table.html</a></p><p><a href=\"https://docs.databricks.com/tables/constraints.html#set-a-check-constraint-in-databricks\">https://docs.databricks.com/tables/constraints.html#set-a-check-constraint-in-databricks</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059434/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>SHOW TBLPROPERTIES sales</p>", "<p>DESCRIBE TABLE sales</p>", "<p>DESCRIBE EXTENDED sales</p>", "<p>SHOW CONSTRAINTS sales</p>", "<p>SHOW CONSTRAINT valid_date</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer added a CHECK constraint to the table using the following command:ALTER TABLE sales ADD CONSTRAINT valid_date CHECK (item_date &gt;= '2023-01-01');Which of the following commands allows the data engineer to verify that the constraint has been successfully added on the table ?", "related_lectures": []}, {"_class": "assessment", "id": 70683108, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is the benefit of Delta Lake File Statistics ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake File Statistics indicate per file:</p><ul><li><p>Total number of records</p></li><li><p>Minimum value in each column of the first 32 columns of the table</p></li><li><p>Maximum value in each column of the first 32 columns of the table</p></li><li><p>Null value counts for in each column of the first 32 columns of the table</p></li></ul><p><br></p><p>These statistics are leveraged for data skipping based on query filters. For example, if you are querying the total number of records in a table, Delta will not calculate the count by scanning all data files. Instead, it will leverage these statistics to generate the query result</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>They are leveraged for process time forecasting when executing selective queries.</p>", "<p>They are leveraged for data skipping when executing selective queries.</p>", "<p>They are leveraged for data compression in order to improve Delta Caching.</p>", "<p>They are used as checksums to check data corruption in parquet files.</p>", "<p>None of the above statements correctly describe the benefit of Delta Lake File Statistics.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following is the benefit of Delta Lake File Statistics ?", "related_lectures": []}, {"_class": "assessment", "id": 70683110, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following two versions of a Delta Lake table before and after an update:</p><p><br></p><p>Before:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-09_23-15-02-86eb1b6b8265c28640996aa4ac713d7d.png\"><p><br></p><p>After:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-09_23-15-03-0923eb9a20be17506234a4a91d21df5c.png\"><p><br></p><p>Which SCD Type is this table ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In a Type 1 SCD table the new data overwrites the existing one. Thus, the existing data is lost as it is not stored anywhere else.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://en.wikipedia.org/wiki/Slowly_changing_dimension\">https://en.wikipedia.org/wiki/Slowly_changing_dimension</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059464/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p><p>For example, our customers_silver table is created as Type 1 SCD. It contains only the latest valid information of each customer</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059482/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>SCD Type 0</p>", "<p>SCD Type 1</p>", "<p>SCD Type 2</p>", "<p>It's a combination of Type 0 and Type 2 SCDs</p>", "<p>More information is needed to determine the SCD Type of this table</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following two versions of a Delta Lake table before and after an update:Before:After:Which SCD Type is this table ?", "related_lectures": []}, {"_class": "assessment", "id": 70683112, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team created a new Databricks job for processing sensitive financial data. A financial analyst asked the team to transfer the \"Owner\" privilege of this job to the \u201cfinance\u201d group.</p><p><br></p><p>A junior data engineer that has the \u201cCAN MANAGE\u201d permission on the job is attempting to make this privilege transfer via Databricks Job UI, but it keeps failing.</p><p><br></p><p>Which of the following explains the cause of this failure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>A job cannot have a group as an owner. If you try to set a group as the owner of a job, you get the error \u201cGroups can not be owners\u201d</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/jobs-acl.html#job-permissions\">https://docs.databricks.com/security/auth-authz/access-control/jobs-acl.html#job-permissions</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059640/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The \"Owner\" privilege is assigned at job creation to the creator and cannot be changed. The job must be re-created using the \u201cfinance\u201d group\u2019s credentials.</p>", "<p>Databricks Jobs UI doesn\u2019t support changing the owners of jobs. Databricks REST API needs to be used instead.</p>", "<p>Having the \u201cCAN MANAGE\u201d permission is not enough to grant \"Owner\" privileges to a group. The data engineer must be the current owner of the job.</p>", "<p>Having the \u201cCAN MANAGE\u201d permission is not enough to grant \"Owner\" privileges to a group. The data engineer must be a workspace administrator.</p>", "<p>Groups can not be owners of Databricks jobs. The owner must be an individual user.</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "The data engineering team created a new Databricks job for processing sensitive financial data. A financial analyst asked the team to transfer the \"Owner\" privilege of this job to the \u201cfinance\u201d group.A junior data engineer that has the \u201cCAN MANAGE\u201d permission on the job is attempting to make this privilege transfer via Databricks Job UI, but it keeps failing.Which of the following explains the cause of this failure?", "related_lectures": []}, {"_class": "assessment", "id": 70683114, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team noticed that a partitioned Delta Lake table is suffering greatly. They are experiencing slowdowns for most general queries on this table.</p><p><br></p><p>The team tried to run an OPTIMIZE command on the table, but this did not help to resolve the issue.</p><p><br></p><p>Which of the following likely explains the cause of these slowdowns?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Data that is over-partitioned or incorrectly partitioned will suffer greatly. Files cannot be combined or compacted across partition boundaries, so partitioned small tables increase storage costs and total number of files to scan. This leads to slowdowns for most general queries. Such an issue requires a full rewrite of all data files to remedy.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/tables/partitions.html\">https://docs.databricks.com/tables/partitions.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059604/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The table has too many old data files that need to be purged. They need to run a VACUUM command instead.</p>", "<p>The table is over-partitioned or incorrectly partitioned. This requires a full rewrite of all data files to resolve the issue.</p>", "<p>They are applying the OPTIMIZE command on the whole table. It must be applied at each partition separately.</p>", "<p>They are applying the OPTIMIZE command without ZORDER. Z-ordering is needed on the partitioning columns.</p>", "<p>The transaction log is too large. Log files older than a certain age must be deleted or archived at partition boundaries.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team noticed that a partitioned Delta Lake table is suffering greatly. They are experiencing slowdowns for most general queries on this table.The team tried to run an OPTIMIZE command on the table, but this did not help to resolve the issue.Which of the following likely explains the cause of these slowdowns?", "related_lectures": []}, {"_class": "assessment", "id": 70683116, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has the following query for processing customers\u2019 requests to be forgotten:</p><p><br></p><pre class=\"prettyprint linenums\">DELETE FROM customers\nWHERE customer_id IN\n(SELECT customer_id FROM delete_requests)</pre><p><br></p><p>Which statement describes the results of executing this query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delete requests, also known as requests to be forgotten, require deleting user data that represent Personally Identifiable Information or PII, such as the name and the email of the user.</p><p><br></p><p>Because of how Delta Lake tables time travel are implemented, deleted values are still present in older versions of the data. Remember, deleting data does not delete the data files from the table directory. Instead, it creates a copy of the affected files without these deleted records. So, to fully commit these deletes, you need to run VACUUM commands on the customers table.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/blog/2022/03/23/implementing-the-gdpr-right-to-be-forgotten-in-delta-lake.html\">https://www.databricks.com/blog/2022/03/23/implementing-the-gdpr-right-to-be-forgotten-in-delta-lake.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37085216/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The identified records will be deleted from the customers tables, and their associated data files will be permanently purged from the table directory.</p>", "<p>The identified will be deleted from the both customers and delete_requests tables, and their associated data files will be permanently purged from the tables directories.</p>", "<p>The identified records will be deleted from the customers table, but they will still be accessible in the table history until a VACUUM command is run.</p>", "<p>The identified records will be deleted from both customers and delete_requests tables, but they will still be accessible in the table history until VACUUM commands are run.</p>", "<p>The identified records will be deleted from the customers tables, but they will still be accessible in the table history until updating the status of the requests in the delete_requests table</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has the following query for processing customers\u2019 requests to be forgotten:DELETE FROM customers\nWHERE customer_id IN\n(SELECT customer_id FROM delete_requests)Which statement describes the results of executing this query ?", "related_lectures": []}, {"_class": "assessment", "id": 70683118, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following commands:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE DATABASE db_hr;\nLOCATION '/mnt/hr_external';\n\nUSE db_hr;\nCREATE TABLE employees;</pre><p><br></p><p>In which of the following locations will the employees table be located?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Since we are creating the database here with the LOCATION keyword, it will be created as an external database under the specified location \u2018/mnt/hr_external\u2019. The database folder has the extension (.db)</p><p>And since we are creating the table without specifying a location, the table becomes a managed table created under the database directory (in db_hr.db folder)</p><p><br></p><p>Reference:<a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html\"> https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>dbfs:/user/hive/warehouse</p>", "<p>dbfs:/user/hive/warehouse/db_hr.db</p>", "<p>/mnt/hr_external</p>", "<p>/mnt/hr_external/db_hr.db</p>", "<p>More information is needed to determine the correct answer</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Given the following commands:CREATE DATABASE db_hr;\nLOCATION '/mnt/hr_external';\n\nUSE db_hr;\nCREATE TABLE employees;In which of the following locations will the employees table be located?", "related_lectures": []}, {"_class": "assessment", "id": 70683120, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a secret scope named \u2018DataOps-Prod\u2019 that contains all secrets needed by DataOps engineers in a production workspace.</p><p><br></p><p>Which of the following is the minimum permission required for the DataOps engineers to use the secrets in this scope ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The secret access permissions are as follows:</p><p><br></p><ul><li><p>MANAGE - Allowed to change ACLs, and read and write to this secret scope.</p></li><li><p>WRITE - Allowed to read and write to this secret scope.</p></li><li><p>READ - Allowed to read this secret scope and list what secrets are available.</p></li></ul><p><br></p><p>Each permission level is a subset of the previous level\u2019s permissions (that is, a principal with WRITE permission for a given scope can perform all actions that require READ permission).</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/secret-acl.html#permission-levels\">https://docs.databricks.com/security/auth-authz/access-control/secret-acl.html#permission-levels</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059672/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>MANAGE permission on the \u201cDataOps-Prod\u201d scope</p>", "<p>READ permission on the \u201cDataOps-Prod\u201d scope</p>", "<p>MANAGE permission on each secret in the \u201cDataOps-Prod\u201d scope</p>", "<p>READ permission on each secret in the \u201cDataOps-Prod\u201d scope</p>", "<p>Workspace Administrator role</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team has a secret scope named \u2018DataOps-Prod\u2019 that contains all secrets needed by DataOps engineers in a production workspace.Which of the following is the minimum permission required for the DataOps engineers to use the secrets in this scope ?", "related_lectures": []}, {"_class": "assessment", "id": 70683122, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is <strong>Not</strong> part of the Ganglia UI ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Lifecycle events of the cluster are not part of Ganglia UI.</p><p><br></p><p>Ganglia allows monitoring the performance of Databricks clusters. Ganglia UI provides you with the overall workload of the cluster, in addition to detailed metrics on memory, CPI, and Network usage.</p><p><br></p><p>Lifecycle events of the cluster are part of the Cluster Event log.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/clusters/clusters-manage.html#view-cluster-logs\">https://docs.databricks.com/clusters/clusters-manage.html#view-cluster-logs</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Memory usage</p>", "<p>Overall workload of the cluster</p>", "<p>CPU usage</p>", "<p>Lifecycle events of the cluster</p>", "<p>Network performance</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which of the following is Not part of the Ganglia UI ?", "related_lectures": []}, {"_class": "assessment", "id": 70683124, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Spark UI, which of the following is <strong>Not</strong> part of the metrics displayed in a stage\u2019s details page ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Spark UI, the stage\u2019s details page shows summary metrics for completed tasks. This includes:</p><p><br></p><ul><li><p>Duration of tasks.</p></li><li><p>GC time: is the total JVM garbage collection time.</p></li><li><p>Shuffle spill (memory): is the size of the deserialized form of the shuffled data in memory.</p></li><li><p>Shuffle spill (disk): is the size of the serialized form of the data on disk.</p></li><li><p>and others \u2026</p></li></ul><p><br></p><p>DBU Cost is not part of Spark UI. DBU stands for Databricks Unit and it is a unit of processing capability per hour for pricing purposes. This depends on your cluster configuration which tells you how much DBUs would be consumed if a virtual machine runs for an hour, and then pays for each DBU consumed.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/latest/web-ui.html\">https://spark.apache.org/docs/latest/web-ui.html</a></p><p><a href=\"https://www.databricks.com/product/pricing\">https://www.databricks.com/product/pricing</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34728190/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p><p><br></p>", "answers": ["<p>Duration</p>", "<p>Spill (Memory)</p>", "<p>Spill (Disk)</p>", "<p>DBU Cost</p>", "<p>GC time</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "In Spark UI, which of the following is Not part of the metrics displayed in a stage\u2019s details page ?", "related_lectures": []}, {"_class": "assessment", "id": 70683126, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stage</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-09_23-28-50-9f5cc531327c31b3a72a6eea41c1f59e.png\"><p><br></p><p>Which conclusion can the data engineer draw from the above statistics ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Usually, if your computation was completely symmetric across tasks, you would see all of the statistics clustered tightly around the 50th percentile value.</p><p><br></p><p>Here, the \u201cMax\u201d metrics task took 10x the time and read about 5x the data of the 75th-percentile task. This suggests a number of \u201cstraggler\u201d tasks that operating over partitions with larger skewed amounts of data.</p><p><br></p>", "answers": ["<p>All task are operating over partitions with even amounts of data</p>", "<p>All task are operating over empty or near empty partitions</p>", "<p>All tasks are operating over partitions with larger skewed amounts of data.</p>", "<p>Number of tasks are operating over partitions with larger skewed amounts of data.</p>", "<p>Number of tasks are operating over empty or near empty partitions</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stageWhich conclusion can the data engineer draw from the above statistics ?", "related_lectures": []}, {"_class": "assessment", "id": 70683128, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is using Databricks REST API to send a GET request to the endpoint \u2018api/2.1/jobs/runs/get\u2019 to retrieve the run\u2019s metadata of a multi-task job using its run_id.</p><p><br></p><p>Which statement correctly describes the response structure of this API call?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Each task of this job run will have a unique run_id to retrieve its output with endpoint \u2018api/2.1/jobs/runs/get-output\u2019</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsGetOutput\">https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsGetOutput</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059662/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Each task of this job run will have a unique task_id</p>", "<p>Each task of this job run will have a unique run_id</p>", "<p>Each task of this job run will have a unique job_id</p>", "<p>Each task of this job run will have a unique orchestration_id</p>", "<p>Tasks does not any unique identifier within a job run</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer is using Databricks REST API to send a GET request to the endpoint \u2018api/2.1/jobs/runs/get\u2019 to retrieve the run\u2019s metadata of a multi-task job using its run_id.Which statement correctly describes the response structure of this API call?", "related_lectures": []}, {"_class": "assessment", "id": 70683130, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has noticed the comment \u2018# Databricks notebook source\u2019 on the first line of each Databricks Python file\u2019s source code pushed to Github.</p><p><br></p><p>Which of the following explain the purpose of this comment ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can convert Python, SQL, Scala, and R scripts to single-cell notebooks by adding a comment to the first cell of the file:</p><p><br></p><p><code># Databricks notebook source</code></p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook\">https://docs.databricks.com/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>This comment makes it easier for humans to understand the source of the generated code from Databricks</p>", "<p>This comment establishes the Python files as Databricks notebooks</p>", "<p>This comment is used for Python auto-generated documentation</p>", "<p>This comment add the Python file to the search index in Databricks workspace</p>", "<p>There is no special purpose for this comment. Comments in Python are ignored by the interpreter.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer has noticed the comment \u2018# Databricks notebook source\u2019 on the first line of each Databricks Python file\u2019s source code pushed to Github.Which of the following explain the purpose of this comment ?", "related_lectures": []}, {"_class": "assessment", "id": 70683132, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes DBFS ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dbfs/index.html\">https://docs.databricks.com/dbfs/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664668/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p>", "answers": ["<p>Database File System that organizes and maintains data files in Databricks workspace</p>", "<p>Database File System to interact with files in cloud-based object storage</p>", "<p>Abstraction on top of Databricks Lakehouse that provides an open solution to share data to any computing platform</p>", "<p>Abstraction on top of scalable object storage that maps Unix-like file system calls to native cloud storage API calls.</p>", "<p>None of the above correctly describes DBFS</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which of the following statements best describes DBFS ?", "related_lectures": []}, {"_class": "assessment", "id": 70683134, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to install a Python wheel scoped to the current notebook\u2019s session, so only the current notebook and any jobs associated with this notebook have access to that library.</p><p><br></p><p>Which of the following commands can the data engineer use to complete this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>\u2018%pip install\u2019 allows you to install a Python wheel scoped to the current notebook\u2019s session. This library will be only accessible in the current notebook and any jobs associated with this notebook.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/libraries/notebooks-python-libraries.html\">https://docs.databricks.com/libraries/notebooks-python-libraries.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>%fs install my_package.whl</p>", "<p>%pip install my_package.whl</p>", "<p>%python install my_package.whl</p>", "<p>%whl install my_package</p>", "<p>Python wheels can not be installed at the notebook level. They can only be installed at the cluster level.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer wants to install a Python wheel scoped to the current notebook\u2019s session, so only the current notebook and any jobs associated with this notebook have access to that library.Which of the following commands can the data engineer use to complete this task?", "related_lectures": []}, {"_class": "assessment", "id": 70683136, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements correctly describes the sys.path Python variable ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The sys.path variable contains a list of directories where the Python interpreter searches for modules.</p><p><br></p><p>To import modules from another directory, you must add it to sys.path</p><p><br></p><pre class=\"prettyprint linenums\">import sys\nsys.path.append(\"/path/to/dir\")</pre><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/files/workspace-modules.html#import-python-and-r-modules-1\">https://docs.databricks.com/files/workspace-modules.html#import-python-and-r-modules-1</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The sys.path variable contains a list of all the parameters passed to a Python notebook.</p>", "<p>The sys.path variable contains a list of all the necessary dependencies for a Python notebook.</p>", "<p>The sys.path variable contains a list of directories where the Python interpreter searches for modules.</p>", "<p>The sys.path variable contains the full pathname of the current working directory of a Python notebook.</p>", "<p>The sys.path variable is an alias for os.path</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Which of the following statements correctly describes the sys.path Python variable ?", "related_lectures": []}, {"_class": "assessment", "id": 70683138, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements correctly describes assertions in unit testing ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Assertions are boolean expressions that enable you to test the assumptions you have made in your code. They are used in unit tests to check if certain assumptions remain true while you're developing your code.</p><p><br></p><p><code>assert func() == expected_value</code></p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/testing.html\">https://docs.databricks.com/notebooks/testing.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37167946/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059482/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>An assertion is a boolean expression that checks if two code blocks are integrated logically and interacted as a group.</p>", "<p>An assertion is a boolean expression that checks if assumptions made in the code remain true while development</p>", "<p>An assertion is a command that logs failed units of code in production for later debugging and analysis</p>", "<p>An assertion is a command that shows the differences between the current version of a code unit and the most recently edited version</p>", "<p>An assertion is a set of actions that simulates a user experience to ensure that the application can run properly under real-world scenarios</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following statements correctly describes assertions in unit testing ?", "related_lectures": []}, {"_class": "assessment", "id": 70683140, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements correctly describes End-to-End Testing ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>End-to-end testing is an approach to ensure that your application can run properly under real-world scenarios. The goal of this testing is to simulate a real user experience from start to finish.</p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37167946/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>It\u2019s an approach to simulate a user experience to ensure that the application can run properly under real-world scenarios</p>", "<p>It\u2019s an approach to test the interaction between subsystems of an application to ensure that modules work properly as a group.</p>", "<p>It\u2019s an approach to test individual units of code to determine whether they still work as expected if new changes are made to them in the future</p>", "<p>It\u2019s an approach to verify if each feature of the application works as per the business requirements</p>", "<p>It\u2019s an approach to measure the reliability, speed, scalability, and responsiveness of an application</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following statements correctly describes End-to-End Testing ?", "related_lectures": []}, {"_class": "assessment", "id": 70683142, "assessment_type": "multiple-choice", "prompt": {"question": "<p>When running an existing job via Databricks REST API, which of the following represents the globally unique identifier of the newly triggered run ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Running an existing job via the endpoint \u2018/api/2.1/jobs/run-now\u2019 returns the run_id of the triggered run. This represents the globally unique identifier of this newly triggered run.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow\">https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059662/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>job_id</p>", "<p>run_id</p>", "<p>run_key</p>", "<p>task_id</p>", "<p>task_key</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "When running an existing job via Databricks REST API, which of the following represents the globally unique identifier of the newly triggered run ?", "related_lectures": []}, {"_class": "assessment", "id": 70683144, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following multi-task job</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-09_23-42-24-32361bde5c3d806916e9c5ebb6192352.png\"><p><br></p><p>If there is an error in the notebook 2 that is associated with Task 2, which statement describes the run result of this job ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>If a task fails during a job run, only the dependent tasks, if any, will be skipped. Parallel tasks will run and complete.</p><p><br></p><p>The failure of a task will always be partial, which means that the operations in the notebook before the code failure will be successfully run and committed, while the operations after the code failure will be skipped.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/jobs/repair-job-failures.html\">https://docs.databricks.com/workflows/jobs/repair-job-failures.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059652/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Task 1 will succeed. Task 2 will partially fail. Task 3 will be skipped</p>", "<p>Task 1 will succeed. Task 2 will completely fail. Task 3 will be skipped</p>", "<p>Tasks 1 and 3 will succeed, while Task 2 will partially fail</p>", "<p>Tasks 1 and 3 will succeed, while Task 2 will completely fail</p>", "<p>All tasks will completely fail</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Given the following multi-task jobIf there is an error in the notebook 2 that is associated with Task 2, which statement describes the run result of this job ?", "related_lectures": []}]}
5888736
~~~
{"count": 60, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70806000, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to attach a notebook to an existing cluster ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_23-50-30-247fbd870dbb25a7161ab99526437604.png\"></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>\u201cCan Attach To\u201d privilege on the cluster</p>", "<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Attach To\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to attach a notebook to an existing cluster ?", "related_lectures": []}, {"_class": "assessment", "id": 70806002, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to start an existing cluster, and attach a notebook to it?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_23-53-24-b6981dd6a88fcc046d0b1ae0a55833cf.png\"></p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>\u201cCan Attach To\u201d privilege on the cluster</p>", "<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Attach To\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to start an existing cluster, and attach a notebook to it?", "related_lectures": []}, {"_class": "assessment", "id": 70806004, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta Lake table created with following query:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE customers_clone\nAS SELECT * FROM customers</pre><p><br></p><p>A data engineer wants to drop the table with the following query:</p><p><br></p><p><code>DROP TABLE customers_clone</code></p><p><br></p><p>Which statement describes the result of running this drop command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The table is created without the LOCATION clause, which means that it\u2019s a managed table. Managed tables are tables whose metadata and data are managed by Databricks.</p><p>When you run DROP TABLE on a managed table, both the metadata and the underlying data files are deleted.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/lakehouse/data-objects.html#what-is-a-managed-table\">https://docs.databricks.com/lakehouse/data-objects.html#what-is-a-managed-table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul>", "answers": ["<p>An error will occur as the table is deep cloned from the customers table</p>", "<p>An error will occur as the table is shallow cloned from the customers table</p>", "<p>Only the table's metadata will be deleted from the catalog, while the data files will be kept in the storage</p>", "<p>Both the table's metadata and the data files will be deleted</p>", "<p>The table will not be dropped until VACUUM command is run</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "The data engineering team has a Delta Lake table created with following query:CREATE TABLE customers_clone\nAS SELECT * FROM customersA data engineer wants to drop the table with the following query:DROP TABLE customers_cloneWhich statement describes the result of running this drop command ?", "related_lectures": []}, {"_class": "assessment", "id": 70806006, "assessment_type": "multiple-choice", "prompt": {"question": "<p>For production Structured Streaming jobs, which of the following retry policies is recommended to use ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In order to restart streaming queries on failure, it\u2019s recommended to configure Structured Streaming jobs with the following job configuration:</p><p><br></p><ul><li><p>Retries: Set to Unlimited.</p></li><li><p>Maximum concurrent runs: Set to 1. There must be only one instance of each query concurrently active.</p></li><li><p>Cluster: Set this always to use a new job cluster and use the latest Spark version (or at least version 2.1). Queries started in Spark 2.1 and above are recoverable after query and Spark version upgrades.</p></li><li><p>Notifications: Set this if you want email notification on failures.</p></li><li><p>Schedule: Do not set a schedule.</p></li><li><p>Timeout: Do not set a timeout. Streaming queries run for an indefinitely long time.</p></li></ul><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/structured-streaming/query-recovery.html#configure-structured-streaming-jobs-to-restart-streaming-queries-on-failure\">https://docs.databricks.com/structured-streaming/query-recovery.html#configure-structured-streaming-jobs-to-restart-streaming-queries-on-failure</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059620/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Unlimited Retries, with 1 Maximum Concurrent Run</p>", "<p>Unlimited Retries, with Unlimited Concurrent Runs</p>", "<p>No Retries, with 1 Maximum Concurrent Run</p>", "<p>No Retries, with Unlimited Concurrent Runs</p>", "<p>1 Retry, with 1 Maximum Concurrent Run</p><p><br></p>"]}, "correct_response": ["a"], "section": "", "question_plain": "For production Structured Streaming jobs, which of the following retry policies is recommended to use ?", "related_lectures": []}, {"_class": "assessment", "id": 70806008, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a MLFlow model logged in a given \u201cmodel_url\u201d. They have registered the model as a Spark UDF using the following code:</p><p><br></p><p><code>predict_udf = mlflow.pyfunc.spark_udf(spark, \"model_url\")</code></p><p><br></p><p>The data engineer wants to apply this model UDF to a test dataset loaded in the \u201ctest_df\u201d DataFrame in order to calculate predictions in a new column \u201cprediction\u201d</p><p><br></p><p>Which of the following code blocks allows the data engineer to accomplish this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In PySpark Dataframe, you can create a new column based on function return value. This can be achieved by calling the function using either:</p><p><br></p><ul><li><p> <strong>Dataframe.withColumn</strong> method:</p></li></ul><p><code>test_df.withColumn(\"prediction\", predict_udf(*column_list))</code></p><p><br></p><ul><li><p>Or using <strong>Dataframe.select</strong> method:</p></li></ul><p><code>test_df.select( predict_udf(*column_list).allas(\"prediction\") )</code></p><p>Dataframe.select allows also to select one or more columns:</p><p><code>test_df.select(\u201crecord_id\u201d, predict_udf(*column_list).allas(\"prediction\"))</code></p><p>or it can be expanded to include all columns using \u201c*\u201d</p><p><code>test_df.select(\u201c*\u201d, predict_udf(*column_list).allas(\"prediction\"))</code></p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html\">https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html</a></p><p><a href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html\">https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html</a></p><p><br></p>", "answers": ["<p>test_df.apply(predict_udf, *column_list).select(\u201crecord_id\u201d, \u201cprediction\")</p>", "<p>test_df.select(\u201crecord_id\u201d, predict_udf(*column_list).allas(\"prediction\"))</p>", "<p>predict_udf(\u201crecord_id\u201d, test_df).select(\u201crecord_id\u201d, \u201cprediction\")</p>", "<p>mlflow.pyfunc.map(predict_udf, test_df, \u201crecord_id\u201d).allas(\"prediction\")</p>", "<p>mlflow.pyfunc.map(predict_udf, test_df, \u201crecord_id\u201d).allas(\"prediction\")</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer has a MLFlow model logged in a given \u201cmodel_url\u201d. They have registered the model as a Spark UDF using the following code:predict_udf = mlflow.pyfunc.spark_udf(spark, \"model_url\")The data engineer wants to apply this model UDF to a test dataset loaded in the \u201ctest_df\u201d DataFrame in order to calculate predictions in a new column \u201cprediction\u201dWhich of the following code blocks allows the data engineer to accomplish this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70806010, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Delta Lake tables, which of the following is the file format for the transaction log ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake builds upon standard data formats. Delta lake table gets stored on the storage in one or more data files in Parquet format, along with transaction logs in JSON format.</p><p><br></p><p>In addition, Databricks automatically creates Parquet checkpoint files every 10 commits to accelerate the resolution of the current table state.</p><p><br></p><p>Reference:</p><ul><li><p><a href=\"https://docs.databricks.com/delta/index.html\">https://docs.databricks.com/delta/index.html</a></p></li><li><p><a href=\"https://www.databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html\">https://www.databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Delta</p>", "<p>Parquet</p>", "<p>JSON</p>", "<p>Hive-specific format</p>", "<p>Both, Parquet and JSON</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "In Delta Lake tables, which of the following is the file format for the transaction log ?", "related_lectures": []}, {"_class": "assessment", "id": 70806012, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes the minimal permissions a data engineer needs to modify permissions of an existing cluster ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can configure two types of cluster permissions:</p><p><br></p><p>1- The \u2018Allow cluster creation\u2019 entitlement controls your ability to create clusters.</p><p><br></p><p>2- Cluster-level permissions control your ability to use and modify a specific cluster. There are four permission levels for a cluster: No Permissions, Can Attach To, Can Restart, and Can Manage. The table lists the abilities for each permission:</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-09_23-57-54-3200dff168162cde39efeeefb4d26e4f.png\"><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html\">https://docs.databricks.com/security/auth-authz/access-control/cluster-acl.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059720/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p>", "answers": ["<p>\u201cCan Restart\u201d privilege on the cluster</p>", "<p>\u201cCan Manage\u201d privilege on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Restart\u201d privileges on the cluster</p>", "<p>Cluster creation allowed + \u201cCan Manage\u201d privileges on the cluster</p>", "<p>Only administrators can modify permissions on existing clusters</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following describes the minimal permissions a data engineer needs to modify permissions of an existing cluster ?", "related_lectures": []}, {"_class": "assessment", "id": 70806014, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is the default target file size when compacting small files of a Delta table by manually running OPTIMIZE command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The OPTIMIZE command compact small data files into larger ones. The default value is 1073741824, which sets the size to 1 GB.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-optimize.html\">https://docs.databricks.com/sql/language-manual/delta-optimize.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37123146/?referralCode=936CBDC941031CE4D795\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>64 MB</p>", "<p>128 MB</p>", "<p>256 MB</p>", "<p>512 MB</p>", "<p>1024 MB</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "Which of the following is the default target file size when compacting small files of a Delta table by manually running OPTIMIZE command ?", "related_lectures": []}, {"_class": "assessment", "id": 70806016, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer is using the following code to de-duplicate raw streaming data and insert them in a target Delta table</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n        .table(\"orders_raw\")\n        .dropDuplicates([\"order_id\", \"order_timestamp\"])\n    .writeStream\n        .option(\"checkpointLocation\", \"dbfs:/checkpoints\")\n        .table(\"orders_unique\")</pre><p><br></p><p>A senior data engineer pointed out that this approach is not enough for having distinct records in the target table when there are late-arriving, duplicate records.</p><p><br></p><p>Which of the following could explain the senior data engineer\u2019s remark?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To perform streaming deduplication, we use dropDuplicates() function to eliminate duplicate records within each new micro batch. In addition, we need to ensure that records to be inserted are not already in the target table. We can achieve this using insert-only merge.</p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html\">https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropDuplicates.html</a></p><p><a href=\"https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables\">https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Watermarking is also needed to only track state information for a window of time in which we expect records could be delayed.</p>", "<p>A ranking function is also needed to ensure processing only the most recent records</p>", "<p>A window function is also needed to apply deduplication for each non-overlapping interval.</p>", "<p>The new records need also to be deduplicated against previously inserted data into the table.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A junior data engineer is using the following code to de-duplicate raw streaming data and insert them in a target Delta tablespark.readStream\n        .table(\"orders_raw\")\n        .dropDuplicates([\"order_id\", \"order_timestamp\"])\n    .writeStream\n        .option(\"checkpointLocation\", \"dbfs:/checkpoints\")\n        .table(\"orders_unique\")A senior data engineer pointed out that this approach is not enough for having distinct records in the target table when there are late-arriving, duplicate records.Which of the following could explain the senior data engineer\u2019s remark?", "related_lectures": []}, {"_class": "assessment", "id": 70806018, "assessment_type": "multiple-choice", "prompt": {"question": "<p>\u201cA feature built into Delta Lake that allows to automatically generate CDC feeds about Delta Lake tables\u201d</p><p><br></p><p>Which of the following is being described in the above statement? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Change Data Feed ,or CDF, is a new feature built into Delta Lake that allows it to automatically generate CDC feeds about Delta Lake tables.</p><p><br></p><p>CDF records row-level changes for all the data written into a Delta table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html\">https://docs.databricks.com/delta/delta-change-data-feed.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Auto Optimize</p>", "<p>Optimized writes</p>", "<p>Spark Watermarking</p>", "<p>Slowly Changing Dimension (SCD)</p>", "<p>Change Data Feed (CDF)</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "\u201cA feature built into Delta Lake that allows to automatically generate CDC feeds about Delta Lake tables\u201dWhich of the following is being described in the above statement?", "related_lectures": []}, {"_class": "assessment", "id": 70806020, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer uses the following SQL query:</p><p><br></p><p><code>GRANT MODIFY ON TABLE employees TO hr_team</code></p><p><br></p><p>Which of the following describes the ability given by the MODIFY privilege ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The MODIFY privilege gives the ability to add, delete, and modify data to or from an object.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658\">Hands-on</a></p></li></ul>", "answers": ["<p>It gives the ability to add data from the table</p>", "<p>It gives the ability to delete data from the table</p>", "<p>It gives the ability to modify data in the table</p>", "<p>All the above abilities are given by the MODIFY privilege</p>", "<p>None of these options correctly describe the ability given by the MODIFY privilege</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer uses the following SQL query:GRANT MODIFY ON TABLE employees TO hr_teamWhich of the following describes the ability given by the MODIFY privilege ?", "related_lectures": []}, {"_class": "assessment", "id": 70806022, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements regarding the retention policy of Delta lake CDF is correct ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks records change data for UPDATE, DELETE, and MERGE operations in the _change_data folder under the table directory.</p><p><br></p><p>The files in the _change_data folder follow the retention policy of the table. Therefore, if you run the VACUUM command, change data feed data is also deleted.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html#change-data-storage\">https://docs.databricks.com/delta/delta-change-data-feed.html#change-data-storage</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Running the VACUUM command on the table deletes CDF data as well</p>", "<p>Running the VACUUM command on the table does not deletes CDF data</p>", "<p>Running the VACUUM command on the table does not deletes CDF data unless CASCADE clause is set to true </p>", "<p>CDF data files can be purged by running VACUUM CHANGES command</p>", "<p>CDF data files can never be permanently purged from Delta Lake</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following statements regarding the retention policy of Delta lake CDF is correct ?", "related_lectures": []}, {"_class": "assessment", "id": 70806024, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.readStream\n        .table(\"orders\")\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"Output_Table\")\n)</pre><p><br></p><p>Which of the following is the trigger Interval for this query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, if you don\u2019t provide any trigger interval, the data will be processed every half second. This is equivalent to trigger(processingTime=\u201d500ms\")</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval\">https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>Every half second</p>", "<p>Every half min</p><p><br></p>", "<p>Every half hour</p>", "<p>The query will run in batch mode to process all available data at once, then the trigger stops.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Given the following Structured Streaming query:(spark.readStream\n        .table(\"orders\")\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"Output_Table\")\n)Which of the following is the trigger Interval for this query ?", "related_lectures": []}, {"_class": "assessment", "id": 70806026, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer run the following CTAS statement in a SQL notebook attached to an All-purpose cluster:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE course_students\nAS ( \tSELECT c.course_name, t.student_id, t.student_name\n        FROM courses c\n        LEFT JOIN (\n            SELECT s.student_id, s.student_name, e.course_id\n            FROM students s \n            INNER JOIN enrollments e\n            ON s.student_id = e.student_id\n        ) t\n        ON c.course_id = t.course_id\n        WHERE c.active = true\n)</pre><p><br></p><p>Which statement describes the resulting course_students table ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>CREATE TABLE AS SELECT</code> statements, or CTAS statements create new Delta tables and populate them using the output of a SELECT query. So, The query result is stored in Delta format in the directory of the newly created table.</p><p><br></p><p><br></p><p>Reference: (cf. AS query clause)</p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664704/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>It\u2019s a virtual table that has no physical data. The SELECT statement will be executed each time the course_students table is queried.</p>", "<p>It\u2019s a cluster-scoped virtual table. The SELECT statement will be executed only the first time the course_students table is queried. The query output will be stored in the memory of the currently active cluster.</p>", "<p>It\u2019s a Delta Lake table. The SELECT statement will be executed at the table creation, and its output will be stored in Delta format on the underlying storage.</p>", "<p>It\u2019s a cluster-scoped table. The SELECT statement will be executed at the table creation, but its output will be stored in the memory of the currently active cluster.</p>", "<p>It\u2019s a session-scoped table. The SELECT statement will be executed at the table creation, but its output will be stored in the cache of the current active Spark session.</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer run the following CTAS statement in a SQL notebook attached to an All-purpose cluster:CREATE TABLE course_students\nAS ( \tSELECT c.course_name, t.student_id, t.student_name\n        FROM courses c\n        LEFT JOIN (\n            SELECT s.student_id, s.student_name, e.course_id\n            FROM students s \n            INNER JOIN enrollments e\n            ON s.student_id = e.student_id\n        ) t\n        ON c.course_id = t.course_id\n        WHERE c.active = true\n)Which statement describes the resulting course_students table ?", "related_lectures": []}, {"_class": "assessment", "id": 70806028, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a streaming job that updates a Delta table named \u2018user_activities\u2019 by the results of a join between a streaming Delta table \u2018activity_logs\u2019 and a static Delta table \u2018users\u2019.</p><p><br></p><p>They noticed that adding new users into the \u2018users\u2019 table does not automatically trigger updates to the \u2018user_activities\u2019 table, even when there were activities for those users in the \u2018activity_logs\u2019 table.</p><p><br></p><p>Which of the following likely explains this issue ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In stream-static join, the streaming portion of this join drives this join process. So, only new data appearing on the streaming side of the join will trigger the processing. While, adding new records into the static table will not automatically trigger updates to the results of the stream-static join.</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/delta-lake.html#performing-stream-static-joins\">https://docs.databricks.com/structured-streaming/delta-lake.html#performing-stream-static-joins</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059536/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059566/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The users table must be refreshed with REFRESH TABLE command for each microbatch of this join</p>", "<p>This stream-static join is not stateful by default unless they set the spark configuration delta.statefulStreamStaticJoin to true.</p>", "<p>The streaming portion of this stream-static join drives the join process. Only new data appearing on the streaming side of the join will trigger the processing.</p>", "<p>The static portion of the stream-static join drives this join process only in batch mode.</p>", "<p>In Delta Lake, static tables can not be joined with streaming tables.</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer has a streaming job that updates a Delta table named \u2018user_activities\u2019 by the results of a join between a streaming Delta table \u2018activity_logs\u2019 and a static Delta table \u2018users\u2019.They noticed that adding new users into the \u2018users\u2019 table does not automatically trigger updates to the \u2018user_activities\u2019 table, even when there were activities for those users in the \u2018activity_logs\u2019 table.Which of the following likely explains this issue ?", "related_lectures": []}, {"_class": "assessment", "id": 70806030, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following query on the Delta table \u2018customers\u2019 on which Change Data Feed is enabled:</p><p><br></p><pre class=\"prettyprint linenums\">spark.read\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter (col(\"_change_type\").isin([\"update_postimage\"]))\n    .write\n        .mode(\"append\")\n        .table(\"customers_updates\")</pre><p><br></p><p>Which statement describes the result of this query each time it is executed ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Reading table\u2019s changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table\u2019s changes (starting from the specified startingVersion) will be read.</p><p><br></p><p>The query in the question then appends the data to the target table at each execution since it\u2019s using the \u2018append\u2019 writing mode.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-batch-queries\">https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-batch-queries</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Newly updated records will be merged into the target table, modifying previous entries with the same primary keys.</p>", "<p>Newly updated records will be appended to the target table.</p>", "<p>Newly updated records will overwrite the target table.</p>", "<p>The entire history of updated records will be appended to the target table at each execution, which leads to duplicate entries.</p>", "<p>The entire history of updated records will overwrite the target table at each execution.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Given the following query on the Delta table \u2018customers\u2019 on which Change Data Feed is enabled:spark.read\n        .option(\"readChangeFeed\", \"true\")\n        .option(\"startingVersion\", 0)\n        .table (\"customers\")\n        .filter (col(\"_change_type\").isin([\"update_postimage\"]))\n    .write\n        .mode(\"append\")\n        .table(\"customers_updates\")Which statement describes the result of this query each time it is executed ?", "related_lectures": []}, {"_class": "assessment", "id": 70806032, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to use Autoloader to ingest input data into a target table, and automatically evolve the schema of the table when new fields are detected.</p><p><br></p><p>They use the below query with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .option(\"cloudFiles.schemaLocation\", checkpointPath)\n        .load(source_path)\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .___________\n        .start(\"target_table\")</pre><p><br></p><p>Which option correctly fills in the blank to meet the specified requirement ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Schema evolution is a feature that allows adding new detected fields to the table. It\u2019s activated by adding .option('mergeSchema', 'true') to your .write or .writeStream Spark command.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/update-schema.html#add-columns-with-automatic-schema-update\">https://docs.databricks.com/delta/update-schema.html#add-columns-with-automatic-schema-update</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37054468/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")</p>", "<p>option(\"cloudFiles.mergeSchema\", True)</p>", "<p>option(\"mergeSchema\", True)</p>", "<p>schema(schema_definition, mergeSchema=True)</p>", "<p>Autoloader can not automatically evolve the schema of the table when new fields are detected</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer wants to use Autoloader to ingest input data into a target table, and automatically evolve the schema of the table when new fields are detected.They use the below query with a blank:spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .option(\"cloudFiles.schemaLocation\", checkpointPath)\n        .load(source_path)\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .___________\n        .start(\"target_table\")Which option correctly fills in the blank to meet the specified requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70806034, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following query:</p><p><br></p><pre class=\"prettyprint linenums\">spark.table(\"stream_sink\")\n        .filter(\"recent = true\")\n        .dropDuplicates([\"item_id\", \"item_timestamp\"])\n    .write\n        .mode (\"overwrite\")\n        .table(\"stream_data_stage\")</pre><p><br></p><p>Which statement describes the result of executing this query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Reading a Delta table using spark.table() function means that you are reading it as a static source. So, each time you run the query, all records in the current version of the \u2018stream_sink\u2019 table will be read, filtered and deduplicated.</p><p><br></p><p>There is no difference between spark.table() and spark.read.table() function. Actually, spark.read.table() internally calls spark.table().</p><p><br></p><p>The query in the question then writes the data in mode \u201coverwrite\u201d to the \u2018stream_data_stage\u2019 table, which completely overwrites the table at each execution.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html\">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>An incremental job will overwrite the stream_sink table by those deduplicated records from stream_data_stage that have been added since the last time the job was run.</p>", "<p>An incremental job will overwrite the stream_data_stage table by those deduplicated records from stream_sink that have been added since the last time the job was run.</p>", "<p>A batch job will overwrite the stream_sink table by deduplicated records calculated from all \u201crecent\u201d items in the stream_data_stage table</p>", "<p>A batch job will overwrite the stream_data_stage table by deduplicated records calculated from all \u201crecent\u201d items in the stream_sink table</p>", "<p>A batch job will overwrite the stream_data_stage table by those deduplicated records from stream_sink that have been added since the last time the job was run.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Given the following query:spark.table(\"stream_sink\")\n        .filter(\"recent = true\")\n        .dropDuplicates([\"item_id\", \"item_timestamp\"])\n    .write\n        .mode (\"overwrite\")\n        .table(\"stream_data_stage\")Which statement describes the result of executing this query ?", "related_lectures": []}, {"_class": "assessment", "id": 70806036, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta table named \u2018users\u2019. A recent CHECK constraint has been added to the table using the following command:</p><p><br></p><pre class=\"prettyprint linenums\">ALTER TABLE users\nADD CONSTRAINT valid_age CHECK (age&gt; 0);</pre><p><br></p><p>The team attempted to insert a batch of new records to the table, but there were some records with negative age values which caused the write to fail because of the constraint violation.</p><p><br></p><p>Which statement describes the outcome of this batch insert?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Write operations failed because of the constraint violation. However, ACID guarantees on Delta Lake ensure that all transactions are atomic. That is, they will either succeed or fail completely. So in this case, none of these records have been inserted into the table, even the ones that don't violate the constraints.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/acid-transactions\">https://www.databricks.com/glossary/acid-transactions</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059434/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>All records except those that violate the table constraint have been inserted in the table. Records violating the constraint have been ignored.</p>", "<p>All records except those that violate the table constraint have been inserted in the table. Records violating the constraint have been recorded into the transaction log.</p>", "<p>Only records processed before reaching the first violating record have been inserted in the table</p>", "<p>None of the records have been inserted into the table.</p>", "<p>The outcome depends on the action defined using the ON VIOLATION clause in the table properties.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "The data engineering team has a Delta table named \u2018users\u2019. A recent CHECK constraint has been added to the table using the following command:ALTER TABLE users\nADD CONSTRAINT valid_age CHECK (age&gt; 0);The team attempted to insert a batch of new records to the table, but there were some records with negative age values which caused the write to fail because of the constraint violation.Which statement describes the outcome of this batch insert?", "related_lectures": []}, {"_class": "assessment", "id": 70806038, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which statement regarding Delta Lake File Statistics is <strong>Not</strong> correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake automatically captures statistics in the transaction log for each added data file of the table. By default, Delta Lake collects the statistics on the first 32 columns of each table. Nested fields count when determining the first 32 columns</p><p><br></p><p>Example: 4 struct fields with 8 nested fields will total to the 32 columns.</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The statistics are leveraged for data skipping when executing selective queries.</p>", "<p>The statistics are generally uninformative for string fields with very high cardinality.</p>", "<p>Delta Lake captures statistics in the transaction log for each added data file</p>", "<p>By default, Delta Lake captures statistics on the first 32 columns of the table.</p>", "<p>Nested fields do not count when determining the first 32 columns in the table.</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "Which statement regarding Delta Lake File Statistics is Not correct?", "related_lectures": []}, {"_class": "assessment", "id": 70806040, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is <strong>Not</strong> a valid Delta Lake File Statistics ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake automatically captures statistics in the transaction log for each added data file of the table. These statistics indicate per file:</p><ul><li><p>Total number of records</p></li><li><p>Minimum value in each column of the first 32 columns of the table</p></li><li><p>Maximum value in each column of the first 32 columns of the table</p></li><li><p>Null value counts for in each column of the first 32 columns of the table</p></li></ul><p><br></p><p>The average value in the columns is Not part of Delta Lake File Statistics</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The total number of records in the added data file.</p>", "<p>The minimum value in each of the first 32 columns</p>", "<p>The maximum value in each of the first 32 columns</p>", "<p>The average value for each of the first 32 columns</p>", "<p>The number of null values for each of the first 32 columns</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which of the following is Not a valid Delta Lake File Statistics ?", "related_lectures": []}, {"_class": "assessment", "id": 70806042, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following definitions correctly describes a Slowly Changing Dimension of Type 0 ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Type 0 SCD tables never change. Tables of this type are usually static. For example, static lookup tables.</p><p><br></p><p>Reference:</p><p><a href=\"https://en.wikipedia.org/wiki/Slowly_changing_dimension\">https://en.wikipedia.org/wiki/Slowly_changing_dimension</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059464/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>It\u2019s a table where no changes are allowed.</p>", "<p>It\u2019s a table where history will be kept in the additional column</p>", "<p>It\u2019s a table where the new arriving data overwrites the existing one.</p>", "<p>It\u2019s a table that stores and manages both current and historical data over time.</p>", "<p>It\u2019s a table that maintains current records, while older records are stored in another table.</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following definitions correctly describes a Slowly Changing Dimension of Type 0 ?", "related_lectures": []}, {"_class": "assessment", "id": 70806044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following two versions of a Delta Lake table before and after an update:</p><p><br></p><p>Before:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-10_00-22-43-cc053810864a4a323d8a7597cb5c47c8.png\"></p><p>After:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-10_04-19-57-967c988e8453d7473f54f8f4cd470301.png\"><p><br></p><p>Which SCD Type is this table ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In a Type 2 SCD table, a new record is added with the changed data values, and this new record becomes the current active record, while the old record is marked as no longer active. So, Type 2 SCD retains the full history of values</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://en.wikipedia.org/wiki/Slowly_changing_dimension\">https://en.wikipedia.org/wiki/Slowly_changing_dimension</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059464/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059458/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>SCD Type 0</p>", "<p>SCD Type 1</p>", "<p>SCD Type 2</p>", "<p>It's a combination of Type 0 and Type 2 SCDs</p>", "<p>More information is needed to determine the SCD Type of this table</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Given the following two versions of a Delta Lake table before and after an update:Before:After:Which SCD Type is this table ?", "related_lectures": []}, {"_class": "assessment", "id": 70806046, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team maintains a Delta Lake table of SCD Type 1. A junior data engineer noticed a folder named \u2018_change_data\u2019 in the table directory, and wants to understand what this folder is used for.</p><p><br></p><p>Which of the following describes the purpose of this folder ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks records change data for UPDATE, DELETE, and MERGE operations in the _change_data folder under the table directory.</p><p><br></p><p>The files in the _change_data folder follow the retention policy of the table. Therefore, if you run the VACUUM command, change data feed data is also deleted.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/delta-change-data-feed.html#change-data-storage\">https://docs.databricks.com/delta/delta-change-data-feed.html#change-data-storage</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059502/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059508/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The \u2018_change_data\u2019 folder is a metadata directory to track any update to the table definition</p>", "<p>The \u2018_change_data\u2019 folder is the default directory to track the evolution in schema definition</p>", "<p>All SCD Type 1 tables have the \u2018_change_data\u2019 folder to track the updates applied on the table\u2019s data.</p>", "<p>Optimized Writes feature is enabled on the table. The \u2018_change_data\u2019 folder is the location where the optimized data is stored</p>", "<p>CDF feature is enabled on the table. The \u2018_change_data\u2019 folder is the location where CDF data is stored</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "The data engineering team maintains a Delta Lake table of SCD Type 1. A junior data engineer noticed a folder named \u2018_change_data\u2019 in the table directory, and wants to understand what this folder is used for.Which of the following describes the purpose of this folder ?", "related_lectures": []}, {"_class": "assessment", "id": 70806118, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has the following streaming query with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n       .table(\"orders_cleaned\")\n       .groupBy(\n           ___________________________,\n           \"author\")\n       .agg(\n           count(\"order_id\").alias(\"orders_count\"),\n           avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"orders_stats\")</pre><p><br></p><p>They want to calculate the orders count and average quantity for each non-overlapping 15-minute interval.</p><p><br></p><p>Which option correctly fills in the blank to meet this requirement ? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Pyspark.sql.functions.window function bucketizes rows into one or more time windows given a timestamp specifying column.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.window.html\">https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.window.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059586/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>trigger(processingTime=\u201d15 minutes\")</p>", "<p>window(\"order_timestamp\", \"15 minutes\")</p>", "<p>withWindow(\"order_timestamp\", \"15 minutes\")</p>", "<p>withWatermark(\"order_timestamp\", \"15 minutes\")</p>", "<p>interval(\"order_timestamp\", \"15 minutes\")</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer has the following streaming query with a blank:spark.readStream\n       .table(\"orders_cleaned\")\n       .groupBy(\n           ___________________________,\n           \"author\")\n       .agg(\n           count(\"order_id\").alias(\"orders_count\"),\n           avg(\"quantity\").alias(\"avg_quantity\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"orders_stats\")They want to calculate the orders count and average quantity for each non-overlapping 15-minute interval.Which option correctly fills in the blank to meet this requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70806048, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team is looking for a simple solution to share part of a large Delta Lake table with the data science team. Only department-specific columns in the table need to be shared, but with different names. In addition, there is some sensitive data that must be filtered out before sharing.</p><p><br></p><p>Which of the following objects can be created to meet the specified requirements ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The solution in this case is to create a view on the table where the required columns can be renamed, and the sensitive data that can be filtered out with the WHERE clause.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672154/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672162/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul><p><br></p>", "answers": ["<p>A new Delta Table created using DEEP CLONE from the existing table</p>", "<p>A new Delta Table created using SHALLOW CLONE from the existing table</p>", "<p>A new Delta Table created using CTAS statement on the existing table</p>", "<p>A stored view on the existing table</p>", "<p>An ad-hoc query on the existing table</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "The data engineering team is looking for a simple solution to share part of a large Delta Lake table with the data science team. Only department-specific columns in the table need to be shared, but with different names. In addition, there is some sensitive data that must be filtered out before sharing.Which of the following objects can be created to meet the specified requirements ?", "related_lectures": []}, {"_class": "assessment", "id": 70806050, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer has created the table \u2018orders_backup\u2019 as a copy of the table \u201corders\u201d. Recently, the team started getting an error when querying the orders_backup indicating that some data files are no longer present. The transaction logs for the orders tables show a recent run of VACUUM command.</p><p><br></p><p>Which of the following explains how the data engineer created the orders_backup table ? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With Shallow Clone, you create a copy of a table by just copying the Delta transaction logs.</p><p>That means that there is no data moving during Shallow Cloning.</p><p>Running the VACUUM command on the source table may purge data files referenced in the transaction log of the clone. In this case, you will get an error when querying the clone indicating that some data files are no longer present.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/clone.html\">https://docs.databricks.com/delta/clone.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p>", "answers": ["<p>The orders_backup table was created using CTAS statement from orders table</p>", "<p>The orders_backup table was created using CRAS statement from orders table</p>", "<p>The orders_backup table was created using Delta Lake\u2019s SHALLOW CLONE functionality from the orders table</p>", "<p>The orders_backup table was created using Delta Lake\u2019s DEEP CLONE functionality from the orders table</p>", "<p>The orders_backup table was created by fully copying the orders table\u2019s directory</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A junior data engineer has created the table \u2018orders_backup\u2019 as a copy of the table \u201corders\u201d. Recently, the team started getting an error when querying the orders_backup indicating that some data files are no longer present. The transaction logs for the orders tables show a recent run of VACUUM command.Which of the following explains how the data engineer created the orders_backup table ?", "related_lectures": []}, {"_class": "assessment", "id": 70806052, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements regarding cloning tables on Databricks is correct ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In either case, deep or shallow cloning, data modifications applied to the cloned version of the table will be tracked and stored separately from the source, so it will not affect the source table.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/clone.html#clone-types-1\">https://docs.databricks.com/delta/clone.html#clone-types-1</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p><p><br></p>", "answers": ["<p>Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.</p>", "<p>Any changes made to deep clones affect only the clones themselves and not the source table. While, changes made to shallow clones affect the source table.</p>", "<p>Any changes made to shallow clones affect only the clones themselves and not the source table. While, changes made to deep clones affect the source table.</p>", "<p>Changes made to either deep or shallow clones affect the source table.</p>", "<p>Changes made to either deep or shallow clones affect the source table unless CASCADE option is False.</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following statements regarding cloning tables on Databricks is correct ?", "related_lectures": []}, {"_class": "assessment", "id": 70806054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to optimize the following join operation by allowing the smaller dataFrame to be sent to all executor nodes in the cluster:</p><p><br></p><pre class=\"prettyprint linenums\">largeDF.join(smallerDF, [\u201ckey\u201d], \"inner\")</pre><p><br></p><p>Which of the following functions can be used to mark a dataFrame as small enough to fit in memory on all executors ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Pyspark.sql.functions.broadcast function marks a DataFrame as small enough for use in broadcast joins.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.broadcast.html\">https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.broadcast.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059482/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>pyspark.sql.functions.distribute</p>", "<p>pyspark.sql.functions.explode</p>", "<p>pyspark.sql.functions.broadcast</p>", "<p>pyspark.sql.functions.diffuse</p>", "<p>pyspark.sql.functions.shuffle</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "A data engineer wants to optimize the following join operation by allowing the smaller dataFrame to be sent to all executor nodes in the cluster:largeDF.join(smallerDF, [\u201ckey\u201d], \"inner\")Which of the following functions can be used to mark a dataFrame as small enough to fit in memory on all executors ?", "related_lectures": []}, {"_class": "assessment", "id": 70806056, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team wants to create a multiplex bronze Delta table from a Kafka source. The Delta Table has the following schema:</p><p><br></p><p>key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG</p><p><br></p><p>Since the \u201cvalue\u201d column contains Personal Identifiable Information (PII) for some topics, the team wants to apply Access Control Lists (ACLs) at partition boundaries to restrict access to this PII data.</p><p><br></p><p>Based on the above schema and the specified requirement, which column is a good candidate for partitioning ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Table partitioning helps improve security. You can separate sensitive and nonsensitive data into different partitions and apply different security controls to the sensitive data.</p><p><br></p><p>* Personally Identifiable Information or PII represents any information that allows identifying individuals by either direct or indirect means, such as the name and the email of the user.</p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059604/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>key</p>", "<p>value</p>", "<p>topic</p>", "<p>partition</p>", "<p>timestamp</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team wants to create a multiplex bronze Delta table from a Kafka source. The Delta Table has the following schema:key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONGSince the \u201cvalue\u201d column contains Personal Identifiable Information (PII) for some topics, the team wants to apply Access Control Lists (ACLs) at partition boundaries to restrict access to this PII data.Based on the above schema and the specified requirement, which column is a good candidate for partitioning ?", "related_lectures": []}, {"_class": "assessment", "id": 70806058, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team wants to know if the tables that they maintain in the Lakehouse are over-partitioned.</p><p><br></p><p>Which of the following is an indicator that a Delta Lake table is over-partitioned ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Data that is over-partitioned or incorrectly partitioned will suffer greatly. Files cannot be combined or compacted across partition boundaries, so partitioned small tables increase storage costs and total number of files to scan. This leads to slowdowns for most general queries.</p><p><br></p><p>If most partitions in a table have less than 1GB of data, the table is likely over-partitioned</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/tables/partitions.html\">https://docs.databricks.com/tables/partitions.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059604/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>If the number of partitions in the table are too low</p>", "<p>If most partitions in the table have less than 1 GB of data</p>", "<p>If most partitions in the table have more than 1 GB of data</p>", "<p>If the partitioning columns are fields of low cardinality</p>", "<p>If the data in the table continues to arrive indefinitely.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team wants to know if the tables that they maintain in the Lakehouse are over-partitioned.Which of the following is an indicator that a Delta Lake table is over-partitioned ?", "related_lectures": []}, {"_class": "assessment", "id": 70806114, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is using the following spark configurations in a pipeline to enable Optimized Writes and Auto Compaction:</p><p><br></p><pre class=\"prettyprint linenums\">spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)</pre><p><br></p><p>They also want to enable Z-order indexing with Auto Compaction to leverage data skipping on all the pipeline\u2019s tables.</p><p><br></p><p>Which of the following solutions allows the data engineer to complete this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Compaction does not support Z-Ordering as Z-Ordering is significantly more expensive than just compaction.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/delta/tune-file-size.html#when-to-run-optimize\">https://docs.databricks.com/delta/tune-file-size.html#when-to-run-optimize</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37123146/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>Use spark.conf.set(\"spark.databricks.delta.autoZorder.enabled\", True)</p>", "<p>Use spark.conf.set(\"spark.databricks.delta.autoCompact.zorder.enabled\", True)</p>", "<p>Z-order indexing with Auto Compaction can only be enabled on each table separately using:</p><p><br></p><p>ALTER TABLE table_name</p><p>SET TBLPROPERTIES (delta.autoOptimize.zorder.enabled = true)</p>", "<p>There is no need for extra configurations. Z-Ordering is enabled by default with Auto Compaction</p>", "<p>There is no way to enable Z-order indexing with Auto Compaction since it does not support Z-Ordering</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer is using the following spark configurations in a pipeline to enable Optimized Writes and Auto Compaction:spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)They also want to enable Z-order indexing with Auto Compaction to leverage data skipping on all the pipeline\u2019s tables.Which of the following solutions allows the data engineer to complete this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70806060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team maintains the following join logic between three Delta tables:</p><p><br></p><pre class=\"prettyprint linenums\">df_students =  spark.table(\"students\")\ndf_courses =  spark.table(\"courses\")\ndf_enrollments =  spark.table(\"enrollments\")\n\ndf_join_1  = (df_students.join(df_enrollments, df_students.student_id == df_enrollments.student_id)\n                        .select(df_students.student_id,\n                                df_students.student_name,\n                                df_enrollments.course_id)\n              )\n\ndf_join_2 = (df_join_1.join(df_courses, df_join_1.course_id == df_courses.course_id)\n                       .select(df_join_1.student_id,\n                               df_join_1.student_name,\n                               df_courses.course_name)\n\t    )\n\n(df_join_2.write\n.mode(\"overwrite\")\n.table(\"students_courses_details\"))</pre><p><br></p><p>Which statement describes the result of this code block each time it is executed ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The query reads three static Delta tables using spark.table() function, which means that all records in the current version of these tables will be read and considered in the join operations.</p><p><br></p><p>There is no difference between spark.table() and spark.read.table() function. Actually, spark.read.table() internally calls spark.table().</p><p><br></p><p>The pyspark.sql.DataFrame.join() function performs inner join operation by default, so the matched records will be written to the target table. In our case, the query writes the data in mode \u201coverwrite\u201d to the target table, which completely overwrites the table.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html\">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html</a></p><p><a href=\"https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\">https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html</a></p><p><br></p>", "answers": ["<p>All records in the current version of the source tables will be considered in the join operations. The matched records will overwrite the students_courses_details table.</p>", "<p>All records in the current version of the source tables will be considered in the join operations. The unmatched records will overwrite the students_courses_details table.</p>", "<p>Only newly added records to any of the source tables will be considered in the join operations. The matched records will overwrite the students_courses_details table.</p>", "<p>Only newly added records to any of the source tables will be considered in the join operations. The unmatched records will overwrite the students_courses_details table.</p>", "<p>These join operations are stateful, meaning that they will wait for unmatched records to be added to the source tables prior to calculating the results.</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "The data engineering team maintains the following join logic between three Delta tables:df_students =  spark.table(\"students\")\ndf_courses =  spark.table(\"courses\")\ndf_enrollments =  spark.table(\"enrollments\")\n\ndf_join_1  = (df_students.join(df_enrollments, df_students.student_id == df_enrollments.student_id)\n                        .select(df_students.student_id,\n                                df_students.student_name,\n                                df_enrollments.course_id)\n              )\n\ndf_join_2 = (df_join_1.join(df_courses, df_join_1.course_id == df_courses.course_id)\n                       .select(df_join_1.student_id,\n                               df_join_1.student_name,\n                               df_courses.course_name)\n\t    )\n\n(df_join_2.write\n.mode(\"overwrite\")\n.table(\"students_courses_details\"))Which statement describes the result of this code block each time it is executed ?", "related_lectures": []}, {"_class": "assessment", "id": 70806062, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a large Delta Lake table named \u2018user_posts\u2019 which is partitioned over the \u2018year\u2019 column. The table is used as an input streaming source in a streaming job. The streaming query is displayed below with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">spark.readStream\n       .table(\"user_posts\")\n       ________________\n       .groupBy(\n           \"post_category\", \"post_date\")\n       .agg(\n           count(\"psot_id\").alias(\"posts_count\"),\n           sum(\"likes\").alias(\"total_likes\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"psots_stats\")</pre><p><br></p><p>They want to remove previous 2 years data from the table without breaking the append-only requirement of streaming sources.</p><p><br></p><p>Which option correctly fills in the blank to enable stream processing from the table after deleting the partitions ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Partitioning on datetime columns can be leveraged when removing data older than a certain age from the table. For example, you can decide to delete previous years data. In this case, file deletion will be cleanly along partition boundaries.</p><p><br></p><p>However, if you are using this table as a streaming source, deleting data breaks the append-only requirement of streaming sources, which makes the table no more streamable. To avoid this, you can use the ignoreDeletes option when streaming from this table. This option enables streaming processing from Delta tables with partition deletes.</p><p><code>option(\"ignoreDeletes\", True)</code></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/structured-streaming/delta-lake.html#ignore-updates-and-deletes\">https://docs.databricks.com/structured-streaming/delta-lake.html#ignore-updates-and-deletes</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059604/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>.withWatermark(\"year\", \"INTERVAL 2 YEARS\")</p>", "<p>.window(\"year\", \"INTERVAL 2 YEARS\")</p>", "<p>.option(\u201cyear\u201d, \"ignoreDeletes\")</p>", "<p>.option(\"ignoreDeletes\", \u201cyear\u201d)</p>", "<p>.option(\"ignoreDeletes\", True)</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "The data engineering team has a large Delta Lake table named \u2018user_posts\u2019 which is partitioned over the \u2018year\u2019 column. The table is used as an input streaming source in a streaming job. The streaming query is displayed below with a blank:spark.readStream\n       .table(\"user_posts\")\n       ________________\n       .groupBy(\n           \"post_category\", \"post_date\")\n       .agg(\n           count(\"psot_id\").alias(\"posts_count\"),\n           sum(\"likes\").alias(\"total_likes\"))\n    .writeStream\n       .option(\"checkpointLocation\", \"dbfs:/path/checkpoint\")\n       .table(\"psots_stats\")They want to remove previous 2 years data from the table without breaking the append-only requirement of streaming sources.Which option correctly fills in the blank to enable stream processing from the table after deleting the partitions ?", "related_lectures": []}, {"_class": "assessment", "id": 70806064, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer created a new table along with a comment using the following query:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE payments\nCOMMENT \"This table contains sensitive information\"\nAS SELECT * FROM bank_transactions</pre><p><br></p><p>Which of the following commands allows the data engineer to review the comment of the table ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>DESCRIBE TABLE EXTENDED</code> or simply <code>DESCRIBE EXTENDED</code> allows you to show none only table\u2019s comment, but also columns\u2019 comments, and other custom table properties</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-table.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-table.html</a></p><p><br></p>", "answers": ["<p>SHOW TABLES payments</p>", "<p>SHOW TBLPROPERTIES payments</p>", "<p>SHOW COMMENTS payments</p>", "<p>DESCRIBE TABLE payments</p>", "<p>DESCRIBE EXTENDED payments</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer created a new table along with a comment using the following query:CREATE TABLE payments\nCOMMENT \"This table contains sensitive information\"\nAS SELECT * FROM bank_transactionsWhich of the following commands allows the data engineer to review the comment of the table ?", "related_lectures": []}, {"_class": "assessment", "id": 70806066, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands allows data engineers to perform an insert-only merge?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The syntax for insert-only merge:</p><p><br></p><pre class=\"prettyprint linenums\">MERGE INTO target_table\nUSING soruce_table\nON merge_condition\nWHEN NOT MATCHED\n    INSERT *</pre><p><br></p><p>You just need to specify the NOT MATCHED clause, which inserts a row when a source row does not match any target row based on the merge_condition (merge keys). Records that have the same keys as an existing record in the table will be simply ignored.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-merge-into.html\">https://docs.databricks.com/sql/language-manual/delta-merge-into.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>MERGE INTO orders</p><p>USING new_orders</p><p>ON orders.orders_id = new_orders.orders_id</p><p>WHEN MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSERT *</p>", "<p>MERGE INTO orders</p><p>USING new_orders</p><p>ON orders.orders_id = new_orders.orders_id</p><p>WHEN NOT MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSERT *</p>", "<p>MERGE INTO orders</p><p>USING new_orders</p><p>ON orders.orders_id = new_orders.orders_id</p><p>WHEN MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSERT *</p><p>WHEN NOT MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;IGNORE *</p>", "<p>MERGE INTO orders</p><p>USING new_orders</p><p>ON orders.orders_id = new_orders.orders_id</p><p>WHEN NOT MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSERT *</p><p>WHEN MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;IGNORE *</p>", "<p>MERGE INTO orders</p><p>USING new_orders</p><p>ON orders.orders_id = new_orders.orders_id</p><p>WHEN NOT MATCHED</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSERT *</p><p>ELSE IGNORE *</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following commands allows data engineers to perform an insert-only merge?", "related_lectures": []}, {"_class": "assessment", "id": 70806068, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is considered a limitation when using the MERGE INTO command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Merge operation can not be performed if multiple source rows matched and attempted to modify the same target row in the table. The result may be ambiguous as it is unclear which source row should be used to update or delete the matching target row.</p><p><br></p><p>For such an issue, you need to preprocess the source table to eliminate the possibility of multiple matches. </p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/error-messages/index.html#delta_multiple_source_row_matching_target_row_in_merge\">https://docs.databricks.com/error-messages/index.html#delta_multiple_source_row_matching_target_row_in_merge</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37177394/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>Merge can not be performed in streaming jobs unless it uses Watermarking</p>", "<p>Merge can not be performed if multiple source rows matched and attempted to modify the same target row in the table</p>", "<p>Merge can not be performed if single source row matched and attempted to modify the multiple target rows in the table</p>", "<p>Merge does not support records deletion. It supports only upsert operations.</p>", "<p>All the above are considered limitations of the MERGE INTO command</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following is considered a limitation when using the MERGE INTO command ?", "related_lectures": []}, {"_class": "assessment", "id": 70806070, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is using a foreachBatch logic to upsert data in a target Delta table.</p><p><br></p><p>The function to be called at each new microbatch processing is displayed below with a blank:</p><p><br></p><pre class=\"prettyprint linenums\">def upsert_data(microBatchDF, batch_id):\n    microBatchDF.createOrReplaceTempView(\"sales_microbatch\")\n    \n    sql_query = \"\"\"\n      MERGE INTO sales_silver a\n      USING sales_microbatch b\n      ON a.item_id=b.item_id AND a.item_timestamp=b.item_timestamp\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\"    \n   \n    ________________\n</pre><p><br></p><p>Which option correctly fills in the blank to execute the sql query in the function on a cluster with recent Databricks Runtime above 10.5 ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Usually, we use spark.sq() function to run SQL queries. However, in this particular case, the spark session can not be accessed from within the microbatch process. Instead, we can access the local spark session from the microbatch dataframe.</p><p><br></p><p>For clusters with recent Databricks Runtime version above 10.5, the syntax to access the local spark session is:</p><p>microBatchDF.sparkSession.sql(sql_query)</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.gcp.databricks.com/structured-streaming/delta-lake.html#language-python\">https://docs.gcp.databricks.com/structured-streaming/delta-lake.html#language-python</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059444/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>spark.sql(sql_query)</p>", "<p>batch_id.sql(sql_query)</p>", "<p>microBatchDF.sql(sql_query)</p>", "<p>microBatchDF.sparkSession.sql(sql_query)</p>", "<p>microBatchDF._jdf.sparkSession().sql(sql_query)</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer is using a foreachBatch logic to upsert data in a target Delta table.The function to be called at each new microbatch processing is displayed below with a blank:def upsert_data(microBatchDF, batch_id):\n    microBatchDF.createOrReplaceTempView(\"sales_microbatch\")\n    \n    sql_query = \"\"\"\n      MERGE INTO sales_silver a\n      USING sales_microbatch b\n      ON a.item_id=b.item_id AND a.item_timestamp=b.item_timestamp\n      WHEN NOT MATCHED THEN INSERT *\n    \"\"\"    \n   \n    ________________\nWhich option correctly fills in the blank to execute the sql query in the function on a cluster with recent Databricks Runtime above 10.5 ?", "related_lectures": []}, {"_class": "assessment", "id": 70806072, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has been asked to develop a nightly batch job for workforce productivity analytics. The job will process events of employees productivity of the previous day, and store the performance of each employee in the Delta table \u201cemployees_performance\u201c. The table has the following schema:</p><p><br></p><p>\"date DATE, employee_id STRING, rating DOUBLE\"</p><p><br></p><p>The data engineering team wants data to be stored in the table with the ability to compare employees' performance across time.</p><p><br></p><p>Which of the following code blocks accomplishes this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><strong>DataFrameWriter.mode</strong> defines the writing behaviour when data or table already exists.</p><p>Options include:</p><ul><li><p><code>append</code>: Append contents of the DataFrame to existing data.</p></li><li><p><code>overwrite</code>: Overwrite existing data.</p></li><li><p><code>error</code> or <code>errorifexists</code>: Throw an exception if data already exists.</p></li><li><p><code>ignore</code>: Silently ignore this operation if data already exists.</p></li></ul><p><br></p><p>This <code>errorifexists</code> or <code>error</code> is the default save mode. If the table already exists, it will throw the error message <strong>Error: pyspark.sql.utils.AnalysisException: table already exists</strong>.</p><p><br></p><p>The \"employees_performance\" table has a <strong>date</strong> column. So, in order to be able to compare employees' performance across time, each new batch of data with new date should be appended into the table using the <code>append</code> mode.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html\">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html</a></p><p><br></p>", "answers": ["<p>performance_df.write.format(\"delta\").saveAsTable(\"employees_performance\")</p>", "<p>performance_df.write.mode(\"append\").saveAsTable(\"employees_performance\")</p>", "<p>performance_df.write.mode(\"overwrite\").saveAsTable(\"employees_performance\")</p>", "<p>performance_df.write.option(\"dateFormat\", \"yyyy-MM-dd\").saveAsTable(\"employees_performance\")</p>", "<p>performance_df.write.partitionBy(\"date\").saveAsTable(\"employees_performance\")</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer has been asked to develop a nightly batch job for workforce productivity analytics. The job will process events of employees productivity of the previous day, and store the performance of each employee in the Delta table \u201cemployees_performance\u201c. The table has the following schema:\"date DATE, employee_id STRING, rating DOUBLE\"The data engineering team wants data to be stored in the table with the ability to compare employees' performance across time.Which of the following code blocks accomplishes this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70806074, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a large Delta table named \u2018user_messages\u2019 with the following schema:</p><p><br></p><p>msg_id INT, user_id INT, msg_time TIMESTAMP, msg_title STRING, msg_body STRING</p><p><br></p><p>The msg_body field represents user messages in free-form text. The table has a performance issue when it\u2019s queried with filters on this field.</p><p><br></p><p>Which of the following could explain the reason for this performance issue ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake automatically captures statistics in the transaction log for each added data file of the table. By default, Delta Lake collects the statistics on the first 32 columns of each table. However, statistics are generally uninformative for string fields with very high cardinality (such as free text fields).</p><p><br></p><p>Calculating statistics on free-form text fields (like user messages, product reviews, etc) can be time consuming. So, you need to omit these fields from statistics collection by setting them later in the schema after the first 32 columns.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/data-skipping.html\">https://docs.databricks.com/delta/data-skipping.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059610/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37067446/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The table does not leverage file skipping because it's not partitioned on the msg_body column.</p>", "<p>The table does not leverage file skipping because it's not optimized with Z-ORDER on the msg_body column.</p>", "<p>The table does not leverage file skipping because Delta Lake statistics are uninformative for string fields with very high cardinality</p>", "<p>The table does not leverage file skipping because Delta Lake statistics are only captured on the first 3 columns in a table.</p>", "<p>The table does not leverage file skipping because Delta Lake statistics are not captured on columns of type STRING</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a large Delta table named \u2018user_messages\u2019 with the following schema:msg_id INT, user_id INT, msg_time TIMESTAMP, msg_title STRING, msg_body STRINGThe msg_body field represents user messages in free-form text. The table has a performance issue when it\u2019s queried with filters on this field.Which of the following could explain the reason for this performance issue ?", "related_lectures": []}, {"_class": "assessment", "id": 70806076, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a pipeline that ingest Kafka source data into a Multiplex bronze table. This Delta table is partitioned based on the topic and month columns.&nbsp; </p><p><br></p><p>A new data engineer notices that the \u2018user_activity\u2019 topic contains Personal Identifiable Information (PII) that needs to to be deleted every two months based on the company\u2019s Service-Level Agreement (SLA).</p><p><br></p><p>Which statement describes how table partitioning can help to meet this requirement ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Partitioning on datetime columns can be leveraged when removing data older than a certain age from the table. For example, you can decide to delete previous months data. In this case, file deletion will be cleanly along partition boundaries.</p><p><br></p><p>Similarly, data could be archived and backed up at partition boundaries to a cheaper storage tier. This drives a huge savings on cloud storage.</p><p><br></p><p>Reference: <a href=\"https://delta.io/blog/2023-01-18-add-remove-partition-delta-lake/\">https://delta.io/blog/2023-01-18-add-remove-partition-delta-lake/</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059596/?referralCode=936CBDC941031CE4D795\">Lecture</a></p>", "answers": ["<p>Table partitioning allows immediate files deletion without running VACUUM command</p>", "<p>Table partitioning allows delete queries to leverage partition boundaries.</p>", "<p>Table partitioning reduces query latency when deleting large data files</p>", "<p>Table partitioning does not allow to time travel the PII data after deletion</p>", "<p>None of the above statements is correct. Table partitioning can not help to meet the specified requirement.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "The data engineering team has a pipeline that ingest Kafka source data into a Multiplex bronze table. This Delta table is partitioned based on the topic and month columns.&nbsp; A new data engineer notices that the \u2018user_activity\u2019 topic contains Personal Identifiable Information (PII) that needs to to be deleted every two months based on the company\u2019s Service-Level Agreement (SLA).Which statement describes how table partitioning can help to meet this requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70806078, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following commands:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE DATABASE db_hr;\n\nUSE db_hr;\nCREATE TABLE employees;</pre><p><br></p><p>In which of the following locations will the employees table be located?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Since we are creating the database here without specifying a location, the database will be created in the default warehouse directory under dbfs:/user/hive/warehouse. The database folder has the extension (.db)</p><p>And since we are creating the table also without specifying a location, the table becomes a managed table created under the database directory (in db_hr.db folder)</p><p><br></p><p>Reference:<a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html\"> https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul>", "answers": ["<p>dbfs:/user/hive/warehouse</p>", "<p>dbfs:/user/hive/warehouse/db_hr.db</p>", "<p>dbfs:/user/hive/warehouse/db_hr</p>", "<p>dbfs:/user/hive/databases/db_hr.db</p>", "<p>More information is needed to determine the correct answer</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Given the following commands:CREATE DATABASE db_hr;\n\nUSE db_hr;\nCREATE TABLE employees;In which of the following locations will the employees table be located?", "related_lectures": []}, {"_class": "assessment", "id": 70806116, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a dynamic view with following definition:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE VIEW students_vw AS\nSELECT * FROM students\nWHERE\n    CASE\n        WHEN is_member(\"instructors\") THEN TRUE\n        ELSE is_active IS FALSE\n    END</pre><p><br></p><p>Which statement describes the results returned by querying this view?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Only members of the instructors group will have full access to the underlying data since the WHERE condition will be True for every record. On the other hand, users that are not members of the specified group will only be able to see records of students with active status = false.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/data-governance/table-acls/object-privileges.html#dynamic-view-functions\">https://docs.databricks.com/data-governance/table-acls/object-privileges.html#dynamic-view-functions</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37134718/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Members of the instructors group will only see the records of active students. While users that are not members of the specified group will only see the records of inactive students.</p>", "<p>Members of the instructors group will only see the records of active students. While users that are not members of the specified group will see null values for the records of inactive students</p>", "<p>Only members of the instructors group will see the records of all students no matter if they are active or not. While users that are not members of the specified group will only see the records of inactive students</p>", "<p>Only members of the instructors group will see the records of all students no matter if they are active or not. While users that are not members of the specified group will see null values for the records of inactive students</p>", "<p>Only members of the instructors group will see the records of all students no matter if they are active or not. While users that are not members of the specified group will see \u201cREDACTED\u201d values for the records of inactive students</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "The data engineering team has a dynamic view with following definition:CREATE VIEW students_vw AS\nSELECT * FROM students\nWHERE\n    CASE\n        WHEN is_member(\"instructors\") THEN TRUE\n        ELSE is_active IS FALSE\n    ENDWhich statement describes the results returned by querying this view?", "related_lectures": []}, {"_class": "assessment", "id": 70806080, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands can a data engineer use to grant full permissions to the HR team on the table employees ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>ALL PRIVILEGES is used to grant full permissions on an object to a user or group of users. It is translated into all the below privileges:</p><ul><li><p>SELECT</p></li><li><p>CREATE</p></li><li><p>MODIFY</p></li><li><p>USAGE</p></li><li><p>READ_METADATA</p></li></ul><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100/?referralCode=F0FA48E9A0546C975F14\">Lecture (Associate course)</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on (Associate course)</a></p></li></ul>", "answers": ["<p>GRANT FULL PRIVILEGES ON TABLE employees TO hr_team</p>", "<p>GRANT FULL PRIVILEGES ON TABLE hr_team TO employees</p>", "<p>GRANT ALL PRIVILEGES ON TABLE employees TO hr_team</p>", "<p>GRANT ALL PRIVILEGES ON TABLE hr_team TO employees</p>", "<p>GRANT SELECT, MODIFY, CREATE, READ_METADATA ON TABLE employees TO hr_team</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Which of the following commands can a data engineer use to grant full permissions to the HR team on the table employees ?", "related_lectures": []}, {"_class": "assessment", "id": 70806082, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a secret scope named \u201cprod-scope\u201d that contains sensitive secrets in a production workspace.</p><p><br></p><p>A data engineer in the team is writing a security and compliance documentation, and wants to explain who could use the secrets in this secret scope. </p><p><br></p><p>Which of the following roles is able to use the secrets in the specified secret scope ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Administrators*, secret creators, and users granted access permission can use Databricks secrets. The secret access permissions are as follows:</p><p><br></p><ul><li><p>MANAGE - Allowed to change ACLs, and read and write to this secret scope.</p></li><li><p>WRITE - Allowed to read and write to this secret scope.</p></li><li><p>READ - Allowed to read this secret scope and list what secrets are available.</p></li></ul><p><br></p><p>Each permission level is a subset of the previous level\u2019s permissions (that is, a principal with WRITE permission for a given scope can perform all actions that require READ permission).</p><p><br></p><p>* Workspace administrators have MANAGE permissions to all secret scopes in the workspace.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/secrets/index.html\">https://docs.databricks.com/security/secrets/index.html</a></p><p><a href=\"https://docs.databricks.com/security/auth-authz/access-control/secret-acl.html#permission-levels\">https://docs.databricks.com/security/auth-authz/access-control/secret-acl.html#permission-levels</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059672/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Workspace Administrators</p>", "<p>Secret creators</p>", "<p>Users with MANAGE permission on the secret scope</p>", "<p>Users with READ permission on the secret scope</p>", "<p>All the above are able to use the secrets in the secret scope </p>"]}, "correct_response": ["e"], "section": "", "question_plain": "The data engineering team has a secret scope named \u201cprod-scope\u201d that contains sensitive secrets in a production workspace.A data engineer in the team is writing a security and compliance documentation, and wants to explain who could use the secrets in this secret scope. Which of the following roles is able to use the secrets in the specified secret scope ?", "related_lectures": []}, {"_class": "assessment", "id": 70806084, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Spark UI, which of the following SQL metrics is displayed on the query\u2019s details page?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Spark UI, the query\u2019s details page displays general information about the query execution time, its duration, the list of associated jobs, and the query execution DAG.</p><p><br></p><p>In addition, it shows SQL metrics in the block of physical operators. The SQL metrics can be useful when we want to dive into the execution details of each operator. For example, \u201cnumber of output rows\u201d can answer how many rows are output after a Filter operator, \u201cSpill size\u201d which is the number of bytes spilled to disk from memory in the operator.</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/latest/web-ui.html\">https://spark.apache.org/docs/latest/web-ui.html</a></p><p><br></p>", "answers": ["<p>Query duration</p>", "<p>Query execution time</p>", "<p>Succeeded Jobs</p>", "<p>Spill size</p>", "<p>Number of input rows</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "In Spark UI, which of the following SQL metrics is displayed on the query\u2019s details page?", "related_lectures": []}, {"_class": "assessment", "id": 70806086, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stage</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-10_00-57-46-9781e80cd919c0d32c8fd1dd84f56770.png\"><p><br></p><p>Which conclusion can the data engineer draw from the above statistics ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Usually, if your computation was completely symmetric across tasks, you would see all of the statistics clustered tightly around the 50th percentile value.</p><p><br></p><p>Here, we see the distribution is reasonable, except that we have a bunch of \u201cMin\u201d values near zero. This suggests that we have almost empty partitions.</p><p><br></p>", "answers": ["<p>All task are operating over partitions with even amounts of data</p>", "<p>All task are operating over empty or near empty partitions </p>", "<p>All tasks are operating over partitions with larger skewed amounts of data.</p>", "<p>Number of tasks are operating over partitions with larger skewed amounts of data.</p>", "<p>Number of tasks are operating over near empty partitions </p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stageWhich conclusion can the data engineer draw from the above statistics ?", "related_lectures": []}, {"_class": "assessment", "id": 70806088, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has heard recently that users who have access to Databricks Secrets could be able to display the values of secrets in notebooks.</p><p><br></p><p>Which of the following could be a workaround to print the value of a Databricks secret in plain text ?</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks redacts secret values that are read using dbutils.secrets.get(). When displayed in notebook cell output, the secret values are replaced with [REDACTED] string.</p><p><br></p><p>However, is there a workaround to print the values of Databricks secrets in plain text by Iterating through the secret and printing each character.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/security/secrets/index.html\">https://docs.databricks.com/security/secrets/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059672/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>db_password = dbutils.secrets.get(\"prod-scope\", \"db-password\")</p><p>display(db_password)</p>", "<p>db_password = dbutils.secrets.get(\"prod-scope\", \"db-password\", redacted=False)</p><p>print(db_password)</p>", "<p>db_password = dbutils.secrets.get(\"prod-scope\", \"db-password\")</p><p>print(db_password, redacted=False)</p>", "<p>db_password = dbutils.secrets.get(\"prod-scope\", \"db-password\")</p><p>for char in db_password:</p><p>&nbsp; &nbsp; &nbsp; &nbsp;print(char)</p>", "<p>There is no workaround to print secrets values in plain text in notebooks. A string \"REDACTED\" will always be displayed when trying to print out a secret value.</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "A data engineer has heard recently that users who have access to Databricks Secrets could be able to display the values of secrets in notebooks.Which of the following could be a workaround to print the value of a Databricks secret in plain text ?", "related_lectures": []}, {"_class": "assessment", "id": 70806090, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wanted to create the job \u2018process-sales\u2019 using Databricks REST API.</p><p><br></p><p>However, they sent by mistake 2 POST requests to the endpoint \u2018api/2.1/jobs/create\u2019</p><p><br></p><p>Which statement describes the result of these requests ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Sending the same job definition in multiple POST requests to the endpoint \u2018api/2.1/jobs/create\u2019 will create a new job for each request, but each job will have its own unique job_id.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsCreate\">https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsCreate</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059662/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Only the first job will be created in the workspace. The second request will fail with an error indicating that a job named \u201cprocess-sales\u201d is already created.</p>", "<p>The second job will overwrite the previous one created using the first request.</p>", "<p>2 jobs will be created in the workspace, but the second one will be renamed to \u201cprocess-sales (1)\u201d</p>", "<p>2 jobs named \u201cprocess-sales\u201d will be created in the workspace with the same job_id</p>", "<p>2 jobs named \u201cprocess-sales\u201d will be created in the workspace, but with different job_id</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "A data engineer wanted to create the job \u2018process-sales\u2019 using Databricks REST API.However, they sent by mistake 2 POST requests to the endpoint \u2018api/2.1/jobs/create\u2019Which statement describes the result of these requests ?", "related_lectures": []}, {"_class": "assessment", "id": 70806092, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to use Databricks REST API to retrieve the metadata of a job run using its run_id.</p><p><br></p><p>Which of the following REST API calls achieves this requirement ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Sending GET requests to the endpoint \u2018/api/2.1/jobs/runs/get\u2019 allows us to retrieve the metadata of a job run using its run_id.</p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsGet\">https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsGet</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059662/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Send POST request to the endpoint \u2018api/2.1/jobs/runs/get\u2019</p>", "<p>Send GET request to the endpoint \u2018api/2.1/jobs/runs/get\u2019</p>", "<p>Send POST request to the endpoint \u2018api/2.1/jobs/runs/get-output\u2019</p>", "<p>Send GET request to the endpoint \u2018api/2.1/jobs/runs/get-output\u2019</p>", "<p>Send GET request to the endpoint \u2018api/2.1/jobs/runs/get-metadata\u2019</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer wants to use Databricks REST API to retrieve the metadata of a job run using its run_id.Which of the following REST API calls achieves this requirement ?", "related_lectures": []}, {"_class": "assessment", "id": 70806094, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands prints the current working directory of a notebook in Databricks Repos ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The %sh magic command allows you to run shell code in a notebook.</p><p><br></p><p>The pwd is an acronym for print working directory.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/notebooks-code.html#mix-languages\">https://docs.databricks.com/notebooks/notebooks-code.html#mix-languages</a></p><p><a href=\"https://en.wikipedia.org/wiki/Pwd\">https://en.wikipedia.org/wiki/Pwd</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>%sh pwd</p>", "<p>print(sys.path)</p>", "<p>os.path.abspath()</p>", "<p>os.environ['PYTHONPATH']</p>", "<p>In Databricks Repos, the working directory of any notebook is \u2018/databricks/driver\u2019 </p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Which of the following commands prints the current working directory of a notebook in Databricks Repos ?", "related_lectures": []}, {"_class": "assessment", "id": 70806096, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following establishes a Python file as a notebook in Databricks ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can convert Python, SQL, Scala, and R scripts to single-cell notebooks by adding a comment to the first cell of the file:</p><p><br></p><p><code># Databricks notebook source</code></p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook\">https://docs.databricks.com/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>The magic command %databricks on the first line of the file\u2019s source code</p>", "<p>The comment \u2018# Databricks notebook source\u2019 on the first line of the file\u2019s source code</p>", "<p>The import of the dbutils.notebook module in the file\u2019s source code</p>", "<p>The creation of a spark session using SparkSession.builder.getOrCreate() in the file\u2019s source code</p>", "<p>None of the above statements is correct. Notebooks are binary files with .pydb file extension.</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following establishes a Python file as a notebook in Databricks ?", "related_lectures": []}, {"_class": "assessment", "id": 70806098, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to upload a CSV file from local system storage to the DBFS of a Databricks workspace. They have Databricks CLI already configured on the local system.</p><p><br></p><p>Which of the following Databricks CLI commands can the data engineer use to complete this task ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks CLI provides the \u2018fs\u2019 utility to interact with DBFS.</p><p><br></p><p>Usage:</p><pre class=\"prettyprint linenums\">databricks fs -h</pre><p><br></p><p>The \u2018fs cp\u2019 command allows copying files to and from DBFS. To upload a csv file from a local system to DBFS, use:</p><p><br></p><pre class=\"prettyprint linenums\">databricks fs cp C:\\source\\file.csv dbfs:/target/file.csv --overwrite</pre><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/dev-tools/cli/dbfs-cli.html\">https://docs.databricks.com/dev-tools/cli/dbfs-cli.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059672/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>workspace</p>", "<p>fs</p>", "<p>jobs</p>", "<p>configure</p>", "<p>libraries</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer wants to upload a CSV file from local system storage to the DBFS of a Databricks workspace. They have Databricks CLI already configured on the local system.Which of the following Databricks CLI commands can the data engineer use to complete this task ?", "related_lectures": []}, {"_class": "assessment", "id": 70806100, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements correctly describes Unit Testing ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Unit testing is an approach to testing units of code, such as functions. So, If you make any changes to them in the future, you can use unit tests to determine whether they still work as you expect them to.</p><p><br></p><p>Assertions are used in unit tests to check if certain assumptions remain true while you're developing your code.</p><p><br></p><p><code>assert func() == expected_value</code></p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/notebooks/testing.html\">https://docs.databricks.com/notebooks/testing.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37167946/?referralCode=936CBDC941031CE4D795\">Lecture</a></p><p><br></p>", "answers": ["<p>It\u2019s an approach to simulate a user experience to ensure that the application can run properly under real-world scenarios</p>", "<p>It\u2019s an approach to test the interaction between subsystems of an application to ensure that modules work properly as a group.</p>", "<p>It\u2019s an approach to test individual units of code to determine whether they still work as expected if new changes are made to them in the future</p>", "<p>It\u2019s an approach to verify if each feature of the application works as per the business requirements</p>", "<p>It\u2019s an approach to measure the reliability, speed, scalability, and responsiveness of an application</p>"]}, "correct_response": ["c"], "section": "", "question_plain": "Which of the following statements correctly describes Unit Testing ?", "related_lectures": []}, {"_class": "assessment", "id": 70806102, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements correctly describes Integration Testing ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Integration Testing is an approach to testing the interaction between subsystems of an application. It tests that the software modules are integrated logically and tested as a group.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://en.wikipedia.org/wiki/Integration_testing\">https://en.wikipedia.org/wiki/Integration_testing</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37167946/?referralCode=936CBDC941031CE4D795\">Lecture</a></p>", "answers": ["<p>It\u2019s an approach to simulate a user experience to ensure that the application can run properly under real-world scenarios</p>", "<p>It\u2019s an approach to test the interaction between subsystems of an application to ensure that modules work properly as a group.</p>", "<p>It\u2019s an approach to test individual units of code to determine whether they still work as expected if new changes are made to them in the future</p>", "<p>It\u2019s an approach to verify if each feature of the application works as per the business requirements</p>", "<p>It\u2019s an approach to measure the reliability, speed, scalability, and responsiveness of an application</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "Which of the following statements correctly describes Integration Testing ?", "related_lectures": []}, {"_class": "assessment", "id": 70806104, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes Cron syntax in Databricks Jobs ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To define a schedule for a Databricks job, you can either interactively specify the period and starting time, or write a Cron Syntax expression. The Cron Syntax allows to represent complex job schedule that can be defined programmatically</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-05-10_01-16-52-f611ea09a5d8c4d3155d9cbf7b2260f2.png\"><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/jobs/schedule-jobs.html\">https://docs.databricks.com/workflows/jobs/schedule-jobs.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059640/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>It\u2019s an expression to represent the maximum concurrent runs of a job</p>", "<p>It\u2019s an expression to represent the retry policy of a job</p>", "<p>It\u2019s an expression to describe the email notification events (start, success, failure)</p>", "<p>It\u2019s an expression to represent the run timeout of a job</p>", "<p>It\u2019s an expression to represent complex job schedule that can be defined programmatically</p>"]}, "correct_response": ["e"], "section": "", "question_plain": "Which of the following describes Cron syntax in Databricks Jobs ?", "related_lectures": []}, {"_class": "assessment", "id": 70806106, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a Job with multiple tasks that takes more than 2 hours to complete. In the last run, the final task unexpectedly failed.</p><p><br></p><p>Which of the following actions can the data engineer perform to complete this run while minimizing the execution time ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can repair failed multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/jobs/repair-job-failures.html\">https://docs.databricks.com/workflows/jobs/repair-job-failures.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059652/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>They can rerun this Job Run to execute all the tasks</p>", "<p>They can repair this Job Run so only the failed tasks will be re-executed</p>", "<p>They need to delete the failed Run, and start a new Run for the Job</p>", "<p>They can keep the failed Run, and simply start a new Run for the Job</p>", "<p>They can run the Job in Production mode which automatically retries execution in case of errors</p>"]}, "correct_response": ["b"], "section": "", "question_plain": "A data engineer has a Job with multiple tasks that takes more than 2 hours to complete. In the last run, the final task unexpectedly failed.Which of the following actions can the data engineer perform to complete this run while minimizing the execution time ?", "related_lectures": []}, {"_class": "assessment", "id": 70806108, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a general rule, before scheduling notebooks in production, which of the following commands should be removed from the code ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Before scheduling notebooks in production, you may need to refactor your code. As a general rule, make sure you comment out:</p><ul><li><p>Unwanted files removal or tables dropping commands added during development</p></li><li><p>Display actions or SQL queries added for debugging purposes</p></li></ul><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059620/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Magic commands</p>", "<p>Overwrite table commands</p>", "<p>Markup language commands</p>", "<p>Display commands</p>", "<p>Import commands</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "As a general rule, before scheduling notebooks in production, which of the following commands should be removed from the code ?", "related_lectures": []}, {"_class": "assessment", "id": 70806110, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes the use of Python wheels in Databricks ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Python wheel is a binary distribution format for installing custom Python code packages on Databricks Clusters.</p><p><br></p><p>A wheel is a ZIP-format archive with the .whl extension.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://packaging.python.org/en/latest/specifications/binary-distribution-format/\">https://packaging.python.org/en/latest/specifications/binary-distribution-format/</a></p><p><a href=\"https://docs.databricks.com/libraries/notebooks-python-libraries.html\">https://docs.databricks.com/libraries/notebooks-python-libraries.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37116960/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>A Python %wheel is a magic command allows to install Python packages on Databricks Clusters</p>", "<p>A Python wheel is a virtual environment for isolating the Python interpreter, libraries and modules in a notebook from other notebooks.</p>", "<p>A Python wheel is a repository for hosting, managing, and distributing Python binaries and artifacts in a Databricks workspace</p>", "<p>A Python wheel is a binary distribution format for installing custom Python code packages on Databricks Clusters</p>", "<p>A Python wheel is package installer tool alternative to \u2018pip\u2019</p>"]}, "correct_response": ["d"], "section": "", "question_plain": "Which of the following statements best describes the use of Python wheels in Databricks ?", "related_lectures": []}, {"_class": "assessment", "id": 70806112, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following multi-task job</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-05-10_01-21-43-bdcecbcc9f1194555fcc1d3924ae0940.png\"><p><br></p><p>If there is an error in the notebook 1 that is associated with Task 1, which statement describes the run result of this job ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>If a task fails during a job run, all dependent tasks will be skipped.</p><p><br></p><p>The failure of a task will always be partial, which means that the operations in the notebook before the code failure will be successfully run and committed, while the operations after the code failure will be skipped.</p><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/jobs/repair-job-failures.html\">https://docs.databricks.com/workflows/jobs/repair-job-failures.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/learn/lecture/37059652/?referralCode=936CBDC941031CE4D795\">Hands-on</a></p><p><br></p>", "answers": ["<p>Task 1 will partially fail. Tasks 2 and 3 will be skipped</p>", "<p>Task 1 will partially fail. Tasks 2 and 3 will run and succeed</p>", "<p>Task 1 will completely fail. Tasks 2 and 3 will be skipped</p>", "<p>Task 1 will completely fail. Tasks 2 and 3 will run and succeed</p>", "<p>All tasks will partially fail</p>"]}, "correct_response": ["a"], "section": "", "question_plain": "Given the following multi-task jobIf there is an error in the notebook 1 that is associated with Task 1, which statement describes the run result of this job ?", "related_lectures": []}]}
