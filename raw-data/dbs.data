4992398
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 56823040, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A gaming company maintains a staging environment for its flagship application which uses a DynamoDB table to keep track of gaming history for the players. This data needs to be kept for only a week and then it can be deleted. The IT manager has noticed that the table has several months of data in the table. The company wants to implement a cost-effective solution to keep only the latest week's data in the table.</p>\n\n<p>Which of the following solutions requires the MINIMUM development effort and on-going maintenance?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>\"Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table\"</p>\n\n<p>You can use DynamoDB Time to Live (TTL) to determine when an item is no longer needed. TTL uses a per-item timestamp to determine items for expiration. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. After you enable TTL on a table, a per-partition scanner background process automatically and continuously evaluates the expiry status of items in the table.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old\" - Although this solution is theoretically possible, but it would require a significant development effort to build and maintain a Glue job to trim the DynamoDB table. There is an additional cost of running the Glue job on a daily basis as well.</p>\n\n<p>\"Add a created_at attribute in the table and then use a cron job on EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute\" - Provisioning an EC2 instance and developing Python script involves significant development effort, maintenance and additional cost. So this option is not correct.</p>\n\n<p>\"Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute\" - Custom code needs to be written for the Lambda function to implement the functionality to trim the DynamoDB table. Lambda function execution is also charged separately. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p>\n", "answers": ["<p>Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old</p>", "<p>Add a created_at attribute in the table and then use a cron job on EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute</p>", "<p>Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute</p>", "<p>Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table</p>"]}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "A gaming company maintains a staging environment for its flagship application which uses a DynamoDB table to keep track of gaming history for the players. This data needs to be kept for only a week and then it can be deleted. The IT manager has noticed that the table has several months of data in the table. The company wants to implement a cost-effective solution to keep only the latest week's data in the table.\n\nWhich of the following solutions requires the MINIMUM development effort and on-going maintenance?", "related_lectures": []}, {"_class": "assessment", "id": 56823042, "assessment_type": "multi-select", "prompt": {"question": "<p>A health-care startup wants to leverage ElastiCache for Redis in cluster mode to enhance the performance and scalability of its existing two-tier application architecture. The ElastiCache cluster is configured to listen on port 6379. The company has hired you as an AWS Certified Database Specialist to make sure that the cache data is secure and protected from unauthorized access so that the solution is HIPAA compliant.</p>\n\n<p>Which of the following steps would address the given use-case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the ElastiCache cluster to have both in-transit as well as at-rest encryption</strong></p>\n\n<p>You can use both in-transit as well as at-rest encryption to guard against unauthorized access of your data on the server. In-transit encryption encrypts your data whenever it is moving from one place to another, such as between nodes in your cluster or between your cluster and your application. At-rest encryption encrypts your on-disk data during sync and backup operations.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p>\n\n<p><strong>Create the cluster with auth-token parameter and make sure that the parameter is included in all subsequent commands to the cluster</strong></p>\n\n<p>Redis authentication tokens enable Redis to require a token (password) before allowing clients to run commands, thereby improving data security. You can require that users enter a token on a token-protected Redis server. You also need to include it in all subsequent commands to the replication group or cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p>\n\n<p><strong>Configure the security group for the ElastiCache cluster with the required rules to allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379</strong></p>\n\n<p>You can create a VPC security group to restrict access to the cluster instances. Configure rules that only allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379. Typically the ElastiCache cluster is accessed from the web servers running on EC2 instances. You can configure the security groups like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the security group for the ElastiCache cluster with the required rules to allow outbound traffic to the cluster's clients on port 6379</strong> - As mentioned in the explanation above, you need to create a security group that allows inbound traffic from the cluster itself as well as from the cluster's clients on port 6379. Creating a security group rule that allows outbound traffic from the cluster on port 6379 is not relevant to the use-case.</p>\n\n<p><strong>Enable CloudWatch Logs to monitor the security credentials for the ElastiCache cluster</strong></p>\n\n<p><strong>Enable CloudTrail to monitor the API Calls for the ElastiCache cluster</strong></p>\n\n<p>Both these options are added as distractors since both CloudWatch Logs and CloudTrail can be used for post-facto analysis to ascertain the series of access events relevant to the cluster. These options will not prevent unauthorized access.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html</a></p>\n", "answers": ["<p>Enable CloudWatch Logs to monitor the security credentials for the ElastiCache cluster</p>", "<p>Configure the security group for the ElastiCache cluster with the required rules to allow outbound traffic to the cluster's clients on port 6379</p>", "<p>Enable CloudTrail to monitor the API Calls for the ElastiCache cluster</p>", "<p>Configure the ElastiCache cluster to have both in-transit as well as at-rest encryption</p>", "<p>Create the cluster with auth-token parameter and make sure that the parameter is included in all subsequent commands to the cluster</p>", "<p>Configure the security group for the ElastiCache cluster with the required rules to allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379</p>"]}, "correct_response": ["d", "e", "f"], "section": "Workload-Specific Database Design", "question_plain": "A health-care startup wants to leverage ElastiCache for Redis in cluster mode to enhance the performance and scalability of its existing two-tier application architecture. The ElastiCache cluster is configured to listen on port 6379. The company has hired you as an AWS Certified Database Specialist to make sure that the cache data is secure and protected from unauthorized access so that the solution is HIPAA compliant.\n\nWhich of the following steps would address the given use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of its disaster recovery testing, an IT company would like to simulate an Availability Zone failure for its RDS for MySQL Multi-AZ DB instance. The testing team wants to record how the application reacts during the DB instance failover activity. The company does not want to make any configuration or code changes for this testing.</p>\n\n<p>As a Database Specialist, which of the following solutions would you outline for the company to test this use-case in the SHORTEST time?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Initiate a manual failover of the RDS DB primary instance by using reboot with failover</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p>\n\n<p>In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ. Failover times are typically 60\u2013120 seconds. You can initiate a manual failover of the RDS DB primary instance by using reboot with failover.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the RDS SQL injection query to simulate the primary instance failure</strong> - SQL injection refers to the placement of malicious code in SQL statements via web page input. This has been added as a distractor and is not relevant to the given use-case.</p>\n\n<p><strong>Use the RDS fault injection query to simulate the primary instance failure</strong> - This is a trick option. You can test the fault tolerance of your Aurora DB cluster by using fault injection queries. Fault injection queries are issued as SQL commands to an Aurora instance.</p>\n\n<p>Exam Alert:</p>\n\n<p>Fault injection queries can be only used with Aurora DB cluster and NOT with an RDS DB cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html</a></p>\n\n<p><strong>Initiate a manual failover of the RDS DB primary instance by terminating the instance</strong> - If you terminate the primary instance then it's gone forever. As the team is just simulating an AZ failure, so the correct action is to use the \"reboot with failover\" option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html</a></p>\n", "answers": ["<p>Use the RDS SQL injection query to simulate the primary instance failure</p>", "<p>Use the RDS fault injection query to simulate the primary instance failure</p>", "<p>Initiate a manual failover of the RDS DB primary instance by using reboot with failover</p>", "<p>Initiate a manual failover of the RDS DB primary instance by terminating the instance</p>"]}, "correct_response": ["c"], "section": "Monitoring and Troubleshooting", "question_plain": "As part of its disaster recovery testing, an IT company would like to simulate an Availability Zone failure for its RDS for MySQL Multi-AZ DB instance. The testing team wants to record how the application reacts during the DB instance failover activity. The company does not want to make any configuration or code changes for this testing.\n\nAs a Database Specialist, which of the following solutions would you outline for the company to test this use-case in the SHORTEST time?", "related_lectures": []}, {"_class": "assessment", "id": 56823046, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IT company manages its technology infrastructure on the AWS Cloud. The Database administration team at the company wants to migrate its RDS MySQL database to Aurora MySQL database.</p>\n\n<p>As a Database Specialist, which of the following solutions would you suggest so that it does not require any custom development and there is minimum downtime when the company migrates the database?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Aurora Replica from the RDS MySQL DB instance and then promote the Replica into a standalone Aurora DB cluster</strong></p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.</p>\n\n<p>You can use Aurora Read Replicas to migrate from an Amazon RDS DB Instance for MySQL to Amazon Aurora. The migration process begins by creating a DB snapshot of the existing DB Instance and then using it as the basis for a fresh Aurora Read Replica. After the replica has been set up, replication is used to bring it up to date with respect to the source. Once the replication lag drops to 0, the replication is complete. At this point, you can make the Aurora Read Replica into a standalone Aurora DB cluster and point your client applications at it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q4-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/\">https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a DB snapshot of the RDS MySQL DB and then migrate this snapshot to create an Aurora MySQL DB cluster</strong> - You can migrate a DB snapshot of an Amazon RDS MySQL DB instance to create an Aurora MySQL DB cluster. The new Aurora MySQL DB cluster is populated with the data from the original Amazon RDS MySQL DB instance. However, to make sure that the entire data is written from RDS MySQL DB to Aurora MySQL DB, you will have to stop the MySQL DB and then take a snapshot. This implies there will be a downtime for the RDS MySQL DB, so this option is incorrect.</p>\n\n<p><strong>Use AWS Glue to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database</strong> - Although AWS Glue can be used to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database, however the ETL script has to be custom developed. Therefore this option is incorrect.</p>\n\n<p><strong>Create a clone of the RDS MySQL DB instance and then promote the clone into a standalone Aurora DB cluster</strong> - Using database cloning in Amazon Aurora, you can quickly and cost-effectively create clones of all of the databases within an Aurora DB cluster.</p>\n\n<p>Exam Alert:</p>\n\n<p>Database cloning is only available for Amazon Aurora DB and not for RDS DB.</p>\n\n<p>So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/\">https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/</a></p>\n", "answers": ["<p>Create a DB snapshot of the RDS MySQL DB and then migrate this snapshot to create an Aurora MySQL DB cluster</p>", "<p>Use AWS Glue to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database</p>", "<p>Create a clone of the RDS MySQL DB instance and then promote the clone into a standalone Aurora DB cluster</p>", "<p>Create an Aurora Replica from the RDS MySQL DB instance and then promote the Replica into a standalone Aurora DB cluster</p>"]}, "correct_response": ["d"], "section": "Deployment and Migration", "question_plain": "An IT company manages its technology infrastructure on the AWS Cloud. The Database administration team at the company wants to migrate its RDS MySQL database to Aurora MySQL database.\n\nAs a Database Specialist, which of the following solutions would you suggest so that it does not require any custom development and there is minimum downtime when the company migrates the database?", "related_lectures": []}, {"_class": "assessment", "id": 56823048, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company uses separate AWS accounts for their business units. The finance team has an encrypted snapshot of an Amazon Relational Database Service (Amazon RDS) instance that uses the default AWS Key Management Service (AWS KMS) key. The finance team wants to share the encrypted snapshot with their Audit team that uses another AWS account.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to address the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the target account to a customer managed key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</strong> - You can't share a snapshot that's encrypted using the default AWS KMS encryption key.</p>\n\n<p>To share an encrypted Amazon RDS DB snapshot:</p>\n\n<p>Add the target account to a custom (non-default) KMS key.</p>\n\n<p>Copy the snapshot using the customer managed key, and then share the snapshot with the target account.</p>\n\n<p>Copy the shared DB snapshot from the target account.</p>\n\n<p>Detailed steps to implement this solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to a customer managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a></p>\n", "answers": ["<p>Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to a customer managed key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to a customer managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to the default AWS KMS key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>"]}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "A multi-national retail company uses separate AWS accounts for their business units. The finance team has an encrypted snapshot of an Amazon Relational Database Service (Amazon RDS) instance that uses the default AWS Key Management Service (AWS KMS) key. The finance team wants to share the encrypted snapshot with their Audit team that uses another AWS account.\n\nAs a Database Specialist, which of the following solutions would you recommend to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823050, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A gaming company is using RDS MySQL DB to re-engineer its flagship application. The development team needs to restore its MySQL database whenever a developer makes a mistake in the schema updates. As a result, the entire development team needs to wait hours for the DB restore to complete. The issue is further compounded as multiple team members are collaborating on the same project, thereby making it hard to find the right restore point for each mistake.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to reduce the downtime during the development phase?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up multiple read replicas and then the developers can make changes to their own promoted replica instances</strong></p>\n\n<p>Read replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. When you create a read replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica whenever there is a change to the primary DB instance.</p>\n\n<p>For the given use-case, each developer can perform DDL operations for the schema changes on the MySQL read replica once the read replica is in sync with its primary DB instance. Then the developer can promote the read replica and direct the application to use the promoted instance during the development phase. This solution isolates the schema changes done by each developer to their own promoted instance. This also avoids the problem of keeping track of the \"correct restore point\" that the team faced while using the same DB instance.</p>\n\n<p>If you need to make changes to the MySQL or MariaDB read replica, you must set the read_only parameter to 0 in the DB parameter group for the read replica. You can then perform all needed DDL operations, such as creating indexes, on the read replica. Actions taken on the read replica don't affect the performance of the primary DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate to Amazon Aurora MySQL and enable the Aurora Backtrack feature</strong> - Although you could migrate to Aurora MySQL DB cluster which supports the Backtrack feature, however the problem of keeping track of the \"correct checkpoint\" would persist if the team continues using the same DB instance. As mentioned above, the right solution is to have the developers set up their own replica instances for the development phase.</p>\n\n<p><strong>Enable the RDS for MySQL Backtrack feature and then the developers can backtrack to a pre-defined checkpoint</strong></p>\n\n<p><strong>Enable the RDS for MySQL Clone feature and then the developers can make changes to their own cloned instances</strong></p>\n\n<p>Exam Alert:</p>\n\n<p>The Backtrack and Clone features are only supported by Aurora DB clusters and NOT by the RDS DB clusters.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n", "answers": ["<p>Set up multiple read replicas and then the developers can make changes to their own promoted replica instances</p>", "<p>Enable the RDS for MySQL Backtrack feature and then the developers can backtrack to a pre-defined checkpoint</p>", "<p>Enable the RDS for MySQL Clone feature and then the developers can make changes to their own cloned instances</p>", "<p>Migrate to Amazon Aurora MySQL and enable the Aurora Backtrack feature</p>"]}, "correct_response": ["a"], "section": "Workload-Specific Database Design", "question_plain": "A gaming company is using RDS MySQL DB to re-engineer its flagship application. The development team needs to restore its MySQL database whenever a developer makes a mistake in the schema updates. As a result, the entire development team needs to wait hours for the DB restore to complete. The issue is further compounded as multiple team members are collaborating on the same project, thereby making it hard to find the right restore point for each mistake.\n\nAs a Database Specialist, which of the following solutions would you recommend to reduce the downtime during the development phase?", "related_lectures": []}, {"_class": "assessment", "id": 56823052, "assessment_type": "multi-select", "prompt": {"question": "<p>A Silicon Valley based unicorn startup is moving its IT operations from an on-premises data center to AWS Cloud. The database team at the startup is evaluating the Multi-AZ and Read Replica capabilities of RDS MySQL vs Aurora MySQL before they implement the solution in their production environment. The startup has hired you as an AWS Certified Database Specialist to provide a detailed report on this technical requirement.</p>\n\n<p>Which of the following would you identify as correct regarding the given use-case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Multi-AZ deployments for RDS MySQL follow synchronous replication whereas Multi-AZ deployments for Aurora MySQL follow asynchronous replication</strong></p>\n\n<p><strong>Read Replicas can be manually promoted to a standalone database instance for RDS MySQL whereas Read Replicas for Aurora MySQL can be promoted to the primary instance</strong></p>\n\n<p><strong>The primary and standby DB instances are upgraded at the same time for RDS MySQL Multi-AZ. All instances are upgraded at the same time for Aurora MySQL</strong></p>\n\n<p>RDS Read replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. When you create a read replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica whenever there is a change to the primary DB instance.</p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster.</p>\n\n<p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p>\n\n<p>To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance. There is a brief interruption during which read and write requests made to the primary instance fail with an exception, and the Aurora Replicas are rebooted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>For RDS MySQL in Multi-AZ configuration, database engine version upgrades happen on both the primary and standby DB instances at the same time. For Aurora MySQL, all instances are upgraded at the same time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q7-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Database engine version upgrades happen on primary for Aurora MySQL whereas all instances are updated together for RDS MySQL</strong></p>\n\n<p><strong>Read Replicas can be manually promoted to a standalone database instance for Aurora MySQL whereas Read Replicas for RDS MySQL can be promoted to the primary instance</strong></p>\n\n<p><strong>Multi-AZ deployments for Aurora MySQL follow synchronous replication whereas Multi-AZ deployments for RDS MySQL follow asynchronous replication</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html</a></p>\n", "answers": ["<p>Multi-AZ deployments for RDS MySQL follow synchronous replication whereas Multi-AZ deployments for Aurora MySQL follow asynchronous replication</p>", "<p>Read Replicas can be manually promoted to a standalone database instance for RDS MySQL whereas Read Replicas for Aurora MySQL can be promoted to the primary instance</p>", "<p>The primary and standby DB instances are upgraded at the same time for RDS MySQL Multi-AZ. All instances are upgraded at the same time for Aurora MySQL</p>", "<p>Database engine version upgrades happen on primary for Aurora MySQL whereas all instances are updated together for RDS MySQL</p>", "<p>Read Replicas can be manually promoted to a standalone database instance for Aurora MySQL whereas Read Replicas for RDS MySQL can be promoted to the primary instance</p>", "<p>Multi-AZ deployments for Aurora MySQL follow synchronous replication whereas Multi-AZ deployments for RDS MySQL follow asynchronous replication</p>"]}, "correct_response": ["a", "b", "c"], "section": "Workload-Specific Database Design", "question_plain": "A Silicon Valley based unicorn startup is moving its IT operations from an on-premises data center to AWS Cloud. The database team at the startup is evaluating the Multi-AZ and Read Replica capabilities of RDS MySQL vs Aurora MySQL before they implement the solution in their production environment. The startup has hired you as an AWS Certified Database Specialist to provide a detailed report on this technical requirement.\n\nWhich of the following would you identify as correct regarding the given use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired an AWS Certified Database Specialist to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and the Database Specialist must validate that the data was migrated accurately from the source to the target before the cutover.</p>\n\n<p>Which of the following solutions will MOST effectively address this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</strong></p>\n\n<p>You can use AWS DMS data validation to ensure that your data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches. In addition, for a CDC-enabled task, AWS DMS compares the incremental changes and reports any mismatches. As part of data validation, DMS compares each row in the source with its corresponding row at the target and verifies that those rows contain the same data. For this comparison, DMS issues appropriate queries to retrieve the data. These queries consume additional resources at the source and the target as well as additional network resources.</p>\n\n<p>DMS data validation overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</strong> - You can use table metrics to capture statistics such as insert, update, delete, and DDL statements completed for the tables being migrated. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q8-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n\n<p><strong>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</strong> - A premigration assessment evaluates specified components of a database migration task to help identify any problems that might prevent a migration task from running as expected. This assessment gives you a chance to identify issues before you run a new or modified task. You can then fix problems before they occur while running the migration task itself. This can avoid delays in completing a given database migration needed to repair data and your database environment. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><strong>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</strong> - You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. You can convert relational OLTP schema or data warehouse schema. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n", "answers": ["<p>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</p>", "<p>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</p>", "<p>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</p>", "<p>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</p>"]}, "correct_response": ["b"], "section": "Monitoring and Troubleshooting", "question_plain": "A multi-national retail company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired an AWS Certified Database Specialist to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and the Database Specialist must validate that the data was migrated accurately from the source to the target before the cutover.\n\nWhich of the following solutions will MOST effectively address this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823056, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A social media company is setting up an Aurora DB cluster with one primary instance and four Aurora Replicas for its flagship application. The Aurora DB cluster has one medium-sized primary instance, one large-sized Replica, one small-sized Replica and two medium-sized Replicas. The database administrator has not assigned a promotion tier to the Replicas.</p>\n\n<p>In case the primary instance fails, which of the following events will occur?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The large-sized Aurora Replica will be promoted</strong></p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance as well as to the Aurora Replicas in the DB cluster.</p>\n\n<p>To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance. There is a brief interruption during which read and write requests made to the primary instance fail with an exception, and the Aurora Replicas are rebooted. If your Aurora DB cluster doesn't include any Aurora Replicas, then your DB cluster will be unavailable for the duration it takes your DB instance to recover from the failure event.</p>\n\n<p>If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>None of the Replicas will be promoted and Aurora will spin up a new primary instance</strong></p>\n\n<p><strong>One of the medium-sized Aurora Replicas will be chosen randomly for promotion</strong></p>\n\n<p><strong>The small-sized Aurora Replica will be promoted</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance</a></p>\n", "answers": ["<p>None of the Replicas will be promoted and Aurora will spin up a new primary instance</p>", "<p>One of the medium-sized Aurora Replicas will be chosen randomly for promotion</p>", "<p>The small-sized Aurora Replica will be promoted</p>", "<p>The large-sized Aurora Replica will be promoted</p>"]}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "A social media company is setting up an Aurora DB cluster with one primary instance and four Aurora Replicas for its flagship application. The Aurora DB cluster has one medium-sized primary instance, one large-sized Replica, one small-sized Replica and two medium-sized Replicas. The database administrator has not assigned a promotion tier to the Replicas.\n\nIn case the primary instance fails, which of the following events will occur?", "related_lectures": []}, {"_class": "assessment", "id": 56823058, "assessment_type": "multi-select", "prompt": {"question": "<p>A Database Specialist is working on modeling data for a DynamoDB production database. To address certain access patterns for the application, he is in the process of creating secondary indexes.</p>\n\n<p>Which of the following options should the database specialist consider for these requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</strong> - Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.</p>\n\n<p><strong>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</strong> - For a table write to succeed, the provisioned throughput settings for the table and all of its global secondary indexes must have enough write capacity to accommodate the write. Otherwise, the write to the main table itself is throttled.</p>\n\n<p><strong>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</strong> - To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A Local Secondary Index maintains an alternate primary key for a given partition key value</strong> - This is an invalid statement. A local secondary index maintains an alternate sort key for a given partition key value. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key.</p>\n\n<p><strong>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</strong> - This statement is incorrect. Global secondary indexes support only eventually consistent data model.</p>\n\n<p><strong>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</strong> - This statement is not correct. Each table in DynamoDB can have up to 20 global secondary indexes (default quota) and 5 local secondary indexes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html</a></p>\n", "answers": ["<p>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</p>", "<p>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</p>", "<p>A Local Secondary Index maintains an alternate primary key for a given partition key value</p>", "<p>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</p>", "<p>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</p>", "<p>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</p>"]}, "correct_response": ["a", "b", "d"], "section": "Workload-Specific Database Design", "question_plain": "A Database Specialist is working on modeling data for a DynamoDB production database. To address certain access patterns for the application, he is in the process of creating secondary indexes.\n\nWhich of the following options should the database specialist consider for these requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An ed-tech company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired an AWS Certified Database Specialist to deploy a solution to create test databases quickly with the LEAST required effort for database administration.</p>\n\n<p>What should the Database Specialist do to address this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use database cloning to create multiple clones of the production DB and use each clone as a test DB</strong></p>\n\n<p>You can quickly create clones of an Aurora DB by using the database cloning feature. In addition, database cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on the source database or the clone database. Cloning is much faster than a manual snapshot of the DB cluster.</p>\n\n<p>For the given use-case, the most optimal solution is to clone the DB cluster. This would allow the performance testing team to have quick access to the production data in an isolated way. The team can iterate over the various test phases by deleting existing test databases and then cloning the production DB to create new test DBs.</p>\n\n<p>You cannot clone databases across AWS regions. The clone databases must be created in the same region as the source databases. Currently, you are limited to 15 clones based on a copy, including clones based on other clones. After that, only copies can be created. However, each copy can also have up to 15 clones.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable database Backtracking on the production DB and let the testing team use the production DB</strong> - Using Backtracking, you can \"rewind\" the DB cluster to any time you specify. One of the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring a DB cluster via point in time restore (PITR) or via a manual DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.</p>\n\n<p>However, as the given use-case is around pre-release testing, it does not make sense to use production DB itself for testing even if backtracking is enabled. The right solution is to use clones of the production DB for testing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q11-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n\n<p><strong>Take a backup of the Aurora MySQL DB instance using the mysqldump utility, create multiple new test DB instances and restore each test DB from the backup</strong> - As the use-case mandates the least effort for database administration, therefore this option is not correct since using the mysqldump utility requires several manual steps to take a backup of a DB and restore into another DB.</p>\n\n<p><strong>Create additional Read Replicas of the Aurora MySQL production DB and use the Read Replicas for testing by promoting each Replica to be its own independent standalone instance</strong> - This option has been added as a distractor as you cannot promote an Aurora Read Replica to a standalone DB instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n", "answers": ["<p>Use database cloning to create multiple clones of the production DB and use each clone as a test DB</p>", "<p>Enable database Backtracking on the production DB and let the testing team use the production DB</p>", "<p>Take a backup of the Aurora MySQL DB instance using the mysqldump utility, create multiple new test DB instances and restore each test DB from the backup</p>", "<p>Create additional Read Replicas of the Aurora MySQL production DB and use the Read Replicas for testing by promoting each Replica to be its own independent standalone instance</p>"]}, "correct_response": ["a"], "section": "Management and Operations", "question_plain": "An ed-tech company is doing pre-launch testing for its new product. The company runs its production database on an Aurora MySQL DB cluster and the performance testing team wants access to multiple test databases that must be re-created from production data. The company has hired an AWS Certified Database Specialist to deploy a solution to create test databases quickly with the LEAST required effort for database administration.\n\nWhat should the Database Specialist do to address this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823062, "assessment_type": "multi-select", "prompt": {"question": "<p>The database administration team at an e-commerce company is doing a root-cause analysis for a recent spike in CPU utilization for an RDS MySQL DB instance that caused the application to perform poorly. The standard metrics available in Amazon CloudWatch are not enough to guide the investigation. The company has hired you as an AWS Certified Database Specialist to determine what caused the CPU spike.</p>\n\n<p>Which of the following steps would you recommend to provide more details about the underlying processes and queries resulting in an increase in the CPU load? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon RDS. You can monitor network throughput, client connections, I/O for read, write, or metadata operations, and burst credit balances for your DB instances. AWS provides various tools that you can use to monitor Amazon RDS.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q12-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><strong>Activate Enhanced Monitoring to view CPU utilization information for the RDS MySQL DB instance</strong></p>\n\n<p>You can use Enhanced Monitoring to capture metrics in real-time for the operating system (OS) that your DB instance runs on. Enhanced Monitoring metrics are useful for the given use-case as it allows you to see how the different processes or threads on a DB instance use the CPU. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q12-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html</a></p>\n\n<p><strong>Enable RDS Performance Insights and review the appropriate dashboard to visualize the database load and filter the load by waits, SQL statements, hosts, or users</strong></p>\n\n<p>Performance Insights allows you to understand your database's performance and help you analyze any issues that affect it. With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users.</p>\n\n<p>You can use Performance Insights for the given use-case as it allows you to see the underlying SQL statements causing an increase in the database load.</p>\n\n<p>The central metric for Performance Insights is DB Load, which represents the average number of active sessions for the DB engine. The DB Load metric is collected every second. An active session is a connection that has submitted work to the DB engine and is waiting for a response. For example, if you submit a SQL query to the DB engine, the database session is active while the DB engine is processing the query.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q12-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Implement ElastiCache in front of the RDS DB instance to reduce the CPU load on the RDS instance</strong> - You can apply a band-aid on the underlying issue by using ElastiCache as it can potentially decrease the CPU load on the RDS DB instance, however it will present no insights about the underlying processes and queries resulting in an increase in the CPU load.</p>\n\n<p><strong>Use Amazon Athena to analyze the SQL statements being run on the RDS instance</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Recently Athena has introduced federated queries that can be used to quickly analyze records from different sources such as RDS MySQL. However, you still cannot use it to analyze the SQL statements being run on the RDS instance.</p>\n\n<p><strong>Activate Amazon CloudWatch Events and review the event data that has the SQL statements behind the CPU spikes</strong> - This is a made-up option as CloudWatch Events do not contain SQL statements in the event data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a></p>\n", "answers": ["<p>Implement ElastiCache in front of the RDS DB instance to reduce the CPU load on the RDS instance</p>", "<p>Activate Enhanced Monitoring to view CPU utilization information for the RDS MySQL DB instance</p>", "<p>Enable RDS Performance Insights and review the appropriate dashboard to visualize the database load and filter the load by waits, SQL statements, hosts, or users</p>", "<p>Use Amazon Athena to analyze the SQL statements being run on the RDS instance</p>", "<p>Activate Amazon CloudWatch Events and review the event data that has the SQL statements behind the CPU spikes</p>"]}, "correct_response": ["b", "c"], "section": "Monitoring and Troubleshooting", "question_plain": "The database administration team at an e-commerce company is doing a root-cause analysis for a recent spike in CPU utilization for an RDS MySQL DB instance that caused the application to perform poorly. The standard metrics available in Amazon CloudWatch are not enough to guide the investigation. The company has hired you as an AWS Certified Database Specialist to determine what caused the CPU spike.\n\nWhich of the following steps would you recommend to provide more details about the underlying processes and queries resulting in an increase in the CPU load? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56823064, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company is using an Aurora DB cluster with Read Replicas for specialized read-only workloads. The database administration team at the company needs to spin up three read-only reporting applications for the Sales, HR and Finance departments at the company. Each application needs to connect to its own set of dedicated Read Replicas. The company has hired you as an AWS Certified Database Specialist to implement a load balanced and highly available solution for this reporting application.</p>\n\n<p>Which of the following options would you recommend for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create three custom endpoints and use one custom endpoint for each of the read-only reporting applications</strong></p>\n\n<p>A custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster.</p>\n\n<p>For the given use-case, you can create three custom endpoints, so that you have three groups of specialized DB instances. This way, Aurora can perform load balancing among all the instances dedicated to each of the three reporting applications. The custom endpoints provide load balancing and high availability for each group of DB instances within your cluster. Because the connection can go to any DB instance that is associated with the custom endpoint, AWS recommends that you make sure that all the DB instances within that group share some similar characteristics. Doing so ensures that the performance, memory capacity, and so on, are consistent for everyone who connects to that endpoint.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create three reader endpoints and use one reader endpoint for each of the read-only reporting applications</strong> - Each Aurora cluster has a single built-in reader endpoint, whose name and other attributes are managed by Aurora. You can't create, delete, or modify this kind of endpoint. So this option is incorrect.</p>\n\n<p><strong>Use one reader endpoint for all three read-only reporting applications</strong> - This does not fit the requirements for the given use-case. Each read-only reporting application must connect to its own set of dedicated Read Replicas. With a single reader endpoint, all three read-only reporting applications would connect to the same set dedicated Read Replicas.</p>\n\n<p><strong>Use one custom endpoint for all three read-only reporting applications</strong> - This does not fit the requirements for the given use-case. Each read-only reporting application must connect to its own set of dedicated Read Replicas. With a single custom endpoint, all three read-only reporting applications would connect to the same set dedicated Read Replicas.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n", "answers": ["<p>Use one reader endpoint for all three read-only reporting applications</p>", "<p>Use one custom endpoint for all three read-only reporting applications</p>", "<p>Create three custom endpoints and use one custom endpoint for each of the read-only reporting applications</p>", "<p>Create three reader endpoints and use one reader endpoint for each of the read-only reporting applications</p>"]}, "correct_response": ["c"], "section": "Workload-Specific Database Design", "question_plain": "An analytics company is using an Aurora DB cluster with Read Replicas for specialized read-only workloads. The database administration team at the company needs to spin up three read-only reporting applications for the Sales, HR and Finance departments at the company. Each application needs to connect to its own set of dedicated Read Replicas. The company has hired you as an AWS Certified Database Specialist to implement a load balanced and highly available solution for this reporting application.\n\nWhich of the following options would you recommend for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823066, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A health-care company has just migrated its IT infrastructure from the on-premises data-center to AWS Cloud. The company is using Aurora MySQL DB cluster as its data store and the HIPAA compliance guidelines mandate that the company must produce audit logs from the production Amazon Aurora MySQL cluster and push the encrypted logs into a third-party compliance management application. The solution should allow the security team to carry out real-time alerting and monitoring outside the Aurora DB cluster.</p>\n\n<p>As a Database Specialist, which approach will you recommend to meet these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Activate database activity streams and then push the streams into a compliance management application via Kinesis Data Streams</strong></p>\n\n<p>Aurora Database activity streams provide a near real-time data stream of the database activity for an Aurora DB cluster. The stream of database activity is pushed from Aurora DB to an Amazon Kinesis Data Stream which can be further consumed by an external compliance management application such as IBM's Security Guardium, McAfee's Data Center Security Suite, and Imperva's SecureSphere Database Audit and Protection. These applications can then use the activity streams to generate alerts and provide auditing of all activity on your Aurora DB clusters. Database activity streams require the use of AWS KMS because the activity streams are always encrypted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html</a></p>\n\n<p>Here is a useful summary of the Logging and Monitoring capabilities supported by Amazon Aurora:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q14-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to capture the audit logs for the DB cluster and the security team can ingest the data from Amazon S3 into the compliance management application</strong></p>\n\n<p>CloudTrail provides a record of actions taken by a user, role, or an AWS service in Amazon Aurora. CloudTrail captures all API calls for Amazon Aurora as events, including calls from the console and code calls to Amazon RDS API operations. Using the information collected by CloudTrail, you can determine the request that was made to Amazon Aurora, the IP address from which the request was made, who made the request, when it was made, and additional details.</p>\n\n<p>CloudTrail cannot be used for real-time alerting and monitoring as it's not a real-time streaming solution. Additionally, CloudTrail cannot deliver the audit logs into an external compliance management application.</p>\n\n<p><strong>Enable Aurora Enhanced Monitoring and then push the metrics into a compliance management application via AWS Lambda</strong> - Using Enhanced Monitoring, Amazon Aurora provides metrics in real-time for the operating system (OS) that your DB cluster runs on. This feature does not facilitate audit logs for real-time alerting and monitoring.</p>\n\n<p><strong>Activate Aurora Event Notifications and then push the notifications into a compliance management application via Kinesis Data Streams</strong> - Amazon RDS groups events into categories that you can subscribe to so that you can be notified when an event in that category occurs. You can subscribe to an event category for a DB instance, DB cluster, DB cluster snapshot, DB parameter group, or DB security group. For example, if you subscribe to the Backup category for a given DB instance, you are notified whenever a backup-related event occurs that affects the DB instance.</p>\n\n<p>This feature does not facilitate audit logs for real-time alerting and monitoring.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html</a></p>\n", "answers": ["<p>Enable Aurora Enhanced Monitoring and then push the metrics into a compliance management application via AWS Lambda</p>", "<p>Activate Aurora Event Notifications and then push the notifications into a compliance management application via Kinesis Data Streams</p>", "<p>Activate database activity streams and then push the streams into a compliance management application via Kinesis Data Streams</p>", "<p>Set up AWS CloudTrail to capture the audit logs for the DB cluster and the security team can ingest the data from Amazon S3 into the compliance management application</p>"]}, "correct_response": ["c"], "section": "Workload-Specific Database Design", "question_plain": "A health-care company has just migrated its IT infrastructure from the on-premises data-center to AWS Cloud. The company is using Aurora MySQL DB cluster as its data store and the HIPAA compliance guidelines mandate that the company must produce audit logs from the production Amazon Aurora MySQL cluster and push the encrypted logs into a third-party compliance management application. The solution should allow the security team to carry out real-time alerting and monitoring outside the Aurora DB cluster.\n\nAs a Database Specialist, which approach will you recommend to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 56823068, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Database Specialist is working on the throughput capacity of a newly provisioned table in Amazon DynamoDB. The database specialist has provisioned 20 Read Capacity Units for the table.</p>\n\n<p>Which of the following options represents the correct throughput that the table will support for the various read modes?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>A read capacity unit (RCU) represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Transactional read requests require two read capacity units to perform one read per second for items up to 4 KB.</p>\n\n<p><strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 40KB/sec</strong></p>\n\n<p>Strongly consistent reads = 20 * 4 = 80KB/sec</p>\n\n<p>Eventually consistent reads = 20 * 4 * 2 = 160KB/sec</p>\n\n<p>Transactional read requests = (20 * 4)/2 = 40KB/sec</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 120KB/sec</strong></p>\n\n<p><strong>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 320KB/sec</strong></p>\n\n<p><strong>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 60KB/sec</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</a></p>\n", "answers": ["<p>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 120KB/sec</p>", "<p>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 40KB/sec</p>", "<p>Read throughput of 80KB/sec with strong consistency, Read throughput of 160KB/sec with eventual consistency, Transactional read throughput of 320KB/sec</p>", "<p>Read throughput of 40KB/sec with strong consistency, Read throughput of 80KB/sec with eventual consistency, Transactional read throughput of 60KB/sec</p>"]}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "A Database Specialist is working on the throughput capacity of a newly provisioned table in Amazon DynamoDB. The database specialist has provisioned 20 Read Capacity Units for the table.\n\nWhich of the following options represents the correct throughput that the table will support for the various read modes?", "related_lectures": []}, {"_class": "assessment", "id": 56823070, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The engineering team at a Big Data Analytics company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. The company has hired you as an AWS Certified Database Specialist to build a serverless solution that involves the least amount of development effort so that is cost-effective and easy to maintain.</p>\n\n<p>Which of the following solutions would you build for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Athena to run SQL based analytics against S3 data</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>AWS Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into DynamoDB on an hourly basis and run the SQL based sanity checks</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Loading the incremental data into DynamoDB implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n", "answers": ["<p>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</p>", "<p>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</p>", "<p>Load the incremental raw zone data into DynamoDB on an hourly basis and run the SQL based sanity checks</p>", "<p>Use Athena to run SQL based analytics against S3 data</p>"]}, "correct_response": ["d"], "section": "Workload-Specific Database Design", "question_plain": "The engineering team at a Big Data Analytics company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL based data sanity checks on the raw zone of the data lake. The company has hired you as an AWS Certified Database Specialist to build a serverless solution that involves the least amount of development effort so that is cost-effective and easy to maintain.\n\nWhich of the following solutions would you build for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823072, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The database administration team at a leading social media company uses Amazon MySQL RDS DB cluster because it simplifies much of the time-consuming administrative tasks typically associated with databases. The team uses Multi-Availability Zone (Multi-AZ) deployment to further automate its database replication and augment data durability. The current cluster configuration also uses Read Replicas. An intern has joined the team and wants to understand the replication capabilities for Multi-AZ as well as Read Replicas for the given RDS cluster.</p>\n\n<p>As a Database Specialist, which of the following capabilities would you identify as correct for the given database?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.</p>\n\n<p>Amazon RDS replicates all databases in the source DB instance. Read Replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison for Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q17-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect Options:</p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n\n<p>These three options contradict the earlier details provided in the explanation. Hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n", "answers": ["<p>Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>", "<p>Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>", "<p>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>", "<p>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read Replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</p>"]}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "The database administration team at a leading social media company uses Amazon MySQL RDS DB cluster because it simplifies much of the time-consuming administrative tasks typically associated with databases. The team uses Multi-Availability Zone (Multi-AZ) deployment to further automate its database replication and augment data durability. The current cluster configuration also uses Read Replicas. An intern has joined the team and wants to understand the replication capabilities for Multi-AZ as well as Read Replicas for the given RDS cluster.\n\nAs a Database Specialist, which of the following capabilities would you identify as correct for the given database?", "related_lectures": []}, {"_class": "assessment", "id": 56823074, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company uses Amazon Aurora DB cluster as its primary database service. The company has now deployed 5 multi-AZ Read Replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding sizes are given in parentheses: tier-1 (8TB), tier-1 (16TB), tier-10 (16TB), tier-15 (8TB), tier-15 (16TB).</p>\n\n<p>In the event of a failover, Amazon Aurora will promote which of the following Read Replicas?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Tier-1 (16TB)</strong></p>\n\n<p>Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency Read Replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).</p>\n\n<p>For Amazon Aurora, each Read Replica is associated with a priority tier (0-15).  In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon Aurora promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Amazon Aurora promotes an arbitrary replica in the same promotion tier.</p>\n\n<p>Therefore, for this problem statement, the Tier-1 (16TB) replica will be promoted.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Tier-15 (16TB)</strong></p>\n\n<p><strong>Tier-1 (8TB)</strong></p>\n\n<p><strong>Tier-10 (16TB)</strong></p>\n\n<p>Given the failover rules described earlier in the explanation, these three options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance</a></p>\n", "answers": ["<p>Tier-15 (16TB)</p>", "<p>Tier-1 (8TB)</p>", "<p>Tier-10 (16TB)</p>", "<p>Tier-1 (16TB)</p>"]}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "A multi-national retail company uses Amazon Aurora DB cluster as its primary database service. The company has now deployed 5 multi-AZ Read Replicas to increase the read throughput and for use as failover target. The replicas have been assigned the following failover priority tiers and corresponding sizes are given in parentheses: tier-1 (8TB), tier-1 (16TB), tier-10 (16TB), tier-15 (8TB), tier-15 (16TB).\n\nIn the event of a failover, Amazon Aurora will promote which of the following Read Replicas?", "related_lectures": []}, {"_class": "assessment", "id": 56823076, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company is looking to migrate its RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster. The company wants the fastest possible solution with minimum downtime and least effort for the migration of its current database which is 2TB in size.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Aurora PostgreSQL Read Replica of the PostgreSQL DB instance and migrate data to the Aurora PostgreSQL DB cluster using this Read Replica. Promote the replica when the replication lag is zero</strong></p>\n\n<p>AWS recommends using an Aurora Read Replica when you need to migrate from a PostgreSQL DB instance to an Aurora PostgreSQL DB cluster. When the replica lag between the PostgreSQL DB instance and the Aurora PostgreSQL Read Replica is zero, you can stop replication. At this point, you can promote the Aurora Read Replica to be a standalone Aurora PostgreSQL DB cluster. This standalone DB cluster can then accept write loads.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Database Migration Service (AWS DMS) to migrate the RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster</strong> - The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. Although it is possible to use DMS to migrate an RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster, however this process involves more configuration and data transfer steps compared to just directly using the Aurora Read Replica for the migration.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q19-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html</a></p>\n\n<p><strong>Create a DB snapshot for the Aurora PostgreSQL DB instance and then migrate data directly from this DB snapshot to the Aurora PostgreSQL DB cluster</strong> - It is possible to create an Aurora PostgreSQL DB cluster using a DB snapshot of an RDS PostgreSQL DB instance. The new Aurora PostgreSQL DB cluster is populated with the data from the original RDS PostgreSQL DB instance. However, the time it takes to create a DB snapshot varies with the size of the database. Additionally, since the snapshot includes the entire storage volume, the size of files, such as temporary files, also affects the amount of time it takes to create the snapshot. As mentioned earlier, AWS recommends using an Aurora Read Replica when you need to migrate from a PostgreSQL DB instance to an Aurora PostgreSQL DB cluster.</p>\n\n<p><strong>Use AWS Glue to create an ETL job for the data migration from RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster</strong> - This option has been added as a distractor. Although AWS Glue can create serverless ETL jobs, however this process involves significant development effort compared to just using Aurora Read Replicas to facilitate the migration.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html</a></p>\n", "answers": ["<p>Use AWS Database Migration Service (AWS DMS) to migrate the RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster</p>", "<p>Create a DB snapshot for the Aurora PostgreSQL DB instance and then migrate data directly from this DB snapshot to the Aurora PostgreSQL DB cluster</p>", "<p>Create an Aurora PostgreSQL Read Replica of the PostgreSQL DB instance and migrate data to the Aurora PostgreSQL DB cluster using this Read Replica. Promote the replica when the replication lag is zero</p>", "<p>Use AWS Glue to create an ETL job for the data migration from RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster</p>"]}, "correct_response": ["c"], "section": "Deployment and Migration", "question_plain": "An analytics company is looking to migrate its RDS PostGreSQL DB instance to Aurora PostGreSQL DB cluster. The company wants the fastest possible solution with minimum downtime and least effort for the migration of its current database which is 2TB in size.\n\nAs a Database Specialist, which of the following solutions would you recommend for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823078, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A database administrator is provisioning a DynamoDB table for an e-commerce application. The development team has given a requirement of 500 Write Capacity Units and 5000 Read Capacity Units for this DynamoDB table. The database administrator is planning to allocate 50GB of space to this table.</p>\n\n<p>How many partitions will be created in the table for this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>[ 50GB/10GB ] = 5, (500/1000) + (5000/3000) = 2.1 ~ 3, Maximum (5,3) = 5 partitions</strong></p>\n\n<p>DynamoDB supports access patterns using the throughput that you provisioned, as long as the traffic against a given partition does not exceed 3,000 RCUs or 1,000 WCUs.</p>\n\n<p>So, partitions required to support throughput = Roundup[(500WCU/1000WCU) + (5000RCU/3000RCU)] = 3 partitions</p>\n\n<p>10GB is the maximum supported size of a partition, so to support the given size requirements, you will need -</p>\n\n<p>50GB/10GB = 5 partitions</p>\n\n<p>Total number of partitions = Max(5, 3) = 5 partitions</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>8 partitions</strong></p>\n\n<p><strong>6 partitions</strong></p>\n\n<p><strong>3 partitions</strong></p>\n\n<p>These three options contradict the details mentioned in the explanation above, hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html</a></p>\n", "answers": ["<p>8 partitions</p>", "<p>6 partitions</p>", "<p>5 partitions</p>", "<p>3 partitions</p>"]}, "correct_response": ["c"], "section": "Management and Operations", "question_plain": "A database administrator is provisioning a DynamoDB table for an e-commerce application. The development team has given a requirement of 500 Write Capacity Units and 5000 Read Capacity Units for this DynamoDB table. The database administrator is planning to allocate 50GB of space to this table.\n\nHow many partitions will be created in the table for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 56823080, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company recently migrated its IT infrastructure from the on-premises data center to AWS Cloud. After a security review by an external auditor, the CTO at the company has mandated that the RDS for PostGreSQL DB cluster should be encrypted at rest using AWS KMS. Due to the high volume of data, it is not possible to develop a custom Extract Transform Load (ETL) solution to reload the data into another encrypted database because of the time constraints involved in developing, testing and deploying the ETL solution.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to address the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a snapshot of the unencrypted RDS DB instance. Create an encrypted snapshot via an encrypted copy of the unencrypted snapshot. Restore this new encrypted snapshot into a new DB instance</strong></p>\n\n<p>To encrypt an unencrypted DB instance, you can encrypt an unencrypted snapshot that you take from the DB instance. Restore a new RDS DB instance from the encrypted snapshot to deploy a new encrypted DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot of the unencrypted RDS DB instance. Create an encrypted snapshot via an encrypted copy of the unencrypted snapshot. Restore this new encrypted snapshot into the existing DB instance</strong> - When you restore data from a DB snapshot, you must create a new DB instance. You can't restore data from a DB snapshot (whether encrypted or unencrypted) to an existing DB instance. So this option is not correct.</p>\n\n<p><strong>Use AWS CLI to activate the AWS KMS encryption setting on the existing RDS DB instance</strong> - This is a made-up option as you cannot change the encryption status of an existing RDS DB instance.</p>\n\n<p><strong>Create an encrypted Read Replica of the RDS DB instance and then promote the Read Replica to its own standalone DB instance</strong> - You cannot create an encrypted Read Replica of an unencrypted RDS DB instance.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Limitations</a></p>\n", "answers": ["<p>Create a snapshot of the unencrypted RDS DB instance. Create an encrypted snapshot via an encrypted copy of the unencrypted snapshot. Restore this new encrypted snapshot into the existing DB instance</p>", "<p>Use AWS CLI to activate the AWS KMS encryption setting on the existing RDS DB instance</p>", "<p>Create an encrypted Read Replica of the RDS DB instance and then promote the Read Replica to its own standalone DB instance</p>", "<p>Create a snapshot of the unencrypted RDS DB instance. Create an encrypted snapshot via an encrypted copy of the unencrypted snapshot. Restore this new encrypted snapshot into a new DB instance</p>"]}, "correct_response": ["d"], "section": "Database Security", "question_plain": "A retail company recently migrated its IT infrastructure from the on-premises data center to AWS Cloud. After a security review by an external auditor, the CTO at the company has mandated that the RDS for PostGreSQL DB cluster should be encrypted at rest using AWS KMS. Due to the high volume of data, it is not possible to develop a custom Extract Transform Load (ETL) solution to reload the data into another encrypted database because of the time constraints involved in developing, testing and deploying the ETL solution.\n\nAs a Database Specialist, which of the following solutions would you recommend to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823082, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global CRM company uses Amazon RDS for MySQL DB cluster for its flagship application but it is running into performance issues despite using Read Replicas. The company has hired you as an AWS Certified Database Specialist to address these performance-related challenges on an urgent basis without moving away from the underlying relational database schema. The company has branch offices across the world and it needs the solution to work on a global scale.</p>\n\n<p>Which of the following will you recommend as the MOST cost-effective and high-performance solution?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Aurora Global Database to enable fast local reads with low latency in each region</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>Amazon Aurora Global Database Features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Global Tables builds upon DynamoDB\u2019s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. Given that the use-case wants you to continue with the underlying schema of the relational database, DynamoDB is not the right choice as it's a NoSQL database.</p>\n\n<p>DynamoDB Global Tables Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><strong>Spin up a Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. Redshift is not suited to be used as a transactional relational database, so this option is not correct.</p>\n\n<p><strong>Spin up EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases</strong> - Setting up EC2 instances in multiple regions with manually managed MySQL databases represents a maintenance nightmare and is not the correct choice for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n", "answers": ["<p>Use Amazon Aurora Global Database to enable fast local reads with low latency in each region</p>", "<p>Spin up EC2 instances in each AWS region, install MySQL databases and migrate the existing data into these new databases</p>", "<p>Use Amazon DynamoDB Global Tables to provide fast, local, read and write performance in each region</p>", "<p>Spin up a Redshift cluster in each AWS region. Migrate the existing data into Redshift clusters</p>"]}, "correct_response": ["a"], "section": "Deployment and Migration", "question_plain": "A global CRM company uses Amazon RDS for MySQL DB cluster for its flagship application but it is running into performance issues despite using Read Replicas. The company has hired you as an AWS Certified Database Specialist to address these performance-related challenges on an urgent basis without moving away from the underlying relational database schema. The company has branch offices across the world and it needs the solution to work on a global scale.\n\nWhich of the following will you recommend as the MOST cost-effective and high-performance solution?", "related_lectures": []}, {"_class": "assessment", "id": 56823084, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global e-commerce company has just migrated its technology infrastructure from the on-premises data center to AWS Cloud. The database management team at the e-commerce company wants to deploy an Online Transactional Processing (OLTP) application with support for relational queries which will have unpredictable spikes in the usage pattern that they do not know in advance.</p>\n\n<p>As a Database Specialist, which of the following database solutions would you recommend for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Aurora Serverless</strong></p>\n\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database.</p>\n\n<p>Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option.</p>\n\n<p>Use-cases for Aurora Serverless:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB with Provisioned Capacity and Auto Scaling</strong></p>\n\n<p><strong>DynamoDB with On-Demand Capacity</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect.</p>\n\n<p><strong>Aurora Global Database</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. This is not the correct choice for the given use-case because the usage pattern is unpredictable and even in case of very low traffic, the company would still need to pay for provisioning Aurora Global Database across multiple AWS Regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n", "answers": ["<p>Aurora Serverless</p>", "<p>Aurora Global Database</p>", "<p>DynamoDB with Provisioned Capacity and Auto Scaling</p>", "<p>DynamoDB with On-Demand Capacity</p>"]}, "correct_response": ["a"], "section": "Workload-Specific Database Design", "question_plain": "A global e-commerce company has just migrated its technology infrastructure from the on-premises data center to AWS Cloud. The database management team at the e-commerce company wants to deploy an Online Transactional Processing (OLTP) application with support for relational queries which will have unpredictable spikes in the usage pattern that they do not know in advance.\n\nAs a Database Specialist, which of the following database solutions would you recommend for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823086, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IT company is using AWS DMS to migrate its Amazon RDS for Oracle DB instance configured in a VPC in the us-east-1 Region to another VPC in the us-west-1 Region.</p>\n\n<p>As a Database Specialist, where would you place the DMS replication instance for the MOST optimal performance?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the replication instance in the same Availability Zone and VPC as the target DB instance</strong></p>\n\n<p>AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. AWS DMS is a server in the AWS Cloud that runs replication software. You create a source and target connection to tell AWS DMS where to extract from and load to. Then you schedule a task that runs on this server to move your data. AWS DMS creates the tables and associated primary keys if they don't exist on the target.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p>\n\n<p>While migrating an Amazon Relational Database Service (Amazon RDS) for Oracle source database to a different AWS Region, AWS suggests that you create the replication instance in the VPC of the target AWS Region. This is sufficient to get your solution working. However, to further optimize your solution, you should consider creating your replication instance in the same Availability Zone and VPC as the target DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the replication instance in the same Region and VPC as the source DB instance</strong></p>\n\n<p><strong>Create the replication instance in the same Region and VPC as the target DB instance</strong></p>\n\n<p><strong>Create the replication instance in the same Availability Zone and VPC as the source DB instance</strong></p>\n\n<p>These three options contradict the details mentioned in the explanation above, hence these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-amazon-rds-for-oracle-database-to-another-aws-account-and-aws-region-using-aws-dms-for-ongoing-replication.html</a></p>\n", "answers": ["<p>Create the replication instance in the same Region and VPC as the target DB instance</p>", "<p>Create the replication instance in the same Region and VPC as the source DB instance</p>", "<p>Create the replication instance in the same Availability Zone and VPC as the target DB instance</p>", "<p>Create the replication instance in the same Availability Zone and VPC as the source DB instance</p>"]}, "correct_response": ["c"], "section": "Deployment and Migration", "question_plain": "An IT company is using AWS DMS to migrate its Amazon RDS for Oracle DB instance configured in a VPC in the us-east-1 Region to another VPC in the us-west-1 Region.\n\nAs a Database Specialist, where would you place the DMS replication instance for the MOST optimal performance?", "related_lectures": []}, {"_class": "assessment", "id": 56822958, "assessment_type": "multi-select", "prompt": {"question": "<p>The database management team at a leading gaming company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\"></p>\n\n<p><strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n", "answers": ["<p>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</p>"]}, "correct_response": ["b", "c"], "section": "Workload-Specific Database Design", "question_plain": "The database management team at a leading gaming company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of users working out together virtually from the comfort of their home.\n\nAs a Database Specialist, which of the following solutions would you recommend? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56822960, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The database administration team at a gaming company is planning to create a Read Replica of an existing Amazon RDS for PostGreSQL Multi-AZ DB instance. However, while using the AWS Management Console to set up the Read Replica, the Team Lead discovers that the source RDS DB instance does not show in the Read Replica source field.</p>\n\n<p>As a Database Specialist, which of the following would you recommend as a solution for this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>\"Enable automatic backups on the source RDS DB instance\"</p>\n\n<p>Read Replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the Read Replica. When you create a Read Replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the Read Replica whenever there is a change to the primary DB instance.</p>\n\n<p>To create a Read Replica for a source DB instance, you must enable automatic backups on the source DB instance by setting the backup retention period to a value other than 0. This requirement also applies to a Read Replica that is the source DB instance for another Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Enable Enhanced Monitoring on the source RDS DB instance\" - You can use Enhanced Monitoring to capture metrics in real-time for the operating system (OS) that your DB instance runs on. It allows you to see how the different processes or threads on a DB instance use the CPU. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics. Enabling Enhanced Monitoring is not a pre-condition for creating Read Replicas for an RDS DB instance.</p>\n\n<p>\"Enable database cloning on the source RDS DB instance\" - Using database cloning in Amazon Aurora, you can quickly and cost-effectively create clones of all of the databases within an Aurora DB cluster.</p>\n\n<p>Exam Alert:</p>\n\n<p>Database cloning is only available for Amazon Aurora DB and not for RDS DB.</p>\n\n<p>You should also note that cloning is different from creating Read Replicas. This option is not correct.</p>\n\n<p>\"Enable database replication on the source RDS DB instance\" - You do have Read Replicas for an RDS DB instance, but there is no such thing as a \"database replication\" feature for an RDS DB instance. This is a made-up option that has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n", "answers": ["<p>Enable Enhanced Monitoring on the source RDS DB instance</p>", "<p>Enable automatic backups on the source RDS DB instance</p>", "<p>Enable database cloning on the source RDS DB instance</p>", "<p>Enable database replication on the source RDS DB instance</p>"]}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "The database administration team at a gaming company is planning to create a Read Replica of an existing Amazon RDS for PostGreSQL Multi-AZ DB instance. However, while using the AWS Management Console to set up the Read Replica, the Team Lead discovers that the source RDS DB instance does not show in the Read Replica source field.\n\nAs a Database Specialist, which of the following would you recommend as a solution for this issue?", "related_lectures": []}, {"_class": "assessment", "id": 56822962, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The database administration team at a major health-care company uses Multi-Availability Zone (Multi-AZ) deployment for its RDS MySQL DB cluster to automate its database replication and augment data durability. The database administration team has scheduled a maintenance window for a database engine level upgrade for the next weekend.</p>\n\n<p>As an AWS Certified Database Specialist, which of the following would you identify as the correct outcome during the maintenance window?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.</p>\n\n<p>Upgrades to the database engine level require downtime. Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your DB instance.</p>\n\n<p>RDS DB Engine Maintenance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n", "answers": ["<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</p>", "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</p>", "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</p>", "<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</p>"]}, "correct_response": ["d"], "section": "Deployment and Migration", "question_plain": "The database administration team at a major health-care company uses Multi-Availability Zone (Multi-AZ) deployment for its RDS MySQL DB cluster to automate its database replication and augment data durability. The database administration team has scheduled a maintenance window for a database engine level upgrade for the next weekend.\n\nAs an AWS Certified Database Specialist, which of the following would you identify as the correct outcome during the maintenance window?", "related_lectures": []}, {"_class": "assessment", "id": 56822964, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global CRM company has recently migrated its technology infrastructure from its on-premises data center to AWS Cloud. The database administration team has provisioned an RDS PostGreSQL DB cluster for the company's flagship CRM application. An analytics workload also runs on the same database which publishes near real-time reports for the senior management of the company. When the analytics workload runs, it slows down the CRM application as well, resulting in bad user experience.</p>\n\n<p>As a Database Specialist, which of the following would you recommend as the MOST cost-optimal solution to fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a Read Replica in the same Region as the Master database and point the analytics workload there</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Creating a Read Replica within the same Region is the correct answer. As we want to minimize the costs, we need to launch the Read Replica in the same Region, because we have to pay for inter-Region data transfer, whereas the transfer of data within a single Region is free.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-28-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison for Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-28-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.</p>\n\n<p><strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Running the application on AWS Lambda will not help, as it will still run against the main database and slow down the application.</p>\n\n<p><strong>For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there</strong> - This option is not correct because we have to pay for inter-Region data transfer for the Read Replica, whereas the transfer of data within a single Region is free. Disaster Recovery is not within the ambit of the requirements mentioned for the given-use. The correct solution needs to optimize on cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n", "answers": ["<p>Create a Read Replica in the same Region as the Master database and point the analytics workload there</p>", "<p>Migrate the analytics application to AWS Lambda</p>", "<p>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</p>", "<p>For Disaster Recovery purposes, create a Read Replica in another Region as the Master database and point the analytics workload there</p>"]}, "correct_response": ["a"], "section": "Workload-Specific Database Design", "question_plain": "A global CRM company has recently migrated its technology infrastructure from its on-premises data center to AWS Cloud. The database administration team has provisioned an RDS PostGreSQL DB cluster for the company's flagship CRM application. An analytics workload also runs on the same database which publishes near real-time reports for the senior management of the company. When the analytics workload runs, it slows down the CRM application as well, resulting in bad user experience.\n\nAs a Database Specialist, which of the following would you recommend as the MOST cost-optimal solution to fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 56822966, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of a new release, the DevOps team at an IT company executed a database migration script that ended up breaking the application because the script had a bug that deleted several records from one of the main tables. An AWS Certified Database Specialist was brought in to assess the event that happened about 8 hours ago. The Specialist noted that the Aurora DB cluster had the \"Backtrack\" feature enabled with a 24 hour backtrack window. In addition, the DevOps team had also taken a manual DB cluster snapshot as well as cloned the DB cluster before the release.</p>\n\n<p>Which of the following solutions can be used to return the affected database to the correct state within the LEAST amount of time and MINIMUM data loss?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the cloned DB cluster with the Backtrack feature enabled and then rewind the cloned cluster to a point in time just before the release. Return the affected database to the correct state by copying the deleted records from the cloned database to the original database</strong></p>\n\n<p>Using Backtracking, you can \"rewind\" the DB cluster to any time you specify.</p>\n\n<p>One of the major advantages of backtracking is that it can rewind the DB cluster much faster compared to restoring a DB cluster via point in time restore (PITR) or via a manual DB cluster snapshot, which can take hours. Backtracking a DB cluster doesn't require a new DB cluster and rewinds the DB cluster in minutes.</p>\n\n<p>You should remember that backtracking affects the entire DB cluster and you can't selectively backtrack a single table. Also, you can't backtrack a database clone to a time before that database clone was created.</p>\n\n<p>For the given use-case, the most optimal solution is to use the cloned DB cluster with the Backtrack feature enabled. This would allow you to just rewind the affected table in the cloned DB cluster to a state prior to deleting the records and then copy those records into the table in the original DB cluster.</p>\n\n<p>In addition, database cloning uses a copy-on-write protocol, in which data is copied only at the time the data changes, either on the source database or the clone database. Cloning is faster than a manual snapshot of the DB cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q29-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q29-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Backtrack the DB cluster to a point in time just before the release</strong> - As mentioned in the explanation above, you cannot selectively backtrack a single table. Backtracking the original DB cluster in place would affect other tables as well. So this option is ruled out.</p>\n\n<p><strong>Restore the DB cluster using the point in time recovery method to a new DB cluster. Return the affected database to the correct state by copying the deleted rows from the recovered database to the original database</strong></p>\n\n<p><strong>Restore the DB cluster using the pre-release manual snapshot to a new DB cluster and update the application configuration to use the new DB cluster</strong></p>\n\n<p>Per the explanation above, we know that using backtracking is a more optimal solution as it can rewind the DB cluster in minutes as compared to restoring a DB cluster via point in time restore (PITR) or via a manual DB cluster snapshot which can take hours. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.Backtrack.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/\">https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n", "answers": ["<p>Use the cloned DB cluster with the Backtrack feature enabled and then rewind the cloned cluster to a point in time just before the release. Return the affected database to the correct state by copying the deleted records from the cloned database to the original database</p>", "<p>Backtrack the DB cluster to a point in time just before the release</p>", "<p>Restore the DB cluster using the point in time recovery method to a new DB cluster. Return the affected database to the correct state by copying the deleted rows from the recovered database to the original database</p>", "<p>Restore the DB cluster using the pre-release manual snapshot to a new DB cluster and update the application configuration to use the new DB cluster</p>"]}, "correct_response": ["a"], "section": "Management and Operations", "question_plain": "As part of a new release, the DevOps team at an IT company executed a database migration script that ended up breaking the application because the script had a bug that deleted several records from one of the main tables. An AWS Certified Database Specialist was brought in to assess the event that happened about 8 hours ago. The Specialist noted that the Aurora DB cluster had the \"Backtrack\" feature enabled with a 24 hour backtrack window. In addition, the DevOps team had also taken a manual DB cluster snapshot as well as cloned the DB cluster before the release.\n\nWhich of the following solutions can be used to return the affected database to the correct state within the LEAST amount of time and MINIMUM data loss?", "related_lectures": []}, {"_class": "assessment", "id": 56822968, "assessment_type": "multi-select", "prompt": {"question": "<p>A company's compliance requirements mandate regular backup of all their production databases.</p>\n\n<p>What should a Database Specialist consider when creating backups for DynamoDB tables? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>DynamoDB backup will preserve the Provisioned Read and Write capacities, Local Secondary Indexes (LSIs) and billing mode of the backed-up table</strong></p>\n\n<p>DynamoDB backups will backup the following from the source table : Global secondary indexes (GSIs), Local secondary indexes (LSIs), billing mode, Provisioned read and write capacity, Encryption settings.</p>\n\n<p><strong>Auto-scaling policies, AWS Identity and Access Management (IAM) policies have to be manually setup for the restored table</strong> - The following must be manually set up on the restored table: Autoscaling policies, AWS Identity and Access Management (IAM) policies, Amazon CloudWatch metrics and alarms, Tags, Stream settings, Time to Live (TTL) settings.</p>\n\n<p><strong>All API and console actions of DynamoDB backup and restore are captured and recorded in AWS CloudTrail</strong> - All backup and restore console and API actions are captured and recorded in AWS CloudTrail for logging, continuous monitoring, and auditing.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When you restore a table, you only use the provisioned Read Capacity Units (RCUs) of the table and Write Capacity Units (WCUs) remain untouched</strong> - This is an incorrect statement. All backups in DynamoDB work without consuming any provisioned throughput on the table. Similarly, you restore a table without consuming any provisioned throughput on the table.</p>\n\n<p><strong>IAM policies as well as encryption settings have to be manually setup for the restored table</strong> - This is an incorrect statement. Encryption settings are copied from the source table as part of the backup, however, AWS Identity and Access Management (IAM) policies will need to be set manually.</p>\n\n<p><strong>To delete backups marked with a 'Backup type' of 'AWS' using the DynamoDB console, you need to have full access permissions on DynamoDB resources</strong> - This is an incorrect statement. You can't delete backups marked with a Backup type of AWS using the DynamoDB console. To manage these backups, use the AWS Backup console.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html</a></p>\n", "answers": ["<p>When you restore a table, you only use the provisioned Read Capacity Units (RCUs) of the table and Write Capacity Units (WCUs) remain untouched</p>", "<p>DynamoDB backup will preserve the Provisioned Read and Write capacities, Local Secondary Indexes (LSIs) and billing mode of the backed-up table</p>", "<p>IAM policies as well as encryption settings have to be manually setup for the restored table</p>", "<p>Auto-scaling policies, AWS Identity and Access Management (IAM) policies have to be manually setup for the restored table</p>", "<p>All API and console actions of DynamoDB backup and restore are captured and recorded in AWS CloudTrail</p>", "<p>To delete backups marked with a 'Backup type' of 'AWS' using the DynamoDB console, you need to have full access permissions on DynamoDB resources</p>"]}, "correct_response": ["b", "d", "e"], "section": "Management and Operations", "question_plain": "A company's compliance requirements mandate regular backup of all their production databases.\n\nWhat should a Database Specialist consider when creating backups for DynamoDB tables? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56822970, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A big data analytics company wants to process IoT data from the field devices of an agricultural sciences company. The database team at the analytics company is designing a new database infrastructure for capturing this IoT data. The database should be resilient with minimal operational overhead and require the least development effort. The application includes a device tracking system that stores the GPS data for all devices. Real-time IoT data, as well as metadata lookups, must be performed with high throughput and microsecond latency.</p>\n\n<p>As a Database Specialist, which of the following options would you recommend as the MOST efficient solution for these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use DynamoDB as the database with DynamoDB Accelerator (DAX)</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable NoSQL database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p>Typically, DynamoDB response times can be measured in single-digit milliseconds. Certain use cases require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Additionally, DAX reduces operational and application complexity by providing a managed service that is API-compatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application. Therefore, this is the correct option.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RDS MySQL as the database with ElastiCache</strong></p>\n\n<p>The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. Also integrating ElastiCache with RDS involves custom code at the application level as well as provisioning and maintaining a separate ElastiCache cluster. This operational overhead goes against the specified requirements.</p>\n\n<p><strong>Use Aurora MySQL as the database with Aurora cluster cache</strong></p>\n\n<p>The given use-case deals with IoT data which has a variable underlying data structure by its nature. Relational databases are not a good fit for capturing such data. In addition, the main benefit of the cluster cache feature is to improve the performance of the new primary/writer instance after failover occurs. So Aurora with cluster cache is not the right fit to handle IoT data with microsecond latency.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q31-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/\">https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/</a></p>\n\n<p><strong>Use DocumentDB as the database with API Gateway</strong></p>\n\n<p>Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully-managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.</p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the front door for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.</p>\n\n<p>This option has been added as a distractor as API Gateway cannot be used to optimize the database infrastructure for the given use-case.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html\">https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/\">https://aws.amazon.com/blogs/database/introduction-to-aurora-postgresql-cluster-cache-management/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n", "answers": ["<p>Use RDS MySQL as the database with ElastiCache</p>", "<p>Use DynamoDB as the database with DynamoDB Accelerator (DAX)</p>", "<p>Use Aurora MySQL as the database with Aurora cluster cache</p>", "<p>Use DocumentDB as the database with API Gateway</p>"]}, "correct_response": ["b"], "section": "Workload-Specific Database Design", "question_plain": "A big data analytics company wants to process IoT data from the field devices of an agricultural sciences company. The database team at the analytics company is designing a new database infrastructure for capturing this IoT data. The database should be resilient with minimal operational overhead and require the least development effort. The application includes a device tracking system that stores the GPS data for all devices. Real-time IoT data, as well as metadata lookups, must be performed with high throughput and microsecond latency.\n\nAs a Database Specialist, which of the following options would you recommend as the MOST efficient solution for these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 56822972, "assessment_type": "multi-select", "prompt": {"question": "<p>An advertising technology startup in Silicon Valley captures browsing metadata to contextually display relevant images, pages, and links to targeted users. A single page load can generate multiple events that need to be stored individually. Each page load must query the user's browsing history to provide targeting recommendations. The startup expects over 1 billion page visits per day. The startup now wants to add a caching layer to support high read volumes.</p>\n\n<p>As a Database Specialist, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management. Therefore, this is a correct option.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or Amazon DynamoDB, providing a high-performance middle tier for applications with extremely high request rates and/or low latency requirements. Therefore, this is also a correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. RDS cannot be used as a caching layer.</p>\n\n<p><strong>Elasticsearch</strong> - Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It cannot be used as a caching layer.</p>\n\n<p><strong>Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used as a caching layer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/faqs/\">https://aws.amazon.com/elasticache/faqs/</a></p>\n", "answers": ["<p>Elasticsearch</p>", "<p>Redshift</p>", "<p>RDS</p>", "<p>DynamoDB Accelerator (DAX)</p>", "<p>ElastiCache</p>"]}, "correct_response": ["d", "e"], "section": "Workload-Specific Database Design", "question_plain": "An advertising technology startup in Silicon Valley captures browsing metadata to contextually display relevant images, pages, and links to targeted users. A single page load can generate multiple events that need to be stored individually. Each page load must query the user's browsing history to provide targeting recommendations. The startup expects over 1 billion page visits per day. The startup now wants to add a caching layer to support high read volumes.\n\nAs a Database Specialist, which of the following AWS services would you recommend as a caching layer for this use-case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56822974, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An advertising technology company is looking at moving their on-premises infrastructure to AWS Cloud. Their flagship application uses a massive PostgreSQL database and the database administration team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an EC2 instance with the optimal storage type on the attached EBS volume.</p>\n\n<p>As a Database Specialist, which of the following configurations would you suggest to the database administration team?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</strong></p>\n\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications.</p>\n\n<p>The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p>Provision IOPS type supports critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume. Examples are large database workloads, such as:\nMongoDB\nCassandra\nMicrosoft SQL Server\nMySQL\nPostgreSQL\nOracle</p>\n\n<p>Therefore, Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type is the right fit for the given use-case.</p>\n\n<p>Please see this detailed overview of the volume types for EBS volumes.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</strong></p>\n\n<p><strong>Amazon EC2 with EBS volume of cold HDD (sc1) type</strong></p>\n\n<p>According to the details provided in the explanation above, these three options are incorrect for the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n", "answers": ["<p>Amazon EC2 with EBS volume of General Purpose SSD (gp2) type</p>", "<p>Amazon EC2 with EBS volume of Provisioned IOPS SSD (io1) type</p>", "<p>Amazon EC2 with EBS volume of Throughput Optimized HDD (st1) type</p>", "<p>Amazon EC2 with EBS volume of cold HDD (sc1) type</p>"]}, "correct_response": ["b"], "section": "Workload-Specific Database Design", "question_plain": "An advertising technology company is looking at moving their on-premises infrastructure to AWS Cloud. Their flagship application uses a massive PostgreSQL database and the database administration team would like to retain control over managing the patches, version upgrades for the database, and consistent performance with high IOPS. The team wants to install the database on an EC2 instance with the optimal storage type on the attached EBS volume.\n\nAs a Database Specialist, which of the following configurations would you suggest to the database administration team?", "related_lectures": []}, {"_class": "assessment", "id": 56822976, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The database management team at a Big Data consultancy is working on the Disaster Recovery (DR) plans for a Redshift cluster deployed in the us-east-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.</p>\n\n<p>As a Database Specialist, which of the following solutions would you suggest to address the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong></p>\n\n<p>To copy snapshots for AWS KMS\u2013encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region.</p>\n\n<p><strong>Create an IAM role in destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region.</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n", "answers": ["<p>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</p>", "<p>Create an IAM role in destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</p>", "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</p>", "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</p>"]}, "correct_response": ["c"], "section": "Database Security", "question_plain": "The database management team at a Big Data consultancy is working on the Disaster Recovery (DR) plans for a Redshift cluster deployed in the us-east-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.\n\nAs a Database Specialist, which of the following solutions would you suggest to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56822978, "assessment_type": "multi-select", "prompt": {"question": "<p>The database team at an e-commerce company has been grappling with an issue on its Amazon Aurora MySQL DB cluster where the Read Replicas are lagging behind the master node with a huge deficit. On further investigation, the team has come across the error message \"Read Replica has fallen behind the master too much. Restarting MySQL\".</p>\n\n<p>As a Database Specialist, what steps and configuration checks will you undertake to resolve the issue? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>AuroraReplicaLag is a measure of lag in milliseconds when replicating updates from the primary Aurora DB instance to the reader nodes in an Aurora DB cluster. To be sure that your changes are propagated among the reader nodes, you must invalidate the cached data for read consistency. In some cases, there can be a delay when propagating changes across the reader nodes. You can observe this through an increase in the AuroraReplicaLag metric in CloudWatch, which leads to an eventual restart.</p>\n\n<p>More on AuroraReplicaLag parameter:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/</a></p>\n\n<p><strong>Check all instances in the cluster have the same specification. A reader node should not have a weaker DB instance class configuration than that of a writer DB instance</strong> - If a reader node has a weaker DB instance class configuration than that of a writer DB instance, the volume of changes can be too big for the reader to invalidate in the cache and catch up. In this scenario, it is a best practice to be sure that all DB instances in the Aurora cluster have the same specification.</p>\n\n<p><strong>A sudden surge of write activity in an already write-heavy production cluster can cause an overload on the writer DB instance. You can visualize this by using Amazon CloudWatch, where you observe sudden bursts of the DMLThroughput, DDLThroughput, and Queries metrics</strong> - A sudden surge of write activity in an already write-heavy production cluster can cause an overload on the writer DB instance. The added stress caused by the surge can cause the reader nodes to fall behind. You can visualize this by using Amazon CloudWatch, where you observe sudden bursts of the DMLThroughput, DDLThroughput, and Queries metrics.</p>\n\n<p>Resolution for Read Replica restart:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q35-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Amazon Aurora DB instance is undergoing instance recovery or failover, forcing the Read Replica to act as Primary node and this resulted in restart of the Read Replica</strong> - This is incorrect. The error clearly indicates that Read Replica has fallen behind the master and hence restarting, so failover scenario is not the issue for Read Replica restart.</p>\n\n<p><strong>Instance in Aurora clusters might have run out of cluster volume and forced a restart on Read Replica</strong> - Instances in Aurora clusters have two types of storage, one of which is storage for persistent data (called the cluster volume). This storage type increases automatically when more space is required. So, this is not the cause of Read Replica restart.</p>\n\n<p><strong>The storage volume that Read Replica is connected to could have crashed</strong> - This statement is incorrect. Aurora Replicas connect to the same storage volume as the primary DB instance and support read operations only.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-read-replica-restart/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-local-storage/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-local-storage/</a></p>\n", "answers": ["<p>Check all instances in the cluster have the same specification. A reader node should not have a weaker DB instance class configuration than that of a writer DB instance</p>", "<p>A sudden surge of write activity in an already write-heavy production cluster can cause an overload on the writer DB instance. You can visualize this by using Amazon CloudWatch, where you observe sudden bursts of the DMLThroughput, DDLThroughput, and Queries metrics</p>", "<p>The Amazon Aurora DB instance is undergoing instance recovery or failover, forcing the Read Replica to act as Primary node and this resulted in a restart of the Read Replica</p>", "<p>An instance in Aurora clusters might have run out of cluster volume and forced a restart on Read Replica</p>", "<p>The storage volume that Read Replica is connected to could have crashed</p>"]}, "correct_response": ["a", "b"], "section": "Monitoring and Troubleshooting", "question_plain": "The database team at an e-commerce company has been grappling with an issue on its Amazon Aurora MySQL DB cluster where the Read Replicas are lagging behind the master node with a huge deficit. On further investigation, the team has come across the error message \"Read Replica has fallen behind the master too much. Restarting MySQL\".\n\nAs a Database Specialist, what steps and configuration checks will you undertake to resolve the issue? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56822980, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A health-care company has just migrated its IT infrastructure from the on-premises data-center to AWS Cloud. The company is using Aurora PostGreSQL DB cluster as its data store and the CTO at the company now wants to build the caching layer of the current architecture for its PostGreSQL DB cluster. He wants the caching layer to have replication and archival support built into the architecture. Most importantly, the solution needs to be end-to-end HIPAA compliant.</p>\n\n<p>Which of the following AWS service offers the capabilities required for the engineering of the caching layer?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache for Redis</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Only ElastiCache for Redis (not Memcached) is HIPAA compliant. Hence this is the correct option.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison sheet for Redis vs Memcached features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q36-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q36-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/compliance/services-in-scope/\">https://aws.amazon.com/compliance/services-in-scope/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ElastiCache for Memcached</strong> - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication, archival snapshots and it's also not HIPAA compliant, so this option is not correct.</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database.</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p><a href=\"https://aws.amazon.com/compliance/services-in-scope/\">https://aws.amazon.com/compliance/services-in-scope/</a></p>\n", "answers": ["<p>DynamoDB Accelerator (DAX)</p>", "<p>ElastiCache for Memcached</p>", "<p>ElastiCache for Redis</p>", "<p>DocumentDB</p>"]}, "correct_response": ["c"], "section": "Workload-Specific Database Design", "question_plain": "A health-care company has just migrated its IT infrastructure from the on-premises data-center to AWS Cloud. The company is using Aurora PostGreSQL DB cluster as its data store and the CTO at the company now wants to build the caching layer of the current architecture for its PostGreSQL DB cluster. He wants the caching layer to have replication and archival support built into the architecture. Most importantly, the solution needs to be end-to-end HIPAA compliant.\n\nWhich of the following AWS service offers the capabilities required for the engineering of the caching layer?", "related_lectures": []}, {"_class": "assessment", "id": 56822982, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data analytics team at a multi-national retail company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the data analytics reporting, the database administration team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>As a Database Specialist, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\">\nvia - <a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift.</p>\n\n<p><strong>Use AWS EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/use-the-aws-database-migration-service-to-stream-change-data-to-amazon-kinesis-data-streams/\">https://aws.amazon.com/blogs/database/use-the-aws-database-migration-service-to-stream-change-data-to-amazon-kinesis-data-streams/</a></p>\n", "answers": ["<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>", "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>", "<p>Use AWS EMR to replicate the data from the databases into Amazon Redshift</p>", "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>"]}, "correct_response": ["a"], "section": "Deployment and Migration", "question_plain": "The data analytics team at a multi-national retail company has been running ad-hoc queries on Oracle and PostgreSQL services on Amazon RDS to prepare daily reports for senior management. To facilitate the data analytics reporting, the database administration team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.\n\nAs a Database Specialist, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?", "related_lectures": []}, {"_class": "assessment", "id": 56822984, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Internet-of-Things (IoT) solutions company is looking to migrate its on-premises infrastructure into the AWS Cloud. The database management team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team is well versed with the expected access patterns for the underlying database and expects the number of concurrent users to touch up to a million so the database should be able to scale elastically.</p>\n\n<p>As a Database Specialist, which of the following AWS services would you recommend for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency.</p>\n\n<p>Exam Alert:</p>\n\n<p>On some of the exam questions, it can be a close call to choose DynamoDB vs DocumentDB. If the use-case talks about known access patterns, then choosing DynamoDB makes more sense.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer.</p>\n\n<p><strong>ElastiCache</strong> - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database.</p>\n\n<p><strong>RDS</strong> - RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/\">https://aws.amazon.com/dynamodb/</a></p>\n", "answers": ["<p>DynamoDB</p>", "<p>DocumentDB</p>", "<p>ElastiCache</p>", "<p>RDS</p>"]}, "correct_response": ["a"], "section": "Workload-Specific Database Design", "question_plain": "An Internet-of-Things (IoT) solutions company is looking to migrate its on-premises infrastructure into the AWS Cloud. The database management team is looking for a fully managed NoSQL persistent data store with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team is well versed with the expected access patterns for the underlying database and expects the number of concurrent users to touch up to a million so the database should be able to scale elastically.\n\nAs a Database Specialist, which of the following AWS services would you recommend for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56822986, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company uses Aurora MySQL DB cluster for running online transaction processing (OLTP) transactions all through the day as well as running a reporting workload at certain times of the day. The reporting workload amounts to just 20% of the OLTP workload. A 10-node cluster is sufficient to meet the entire workload. The company wants to optimize the cluster size so that it pays the minimum charges while still maintaining a sufficient number of nodes to support any changes in the overall workload.</p>\n\n<p>As a Database Specialist, which of the following would you recommend as the MOST cost-optimal solution with minimum configuration changes?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Aurora Auto Scaling to enable your Aurora DB cluster to handle variations in workload by provisioning Aurora Replicas</strong></p>\n\n<p>You can use Aurora Auto Scaling to enable your Aurora DB cluster to handle sudden increases in connectivity or workload. Aurora Auto Scaling dynamically adjusts the number of Aurora Replicas provisioned for an Aurora DB cluster using single-master replication. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL. Based on the scaling policy, Aurora Auto Scaling adjusts the number of Aurora Replicas up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q39-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q39-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create two separate DB clusters - use one for OLTP workload and the other for reporting workload. Use AWS DMS to replicate data between the two clusters</strong> - Two-way replication via DMS between the new clusters would be a complex solution to implement. As the use-case requires minimum configuration changes, so this option is ruled out.</p>\n\n<p><strong>Use CloudWatch alarm to stop all the nodes of the DB cluster during times of minimal workload. The cluster can be restarted again if the workload goes above a pre-defined threshold</strong> - This is a distractor as once the cluster has been stopped, you will not have any workload metrics available from within the cluster to restart.</p>\n\n<p><strong>Use Aurora Auto Scaling to enable your Aurora DB cluster to handle variations in workload by provisioning Aurora Multi-AZ instances</strong> - Aurora Auto Scaling provisions Aurora Replicas to handle variations in the workload. You cannot use Aurora Multi-AZ instances for Aurora Auto Scaling.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p>\n", "answers": ["<p>Create two separate DB clusters - use one for OLTP workload and the other for reporting workload. Use AWS DMS to replicate data between the two clusters</p>", "<p>Use CloudWatch alarm to stop all the nodes of the DB cluster during times of minimal workload. The cluster can be restarted again if the workload goes above a pre-defined threshold</p>", "<p>Use Aurora Auto Scaling to enable your Aurora DB cluster to handle variations in workload by provisioning Aurora Replicas</p>", "<p>Use Aurora Auto Scaling to enable your Aurora DB cluster to handle variations in workload by provisioning Aurora Multi-AZ instances</p>"]}, "correct_response": ["c"], "section": "Workload-Specific Database Design", "question_plain": "An analytics company uses Aurora MySQL DB cluster for running online transaction processing (OLTP) transactions all through the day as well as running a reporting workload at certain times of the day. The reporting workload amounts to just 20% of the OLTP workload. A 10-node cluster is sufficient to meet the entire workload. The company wants to optimize the cluster size so that it pays the minimum charges while still maintaining a sufficient number of nodes to support any changes in the overall workload.\n\nAs a Database Specialist, which of the following would you recommend as the MOST cost-optimal solution with minimum configuration changes?", "related_lectures": []}, {"_class": "assessment", "id": 56822988, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A sports analytics firm uses AWS DynamoDB to store information about user's favorite sports teams and allows the information to be searchable from their home page. There is a daily requirement that all 10 million records in the table should be deleted then re-loaded at 3:00 AM each night.</p>\n\n<p>Which option is an efficient way to delete with minimal costs?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete then re-create the table</strong></p>\n\n<p>The DeleteTable operation deletes a table and all of its items. After a <code>DeleteTable</code> request, the specified table is in the <code>DELETING</code> state until DynamoDB completes the deletion. This is the most cost-efficient option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Scan and delete items using batch mode</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Scan and delete items individually</strong> - Scan is a very slow operation for 10 million items and this is not the best-fit option for the given use-case.</p>\n\n<p><strong>Use the purge table option</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html\">https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html</a></p>\n", "answers": ["<p>Scan and delete items using batch mode</p>", "<p>Scan and delete items individually</p>", "<p>Delete then re-create the table</p>", "<p>Use the purge table option</p>"]}, "correct_response": ["c"], "section": "Management and Operations", "question_plain": "A sports analytics firm uses AWS DynamoDB to store information about user's favorite sports teams and allows the information to be searchable from their home page. There is a daily requirement that all 10 million records in the table should be deleted then re-loaded at 3:00 AM each night.\n\nWhich option is an efficient way to delete with minimal costs?", "related_lectures": []}, {"_class": "assessment", "id": 56822990, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The database team at an e-commerce company wants to connect to the RDS PostGreSQL Multi-AZ DB instance using Secure Socket Layer or Transport Layer Security (SSL/TLS). The Team Lead has set the value of the static parameter <code>rds.force_ssl</code> as 1 in the custom parameter group currently associated with the production RDS PostGreSQL Multi-AZ DB instance. This change has been approved for a specific maintenance window to help minimize the impact on users.</p>\n\n<p>As a Database Specialist, which of the following steps would you recommend to apply the parameter group change for the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Propagate the parameter group update manually by rebooting the DB instance during the approved maintenance window</strong></p>\n\n<p>You can use a DB parameter group to act as a container for engine configuration values that are applied to one or more DB instances.</p>\n\n<p>If you create a DB instance without specifying a DB parameter group, the DB instance uses a default DB parameter group. Each default DB parameter group contains database engine defaults and Amazon RDS system defaults. You can't modify the parameter settings of a default parameter group. Instead, you create your own parameter group where you choose your own parameter settings. Not all DB engine parameters can be changed in a parameter group that you create.</p>\n\n<p>For a dynamic parameter, the changes are applied immediately. However, for a static parameter, the change takes effect after you manually reboot the DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the \"Apply Immediately\" setting to enforce the parameter update</strong> - As <code>rds.force_ssl</code> is a static parameter, you cannot use the \"Apply Immediately\" setting because that setting is only relevant for a dynamic parameter.</p>\n\n<p><strong>Go to the AWS RDS console and reboot the instance immediately</strong> - Although rebooting the instance immediately will propagate the change for the <code>rds.force_ssl</code> parameter, but this falls outside the approved maintenance window. Hence this option is not correct.</p>\n\n<p><strong>Let the approved maintenance window automatically apply the change for the static parameter</strong> - If a DB instance isn't using the latest changes to its associated DB parameter group, the AWS Management Console shows the DB parameter group with a status of pending-reboot. The pending-reboot parameter group status doesn't result in an automatic reboot during the approved maintenance window. To apply the latest parameter changes to that DB instance, you need to manually reboot the DB instance.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html</a></p>\n", "answers": ["<p>Use the \"Apply Immediately\" setting to enforce the parameter update</p>", "<p>Let the approved maintenance window automatically apply the change for the static parameter</p>", "<p>Go to the AWS RDS console and reboot the instance immediately</p>", "<p>Propagate the parameter group update manually by rebooting the DB instance during the approved maintenance window</p>"]}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "The database team at an e-commerce company wants to connect to the RDS PostGreSQL Multi-AZ DB instance using Secure Socket Layer or Transport Layer Security (SSL/TLS). The Team Lead has set the value of the static parameter rds.force_ssl as 1 in the custom parameter group currently associated with the production RDS PostGreSQL Multi-AZ DB instance. This change has been approved for a specific maintenance window to help minimize the impact on users.\n\nAs a Database Specialist, which of the following steps would you recommend to apply the parameter group change for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56822992, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple EC2 instances. The development team has identified the root cause as unhealthy servers causing session data to be lost. The Lead Developer has asked the database management team to implement a distributed in-memory cache-based session management solution.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Elasticache for distributed cache-based session management</strong></p>\n\n<p>Amazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.</p>\n\n<p>Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n\n<p>How ElastiCache Works:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_how-it-works.ec509f8b878f549b7fb8a49669bf2547878303f6.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RDS for distributed in-memory cache-based session management</strong> - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option is incorrect.</p>\n\n<p><strong>Use DynamoDB for distributed in-memory cache-based session management</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based session management solution.</p>\n\n<p><strong>Continue with application-based sticky sessions</strong> - Although sticky sessions enable each user to interact with one server and one server only, however, in case of an unhealthy server, all the session data is gone as well. Therefore Elasticache powered distributed in-memory cache-based session management is a better solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n", "answers": ["<p>Use RDS for distributed in-memory cache based session management</p>", "<p>Use Elasticache for distributed in-memory cache based session management</p>", "<p>Continue with application-based sticky sessions</p>", "<p>Use DynamoDB for distributed in-memory cache based session management</p>"]}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "An e-commerce application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple EC2 instances. The development team has identified the root cause as unhealthy servers causing session data to be lost. The Lead Developer has asked the database management team to implement a distributed in-memory cache-based session management solution.\n\nAs a Database Specialist, which of the following solutions would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 56822994, "assessment_type": "multi-select", "prompt": {"question": "<p>An in-home fitness company has recently transitioned to AWS Cloud from its on-premises data center. The CTO at the company wants to augment the existing application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's health metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable rider data for the community of users riding together virtually from the comfort of their home. The leaderboard would be accessed by millions of users simultaneously.</p>\n\n<p>As a Database Specialist, which of the following options would you enlist to advocate for using ElastiCache for the given requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for read-heavy application workloads</strong></p>\n\n<p><strong>Use ElastiCache to improve performance of compute-intensive workloads</strong></p>\n\n<p>Amazon ElastiCache allows you to run in-memory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p>Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing, leaderboard and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.</p>\n\n<p>Overview of Amazon ElastiCache features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q43-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ElastiCache to improve latency and throughput for write-heavy application workloads</strong> - As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads. Caching is not a good fit for write-heavy applications as the cache goes stale at a very fast rate.</p>\n\n<p><strong>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</strong> - ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.</p>\n\n<p><strong>Use ElastiCache to run highly complex JOIN queries</strong> - Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/features/\">https://aws.amazon.com/elasticache/features/</a></p>\n", "answers": ["<p>Use ElastiCache to improve latency and throughput for read-heavy application workloads</p>", "<p>Use ElastiCache to improve latency and throughput for write-heavy application workloads</p>", "<p>Use ElastiCache to improve performance of Extract-Transform-Load (ETL) workloads</p>", "<p>Use ElastiCache to improve performance of compute-intensive workloads</p>", "<p>Use ElastiCache to run highly complex JOIN queries</p>"]}, "correct_response": ["a", "d"], "section": "Management and Operations", "question_plain": "An in-home fitness company has recently transitioned to AWS Cloud from its on-premises data center. The CTO at the company wants to augment the existing application by adding a leaderboard that uses a complex proprietary algorithm based on the participating user's health metrics to identify the top users on a real-time basis. The technical requirements mandate high elasticity, low latency, and real-time processing to deliver customizable rider data for the community of users riding together virtually from the comfort of their home. The leaderboard would be accessed by millions of users simultaneously.\n\nAs a Database Specialist, which of the following options would you enlist to advocate for using ElastiCache for the given requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56822996, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web-application on EC2 based web servers and uses RDS PostGreSQL DB as the data store. The PostGreSQL DB is set up in a private subnet that allows inbound traffic from selected EC2 instances. The DB also uses AWS KMS for encrypting data at rest.</p>\n\n<p>As a Database Specialist, which of the following steps would you suggest to facilitate secure access to the database?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure RDS to use SSL for data in transit</strong></p>\n\n<p>You can use Secure Socket Layer / Transport Layer Security (SSL/TLS) connections to encrypt data in transit. Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when the instance is provisioned. For MySQL, you launch the mysql client using the --ssl_ca parameter to reference the public key in order to encrypt connections. Using SSL, you can encrypt a PostgreSQL connection between your applications and your PostgreSQL DB instances. You can also force all connections to your PostgreSQL DB instance to use SSL.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/security/\">https://aws.amazon.com/rds/features/security/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q44-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q44-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use IAM authentication to access the DB instead of the database user's access credentials</strong> - You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p>\n\n<p>IAM authentication is just another way to authenticate the user's credentials while accessing the database. It would not significantly enhance the security in a way that enabling SSL does by facilitating the in-transit encryption for the database.</p>\n\n<p><strong>Create a new security group that blocks SSH from the selected EC2 instances into the DB</strong></p>\n\n<p><strong>Create a new Network Access Control List (NACL) that blocks SSH from the entire EC2 subnet into the DB</strong></p>\n\n<p>Both these options are added as distractors. You cannot SSH into an RDS DB instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/security/\">https://aws.amazon.com/rds/features/security/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.SSLSupport</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/\">https://aws.amazon.com/blogs/database/using-iam-authentication-to-connect-with-pgadmin-amazon-aurora-postgresql-or-amazon-rds-for-postgresql/</a></p>\n", "answers": ["<p>Use IAM authentication to access the DB instead of the database user's access credentials</p>", "<p>Configure RDS to use SSL for data in transit</p>", "<p>Create a new security group that blocks SSH from the selected EC2 instances into the DB</p>", "<p>Create a new Network Access Control List (NACL) that blocks SSH from the entire EC2 subnet into the DB</p>"]}, "correct_response": ["b"], "section": "Database Security", "question_plain": "A financial services company uses AWS Cloud to manage its technology infrastructure. The company has deployed its consumer-focused web-application on EC2 based web servers and uses RDS PostGreSQL DB as the data store. The PostGreSQL DB is set up in a private subnet that allows inbound traffic from selected EC2 instances. The DB also uses AWS KMS for encrypting data at rest.\n\nAs a Database Specialist, which of the following steps would you suggest to facilitate secure access to the database?", "related_lectures": []}, {"_class": "assessment", "id": 56822998, "assessment_type": "multi-select", "prompt": {"question": "<p>A financial services company has recently transitioned its technology infrastructure from the on-premises data center to AWS Cloud. The security team at the company is evaluating the security features offered by RDS MySQL. You have been hired as an AWS Certified Database Specialist to provide guidance on RDS MySQL security offerings.</p>\n\n<p>Which of the following statements would you identify as correct for the given use-case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>You can map multiple IAM users or roles to the same database user account</strong> - It is possible to map multiple IAM users or roles to the same database user account.</p>\n\n<p><strong>If your application is running on Amazon Elastic Compute Cloud (Amazon EC2), then you can use EC2 instance profile credentials to access the database</strong> - If your application is running on Amazon Elastic Compute Cloud (Amazon EC2), then you can use EC2 instance profile credentials to access the database. You don't need to store database passwords on your instance.</p>\n\n<p><strong>IAM database authentication should only be used if your application requires no more than 200 new connections per second</strong> - When using IAM database authentication with MySQL, you are limited to a maximum of 200 new connections per second. If you are using a db.t2.micro DB instance class, the limit is 10 connections per second.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>All data transmitted to and from RDS DB instance for IAM Database Authentication is encrypted using KMS service</strong> - This statement is incorrect. All data transmitted to and from your RDS DB instance for IAM Database Authentication requires SSL connection for encryption.</p>\n\n<p><strong>IAM user/role and database user hold a one to one relationship. That is, each IAM user or role needs a separate database user account mapped to their IAM account for access to the RDS MySQL database</strong> - This is incorrect. You can map multiple IAM users or roles to the same database user account.</p>\n\n<p><strong>IAM database authentication tokens are generated using AWS access keys. These Authentication tokens do not expire once they are generated. They need to be manually expired by the application after using them</strong> - This is incorrect. Authentication tokens have a lifespan of 15 minutes, so you don't need to enforce password resets or manually expire the tokens in your application.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.IAMPolicy.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.IAMPolicy.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/users-connect-rds-iam/\">https://aws.amazon.com/premiumsupport/knowledge-center/users-connect-rds-iam/</a></p>\n", "answers": ["<p>All data transmitted to and from RDS DB instance for IAM Database Authentication is encrypted using KMS service</p>", "<p>IAM user/role and database user hold a one to one relationship. That is, each IAM user or role need a separate database user account mapped to their IAM account for access to the RDS MySQL database</p>", "<p>You can map multiple IAM users or roles to the same database user account</p>", "<p>If your application is running on Amazon Elastic Compute Cloud (Amazon EC2), then you can use EC2 instance profile credentials to access the database</p>", "<p>IAM database authentication tokens are generated using AWS access keys. These Authentication tokens do not expire once they are generated. They need to be manually expired by the application after using them</p>", "<p>IAM database authentication should only be used if your application requires no more than 200 new connections per second</p>"]}, "correct_response": ["c", "d", "f"], "section": "Database Security", "question_plain": "A financial services company has recently transitioned its technology infrastructure from the on-premises data center to AWS Cloud. The security team at the company is evaluating the security features offered by RDS MySQL. You have been hired as an AWS Certified Database Specialist to provide guidance on RDS MySQL security offerings.\n\nWhich of the following statements would you identify as correct for the given use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823000, "assessment_type": "multi-select", "prompt": {"question": "<p>The development team at an e-commerce company manages 5 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL DB instance. As a Database Specialist, you have been asked to make the database instance resilient from a disaster recovery perspective.</p>\n\n<p>Which of the following features will help you prepare for database disaster recovery? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use cross-Region Read Replicas</strong></p>\n\n<p>In addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a cross-Region Read Replica can help ensure that you get back up and running if you experience a regional availability issue.</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. Amazon RDS uses several different technologies to provide failover support. Multi-AZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.</p>\n\n<p>The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. If it\u2019s a Multi-AZ configuration, backups occur on the standby to reduce I/O impact on the primary. Amazon RDS supports Cross-Region Automated Backups. Manual snapshots and Read Replicas are also supported across multiple Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</strong> - This is an incorrect statement. Automated backups can be created across AWS Regions.</p>\n\n<p><strong>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</strong> - Amazon RDS Provisioned IOPS Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.</p>\n\n<p><strong>Use database cloning feature of the RDS DB cluster</strong> - This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/\">https://aws.amazon.com/rds/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n", "answers": ["<p>Use cross-Region Read Replicas</p>", "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions</p>", "<p>Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage</p>", "<p>Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region</p>", "<p>Use database cloning feature of the RDS DB cluster</p>"]}, "correct_response": ["a", "b"], "section": "Monitoring and Troubleshooting", "question_plain": "The development team at an e-commerce company manages 5 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for PostgreSQL DB instance. As a Database Specialist, you have been asked to make the database instance resilient from a disaster recovery perspective.\n\nWhich of the following features will help you prepare for database disaster recovery? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56823002, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are a database administrator for an IT company that recently moved its production application to AWS and migrated data from PostGreSQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added at the outset when you are first creating tables, otherwise, changes cannot be done once the table is created.</p>\n\n<p>As a Database Specialist, which of the following actions would you suggest?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an LSI</strong></p>\n\n<p>LSI stands for Local Secondary Index.</p>\n\n<p>Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.</p>\n\n<p>Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist.</p>\n\n<p>Differences between GSI and LSI:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create DynamoDB Streams</strong> - DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time. This option is not relevant to the given use-case.</p>\n\n<p><strong>Create a GSI</strong> - GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table. Global secondary indexes can be created at the same time that you create a table. You can also add a new global secondary index to an existing table, or delete an existing global secondary index.</p>\n\n<p><strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code development.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a></p>\n", "answers": ["<p>Create a GSI</p>", "<p>Create DynamoDB Streams</p>", "<p>Create an LSI</p>", "<p>Migrate away from DynamoDB</p>"]}, "correct_response": ["c"], "section": "Monitoring and Troubleshooting", "question_plain": "You are a database administrator for an IT company that recently moved its production application to AWS and migrated data from PostGreSQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added at the outset when you are first creating tables, otherwise, changes cannot be done once the table is created.\n\nAs a Database Specialist, which of the following actions would you suggest?", "related_lectures": []}, {"_class": "assessment", "id": 56823004, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An ad-tech company runs a leading ad targeting platform that captures various kinds of marketing data such as user profiles, user events, clicks, and visited links. The company is looking to migrate its IT infrastructure from the on-premises data center to AWS Cloud. The technical requirements for the advertising platform mandate a high request rate (millions of requests per second), low predictable latency and high reliability. The company also needs to deploy this ad targeting platform in more than one AWS Regions. The maximum size of an event is 200 KB and the average size is 10 KB. The structure of the incoming data varies depending on the event.</p>\n\n<p>As a Database Specialist, which of the following database solutions would you recommend to address these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>DynamoDB Global Tables</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>Global Tables builds upon DynamoDB\u2019s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read, and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions.</p>\n\n<p>DynamoDB Global Tables Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q48-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p>For the given use-case, DynamoDB is the right choice as it supports a high request rate (millions of requests per second) with low predictable latency and high reliability. DynamoDB supports a maximum item size of 400 KB which meets the requirement for the maximum size of an event is 200 KB and the average size is 10 KB. As DynamoDB is a NoSQL database, so it can be used to store the input data even with the variable structure of the metadata for different events. As the company wants to deploy this ad targeting platform in more than one AWS Region, therefore DynamoDB Global Tables fits the overall requirement.</p>\n\n<p>For further deep dive on ad-tech use-cases for DynamoDB, please review this excellent blog:\n<a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully-managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. Although DocumentDB is a NoSQL database just like DynamoDB, it can only be deployed to a specific AWS Region. Therefore it does not meet all the requirements for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q48-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html\">https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html</a></p>\n\n<p><strong>Aurora Serverless</strong> - Amazon Aurora Serverless is an on-demand, auto-scaling configuration for the MySQL-compatible and PostgreSQL-compatible editions of Amazon Aurora. An Aurora Serverless DB cluster automatically starts up, shuts down, and scales capacity up or down based on your application's needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.</p>\n\n<p><strong>Aurora Global Database</strong> - Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p>\n\n<p>Both Aurora Serverless and Aurora Global Databases are relational databases and need a consistent schema for the incoming data, therefore these can not be used to store the input data with variable structure of the incoming data for different events.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-ad-tech-use-cases-and-design-patterns/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html\">https://docs.aws.amazon.com/documentdb/latest/developerguide/replication.html</a></p>\n", "answers": ["<p>DocumentDB</p>", "<p>DynamoDB Global Tables</p>", "<p>Aurora Serverless</p>", "<p>Aurora Global Database</p>"]}, "correct_response": ["b"], "section": "Workload-Specific Database Design", "question_plain": "An ad-tech company runs a leading ad targeting platform that captures various kinds of marketing data such as user profiles, user events, clicks, and visited links. The company is looking to migrate its IT infrastructure from the on-premises data center to AWS Cloud. The technical requirements for the advertising platform mandate a high request rate (millions of requests per second), low predictable latency and high reliability. The company also needs to deploy this ad targeting platform in more than one AWS Regions. The maximum size of an event is 200 KB and the average size is 10 KB. The structure of the incoming data varies depending on the event.\n\nAs a Database Specialist, which of the following database solutions would you recommend to address these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 56823006, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data analytics company wants to deploy a global IoT solution in multiple AWS Regions. The company wants to store IoT data in DynamoDB tables in each Region as per the user's geographic location. The DevOps team at the company wants an automated solution that can deploy the database along with the rest of the application with an identical configuration in new Regions as per the business requirement. The solution should also propagate configuration updates across all Regions.</p>\n\n<p>As a Database Specialist, which of the following options would you recommend to address the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>\"Create a CloudFormation template and deploy the template to multiple Regions via a stack set\"</p>\n\n<p>AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires.</p>\n\n<p>For the given use-case, the company wants to deploy the IoT solution in multiple AWS Regions as well as automate any future configuration updates to all Regions. Therefore, you must use stack sets to facilitate the deployment solution via CloudFormation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q49-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Create DynamoDB global tables via AWS CLI\" - Global tables provide you with a fully managed, multi-region, and multi-master DynamoDB database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p>\n\n<p>Global table is not the right fit for this use-case because the solution must support deployment for the entire application not just the database, across multiple AWS Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q49-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p>\"Create DynamoDB tables using the AWS Console in multiple Regions and create a detailed guide for future updates to the deployment\" - This option just focuses on provisioning DynamoDB tables manually and it's not an automated solution. So this option is not correct.</p>\n\n<p>\"Use a CloudFormation template to deploy the solution to multiple Regions\" - An AWS CloudFormation template is a JSON or YAML formatted text file. CloudFormation uses these templates as blueprints for building your AWS resources. For example, in a template, you can describe an Amazon EC2 instance, such as the instance type, the AMI ID, block device mappings, and its Amazon EC2 key pair name. Whenever you create a stack, you also specify a template that AWS CloudFormation uses to create whatever you described in the template.</p>\n\n<p>A template by itself cannot be used to deploy resources across multiple AWS Regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n", "answers": ["<p>Create DynamoDB global tables via AWS CLI</p>", "<p>Create DynamoDB tables using the AWS Console in multiple Regions and create a detailed guide for future updates to the deployment</p>", "<p>Use a CloudFormation template to deploy the solution to multiple Regions</p>", "<p>Create a CloudFormation template and deploy the template to multiple Regions via a stack set</p>"]}, "correct_response": ["d"], "section": "Deployment and Migration", "question_plain": "A data analytics company wants to deploy a global IoT solution in multiple AWS Regions. The company wants to store IoT data in DynamoDB tables in each Region as per the user's geographic location. The DevOps team at the company wants an automated solution that can deploy the database along with the rest of the application with an identical configuration in new Regions as per the business requirement. The solution should also propagate configuration updates across all Regions.\n\nAs a Database Specialist, which of the following options would you recommend to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823008, "assessment_type": "multi-select", "prompt": {"question": "<p>A media company is planning to move their on-premises database to Amazon RDS. As part of disaster recovery planning, the team is looking into the various database backup options available with RDS. As a Database Specialist, you have been roped in to understand and weigh-in on the RDS automated backups and snapshot features.</p>\n\n<p>Which of the following statements would you identify as correct for the given context? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>The default backup retention period is one day if you create the DB instance using the Amazon RDS API</strong> - The default backup retention period is one day if you create the DB instance using the Amazon RDS API or the AWS CLI. The default backup retention period is seven days if you create the DB instance using the console.</p>\n\n<p><strong>DB Snapshots can be created with the AWS Management Console, CreateDBSnapshot API, or create-db-snapshot command</strong> - DB Snapshots can be created with the AWS Management Console, CreateDBSnapshot API, or create-db-snapshot command and are kept until you explicitly delete them.</p>\n\n<p><strong>Disabling automatic backups for a DB instance deletes all existing automated backups for the instance</strong> - Disabling automatic backups for a DB instance deletes all existing automated backups for the instance. If you disable and then re-enable automated backups, you are only able to restore starting from the time you re-enabled automated backups.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DB Snapshots can be created with the AWS Management Console and create-db-snapshot command only</strong> - This is an incorrect statement. DB Snapshots can be created with the AWS Management Console, CreateDBSnapshot API, or create-db-snapshot command.</p>\n\n<p><strong>When a DB instance is deleted, manually created DB snapshots as well as automated backups are deleted</strong> - This is an incorrect statement. Automated backups are deleted when the DB instance is deleted. Only manually created DB Snapshots are retained after the DB Instance is deleted.</p>\n\n<p><strong>The total number of automated backups and manual snapshots, put together, should not exceed 100 per Region</strong> - This is an incorrect statement. Manual snapshot limits (of 100 per region) does not apply to automated backups.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html</a></p>\n", "answers": ["<p>The default backup retention period is one day if you create the DB instance using the Amazon RDS API</p>", "<p>DB Snapshots can be created with the AWS Management Console, CreateDBSnapshot API, or create-db-snapshot command</p>", "<p>DB Snapshots can be created with the AWS Management Console and create-db-snapshot command only</p>", "<p>Disabling automatic backups for a DB instance deletes all existing automated backups for the instance</p>", "<p>When a DB instance is deleted, manually created DB snapshots as well as automated backups are deleted</p>", "<p>The total number of automated backups and manual snapshots, put together, should not exceed 100 per Region</p>"]}, "correct_response": ["a", "b", "d"], "section": "Management and Operations", "question_plain": "A media company is planning to move their on-premises database to Amazon RDS. As part of disaster recovery planning, the team is looking into the various database backup options available with RDS. As a Database Specialist, you have been roped in to understand and weigh-in on the RDS automated backups and snapshot features.\n\nWhich of the following statements would you identify as correct for the given context? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823010, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Due to the COVID-19 pandemic, a large retail company is planning to close operations for a month. The technology team at the company needs to stop all applications along with all the RDS PostGreSQL DB instances to ensure employees do not have access to the systems during this time. The Team Lead has written and executed a script to stop all the RDS PostGreSQL DB instances. However, while reviewing the logs, the Team Lead finds that the RDS PostGreSQL DB instances with Read Replicas did not stop.</p>\n\n<p>As an AWS Certified Database Specialist, how would you modify the script to fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Delete all Read Replicas before stopping the corresponding primary RDS DB instance</strong></p>\n\n<p>Read Replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the Read Replica. When you create a Read Replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the Read Replica whenever there is a change to the primary DB instance.</p>\n\n<p>You can stop your Amazon RDS DB instance if it is running any the following engines - MariaDB, Microsoft SQL Server, MySQL, Oracle and PostgreSQL. While your DB instance is stopped, you are charged for provisioned storage (including Provisioned IOPS) and backup storage (including manual snapshots and automated backups within your specified retention window), but not for DB instance hours.</p>\n\n<p>You can't stop a DB instance if that instance has a read replica or that instance itself is a read replica. Therefore, for the given use-case, you must delete all Read Replicas before stopping the corresponding primary RDS DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q51-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stop the primary RDS DB instance before stopping all Read Replicas</strong></p>\n\n<p><strong>Stop all Read Replicas before stopping the primary RDS DB instance</strong></p>\n\n<p>Both these options contradict the details provided in the explanation above, so these options are incorrect.</p>\n\n<p><strong>Use a schedule based CloudWatch Events rule to stop the Read Replicas and the primary RDS DB instance at the same time</strong> - This option has been added as a distractor. You can't stop a DB instance if that instance has a read replica or that instance itself is a read replica. The sequence of events does not matter.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html</a></p>\n", "answers": ["<p>Delete all Read Replicas before stopping the corresponding primary RDS DB instance</p>", "<p>Stop the primary RDS DB instance before stopping all Read Replicas</p>", "<p>Stop all Read Replicas before stopping the primary RDS DB instance</p>", "<p>Use a schedule based CloudWatch Events rule to stop the Read Replicas and the primary RDS DB instance at the same time</p>"]}, "correct_response": ["a"], "section": "Monitoring and Troubleshooting", "question_plain": "Due to the COVID-19 pandemic, a large retail company is planning to close operations for a month. The technology team at the company needs to stop all applications along with all the RDS PostGreSQL DB instances to ensure employees do not have access to the systems during this time. The Team Lead has written and executed a script to stop all the RDS PostGreSQL DB instances. However, while reviewing the logs, the Team Lead finds that the RDS PostGreSQL DB instances with Read Replicas did not stop.\n\nAs an AWS Certified Database Specialist, how would you modify the script to fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 56823012, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are a Database Specialist working for an analytics company. You have been tasked with building a reporting application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 8 KB in size each.</p>\n\n<p>How many Read Capacity Units (RCUs) are needed?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>Before proceeding with the calculations, please review the following:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q52-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q52-i2.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n\n<p><strong>20</strong></p>\n\n<p>One Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p>\n\n<p>1) Item Size / 4KB, rounding to the nearest whole number.</p>\n\n<p>So, in the above case, 8KB / 4 KB = 2 read capacity units per item.</p>\n\n<p>2) 2 read capacity units per item (since strongly consistent read) \u00d7 No of reads per second</p>\n\n<p>So, in the above case, 2 x 10 = 20 read capacity units.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>40</strong></p>\n\n<p><strong>10</strong></p>\n\n<p><strong>5</strong></p>\n\n<p>These three options contradict the details provided in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a></p>\n", "answers": ["<p>10</p>", "<p>40</p>", "<p>20</p>", "<p>5</p>"]}, "correct_response": ["c"], "section": "Management and Operations", "question_plain": "You are a Database Specialist working for an analytics company. You have been tasked with building a reporting application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 8 KB in size each.\n\nHow many Read Capacity Units (RCUs) are needed?", "related_lectures": []}, {"_class": "assessment", "id": 56823014, "assessment_type": "multi-select", "prompt": {"question": "<p>A social networking application supports the transfer of gift vouchers between users. When a user reaches a certain number of followers on the application, that user earns a gift voucher that can be redeemed or transferred to another user. The database administration team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend as the best-fit options to meet the given requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</strong></p>\n\n<p>You can use DynamoDB transactions to make coordinated all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.</p>\n\n<p>DynamoDB Transactions Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q53-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><strong>Complete both operations on RDS MySQL in a single transaction block</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database with support for transactions in the cloud. A relational database is a collection of data items with pre-defined relationships between them. RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance Online Transaction Processing (OLTP) applications, and the other for cost-effective general-purpose use.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q53-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</strong> - DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. Read consistency does not facilitate DynamoDB transactions and this option has been added as a distractor.</p>\n\n<p><strong>Complete both operations on Amazon RedShift in a single transaction block</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. It cannot be used to manage database transactions.</p>\n\n<p><strong>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It cannot be used to manage database transactions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/relational-database/\">https://aws.amazon.com/relational-database/</a></p>\n", "answers": ["<p>Perform DynamoDB read and write operations with ConsistentRead parameter set to true</p>", "<p>Complete both operations on Amazon RedShift in a single transaction block</p>", "<p>Use the Amazon Athena transactional read and write APIs on the table items as a single, all-or-nothing operation</p>", "<p>Use the DynamoDB transactional read and write APIs on the table items as a single, all-or-nothing operation</p>", "<p>Complete both operations on RDS MySQL in a single transaction block</p>"]}, "correct_response": ["d", "e"], "section": "Database Security", "question_plain": "A social networking application supports the transfer of gift vouchers between users. When a user reaches a certain number of followers on the application, that user earns a gift voucher that can be redeemed or transferred to another user. The database administration team wants to ensure that this transfer is captured in the database such that the records for both users are either written successfully with the new gift vouchers or the status quo is maintained.\n\nAs a Database Specialist, which of the following solutions would you recommend as the best-fit options to meet the given requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56823016, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Ahead of the upcoming holiday sale season, a group of database administrators at an e-commerce company is analyzing performance issues for an RDS for PostgreSQL DB instance and is reviewing related metrics. The group wants to understand the database wait events in more detail.</p>\n\n<p>As a Database Specialist, which of the following steps would you suggest for the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon RDS. You can monitor network throughput, client connections, I/O for read, write, or metadata operations, and burst credit balances for your DB instances. AWS provides various tools that you can use to monitor Amazon RDS.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q54-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n\n<p><strong>Enable Amazon RDS Performance Insights and review the appropriate dashboard to analyze the wait events</strong></p>\n\n<p>Performance Insights allows you to understand your database's performance and help you analyze any issues that affect it. With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users.</p>\n\n<p>The central metric for Performance Insights is DB Load, which represents the average number of active sessions for the DB engine. The DB Load metric is collected every second. An active session is a connection that has submitted work to the DB engine and is waiting for a response. For example, if you submit a SQL query to the DB engine, the database session is active while the DB engine is processing the query.</p>\n\n<p>A wait event causes a SQL statement to wait for a specific event to happen before it can continue running. For example, a SQL statement might wait until a locked resource is unlocked. By combining DB Load with wait events, you can get a complete picture of the session state.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q54-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the database logs to be pushed to Amazon CloudWatch Logs for detailed analysis for the wait events</strong> - With CloudWatch Logs, you can perform real-time analysis of the log data, store the data in highly durable storage, and manage the data with the CloudWatch Logs Agent. AWS retains log data published to CloudWatch Logs for an indefinite time period unless you specify a retention period.</p>\n\n<p>You can publish the following log types to CloudWatch Logs for RDS for PostgreSQL:</p>\n\n<p>Postgresql log</p>\n\n<p>Upgrade log (not available for Aurora PostgreSQL)</p>\n\n<p>The data specific to the wait events is not part of this log data, so this option is incorrect.</p>\n\n<p><strong>Use the Trusted Advisor dashboard to analyze the wait events</strong> - Using the Trusted Advisor dashboard, you can review things such as RDS Idle DB Instances, RDS Security Group Access Risk, RDS Backups and RDS Multi-AZ. You cannot use Trusted Advisor dashboard to analyze the wait events.</p>\n\n<p><strong>Use Amazon RDS Enhanced Monitoring to analyze the wait events</strong> - Amazon RDS provides metrics in real-time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs. You cannot use Enhanced Monitoring to analyze the wait events.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.UsingDashboard.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</a></p>\n", "answers": ["<p>Configure the database logs to be pushed to Amazon CloudWatch Logs for detailed analysis for the wait events</p>", "<p>Use the Trusted Advisor dashboard to analyze the wait events</p>", "<p>Use Amazon RDS Enhanced Monitoring to analyze the wait events</p>", "<p>Enable Amazon RDS Performance Insights and review the appropriate dashboard to analyze the wait events</p>"]}, "correct_response": ["d"], "section": "Monitoring and Troubleshooting", "question_plain": "Ahead of the upcoming holiday sale season, a group of database administrators at an e-commerce company is analyzing performance issues for an RDS for PostgreSQL DB instance and is reviewing related metrics. The group wants to understand the database wait events in more detail.\n\nAs a Database Specialist, which of the following steps would you suggest for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823018, "assessment_type": "multi-select", "prompt": {"question": "<p>A multi-national company has just floated their latest security policy. As per this policy, data needs to be stored in encrypted form only. The company uses different Amazon RDS solutions for their various products. As a Database Specialist, you are roped in to facilitate the learning of RDS encryption features and configurations.</p>\n\n<p>Which of the following statements would you identify as correct for the given context? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>An unencrypted RDS DB instance will result in unencrypted Read Replicas only, You can't have an encrypted Read Replica of an unencrypted DB instance</strong> - This is a correct statement. You can't have an encrypted read replica of an unencrypted RDS DB instance or an unencrypted Read Replica of an encrypted RDS DB instance.</p>\n\n<p>Limitations of Amazon RDS Encrypted DB Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p>\n\n<p><strong>You can restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster</strong> - Aurora DB offers this facility, though this is not possible for RDS DB instances.\nYou can't convert an unencrypted Aurora DB cluster to an encrypted one. However, you can restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster. To do this, specify a KMS encryption key when you restore from the unencrypted DB cluster snapshot.</p>\n\n<p>Limitations of Amazon Aurora Encrypted DB Clusters:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q55-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html</a></p>\n\n<p><strong>To copy an encrypted snapshot from one AWS Region to another, you must specify the KMS key identifier of the destination AWS Region</strong> - To copy an encrypted snapshot from one AWS Region to another, you must specify the KMS key identifier of the destination AWS Region. This is because KMS encryption keys are specific to the AWS Region that they are created in.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can enable encryption for an Amazon RDS DB instance when you create it, or when you pull it down for maintenance. Encryption settings cannot be changed when the database is up and running</strong> - This is an incorrect statement. You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created.</p>\n\n<p><strong>You can restore an unencrypted backup or snapshot of an Amazon RDS DB instance to an encrypted DB instance by specifying the KMS key identifier</strong> - This is incorrect. You can't restore an unencrypted backup or snapshot to an encrypted Amazon RDS DB instance.</p>\n\n<p><strong>Since Read Replica of an Amazon RDS encrypted instance is also encrypted using the same key as the primary DB instance, creating Read Replicas in a different Region is not possible</strong> - This is incorrect. A Read Replica of an Amazon RDS encrypted instance is also encrypted using the same key as the primary DB instance when both are in the same AWS Region. If the primary DB instance and Read Replica are in different AWS Regions, you encrypt using the encryption key for that AWS Region.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.Encryption.html</a></p>\n", "answers": ["<p>You can enable encryption for an Amazon RDS DB instance when you create it, or when you pull it down for maintenance. Encryption settings cannot be changed when the database is up and running</p>", "<p>An unencrypted RDS DB instance will only result in unencrypted Read Replicas. You can't have an encrypted Read Replica of an unencrypted DB instance</p>", "<p>You can restore an unencrypted Aurora DB cluster snapshot to an encrypted Aurora DB cluster</p>", "<p>To copy an encrypted snapshot from one AWS Region to another, you must specify the KMS key identifier of the destination AWS Region</p>", "<p>You can restore an unencrypted backup or snapshot of an Amazon RDS DB instance to an encrypted DB instance by specifying the KMS key identifier</p>", "<p>Since Read Replica of an Amazon RDS encrypted instance is also encrypted using the same key as the primary DB instance, creating Read Replicas in a different Region is not possible</p>"]}, "correct_response": ["b", "c", "d"], "section": "Database Security", "question_plain": "A multi-national company has just floated their latest security policy. As per this policy, data needs to be stored in encrypted form only. The company uses different Amazon RDS solutions for their various products. As a Database Specialist, you are roped in to facilitate the learning of RDS encryption features and configurations.\n\nWhich of the following statements would you identify as correct for the given context? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823020, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The technology team at a Wall Street trading firm uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.</p>\n\n<p>As a Database Specialist, which of the following actions would you recommend to make sure that only the last updated value of any item is used in the application?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use ConsistentRead = true while doing GetItem operation for any item</strong></p>\n\n<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>\n\n<p>Eventually Consistent Reads</p>\n\n<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>\n\n<p>Strongly Consistent Reads</p>\n\n<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.</p>\n\n<p>DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use ConsistentRead = true while doing UpdateItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = true while doing PutItem operation for any item</strong></p>\n\n<p><strong>Use ConsistentRead = false while doing PutItem operation for any item</strong></p>\n\n<p>As mentioned in the explanation above, strongly consistent reads apply only while using the read operations (such as GetItem, Query, and Scan). So these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html</a></p>\n", "answers": ["<p>Use ConsistentRead = true while doing GetItem operation for any item</p>", "<p>Use ConsistentRead = true while doing UpdateItem operation for any item</p>", "<p>Use ConsistentRead = true while doing PutItem operation for any item</p>", "<p>Use ConsistentRead = false while doing PutItem operation for any item</p>"]}, "correct_response": ["a"], "section": "Database Security", "question_plain": "The technology team at a Wall Street trading firm uses DynamoDB to facilitate high-frequency trading where multiple trades can try and update an item at the same time.\n\nAs a Database Specialist, which of the following actions would you recommend to make sure that only the last updated value of any item is used in the application?", "related_lectures": []}, {"_class": "assessment", "id": 56823022, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company is migrating its infrastructure from the on-premises data-center to AWS Cloud. The company wants to deploy its two-tier application with the EC2 instance based web servers in a public subnet and PostGreSQL RDS based database layer in a private subnet. The company wants to ensure that the database access credentials used by the web servers are handled securely as well as these credentials are changed every 90 days in an automated way using an off-the-shelf solution.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend for the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q57-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the database access credentials as the EC2 instance user data. Configure the application web servers to retrieve the credentials from the user data while bootstrapping. Write custom code to change the database access credentials stored in the user data after 90 days</strong> - The given use-case mandates that the database secrets are rotated every 90 days using an off-the-shelf solution in an automated way. Writing custom code to change these credentials after 90 days violates this key requirement.</p>\n\n<p><strong>Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong></p>\n\n<p><strong>Store the database access credentials in a KMS encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</strong></p>\n\n<p>A key requirement of the use-case is to automate the database access secrets rotation every 90 days using an off-the-shelf solution. Both these options involve writing custom code to change the database access credentials after 90 days. In addition, storing database access credentials in an external file (even if encrypted) is not a best practice, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n", "answers": ["<p>Store the database access credentials in an SSE-S3 encrypted text file on S3. Configure the application web servers to retrieve the credentials from S3 on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</p>", "<p>Store the database access credentials as the EC2 instance user data. Configure the application web servers to retrieve the credentials from the user data while bootstrapping. Write custom code to change the database access credentials stored in the user data after 90 days</p>", "<p>Use AWS Secrets Manager to store the database access credentials with the rotation interval configured to 90 days. Set up the application web servers to retrieve the credentials from the Secrets Manager</p>", "<p>Store the database access credentials in a KMS encrypted text file on EFS. Configure the application web servers to retrieve the credentials from EFS on system boot. Write custom code to change the database access credentials stored on the encrypted file after 90 days</p>"]}, "correct_response": ["c"], "section": "Database Security", "question_plain": "A retail company is migrating its infrastructure from the on-premises data-center to AWS Cloud. The company wants to deploy its two-tier application with the EC2 instance based web servers in a public subnet and PostGreSQL RDS based database layer in a private subnet. The company wants to ensure that the database access credentials used by the web servers are handled securely as well as these credentials are changed every 90 days in an automated way using an off-the-shelf solution.\n\nAs a Database Specialist, which of the following solutions would you recommend for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 56823024, "assessment_type": "multi-select", "prompt": {"question": "<p>The DevOps team at an IT company created an AWS CloudFormation stack with an RDS DB instance. An intern has accidentally deleted the stack and most of the recent data has been lost. You have been hired as an AWS Certified Database Specialist to configure\nRDS settings to the CloudFormation template to mitigate such accidental instance data loss in the future.</p>\n\n<p>Which of the following settings will address this use-case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>Here are some of the ways to prevent deletion or updates to resources in an AWS CloudFormation stack:</p>\n\n<p>Set the DeletionPolicy attribute to prevent the deletion of an individual resource at the stack level.</p>\n\n<p>Use AWS Identity and Access Management (IAM) policies to restrict the ability of users to delete or update a stack and its resources.</p>\n\n<p>Enable termination protection to prevent users from deleting the stack from the AWS CloudFormation console or AWS Command Line Interface (AWS CLI).</p>\n\n<p><strong>Enable Termination Protection section of the CloudFormation stack via the AWS Management Console</strong> - You can prevent a stack from being accidentally deleted by enabling termination protection on the stack. If a user attempts to delete a stack with termination protection enabled, the deletion fails and the stack--including its status--remains unchanged. To enable termination protection when creating a stack, you need to set the value in the Termination Protection section to \"Enable\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q58-i1a.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>You may get CloudFormation YAML/JSON template configuration options such as use TerminationProtection or DeletionProtection (notice that these are in Pascal case and different from the configuration section names shown on the AWS Management Console). In this case, you must select the option that sets DeletionProtection to True like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q58-i1b.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><strong>Configure DeleteAutomatedBackups to False</strong> - You can use DeleteAutomatedBackups to remove automated backups immediately after the DB instance is deleted. This parameter isn't case-sensitive. The default is to remove automated backups immediately after the DB instance is deleted. You can set this value to False to keep the automated backups even after the database is deleted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q58-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DeleteDBInstance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DeleteDBInstance.html</a></p>\n\n<p><strong>Configure DeletionPolicy to Retain</strong> - Using the DeletionPolicy attribute, you can preserve a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify Retain for that resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q58-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Deletion Protection section of the CloudFormation stack via the AWS Management Console</strong> - This has been added as a distractor. There is no such thing as Deletion Protection section for a CloudFormation stack on the AWS Management Console. You can use the Termination Protection section and set the value to \"Enable\" for ensuring that the stack is not deleted.</p>\n\n<p><strong>Configure Read Replica to True</strong></p>\n\n<p><strong>Configure Multi-AZ to True</strong></p>\n\n<p>These two options have been added as distractors since Read Replicas or Multi-AZ have no bearing on preventing accidental data loss for a CloudFormation stack.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DeleteDBInstance.html\">https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DeleteDBInstance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n", "answers": ["<p>Enable Termination Protection section of the CloudFormation stack via the AWS Management Console</p>", "<p>Configure DeleteAutomatedBackups to False</p>", "<p>Configure DeletionPolicy to Retain</p>", "<p>Configure Multi-AZ to True</p>", "<p>Enable Deletion Protection section of the CloudFormation stack via the AWS Management Console</p>", "<p>Configure Read Replica to True</p>"]}, "correct_response": ["a", "b", "c"], "section": "Refactoring", "question_plain": "The DevOps team at an IT company created an AWS CloudFormation stack with an RDS DB instance. An intern has accidentally deleted the stack and most of the recent data has been lost. You have been hired as an AWS Certified Database Specialist to configure\nRDS settings to the CloudFormation template to mitigate such accidental instance data loss in the future.\n\nWhich of the following settings will address this use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56823026, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A CRM company uses a two-tier architecture with application servers in the public subnet and an RDS based database in a private subnet. The database management team is able to use a bastion host in the public subnet to log in to MySQL and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the database team notices several \"could not connect to server: connection timed out\" error messages.</p>\n\n<p>As a Database Specialist, which of the following would you identify as the root cause behind this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</strong></p>\n\n<p>You should use security groups to control the inbound and outbound traffic for your DB instance. For your application servers, create a security group with inbound rules that use the IP addresses of the client application as the source. This security group allows your client application to connect to your application servers. Then create a second security group for your DB instance and create a new rule by specifying the security group that you created earlier as the source for this DB specific security group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p>Please follow these troubleshooting guidelines for resolving RDS DB connection issues:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</strong> - As mentioned in the explanation above, the application servers don't need inbound connections from the DB instance, rather the DB instance needs the correct inbound rule with application servers' security group as the source.</p>\n\n<p><strong>The database user credentials (username and password) configured for the application are incorrect</strong></p>\n\n<p><strong>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</strong></p>\n\n<p>These two options have been added as a distractor since the error mentions a \"connection timeout\" issue rather than an \"access denied\" error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-cannot-connect/</a></p>\n", "answers": ["<p>The security group configuration for the DB instance does not have the correct rules to allow inbound connections from the application servers</p>", "<p>The database user credentials (username and password) configured for the application are incorrect</p>", "<p>The security group configuration for the application servers does not have the correct rules to allow inbound connections from the DB instance</p>", "<p>The database user credentials (username and password) configured for the application do not have the required privilege for the given database</p>"]}, "correct_response": ["a"], "section": "Database Security", "question_plain": "A CRM company uses a two-tier architecture with application servers in the public subnet and an RDS based database in a private subnet. The database management team is able to use a bastion host in the public subnet to log in to MySQL and run queries from the bastion host. However, end-users are reporting application errors. Upon inspecting application logs, the database team notices several \"could not connect to server: connection timed out\" error messages.\n\nAs a Database Specialist, which of the following would you identify as the root cause behind this issue?", "related_lectures": []}, {"_class": "assessment", "id": 56823028, "assessment_type": "multi-select", "prompt": {"question": "<p>A multi-national retail company is planning to move their on-premises database to Amazon RDS. As an AWS Certified Database Specialist, you have been roped in to understand and weigh-in on the replication features of RDS.</p>\n\n<p>Which of the following statements would you identify as correct for the given context? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon RDS for MySQL allows you to create a second-tier read replica from an existing first-tier Read Replica</strong> - Amazon Aurora, Amazon RDS for MySQL and MariaDB allow you to create a second-tier read replica from an existing first-tier read replica.</p>\n\n<p>More on Second-tier Read Replicas:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q60-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p><strong>You can configure RDS source DB instance as a Multi-AZ deployment to avoid I/O suspension that occurs when a Read Replica is initiated</strong></p>\n\n<p>When you initiate the creation of a Read Replica, Amazon RDS takes a snapshot of your source DB instance and begins replication. As a result, you will experience a brief I/O suspension on your source DB instance as the snapshot occurs. The I/O suspension typically lasts on the order of one minute, and is avoided if the source DB instance is a Multi-AZ deployment (in the case of Multi-AZ deployments, snapshots are taken from the standby).</p>\n\n<p>RDS source DB instance as a Multi-AZ deployment:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q60-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon RDS for PostgreSQL allows you to create a second-tier Read Replica from an existing first-tier Read Replica</strong> - Amazon RDS for PostgreSQL, Oracle and SQL Server do not support Read Replicas of Read Replicas.</p>\n\n<p><strong>When running an RDS DB instance as a Multi-AZ deployment, the standby DB can be used for read or write operations</strong> - This is an incorrect option. A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits.</p>\n\n<p><strong>If you delete an Amazon RDS for MySQL DB Instance that has Read Replicas, all Read Replicas will be promoted to standalone DB Instances</strong> - This is an incorrect statement. An Amazon RDS for MySQL or MariaDB read replica will stay active and continue accepting read traffic even after its corresponding source DB instance has been deleted. If you delete an Amazon RDS for PostgreSQL DB Instance that has Read Replicas, all Read Replicas will be promoted to standalone DB Instances and will be able to accept both read and write traffic.</p>\n\n<p>Deleting RDS Read Replicas for different database engines:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q60-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n", "answers": ["<p>Amazon RDS for MySQL allows you to create a second-tier read replica from an existing first-tier Read Replica</p>", "<p>You can configure RDS source DB instance as a Multi-AZ deployment to avoid I/O suspension that occurs when a Read Replica is initiated</p>", "<p>Amazon RDS for PostgreSQL allows you to create a second-tier Read Replica from an existing first-tier Read Replica</p>", "<p>When running an RDS DB instance as a Multi-AZ deployment, the standby DB can be used for read or write operations</p>", "<p>If you delete an Amazon RDS for MySQL DB Instance that has Read Replicas, all Read Replicas will be promoted to standalone DB Instances</p>"]}, "correct_response": ["a", "b"], "section": "Management and Operations", "question_plain": "A multi-national retail company is planning to move their on-premises database to Amazon RDS. As an AWS Certified Database Specialist, you have been roped in to understand and weigh-in on the replication features of RDS.\n\nWhich of the following statements would you identify as correct for the given context? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56823030, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The technology team at a retail organization is running batch workloads on AWS Cloud. The team has embedded RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to meet this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Secrets Manager</strong></p>\n\n<p>AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.</p>\n\n<p>Benefits of Secrets Manager:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q61-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSM Parameter Store</strong> - AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. SSM Parameter Store cannot be used to automatically rotate the database credentials.</p>\n\n<p><strong>Systems Manager</strong> - AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Systems Manager cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p><strong>KMS</strong> - AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. KMS cannot be used to store your secrets securely and automatically rotate the database credentials.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a></p>\n", "answers": ["<p>SSM Parameter Store</p>", "<p>Secrets Manager</p>", "<p>Systems Manager</p>", "<p>KMS</p>"]}, "correct_response": ["b"], "section": "Database Security", "question_plain": "The technology team at a retail organization is running batch workloads on AWS Cloud. The team has embedded RDS database connection strings within each web server hosting the flagship application. After failing a security audit, the team is looking at a different approach to store the database secrets securely and automatically rotate the database credentials.\n\nAs a Database Specialist, which of the following solutions would you recommend to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 56823032, "assessment_type": "multi-select", "prompt": {"question": "<p>A media company has recently migrated their technology infrastructure to AWS Cloud. The database team is centralizing database access credentials to align with IAM based authentication.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p><strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS SQL Server</strong></p>\n\n<p>These two options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p><strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n", "answers": ["<p>RDS Oracle</p>", "<p>RDS Db2</p>", "<p>RDS MySQL</p>", "<p>RDS PostGreSQL</p>", "<p>RDS SQL Server</p>"]}, "correct_response": ["c", "d"], "section": "Database Security", "question_plain": "A media company has recently migrated their technology infrastructure to AWS Cloud. The database team is centralizing database access credentials to align with IAM based authentication.\n\nWhich of the following AWS database engines can be configured with IAM Database Authentication? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56823034, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Silicon Valley based startup is experimenting with DynamoDB in its new test environment. The database team has discovered that some of the write operations have been overwriting existing items which have that specific primary key. This has corrupted their data leading to data discrepancies.</p>\n\n<p>As a Database Specialist, which DynamoDB write option would you select to prevent this kind of overwriting?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Conditional writes</strong> - DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.</p>\n\n<p>For example, you might want a PutItem operation to succeed only if there is no other item with that same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. This is the right choice for the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Batch writes</strong> - Bath operations (read and write) help reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Applications benefit from this parallelism without having to manage concurrency or threading. But, this is of no use in the current scenario of overwriting changes.</p>\n\n<p><strong>Atomic Counters</strong> - Atomic Counters is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. You might use an atomic counter to track the number of visitors to a website. This functionality is not useful for the current scenario.</p>\n\n<p><strong>Use Scan operation</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. This is given as a distractor and not related to DynamoDB item updates.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate</a></p>\n", "answers": ["<p>Batch writes</p>", "<p>Conditional writes</p>", "<p>Atomic Counters</p>", "<p>Use Scan operation</p>"]}, "correct_response": ["b"], "section": "Database Security", "question_plain": "A Silicon Valley based startup is experimenting with DynamoDB in its new test environment. The database team has discovered that some of the write operations have been overwriting existing items which have that specific primary key. This has corrupted their data leading to data discrepancies.\n\nAs a Database Specialist, which DynamoDB write option would you select to prevent this kind of overwriting?", "related_lectures": []}, {"_class": "assessment", "id": 56823036, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IT company has a three-year contract with a health-care provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit for backup retention for automated backups on Amazon Relational Database Service (RDS) does not meet your requirements.</p>\n\n<p>Which of the following solutions can help you meet your requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a schedule based CloudWatch Events rule that invokes an AWS Lambda function that further triggers the database snapshot</strong> - There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select \u201cSchedule\u201d as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select \u201cLambda Function\u201d as the Target. Your Lambda will have the necessary code for snapshot functionality.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable RDS automatic backups</strong> - You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.</p>\n\n<p><strong>Enable RDS Read replicas</strong> - Amazon RDS server's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy read-only data workloads. These are not suitable for the given use-case.</p>\n\n<p><strong>Enable RDS Multi-AZ</strong> - Multi-AZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html</a></p>\n", "answers": ["<p>Create a schedule based CloudWatch Events rule that invokes an AWS Lambda function that further triggers the database snapshot</p>", "<p>Enable RDS automatic backups</p>", "<p>Enable RDS Read replicas</p>", "<p>Enable RDS Multi-AZ</p>"]}, "correct_response": ["a"], "section": "Database Security", "question_plain": "An IT company has a three-year contract with a health-care provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit for backup retention for automated backups on Amazon Relational Database Service (RDS) does not meet your requirements.\n\nWhich of the following solutions can help you meet your requirements?", "related_lectures": []}, {"_class": "assessment", "id": 56823038, "assessment_type": "multi-select", "prompt": {"question": "<p>As a Database Specialist, you have been asked to implement a disaster recovery strategy for all the Amazon RDS databases that a company owns.</p>\n\n<p>Which of the following points do you need to consider for creating a robust recovery plan (Select three)?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Recovery time objective (RTO) represents the number of hours it takes, to return the  Amazon RDS database to a working state after a disaster</strong> - Recovery time objective (RTO) and recovery point objective (RPO) are two key metrics to consider when developing a DR plan. RTO represents how many hours it takes you to return to a working state after a disaster.</p>\n\n<p>More info on RTO and RPO:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q65-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n\n<p><strong>Automated backups, manual snapshots and Read Replicas are supported across multiple Regions</strong> - Automated backups, manual snapshots and Read Replicas are all supported across multiple Regions.</p>\n\n<p><strong>Database snapshots are user-initiated backups of your complete DB instance that serve as full backups. These snapshots can be copied and shared to different Regions and accounts</strong> - Database snapshots are manual (user-initiated) backups of your complete DB instance that serve as full backups. They\u2019re stored in Amazon S3 and are retained until you explicitly delete them. These snapshots can be copied and shared to different Regions and accounts. Because DB snapshots include the entire DB instance, including data files and temporary files, the size of the instance affects the amount of time it takes to create the snapshot.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Recovery time objective (RTO), expressed in hours, represents how much data you could lose when a disaster happens</strong> - RTO represents how many hours it takes you to return to a working state after a disaster. RPO, which is also expressed in hours, represents how much data you could lose when a disaster happens.</p>\n\n<p><strong>You can share automated Amazon RDS snapshots with up to 20 AWS accounts</strong> - This statement is incorrect. Amazon RDS enables you to share DB snapshots or cluster snapshots with other AWS accounts. You can share manual DB snapshots with up to 20 AWS accounts. Automated Amazon RDS snapshots cannot be shared directly with other AWS accounts.</p>\n\n<p><strong>Similar to an Amazon RDS Multi-AZ configuration, failover to a Read Replica is an automated process that requires no manual intervention after initial configurations</strong> - Unlike an Amazon RDS Multi-AZ configuration, failover to a Read Replica is not an automated process. If you are using cross-Region Read Replicas, you should be certain that you want to switch your AWS resources between Regions. Cross-Region traffic can experience latency, and reconfiguring applications can be complicated.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/\">https://aws.amazon.com/about-aws/whats-new/2021/07/amazon-rds-cross-region-automated-backups-regional-expansion/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n", "answers": ["<p>Recovery time objective (RTO) represents the number of hours it takes, to return the Amazon RDS database to a working state after a disaster</p>", "<p>Recovery time objective (RTO), expressed in hours, represents how much data you could lose when a disaster happens</p>", "<p>Automated backups, manual snapshots and Read Replicas are supported across multiple Regions</p>", "<p>Database snapshots are user-initiated backups of your complete DB instance that serve as full backups. These snapshots can be copied and shared to different Regions and accounts</p>", "<p>You can share automated Amazon RDS snapshots with up to 20 AWS accounts</p>", "<p>Similar to an Amazon RDS Multi-AZ configuration, failover to a Read Replica is an automated process that requires no manual intervention after initial configurations</p>"]}, "correct_response": ["a", "c", "d"], "section": "Management and Operations", "question_plain": "As a Database Specialist, you have been asked to implement a disaster recovery strategy for all the Amazon RDS databases that a company owns.\n\nWhich of the following points do you need to consider for creating a robust recovery plan (Select three)?", "related_lectures": []}]}
4992420
~~~
{"count": 10, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 32470028, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old</p>", "<p>Add a created_at attribute in the table and then use a cron job on EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute</p>", "<p>Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute</p>", "<p>Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table</p>"], "question": "<p>A gaming company maintains a staging environment for its flagship application which uses a DynamoDB table to keep track of gaming history for the players. This data needs to be kept for only a week and then it can be deleted. The IT manager has noticed that the table has several months of data in the table. The company wants to implement a cost-effective solution to keep only the latest week's data in the table.</p>\n\n<p>Which of the following solutions requires the MINIMUM development effort and on-going maintenance?</p>\n", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>\"Add a new attribute in the table to track the expiration time and enable time to live (TTL) on the table\"</p>\n\n<p>You can use DynamoDB Time to Live (TTL) to determine when an item is no longer needed. TTL uses a per-item timestamp to determine items for expiration. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. After you enable TTL on a table, a per-partition scanner background process automatically and continuously evaluates the expiry status of items in the table.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Add a new attribute in the table to track the expiration time and set up a Glue job to delete items that are more than a week old\" - Although this solution is theoretically possible, but it would require a significant development effort to build and maintain a Glue job to trim the DynamoDB table. There is an additional cost of running the Glue job on a daily basis as well.</p>\n\n<p>\"Add a created_at attribute in the table and then use a cron job on EC2 instance to invoke a Python script daily. The script deletes items older than a week on the basis of this attribute\" - Provisioning an EC2 instance and developing Python script involves significant development effort, maintenance and additional cost. So this option is not correct.</p>\n\n<p>\"Add a created_at attribute in the table and then use a CloudWatch Events rule to invoke a Lambda function daily. The Lambda function deletes items older than a week on the basis of this attribute\" - Custom code needs to be written for the Lambda function to implement the functionality to trim the DynamoDB table. Lambda function execution is also charged separately. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></p>\n", "relatedLectureIds": ""}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "A gaming company maintains a staging environment for its flagship application which uses a DynamoDB table to keep track of gaming history for the players. This data needs to be kept for only a week and then it can be deleted. The IT manager has noticed that the table has several months of data in the table. The company wants to implement a cost-effective solution to keep only the latest week's data in the table.\n\nWhich of the following solutions requires the MINIMUM development effort and on-going maintenance?", "related_lectures": []}, {"_class": "assessment", "id": 32470030, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", "", ""], "relatedLectureIds": "", "question": "<p>A health-care startup wants to leverage ElastiCache for Redis in cluster mode to enhance the performance and scalability of its existing two-tier application architecture. The ElastiCache cluster is configured to listen on port 6379. The company has hired you as an AWS Certified Database Specialist to make sure that the cache data is secure and protected from unauthorized access so that the solution is HIPAA compliant.</p>\n\n<p>Which of the following steps would address the given use-case? (Select three)</p>\n", "answers": ["<p>Enable CloudWatch Logs to monitor the security credentials for the ElastiCache cluster</p>", "<p>Configure the security group for the ElastiCache cluster with the required rules to allow outbound traffic to the cluster's clients on port 6379</p>", "<p>Enable CloudTrail to monitor the API Calls for the ElastiCache cluster</p>", "<p>Configure the ElastiCache cluster to have both in-transit as well as at-rest encryption</p>", "<p>Create the cluster with auth-token parameter and make sure that the parameter is included in all subsequent commands to the cluster</p>", "<p>Configure the security group for the ElastiCache cluster with the required rules to allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379</p>"], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the ElastiCache cluster to have both in-transit as well as at-rest encryption</strong></p>\n\n<p>You can use both in-transit as well as at-rest encryption to guard against unauthorized access of your data on the server. In-transit encryption encrypts your data whenever it is moving from one place to another, such as between nodes in your cluster or between your cluster and your application. At-rest encryption encrypts your on-disk data during sync and backup operations.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p>\n\n<p><strong>Create the cluster with auth-token parameter and make sure that the parameter is included in all subsequent commands to the cluster</strong></p>\n\n<p>Redis authentication tokens enable Redis to require a token (password) before allowing clients to run commands, thereby improving data security. You can require that users enter a token on a token-protected Redis server. You also need to include it in all subsequent commands to the replication group or cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p>\n\n<p><strong>Configure the security group for the ElastiCache cluster with the required rules to allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379</strong></p>\n\n<p>You can create a VPC security group to restrict access to the cluster instances. Configure rules that only allow inbound traffic from the cluster itself as well as from the cluster's clients on port 6379. Typically the ElastiCache cluster is accessed from the web servers running on EC2 instances. You can configure the security groups like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q2-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the security group for the ElastiCache cluster with the required rules to allow outbound traffic to the cluster's clients on port 6379</strong> - As mentioned in the explanation above, you need to create a security group that allows inbound traffic from the cluster itself as well as from the cluster's clients on port 6379. Creating a security group rule that allows outbound traffic from the cluster on port 6379 is not relevant to the use-case.</p>\n\n<p><strong>Enable CloudWatch Logs to monitor the security credentials for the ElastiCache cluster</strong></p>\n\n<p><strong>Enable CloudTrail to monitor the API Calls for the ElastiCache cluster</strong></p>\n\n<p>Both these options are added as distractors since both CloudWatch Logs and CloudTrail can be used for post-facto analysis to ascertain the series of access events relevant to the cluster. These options will not prevent unauthorized access.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html</a></p>\n"}, "correct_response": ["d", "e", "f"], "section": "Workload-Specific Database Design", "question_plain": "A health-care startup wants to leverage ElastiCache for Redis in cluster mode to enhance the performance and scalability of its existing two-tier application architecture. The ElastiCache cluster is configured to listen on port 6379. The company has hired you as an AWS Certified Database Specialist to make sure that the cache data is secure and protected from unauthorized access so that the solution is HIPAA compliant.\n\nWhich of the following steps would address the given use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 32470032, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Initiate a manual failover of the RDS DB primary instance by using reboot with failover</strong></p>\n\n<p>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p>\n\n<p>In the event of a planned or unplanned outage of your DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if you have enabled Multi-AZ. Failover times are typically 60\u2013120 seconds. You can initiate a manual failover of the RDS DB primary instance by using reboot with failover.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the RDS SQL injection query to simulate the primary instance failure</strong> - SQL injection refers to the placement of malicious code in SQL statements via web page input. This has been added as a distractor and is not relevant to the given use-case.</p>\n\n<p><strong>Use the RDS fault injection query to simulate the primary instance failure</strong> - This is a trick option. You can test the fault tolerance of your Aurora DB cluster by using fault injection queries. Fault injection queries are issued as SQL commands to an Aurora instance.</p>\n\n<p>Exam Alert:</p>\n\n<p>Fault injection queries can be only used with Aurora DB cluster and NOT with an RDS DB cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q3-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html</a></p>\n\n<p><strong>Initiate a manual failover of the RDS DB primary instance by terminating the instance</strong> - If you terminate the primary instance then it's gone forever. As the team is just simulating an AZ failure, so the correct action is to use the \"reboot with failover\" option.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Managing.FaultInjectionQueries.html</a></p>\n", "question": "<p>As part of its disaster recovery testing, an IT company would like to simulate an Availability Zone failure for its RDS for MySQL Multi-AZ DB instance. The testing team wants to record how the application reacts during the DB instance failover activity. The company does not want to make any configuration or code changes for this testing.</p>\n\n<p>As a Database Specialist, which of the following solutions would you outline for the company to test this use-case in the SHORTEST time?</p>\n", "answers": ["<p>Use the RDS SQL injection query to simulate the primary instance failure</p>", "<p>Use the RDS fault injection query to simulate the primary instance failure</p>", "<p>Initiate a manual failover of the RDS DB primary instance by using reboot with failover</p>", "<p>Initiate a manual failover of the RDS DB primary instance by terminating the instance</p>"], "relatedLectureIds": ""}, "correct_response": ["c"], "section": "Monitoring and Troubleshooting", "question_plain": "As part of its disaster recovery testing, an IT company would like to simulate an Availability Zone failure for its RDS for MySQL Multi-AZ DB instance. The testing team wants to record how the application reacts during the DB instance failover activity. The company does not want to make any configuration or code changes for this testing.\n\nAs a Database Specialist, which of the following solutions would you outline for the company to test this use-case in the SHORTEST time?", "related_lectures": []}, {"_class": "assessment", "id": 32470034, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An IT company manages its technology infrastructure on the AWS Cloud. The Database administration team at the company wants to migrate its RDS MySQL database to Aurora MySQL database.</p>\n\n<p>As a Database Specialist, which of the following solutions would you suggest so that it does not require any custom development and there is minimum downtime when the company migrates the database?</p>\n", "answers": ["<p>Create a DB snapshot of the RDS MySQL DB and then migrate this snapshot to create an Aurora MySQL DB cluster</p>", "<p>Use AWS Glue to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database</p>", "<p>Create a clone of the RDS MySQL DB instance and then promote the clone into a standalone Aurora DB cluster</p>", "<p>Create an Aurora Replica from the RDS MySQL DB instance and then promote the Replica into a standalone Aurora DB cluster</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an Aurora Replica from the RDS MySQL DB instance and then promote the Replica into a standalone Aurora DB cluster</strong></p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.</p>\n\n<p>You can use Aurora Read Replicas to migrate from an Amazon RDS DB Instance for MySQL to Amazon Aurora. The migration process begins by creating a DB snapshot of the existing DB Instance and then using it as the basis for a fresh Aurora Read Replica. After the replica has been set up, replication is used to bring it up to date with respect to the source. Once the replication lag drops to 0, the replication is complete. At this point, you can make the Aurora Read Replica into a standalone Aurora DB cluster and point your client applications at it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q4-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/\">https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a DB snapshot of the RDS MySQL DB and then migrate this snapshot to create an Aurora MySQL DB cluster</strong> - You can migrate a DB snapshot of an Amazon RDS MySQL DB instance to create an Aurora MySQL DB cluster. The new Aurora MySQL DB cluster is populated with the data from the original Amazon RDS MySQL DB instance. However, to make sure that the entire data is written from RDS MySQL DB to Aurora MySQL DB, you will have to stop the MySQL DB and then take a snapshot. This implies there will be a downtime for the RDS MySQL DB, so this option is incorrect.</p>\n\n<p><strong>Use AWS Glue to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database</strong> - Although AWS Glue can be used to create an ETL job to migrate the RDS MySQL database to Aurora MySQL database, however the ETL script has to be custom developed. Therefore this option is incorrect.</p>\n\n<p><strong>Create a clone of the RDS MySQL DB instance and then promote the clone into a standalone Aurora DB cluster</strong> - Using database cloning in Amazon Aurora, you can quickly and cost-effectively create clones of all of the databases within an Aurora DB cluster.</p>\n\n<p>Exam Alert:</p>\n\n<p>Database cloning is only available for Amazon Aurora DB and not for RDS DB.</p>\n\n<p>So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/\">https://aws.amazon.com/blogs/aws/new-create-an-amazon-aurora-read-replica-from-a-mysql-db-instance/</a></p>\n"}, "correct_response": ["d"], "section": "Deployment and Migration", "question_plain": "An IT company manages its technology infrastructure on the AWS Cloud. The Database administration team at the company wants to migrate its RDS MySQL database to Aurora MySQL database.\n\nAs a Database Specialist, which of the following solutions would you suggest so that it does not require any custom development and there is minimum downtime when the company migrates the database?", "related_lectures": []}, {"_class": "assessment", "id": 32470036, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to a customer managed key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to a customer managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>", "<p>Add the target account to the default AWS KMS key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</p>"], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the target account to a customer managed key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account</strong> - You can't share a snapshot that's encrypted using the default AWS KMS encryption key.</p>\n\n<p>To share an encrypted Amazon RDS DB snapshot:</p>\n\n<p>Add the target account to a custom (non-default) KMS key.</p>\n\n<p>Copy the snapshot using the customer managed key, and then share the snapshot with the target account.</p>\n\n<p>Copy the shared DB snapshot from the target account.</p>\n\n<p>Detailed steps to implement this solution:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a></p>\n\n<p>Incorrect options:</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to a customer managed key. Copy the snapshot using the default AWS KMS key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>\"Add the target account to the default AWS KMS key. Copy the snapshot using the customer managed key, and then share the snapshot with the target account. Copy the shared DB snapshot from the target account\"</p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/\">https://aws.amazon.com/premiumsupport/knowledge-center/share-encrypted-rds-snapshot-kms-key/</a></p>\n", "feedbacks": ["", "", "", ""], "relatedLectureIds": "", "question": "<p>A multi-national retail company uses separate AWS accounts for their business units. The finance team has an encrypted snapshot of an Amazon Relational Database Service (Amazon RDS) instance that uses the default AWS Key Management Service (AWS KMS) key. The finance team wants to share the encrypted snapshot with their Audit team that uses another AWS account.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to address the given use-case?</p>\n"}, "correct_response": ["b"], "section": "Management and Operations", "question_plain": "A multi-national retail company uses separate AWS accounts for their business units. The finance team has an encrypted snapshot of an Amazon Relational Database Service (Amazon RDS) instance that uses the default AWS Key Management Service (AWS KMS) key. The finance team wants to share the encrypted snapshot with their Audit team that uses another AWS account.\n\nAs a Database Specialist, which of the following solutions would you recommend to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 32470038, "assessment_type": "multiple-choice", "prompt": {"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up multiple read replicas and then the developers can make changes to their own promoted replica instances</strong></p>\n\n<p>Read replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. When you create a read replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica whenever there is a change to the primary DB instance.</p>\n\n<p>For the given use-case, each developer can perform DDL operations for the schema changes on the MySQL read replica once the read replica is in sync with its primary DB instance. Then the developer can promote the read replica and direct the application to use the promoted instance during the development phase. This solution isolates the schema changes done by each developer to their own promoted instance. This also avoids the problem of keeping track of the \"correct restore point\" that the team faced while using the same DB instance.</p>\n\n<p>If you need to make changes to the MySQL or MariaDB read replica, you must set the read_only parameter to 0 in the DB parameter group for the read replica. You can then perform all needed DDL operations, such as creating indexes, on the read replica. Actions taken on the read replica don't affect the performance of the primary DB instance.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate to Amazon Aurora MySQL and enable the Aurora Backtrack feature</strong> - Although you could migrate to Aurora MySQL DB cluster which supports the Backtrack feature, however the problem of keeping track of the \"correct checkpoint\" would persist if the team continues using the same DB instance. As mentioned above, the right solution is to have the developers set up their own replica instances for the development phase.</p>\n\n<p><strong>Enable the RDS for MySQL Backtrack feature and then the developers can backtrack to a pre-defined checkpoint</strong></p>\n\n<p><strong>Enable the RDS for MySQL Clone feature and then the developers can make changes to their own cloned instances</strong></p>\n\n<p>Exam Alert:</p>\n\n<p>The Backtrack and Clone features are only supported by Aurora DB clusters and NOT by the RDS DB clusters.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n", "feedbacks": ["", "", "", ""], "relatedLectureIds": "", "question": "<p>A gaming company is using RDS MySQL DB to re-engineer its flagship application. The development team needs to restore its MySQL database whenever a developer makes a mistake in the schema updates. As a result, the entire development team needs to wait hours for the DB restore to complete. The issue is further compounded as multiple team members are collaborating on the same project, thereby making it hard to find the right restore point for each mistake.</p>\n\n<p>As a Database Specialist, which of the following solutions would you recommend to reduce the downtime during the development phase?</p>\n", "answers": ["<p>Set up multiple read replicas and then the developers can make changes to their own promoted replica instances</p>", "<p>Enable the RDS for MySQL Backtrack feature and then the developers can backtrack to a pre-defined checkpoint</p>", "<p>Enable the RDS for MySQL Clone feature and then the developers can make changes to their own cloned instances</p>", "<p>Migrate to Amazon Aurora MySQL and enable the Aurora Backtrack feature</p>"]}, "correct_response": ["a"], "section": "Workload-Specific Database Design", "question_plain": "A gaming company is using RDS MySQL DB to re-engineer its flagship application. The development team needs to restore its MySQL database whenever a developer makes a mistake in the schema updates. As a result, the entire development team needs to wait hours for the DB restore to complete. The issue is further compounded as multiple team members are collaborating on the same project, thereby making it hard to find the right restore point for each mistake.\n\nAs a Database Specialist, which of the following solutions would you recommend to reduce the downtime during the development phase?", "related_lectures": []}, {"_class": "assessment", "id": 32470040, "assessment_type": "multi-select", "prompt": {"answers": ["<p>Multi-AZ deployments for RDS MySQL follow synchronous replication whereas Multi-AZ deployments for Aurora MySQL follow asynchronous replication</p>", "<p>Read Replicas can be manually promoted to a standalone database instance for RDS MySQL whereas Read Replicas for Aurora MySQL can be promoted to the primary instance</p>", "<p>The primary and standby DB instances are upgraded at the same time for RDS MySQL Multi-AZ. All instances are upgraded at the same time for Aurora MySQL</p>", "<p>Database engine version upgrades happen on primary for Aurora MySQL whereas all instances are updated together for RDS MySQL</p>", "<p>Read Replicas can be manually promoted to a standalone database instance for Aurora MySQL whereas Read Replicas for RDS MySQL can be promoted to the primary instance</p>", "<p>Multi-AZ deployments for Aurora MySQL follow synchronous replication whereas Multi-AZ deployments for RDS MySQL follow asynchronous replication</p>"], "feedbacks": ["", "", "", "", "", ""], "relatedLectureIds": "", "question": "<p>A Silicon Valley based unicorn startup is moving its IT operations from an on-premises data center to AWS Cloud. The database team at the startup is evaluating the Multi-AZ and Read Replica capabilities of RDS MySQL vs Aurora MySQL before they implement the solution in their production environment. The startup has hired you as an AWS Certified Database Specialist to provide a detailed report on this technical requirement.</p>\n\n<p>Which of the following would you identify as correct regarding the given use-case? (Select three)</p>\n", "explanation": "<p>Correct options:</p>\n\n<p><strong>Multi-AZ deployments for RDS MySQL follow synchronous replication whereas Multi-AZ deployments for Aurora MySQL follow asynchronous replication</strong></p>\n\n<p><strong>Read Replicas can be manually promoted to a standalone database instance for RDS MySQL whereas Read Replicas for Aurora MySQL can be promoted to the primary instance</strong></p>\n\n<p><strong>The primary and standby DB instances are upgraded at the same time for RDS MySQL Multi-AZ. All instances are upgraded at the same time for Aurora MySQL</strong></p>\n\n<p>RDS Read replicas are a special type of DB instances that make use of the built-in replication functionality for RDS. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. When you create a read replica, you first specify an existing DB instance as the source. Then Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot. Amazon RDS then uses the asynchronous replication method for the DB engine to update the read replica whenever there is a change to the primary DB instance.</p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance and to Aurora Replicas in the DB cluster.</p>\n\n<p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p>\n\n<p>To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance. There is a brief interruption during which read and write requests made to the primary instance fail with an exception, and the Aurora Replicas are rebooted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>For RDS MySQL in Multi-AZ configuration, database engine version upgrades happen on both the primary and standby DB instances at the same time. For Aurora MySQL, all instances are upgraded at the same time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q7-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Database engine version upgrades happen on primary for Aurora MySQL whereas all instances are updated together for RDS MySQL</strong></p>\n\n<p><strong>Read Replicas can be manually promoted to a standalone database instance for Aurora MySQL whereas Read Replicas for RDS MySQL can be promoted to the primary instance</strong></p>\n\n<p><strong>Multi-AZ deployments for Aurora MySQL follow synchronous replication whereas Multi-AZ deployments for RDS MySQL follow asynchronous replication</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html</a></p>\n"}, "correct_response": ["a", "b", "c"], "section": "Workload-Specific Database Design", "question_plain": "A Silicon Valley based unicorn startup is moving its IT operations from an on-premises data center to AWS Cloud. The database team at the startup is evaluating the Multi-AZ and Read Replica capabilities of RDS MySQL vs Aurora MySQL before they implement the solution in their production environment. The startup has hired you as an AWS Certified Database Specialist to provide a detailed report on this technical requirement.\n\nWhich of the following would you identify as correct regarding the given use-case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 32470042, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "feedbacks": ["", "", "", ""], "answers": ["<p>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</p>", "<p>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</p>", "<p>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</p>", "<p>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</p>"], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</strong></p>\n\n<p>You can use AWS DMS data validation to ensure that your data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches. In addition, for a CDC-enabled task, AWS DMS compares the incremental changes and reports any mismatches. As part of data validation, DMS compares each row in the source with its corresponding row at the target and verifies that those rows contain the same data. For this comparison, DMS issues appropriate queries to retrieve the data. These queries consume additional resources at the source and the target as well as additional network resources.</p>\n\n<p>DMS data validation overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</strong> - You can use table metrics to capture statistics such as insert, update, delete, and DDL statements completed for the tables being migrated. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q8-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n\n<p><strong>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</strong> - A premigration assessment evaluates specified components of a database migration task to help identify any problems that might prevent a migration task from running as expected. This assessment gives you a chance to identify issues before you run a new or modified task. You can then fix problems before they occur while running the migration task itself. This can avoid delays in completing a given database migration needed to repair data and your database environment. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><strong>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</strong> - You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. You can convert relational OLTP schema or data warehouse schema. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n", "question": "<p>A multi-national retail company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired an AWS Certified Database Specialist to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and the Database Specialist must validate that the data was migrated accurately from the source to the target before the cutover.</p>\n\n<p>Which of the following solutions will MOST effectively address this use-case?</p>\n"}, "correct_response": ["b"], "section": "Monitoring and Troubleshooting", "question_plain": "A multi-national retail company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired an AWS Certified Database Specialist to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and the Database Specialist must validate that the data was migrated accurately from the source to the target before the cutover.\n\nWhich of the following solutions will MOST effectively address this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 32470044, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "relatedLectureIds": "", "explanation": "<p>Correct option:</p>\n\n<p><strong>The large-sized Aurora Replica will be promoted</strong></p>\n\n<p>Aurora Replicas are independent endpoints in an Aurora DB cluster, best used for scaling read operations and increasing availability. Up to 15 Aurora Replicas can be distributed across the Availability Zones that a DB cluster spans within an AWS Region. The DB cluster volume is made up of multiple copies of the data for the DB cluster. However, the data in the cluster volume is represented as a single, logical volume to the primary instance as well as to the Aurora Replicas in the DB cluster.</p>\n\n<p>To increase availability, you can use Aurora Replicas as failover targets. That is, if the primary instance fails, an Aurora Replica is promoted to the primary instance. There is a brief interruption during which read and write requests made to the primary instance fail with an exception, and the Aurora Replicas are rebooted. If your Aurora DB cluster doesn't include any Aurora Replicas, then your DB cluster will be unavailable for the duration it takes your DB instance to recover from the failure event.</p>\n\n<p>If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is largest in size.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dbs-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>None of the Replicas will be promoted and Aurora will spin up a new primary instance</strong></p>\n\n<p><strong>One of the medium-sized Aurora Replicas will be chosen randomly for promotion</strong></p>\n\n<p><strong>The small-sized Aurora Replica will be promoted</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance</a></p>\n", "answers": ["<p>None of the Replicas will be promoted and Aurora will spin up a new primary instance</p>", "<p>One of the medium-sized Aurora Replicas will be chosen randomly for promotion</p>", "<p>The small-sized Aurora Replica will be promoted</p>", "<p>The large-sized Aurora Replica will be promoted</p>"], "question": "<p>A social media company is setting up an Aurora DB cluster with one primary instance and four Aurora Replicas for its flagship application. The Aurora DB cluster has one medium-sized primary instance, one large-sized Replica, one small-sized Replica and two medium-sized Replicas. The database administrator has not assigned a promotion tier to the Replicas.</p>\n\n<p>In case the primary instance fails, which of the following events will occur?</p>\n"}, "correct_response": ["d"], "section": "Management and Operations", "question_plain": "A social media company is setting up an Aurora DB cluster with one primary instance and four Aurora Replicas for its flagship application. The Aurora DB cluster has one medium-sized primary instance, one large-sized Replica, one small-sized Replica and two medium-sized Replicas. The database administrator has not assigned a promotion tier to the Replicas.\n\nIn case the primary instance fails, which of the following events will occur?", "related_lectures": []}, {"_class": "assessment", "id": 32470046, "assessment_type": "multi-select", "prompt": {"explanation": "<p>Correct options:</p>\n\n<p><strong>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</strong> - Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.</p>\n\n<p><strong>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</strong> - For a table write to succeed, the provisioned throughput settings for the table and all of its global secondary indexes must have enough write capacity to accommodate the write. Otherwise, the write to the main table itself is throttled.</p>\n\n<p><strong>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</strong> - To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>A Local Secondary Index maintains an alternate primary key for a given partition key value</strong> - This is an invalid statement. A local secondary index maintains an alternate sort key for a given partition key value. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key.</p>\n\n<p><strong>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</strong> - This statement is incorrect. Global secondary indexes support only eventually consistent data model.</p>\n\n<p><strong>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</strong> - This statement is not correct. Each table in DynamoDB can have up to 20 global secondary indexes (default quota) and 5 local secondary indexes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html</a></p>\n", "relatedLectureIds": "", "question": "<p>A Database Specialist is working on modeling data for a DynamoDB production database. To address certain access patterns for the application, he is in the process of creating secondary indexes.</p>\n\n<p>Which of the following options should the database specialist consider for these requirements? (Select three)</p>\n", "answers": ["<p>Applications that need to perform many kinds of queries, using a variety of different attributes for their query criteria should use Global Secondary Indexes</p>", "<p>If the writes are throttled on the Global Secondary Indexes, then the main table will be throttled, even though the Write Capacity Units on the main tables are fine</p>", "<p>A Local Secondary Index maintains an alternate primary key for a given partition key value</p>", "<p>A Global Secondary Index contains a selection of attributes from the base table which are organized by a primary key that is different from that of the table</p>", "<p>Global secondary indexes support both data models - eventually consistent or strongly consistent reads</p>", "<p>For greater query or scan flexibility, you can create up to twenty Local Secondary Indexes per table</p>"], "feedbacks": ["", "", "", "", "", ""]}, "correct_response": ["a", "b", "d"], "section": "Workload-Specific Database Design", "question_plain": "A Database Specialist is working on modeling data for a DynamoDB production database. To address certain access patterns for the application, he is in the process of creating secondary indexes.\n\nWhich of the following options should the database specialist consider for these requirements? (Select three)", "related_lectures": []}]}
