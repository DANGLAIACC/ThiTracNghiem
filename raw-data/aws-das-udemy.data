5450714
~~~
{"count": 10, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 56324352, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The analytics team at an e-commerce company uses Apache Hive on Amazon EMR. Several analysts have reported sub-par performance for the cluster during the morning peak load hours when 95% of the daily queries are executed by the analysts. The analytics team has also noted that HDFS's (Hadoop Distributed File System) usage never surpasses 10%.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following solutions would you recommend to resolve these performance issues?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the instance groups</strong></p>\n\n<p>Apache Hive is an open-source, distributed, fault-tolerant system that provides data warehouse-like query capabilities. It enables users to read, write, and manage petabytes of data using a SQL-like interface. Learn more about Apache Hive here.</p>\n\n<p>Apache Hive is natively supported in Amazon EMR, and you can quickly and easily create managed Apache Hive clusters from the AWS Management Console, AWS CLI, or the Amazon EMR API. When you create a cluster and specify the configuration of the master node, core nodes, and task nodes, you have two configuration options. You can use instance fleets or uniform instance groups.</p>\n\n<p>Apache Hive Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/emr/features/hive/\">https://aws.amazon.com/emr/features/hive/</a></p>\n\n<p>The instance fleets configuration offers the widest variety of provisioning options for Amazon EC2 instances. Each node type has a single instance fleet, and using a task instance fleet is optional. You can specify up to five EC2 instance types per fleet, or 30 EC2 instance types per fleet when you create a cluster using the AWS CLI or Amazon EMR API and an allocation strategy for On-Demand and Spot Instances.</p>\n\n<p>Each Amazon EMR cluster can include up to 50 instance groups: one master instance group that contains one Amazon EC2 instance, a core instance group that contains one or more EC2 instances, and up to 48 optional task instance groups. Each core and task instance group can contain any number of Amazon EC2 instances.</p>\n\n<p>Instance Groups vs Instance Fleets:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html</a></p>\n\n<p>For the given use case, the correct solution should support automatic scaling. You can set up automatic scaling in Amazon EMR for an instance group, adding and removing instances automatically based on the value of an Amazon CloudWatch metric that you specify. The metric <code>YARNMemoryAvailablePercentage</code> represents the percentage of remaining memory available to YARN (YARNMemoryAvailablePercentage = MemoryAvailableMB / MemoryTotalMB). This value is useful for scaling cluster resources based on YARN memory usage.</p>\n\n<p>Amazon EMR metrics:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q1-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong></p>\n\n<p><strong>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the spot fleet</strong></p>\n\n<p>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. Spot fleet is applicable to EC2 instances and cannot be used directly with EMR. So both these options are incorrect. With EMR, you need to use the instance fleet option which does support automatic scaling.</p>\n\n<p><strong>Set up instance group configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the instance groups</strong> - The metric <code>CapacityRemainingGB</code> represents the amount of remaining HDFS disk capacity. It can be used to monitor cluster progress and monitor cluster health. It cannot be used to scale cluster resources. In addition, the use-case states that HDFS usage never surpasses 10%, so this metric cannot be a criterion for right-sizing the cluster.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/emr/features/hive/\">https://aws.amazon.com/emr/features/hive/</a></p>\n", "answers": ["<p>Set up instance group configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the instance groups</p>", "<p>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch YARNMemoryAvailablePercentage metric to configure automatic scaling policies to scale out/scale in the spot fleet</p>", "<p>Set up spot fleet configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the spot fleet</p>", "<p>Set up instance group configurations for core and task nodes. Leverage the CloudWatch CapacityRemainingGB metric to configure automatic scaling policies to scale out/scale in the instance groups</p>"]}, "correct_response": ["a"], "section": "Domain 3: Processing", "question_plain": "The analytics team at an e-commerce company uses Apache Hive on Amazon EMR. Several analysts have reported sub-par performance for the cluster during the morning peak load hours when 95% of the daily queries are executed by the analysts. The analytics team has also noted that HDFS's (Hadoop Distributed File System) usage never surpasses 10%.\n\nAs an AWS Certified Data Analytics Specialist, which of the following solutions would you recommend to resolve these performance issues?", "related_lectures": []}, {"_class": "assessment", "id": 56324354, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A research agency has deployed two autonomous underwater vehicles in the ocean to track parameters such as salinity, temperature, speed and direction of currents, etc. Vehicle A has twenty sensors whereas Vehicle B has ten sensors. Each sensor is identified by a unique ID. Amazon Kinesis Data Streams is being used to gather data from each sensor. A single Amazon Kinesis Data Stream with two shards is configured based on the total incoming and outgoing data throughput. Two partition keys are generated based on the name of the vehicles. During initial testing, data from Vehicle A experiences a bottleneck whereas data from Vehicle B does not. The overall stream throughput has been validated to be less than the assigned Kinesis Data Streams throughput.</p>\n\n<p>Which of the following solutions would you use to address this bottleneck without increasing the total cost and complexity of the system?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the partition key to use the sensor ID instead of the name of the vehicle</strong></p>\n\n<p>Amazon Kinesis Data Streams is a fully managed, serverless streaming data service that makes it easy to elastically ingest and store logs, events, clickstreams, and other forms of streaming data in real-time. Kinesis Data Streams has two capacity modes: on-demand and provisioned, and both come with specific billing options.</p>\n\n<p>In the on-demand mode, pricing is based on the volume of data ingested and retrieved along with a per-hour charge for each data stream in your account.</p>\n\n<p>With provisioned capacity mode, you specify the number of shards necessary for your application based on its write and read request rate. A shard is a unit of capacity that provides 1 MB/second of write and 2 MB/second of read throughout. A record is the data that your data producer adds to your Amazon Kinesis data stream. A PUT Payload Unit is counted in 25 KB payload \u201cchunks\u201d that comprise a record. For example, a 5 KB record contains one PUT Payload Unit, a 45 KB record contains two PUT Payload Units, and a 1 MB record contains 40 PUT Payload Units. PUT Payload Unit is charged a per-million PUT Payload Units rate. In the provisioned mode (applicable for the given scenario), you pay by the shard hour (1MB/second ingress, 2MB/second egress) and PUT Payload Units, per 1,000,000 units.</p>\n\n<p>For the existing use-case, the Kinesis Data Stream has been partitioned into two shards, one for each of the two vehicles. Therefore, data from the 20 sensors of Vehicle A is going to the first shard and the data from the ten sensors of Vehicle B is going to the other shard, causing an imbalance and thereby resulting in a throughput bottleneck for Vehicle A. The correct solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards. The use-case already states that the overall stream throughput is less than the assigned Kinesis Data Streams throughput, so there is no need to provision extra shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the number of shards in Kinesis Data Streams to support throughput from both vehicles</strong> - As mentioned in the explanation above, there is no need to provision extra shards as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned.</p>\n\n<p><strong>Set up another Kinesis Data Stream for Vehicle A with twenty shards and then direct Vehicle A sensor data to this new Kinesis Data Stream</strong></p>\n\n<p><strong>Set up another Kinesis Data Stream for Vehicle B with ten shards and then direct Vehicle B sensor data to this new Kinesis Data Stream</strong></p>\n\n<p>Both these options have been added as distractors. There is no need to provision extra shards for any of the vehicles, as it would result in additional costs. The right solution is to partition the shards on the basis of the sensor ID which will result in an even distribution across the two shards already provisioned.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/pricing/\">https://aws.amazon.com/kinesis/data-streams/pricing/</a></p>\n", "answers": ["<p>Increase the number of shards in Kinesis Data Streams to support throughput from both vehicles</p>", "<p>Change the partition key to use the sensor ID instead of the name of the vehicle</p>", "<p>Set up another Kinesis Data Stream for Vehicle A with twenty shards and then direct Vehicle A sensor data to this new Kinesis Data Stream</p>", "<p>Set up another Kinesis Data Stream for Vehicle B with ten shards and then direct Vehicle B sensor data to this new Kinesis Data Stream</p>"]}, "correct_response": ["b"], "section": "Domain 1: Collection", "question_plain": "A research agency has deployed two autonomous underwater vehicles in the ocean to track parameters such as salinity, temperature, speed and direction of currents, etc. Vehicle A has twenty sensors whereas Vehicle B has ten sensors. Each sensor is identified by a unique ID. Amazon Kinesis Data Streams is being used to gather data from each sensor. A single Amazon Kinesis Data Stream with two shards is configured based on the total incoming and outgoing data throughput. Two partition keys are generated based on the name of the vehicles. During initial testing, data from Vehicle A experiences a bottleneck whereas data from Vehicle B does not. The overall stream throughput has been validated to be less than the assigned Kinesis Data Streams throughput.\n\nWhich of the following solutions would you use to address this bottleneck without increasing the total cost and complexity of the system?", "related_lectures": []}, {"_class": "assessment", "id": 56324356, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare analytics company wants to use Amazon QuickSight to develop dashboards for analyzing health metrics. A team of 10 analysts will author these dashboards that will later be shared with 1000 healthcare professionals. The given health data is gathered from multiple research institutes and the data is later uploaded to Amazon S3 every 24 hours. The data is divided into years and months and saved in the Apache Parquet format. The company's primary data catalog is AWS Glue Data Catalog and the data querying is handled via Amazon Athena. At any point in time, the dashboards query from a total of 200 GB of uncompressed data.</p>\n\n<p>Which of the following represents the MOST cost-effective solution to address the given scenario?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up QuickSight Enterprise edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE which is then automatically refreshed every 24 hours</strong></p>\n\n<p>Amazon QuickSight is a cloud-powered business analytics service that allows users to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data.</p>\n\n<p>Amazon QuickSight is built with \"SPICE\" \u2013 a Super-fast, Parallel, In-memory Calculation Engine. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations and machine code generation to run interactive queries on large datasets and get rapid responses.</p>\n\n<p>Amazon QuickSight offers Standard and Enterprise editions. Both editions offer a full set of features for creating and sharing data visualizations. Enterprise edition additionally offers encryption at rest and Microsoft Active Directory integration. In the Enterprise edition, you select a Microsoft Active Directory directory in AWS Directory Service. You use that active directory to identify and manage your Amazon QuickSight users and administrators.</p>\n\n<p>A QuickSight Author is a user who can connect to data sources (within AWS or outside), create visuals and analyze data. Authors can create interactive dashboards using advanced QuickSight capabilities such as parameters and calculated fields, and publish dashboards with other users in the account.</p>\n\n<p>A QuickSight Reader is a user who consumes interactive dashboards. Readers can log in via their organization\u2019s preferred authentication mechanism (QuickSight username/password, SAML portal or AD auth), view shared dashboards, filter data, drill down to details or export data as a CSV file, using a web browser or mobile app. Readers do not have any allocated SPICE capacity.</p>\n\n<p>For the given use-case, the dashboard will query from a total of 200 GB of uncompressed data, so the source dataset must support at least 200 GB of data, which is only possible in the Enterprise edition of QuickSight. You should also note that the data stored in SPICE can be reused multiple times without incurring additional costs, hence this option is the best fit for the given scenario.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html\">https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up QuickSight Enterprise edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and use the direct query option</strong> - You should note that data stored in SPICE can be reused multiple times without incurring additional costs. If you use a data source that charges per query, such as Athena as mentioned for this option, you're charged for querying the data when you first create the dataset and later when you refresh the dataset. So this option is not cost-effective. In addition, the given use case states that the data is stored in S3 in parquet format which is then queried via Athena. Now, parquet files store data in a compressed format, which goes against the requirement that the dashboards should query from uncompressed data. Therefore this option is incorrect.</p>\n\n<p><strong>Set up QuickSight Standard edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE which is then automatically refreshed every 24 hours</strong></p>\n\n<p><strong>Set up QuickSight Standard edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and use the direct query option</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q3-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/quicksight/pricing/\">https://aws.amazon.com/quicksight/pricing/</a></p>\n\n<p>You cannot create Readers using the Standard edition of QuickSight, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/quicksight/pricing/\">https://aws.amazon.com/quicksight/pricing/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html\">https://docs.aws.amazon.com/quicksight/latest/user/data-source-limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/spice.html\">https://docs.aws.amazon.com/quicksight/latest/user/spice.html</a></p>\n", "answers": ["<p>Set up QuickSight Enterprise edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and use the direct query option</p>", "<p>Set up QuickSight Standard edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE which is then automatically refreshed every 24 hours</p>", "<p>Set up QuickSight Enterprise edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and import the data into SPICE which is then automatically refreshed every 24 hours</p>", "<p>Set up QuickSight Standard edition. Create 10 author users and 1,000 reader users. Configure an Athena data source and use the direct query option</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A healthcare analytics company wants to use Amazon QuickSight to develop dashboards for analyzing health metrics. A team of 10 analysts will author these dashboards that will later be shared with 1000 healthcare professionals. The given health data is gathered from multiple research institutes and the data is later uploaded to Amazon S3 every 24 hours. The data is divided into years and months and saved in the Apache Parquet format. The company's primary data catalog is AWS Glue Data Catalog and the data querying is handled via Amazon Athena. At any point in time, the dashboards query from a total of 200 GB of uncompressed data.\n\nWhich of the following represents the MOST cost-effective solution to address the given scenario?", "related_lectures": []}, {"_class": "assessment", "id": 56324358, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company uses Amazon RDS to store the sales data. For the analytics workloads that require high performance, only the last six months of data (approximately 50 TB) will be frequently queried. At the end of each month, the monthly sales data will be merged with the historical sales data for the last 5 years, which should also be available for analysis. The CTO at the company is looking at a cost-optimal solution that offers the best performance for this use case.</p>\n\n<p>Which of the following would you select for the given requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Load and store the last six months of data from S3 in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all the historical data in S3</strong></p>\n\n<p>You can use AWS data pipeline to automate the movement and transformation of data. It allows you to quickly define a dependent chain of data sources, destinations, and predefined or custom data processing activities called a pipeline. Based on a schedule you define, your pipeline regularly performs processing activities such as distributed data copy, SQL transforms, MapReduce applications, or custom scripts against destinations such as Amazon S3, Amazon RDS, Amazon Redshift or Amazon DynamoDB.</p>\n\n<p>You can use AWS data pipeline to set up a one-time export of RDS data to S3 as well as an ongoing incremental copy of RDS data to S3. You can also use AWS data pipeline to load data from S3 to Redshift.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html</a></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. For the given use case, you can use AWS data pipeline to load data from S3 to Redshift and then use Redshift to query the last six months of data to ensure high performance. Further, you can use Amazon Redshift Spectrum to query the entire historical data in S3 which would be a cost-effective solution to facilitate analysis of the historical data.</p>\n\n<p>Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent\u2014you can access your Amazon S3 data from any number of Amazon Redshift clusters.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q4-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a read replica of the RDS database to store the last six months of data and execute more frequent queries on the read replica. Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the historical data in S3</strong> - You cannot use RDS to facilitate queries on the last six months of data as it would not match the high performance offered by Redshift, which is a better option for the given requirement.</p>\n\n<p><strong>Use AWS data pipeline to incrementally load the last six months of data into Amazon Redshift and execute more frequent queries on Redshift. Set up a read replica of the RDS database to run queries on the historical data</strong> - Using a read replica on RDS to store and analyze the entire historical data would turn out to be much costlier than storing the data on S3 and enable analysis using Redshift spectrum. So this option is incorrect.</p>\n\n<p><strong>Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the entire data in S3</strong> - It is certainly possible to query the entire data in S3 using Athena, however, it will not be able to match the high performance offered by Redshift to query the last six months of data. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-incrementalcopyrdstos3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshiftrdsincremental.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-redshift-create.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n", "answers": ["<p>Configure a read replica of the RDS database to store the last six months of data and execute more frequent queries on the read replica. Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the historical data in S3</p>", "<p>Use AWS data pipeline to incrementally load the last six months of data into Amazon Redshift and execute more frequent queries on Redshift. Set up a read replica of the RDS database to run queries on the historical data</p>", "<p>Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Configure an AWS Glue Data Catalog of the data in S3 and use Amazon Athena to query the entire data in S3</p>", "<p>Export RDS data to S3 and schedule an AWS data pipeline for incremental copy of RDS data to S3. Load and store the last six months of data from S3 in Amazon Redshift. Configure an Amazon Redshift Spectrum table to connect to all the historical data in S3</p>"]}, "correct_response": ["d"], "section": "Domain 2: Storage and Data Management", "question_plain": "A retail company uses Amazon RDS to store the sales data. For the analytics workloads that require high performance, only the last six months of data (approximately 50 TB) will be frequently queried. At the end of each month, the monthly sales data will be merged with the historical sales data for the last 5 years, which should also be available for analysis. The CTO at the company is looking at a cost-optimal solution that offers the best performance for this use case.\n\nWhich of the following would you select for the given requirement?", "related_lectures": []}, {"_class": "assessment", "id": 56324360, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A trading firm collects daily stock trading data from exchanges and stores it in a data warehouse. The analytics team at the firm needs a solution that streams data directly into the data repository but should also allow SQL-based data modifications when needed. The solution should facilitate complex analytical queries that execute in the fastest possible time. The solution should also offer a business intelligence dashboard that highlights any stock price anomalies.</p>\n\n<p>Which of the following solutions represents the best fit for the given scenario?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</strong></p>\n\n<p>Amazon Kinesis Data Firehose is an extract, transform, and load (ETL) service that reliably captures, transforms, and delivers streaming data to data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards.</p>\n\n<p>Kinesis Data Firehose Overview:\n<img src=\"https://d1.awsstatic.com/pdp-how-it-works-assets/product-page-diagram_Amazon-KDF_HIW-V2-Updated-Diagram@2x.6e531854393eabf782f5a6d6d3b63f2e74de0db4.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with a number of AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.</p>\n\n<p>Key Concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q5-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a></p>\n\n<p>For the given use case, you can use Kinesis Data Firehose to stream data to Amazon Redshift. For a Redshift destination, streaming data is delivered to an S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket. Once the data is in Redshift, you can use Quicksight to create a business intelligence dashboard that has Redshift as the data source.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</strong></p>\n\n<p>You cannot stream data from Kinesis Data Streams directly to Redshift. You need to use Kinesis Data Firehose to deliver streaming data to Redshift. So this option is incorrect.</p>\n\n<p><strong>Set up Amazon Kinesis Data Firehose to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong></p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</strong></p>\n\n<p>Storing the data in S3 and then querying the data via Athena would not facilitate the execution of complex queries in the fastest possible time since Redshift has much better performance than Athena for complex analytical queries. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/getting-started/\">https://aws.amazon.com/kinesis/data-streams/getting-started/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/\">https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-redshift-private-connection/</a></p>\n", "answers": ["<p>Set up Amazon Kinesis Data Streams to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</p>", "<p>Set up Amazon Kinesis Data Firehose to stream data to Amazon Redshift. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Redshift as a data source</p>", "<p>Set up Amazon Kinesis Data Firehose to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</p>", "<p>Set up Amazon Kinesis Data Streams to stream data to Amazon S3. Create a business intelligence dashboard by using Amazon QuickSight that has Amazon Athena as a data source</p>"]}, "correct_response": ["b"], "section": "Domain 3: Processing", "question_plain": "A trading firm collects daily stock trading data from exchanges and stores it in a data warehouse. The analytics team at the firm needs a solution that streams data directly into the data repository but should also allow SQL-based data modifications when needed. The solution should facilitate complex analytical queries that execute in the fastest possible time. The solution should also offer a business intelligence dashboard that highlights any stock price anomalies.\n\nWhich of the following solutions represents the best fit for the given scenario?", "related_lectures": []}, {"_class": "assessment", "id": 56324362, "assessment_type": "multi-select", "prompt": {"question": "<p>A company runs multiple gaming platforms that need to store game state, player data, session history, and leaderboards. The company is looking to move to AWS Cloud to scale reliably to millions of concurrent users and requests while ensuring consistently low latency measured in single-digit milliseconds. The development team at the company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of its users.</p>\n\n<p>Which of the following solutions would you recommend? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.\nElastiCache for Redis can be used to power the live leaderboard, so this option is correct.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\"></p>\n\n<p><strong>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-Region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. So DynamoDB with DAX can be used to power the live leaderboard.</p>\n\n<p>DAX Overview:\n<img src=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/dax_high_level.png\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Develop the leaderboard using Amazon Redshift as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Redshift is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</strong> - DynamoDB is not an in-memory database, so this option is not correct.</p>\n\n<p><strong>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</strong> - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n", "answers": ["<p>Develop the leaderboard using DynamoDB as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using ElastiCache Redis as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using DynamoDB with DynamoDB Accelerator (DAX) as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using AWS Neptune as it meets the in-memory, high availability, low latency requirements</p>", "<p>Develop the leaderboard using RDS Aurora as it meets the in-memory, high availability, low latency requirements</p>"]}, "correct_response": ["b", "c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A company runs multiple gaming platforms that need to store game state, player data, session history, and leaderboards. The company is looking to move to AWS Cloud to scale reliably to millions of concurrent users and requests while ensuring consistently low latency measured in single-digit milliseconds. The development team at the company is evaluating multiple in-memory data stores with the ability to power its on-demand, live leaderboard. The company's leaderboard requires high availability, low latency, and real-time processing to deliver customizable user data for the community of its users.\n\nWhich of the following solutions would you recommend? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 56324364, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company stores all transaction data in Amazon RDS in the us-east-1 Region. The transformed transaction data is also kept in the us-east-1 Region in Amazon Redshift. The analytics team wants to improve the user experience by developing a business intelligence (BI) dashboard that highlights the sales trends over the last year. A team in India configured Amazon QuickSight in ap-south-1 Region during development. The team is experiencing connectivity issues between Amazon QuickSight in ap-south-1 Region and Amazon Redshift in us-east-1 Region.</p>\n\n<p>Which of the following solutions would you recommend to address this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong></p>\n\n<p>Amazon QuickSight is a cloud-powered business analytics service that allows users to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data.</p>\n\n<p>Amazon QuickSight is built with \"SPICE\" \u2013 a Super-fast, Parallel, In-memory Calculation Engine. Built from the ground up for the cloud, SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations and machine code generation to run interactive queries on large datasets and get rapid responses.</p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale.</p>\n\n<p>For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in that AWS Region. You can configure such a security group irrespective of the Redshift cluster having been created in a VPC or not.</p>\n\n<p>Manually enabling access to an Amazon Redshift cluster in a VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n\n<p>Manually enabling access to an Amazon Redshift cluster that is not in a VPC:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q7-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new Network Access Control List (NACL) for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</strong> - A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACLs are associated with subnets, which are in turn created within VPCs. You should note that a Redshift cluster, as well as a QuickSight dashboard, can be created outside of a VPC. So this option is incorrect for the given use case.</p>\n\n<p><strong>Set up a VPC endpoint from the Amazon QuickSight VPC in ap-south-1 Region to the Amazon Redshift VPC in us-east-1 Region, so QuickSight can privately access data from Redshift</strong> - Only QuickSight Enterprise edition can be integrated with the Amazon VPC service. A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC. VPC endpoints can only be used to access resources in the same Region as the endpoint. Since the QuickSight dashboard and Redshift cluster are in separate Regions, you cannot use VPC endpoint for the given scenario.</p>\n\n<p><strong>Set up a daily cross-Region snapshot for Redshift and set the destination Region as ap-south-1. Restore the Amazon Redshift Cluster from the snapshot ap-south-1 Region and connect the  QuickSight dashboard in ap-south-1 to this redshift cluster</strong> - You cannot set a direct cross-Region snapshot in Redshift. You need to configure an automated snapshot in the same Region as the Redshift cluster and then you can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region. When a snapshot is created in the cluster's primary AWS Region, it's copied to a secondary AWS Region. The two AWS Regions are known respectively as the source AWS Region and destination AWS Region. In addition, you should note that using a snapshot copy can make the data available in the other Region which can be used for the QuickSight dashboard, however, this would turn out to be an expensive solution. So this option is not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/regions.html\">https://docs.aws.amazon.com/quicksight/latest/user/regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/welcome.html\">https://docs.aws.amazon.com/quicksight/latest/user/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/regions.html\">https://docs.aws.amazon.com/quicksight/latest/user/regions.html</a></p>\n", "answers": ["<p>Configure a new security group for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</p>", "<p>Configure a new Network Access Control List (NACL) for Amazon Redshift in us-east-1 with an inbound rule authorizing access from the appropriate CIDR address block for the Amazon QuickSight servers in ap-south-1</p>", "<p>Set up a VPC endpoint from the Amazon QuickSight VPC in ap-south-1 Region to the Amazon Redshift VPC in us-east-1 Region, so QuickSight can privately access data from Redshift</p>", "<p>Set up a daily cross-Region snapshot for Redshift and set the destination Region as ap-south-1. Restore the Amazon Redshift Cluster from the snapshot ap-south-1 Region and connect the  QuickSight dashboard in ap-south-1 to this redshift cluster</p>"]}, "correct_response": ["a"], "section": "Domain 4: Analysis and Visualization", "question_plain": "An e-commerce company stores all transaction data in Amazon RDS in the us-east-1 Region. The transformed transaction data is also kept in the us-east-1 Region in Amazon Redshift. The analytics team wants to improve the user experience by developing a business intelligence (BI) dashboard that highlights the sales trends over the last year. A team in India configured Amazon QuickSight in ap-south-1 Region during development. The team is experiencing connectivity issues between Amazon QuickSight in ap-south-1 Region and Amazon Redshift in us-east-1 Region.\n\nWhich of the following solutions would you recommend to address this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 56324366, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to use AWS for its connected cab application that would collect sensor data from its electric cab fleet to give drivers dynamically updated map information. The company would like to build its new sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the company does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. The company has hired you to provide consultancy for this strategic initiative.</p>\n\n<p>Given these constraints, which of the following solutions would you suggest as the BEST fit to develop this service?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:\n<strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda event source mappings support standard queues and first-in, first-out (FIFO) queues. With Amazon SQS, you can offload tasks from one component of your application by sending them to a queue and processing them asynchronously.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p>\n\n<p>AWS manages all ongoing operations and underlying infrastructure needed to provide a highly available and scalable message queuing service. With SQS, there is no upfront cost, no need to acquire, install, and configure messaging software, and no time-consuming build-out and maintenance of supporting infrastructure. SQS queues are dynamically created and scale automatically so you can build and grow applications quickly and efficiently.\nAs there is no need to manually provision the capacity, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.</p>\n\n<p>Firehose cannot directly write into a DynamoDB table, so this option is incorrect.</p>\n\n<p><strong>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p><strong>Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</strong></p>\n\n<p>Using an application on an EC2 instance is ruled out as the company wants to use fully serverless components. So both these options are incorrect.</p>\n\n<p>References:\n<a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n", "answers": ["<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by a Lambda function in batches and the data is written into an auto-scaled DynamoDB table for downstream processing</p>", "<p>Ingest the sensor data in an Amazon SQS standard queue, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</p>", "<p>Ingest the sensor data in Kinesis Data Firehose, which directly writes the data into an auto-scaled DynamoDB table for downstream processing</p>", "<p>Ingest the sensor data in a Kinesis Data Stream, which is polled by an application running on an EC2 instance and the data is written into an auto-scaled DynamoDB table for downstream processing</p>"]}, "correct_response": ["a"], "section": "Domain 1: Collection", "question_plain": "A company wants to use AWS for its connected cab application that would collect sensor data from its electric cab fleet to give drivers dynamically updated map information. The company would like to build its new sensor service by leveraging fully serverless components that are provisioned and managed automatically by AWS. The development team at the company does not want an option that requires the capacity to be manually provisioned, as it does not want to respond manually to changing volumes of sensor data. The company has hired you to provide consultancy for this strategic initiative.\n\nGiven these constraints, which of the following solutions would you suggest as the BEST fit to develop this service?", "related_lectures": []}, {"_class": "assessment", "id": 56324368, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses two AWS accounts for accessing various AWS services. The analytics team has just configured an Amazon S3 bucket in AWS account A for writing data from the Amazon Redshift cluster provisioned in AWS account B. The team has noticed that the files created in the S3 bucket using UNLOAD command from the Redshift cluster are not accessible to the bucket owner user of the AWS account A that created the S3 bucket.</p>\n\n<p>What could be the reason for this denial of permission for resources belonging to the same AWS account?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by the Redshift cluster</strong> - By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files.</p>\n\n<p>To get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket:</p>\n\n<ol>\n<li><p>From the account of the S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket.</p></li>\n<li><p>From the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role.</p></li>\n<li><p>Update the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role.</p></li>\n<li><p>From the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role.</p></li>\n</ol>\n\n<p>This resolution doesn't apply to Amazon Redshift clusters or S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When objects are uploaded to an S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. It is an upload error that can be fixed by providing manual access from AWS console</strong> - By default, an S3 object is owned by the AWS account that uploaded it. So, the bucket owner will not have any default permissions on the objects.</p>\n\n<p><strong>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, it is possible that the write operation is still in progress</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p><strong>When two different AWS accounts are accessing an S3 bucket, both the accounts need to share the bucket policies, explicitly defining the actions possible for each account. An erroneous policy can lead to such permission failures</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/</a></p>\n", "answers": ["<p>When objects are uploaded to an S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. It is an upload error that can be fixed by providing manual access from AWS console</p>", "<p>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, it is possible that the write operation is still in progress</p>", "<p>When two different AWS accounts are accessing an S3 bucket, both the accounts need to share the bucket policies, explicitly defining the actions possible for each account. An erroneous policy can lead to such permission failures</p>", "<p>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by the Redshift cluster</p>"]}, "correct_response": ["d"], "section": "Domain 5: Security", "question_plain": "A company uses two AWS accounts for accessing various AWS services. The analytics team has just configured an Amazon S3 bucket in AWS account A for writing data from the Amazon Redshift cluster provisioned in AWS account B. The team has noticed that the files created in the S3 bucket using UNLOAD command from the Redshift cluster are not accessible to the bucket owner user of the AWS account A that created the S3 bucket.\n\nWhat could be the reason for this denial of permission for resources belonging to the same AWS account?", "related_lectures": []}, {"_class": "assessment", "id": 56324350, "assessment_type": "multi-select", "prompt": {"question": "<p>The analytics team at a company is building an ElasticSearch based index for all the existing files in S3. To build this index, it only needs to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, adding up to 50TB of data.</p>\n\n<p>Which of the following solutions can be used to build this index MOST efficiently? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in ElasticSearch</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. With Amazon ES, you get direct access to the Elasticsearch APIs; existing code and applications work seamlessly with the service.</p>\n\n<p>How ElasticSearch Works:\n<img src=\"https://d1.awsstatic.com/re19/ESS_HIW.db99588614cbf44e2d62ef9c9c173ebfe41e2834.png\">\nvia - <a href=\"https://aws.amazon.com/elasticsearch-service/\">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p>Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.</p>\n\n<p>A byte-range request is a perfect way to get the beginning of a file and ensure that we remain efficient during our scan of our S3 bucket. You can then store the relevant information in the form of a JSON document in ElasticSearch.</p>\n\n<p><strong>Create an application that will use the S3 Select ScanRange parameter to get the first 250 bytes and store that information in ElasticSearch</strong></p>\n\n<p>With Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte). You can then store the relevant information in the form of a JSON document in ElasticSearch.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the ElasticSearch Import feature to load the entire data from S3 to ElasticSearch and then ElasticSearch would automatically build the index</strong> - This option has been added as a distractor as there is no ElasticSearch Import feature to load data from S3.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, read the entire files one by one, extract the first 250 bytes, and store that information in ElasticSearch</strong> - If you build an application that loads all the files from S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p><strong>Use the Database Migration Service to load the entire data from S3 to ElasticSearch and then ElasticSearch would automatically build the index</strong> - Although you could use Database Migration Service to load the entire data from S3 to ElasticSearch, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opensearch-service/\">https://aws.amazon.com/opensearch-service/</a></p>\n", "answers": ["<p>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in ElasticSearch</p>", "<p>Use the ElasticSearch Import feature to load the entire data from S3 to ElasticSearch and then ElasticSearch would automatically build the index</p>", "<p>Create an application that will traverse the S3 bucket, read the entire files one by one, extract the first 250 bytes, and store that information in ElasticSearch</p>", "<p>Create an application that will use the S3 Select ScanRange parameter to get the first 250 bytes and store that information in ElasticSearch</p>", "<p>Use the Database Migration Service to load the entire data from S3 to ElasticSearch and then ElasticSearch would automatically build the index</p>"]}, "correct_response": ["a", "d"], "section": "Domain 5: Security", "question_plain": "The analytics team at a company is building an ElasticSearch based index for all the existing files in S3. To build this index, it only needs to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, adding up to 50TB of data.\n\nWhich of the following solutions can be used to build this index MOST efficiently? (Select two)", "related_lectures": []}]}
5450716
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 54888236, "assessment_type": "multi-select", "prompt": {"question": "<p>The web development team at an IT company has about 200 TB of web-log data that is stored in an Amazon S3 bucket as raw text. Each log file is identified by a key of the type year-month-day_log_HHmmss.txt where HHmmss denotes the time the log file was created. The data analytics team has created an Amazon Athena table that links to the given S3 bucket. The data analytics team executes several queries every hour against a subset of the table's columns. The company wants a Hive-metastore compatible solution that costs less and requires less maintenance to support the ongoing analytics on this log data.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following solutions would you combine to address these requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Change the log files to Apache Parquet format</strong></p>\n\n<p><strong>Partition the data by using a key prefix of the form date=year-month-day/ to the S3 objects</strong></p>\n\n<p><strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>MSCK REPAIR TABLE</code> statement</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data stored in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme.</p>\n\n<p>Athena can use Apache Hive style partitions, whose data paths contain key value pairs connected by equal signs (for example, country=us/... or year=2021/month=01/day=26/...). Thus, the paths include both the names of the partition keys and the values that each path represents.</p>\n\n<p>Athena can also use non-Hive style partitioning schemes. For example, CloudTrail logs and Kinesis Data Firehose delivery streams use separate path components for date parts such as data/2021/01/26/us/6fc7845e.json. For such non-Hive compatible data, you use ALTER TABLE ADD PARTITION to add the partitions manually.</p>\n\n<p>Since the given use case needs a hive-metastore compatible solution, you can use a key prefix of the form date=year-month-day/ for partitioning data and use <code>MSCK REPAIR TABLE</code> statement to load the partitions.</p>\n\n<p>Considerations and Limitations for Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n\n<p>Avro is a row-based storage format whereas Parquet is a columnar-based storage format. Writing operations in Avro are more efficient than Parquet whereas Parquet is much better for analytical operations since the reads and querying are much more efficient than writing. Parquet is better suited for querying a subset of columns in a multi-column table whereas Avro is better suited for ETL operations where we need to query all the columns.</p>\n\n<p>For the given use case, several queries are executed every hour, so Parquet is a better format than Avro.</p>\n\n<p>Highly recommend the following blog on the top performance tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>ALTER TABLE ADD PARTITION</code> statement</strong></p>\n\n<p><strong>Partition the data by using a key prefix of the form year-month-day/ to the S3 objects</strong></p>\n\n<p><strong>Change the log files to Apache Avro format</strong></p>\n\n<p>Per the explanation provided above, these three options do not meet the requirements for the given use case, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.clairvoyant.ai/blog/big-data-file-formats\">https://www.clairvoyant.ai/blog/big-data-file-formats</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/connect-to-data-source-hive.html\">https://docs.aws.amazon.com/athena/latest/ug/connect-to-data-source-hive.html</a></p>\n", "answers": ["<p>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>ALTER TABLE ADD PARTITION</code> statement</p>", "<p>Partition the data by using a key prefix of the form year-month-day/ to the S3 objects</p>", "<p>Change the log files to Apache Avro format</p>", "<p>Change the log files to Apache Parquet format</p>", "<p>Partition the data by using a key prefix of the form date=year-month-day/ to the S3 objects</p>", "<p>Drop and recreate the table with the PARTITIONED BY clause. Load the partitions by executing the <code>MSCK REPAIR TABLE</code> statement</p>"]}, "correct_response": ["d", "e", "f"], "section": "Domain 4: Analysis and Visualization", "question_plain": "The web development team at an IT company has about 200 TB of web-log data that is stored in an Amazon S3 bucket as raw text. Each log file is identified by a key of the type year-month-day_log_HHmmss.txt where HHmmss denotes the time the log file was created. The data analytics team has created an Amazon Athena table that links to the given S3 bucket. The data analytics team executes several queries every hour against a subset of the table's columns. The company wants a Hive-metastore compatible solution that costs less and requires less maintenance to support the ongoing analytics on this log data.\n\nAs an AWS Certified Data Analytics Specialist, which of the following solutions would you combine to address these requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 54888238, "assessment_type": "multi-select", "prompt": {"question": "<p>A data analytics company wants to store data from Athena CTAS (<code>CREATE TABLE AS SELECT</code>) query results in Amazon S3. A junior analyst wants to understand the distinction between partitioning scheme vs bucketing scheme for storing data via such CTAS queries.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following options would you identify as the right fit for choosing bucketing vs partitioning? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have low cardinality</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values</strong></p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>When you run a CTAS query, Athena writes the results to a specified location in Amazon S3. If you specify partitions, it creates them and stores each partition in a separate partition folder in the same location. Having partitions in Amazon S3 helps with Athena query performance because this helps you run targeted queries for only specific partitions. As a best practice, you should partition data by those columns that have similar characteristics, such as records from the same department, and that can have a limited number of possible values (low cardinality), such as a limited number of distinct departments in an organization.</p>\n\n<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values. For example, columns storing timestamp data could potentially have a very large number of distinct values, and their data is evenly distributed across the data set. To choose the column by which to bucket the CTAS query results, use the column that has a high number of values (high cardinality) and whose data can be split for storage into many buckets that will have roughly the same amount of data. Columns that are sparsely populated with values are not good candidates for bucketing.</p>\n\n<p>You should also note that you can partition and use bucketing for storing results of the same CTAS query. These techniques for writing data do not exclude each other. Typically, the columns you use for bucketing differ from those you use for partitioning.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html\">https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have high cardinality</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has low cardinality and evenly distributed values</strong></p>\n\n<p><strong>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and sparsely distributed values</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html\">https://docs.aws.amazon.com/athena/latest/ug/bucketing-vs-partitioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n", "answers": ["<p>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have low cardinality</p>", "<p>Partitioning CTAS query results works well when the number of partitions you plan to have is limited and the partitioned columns have high cardinality</p>", "<p>Bucketing CTAS query results works well when you bucket data by the column that has low cardinality and evenly distributed values</p>", "<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and sparsely distributed values</p>", "<p>Bucketing CTAS query results works well when you bucket data by the column that has high cardinality and evenly distributed values</p>"]}, "correct_response": ["a", "e"], "section": "Domain 2: Storage and Data Management", "question_plain": "A data analytics company wants to store data from Athena CTAS (CREATE TABLE AS SELECT) query results in Amazon S3. A junior analyst wants to understand the distinction between partitioning scheme vs bucketing scheme for storing data via such CTAS queries.\n\nAs an AWS Certified Data Analytics Specialist, which of the following options would you identify as the right fit for choosing bucketing vs partitioning? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888240, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A news media company uses an ad-hoc Kinesis Firehose based solution to ingest raw data in JSON format and then deliver it to an Amazon S3 bucket. The data analytics team at the company uses Apache Spark SQL to analyze this data via Amazon EMR, which is configured to use AWS Glue Data Catalog as the metastore. An AWS Glue crawler runs every four hours to update the schema of the data catalog. The team has noticed that it sometimes obtains outdated data. You have been hired by the company as an AWS Certified Data Analytics Specialist to build a solution for ensuring that the team always has access to the current data.</p>\n\n<p>Which of the following represents the best solution to meet this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Invoke the AWS Glue crawler via an AWS Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p>An AWS Glue crawler is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog. You can use a Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket to invoke the AWS Glue crawler on-demand. This obviates the need to periodically run the crawler on a schedule to update the new data into the existing data catalog tables.</p>\n\n<p>Data Lake Architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q3-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/\">https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon CloudWatch Events with the rate (5 minutes) expression to execute the AWS Glue crawler every 5 minutes</strong> - CloudWatch Events do not support AWS Glue crawler as a destination type, so this option just acts as a distractor.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q3-i3.jpg\"></p>\n\n<p><strong>Modify the execution schedule of the AWS Glue crawler from 4 hours to 1 minute</strong> - For AWS Glue crawlers, the minimum precision for a schedule is 5 minutes. So this option just acts as a distractor.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q3-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html</a></p>\n\n<p><strong>Use Amazon Athena to directly analyze the current data in Amazon S3</strong> - The use case mentions that the analysis must be done by using Apache Spark SQL on Amazon EMR, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/\">https://aws.amazon.com/blogs/big-data/build-and-automate-a-serverless-data-lake-using-an-aws-glue-trigger-for-the-data-catalog-and-etl-jobs/</a></p>\n", "answers": ["<p>Invoke the AWS Glue crawler via an AWS Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket</p>", "<p>Use Amazon CloudWatch Events with the rate (5 minutes) expression to execute the AWS Glue crawler every 5 minutes</p>", "<p>Use Amazon Athena to directly analyze the current data in Amazon S3</p>", "<p>Modify the execution schedule of the AWS Glue crawler from 4 hours to 1 minute</p>"]}, "correct_response": ["a"], "section": "Domain 3: Processing", "question_plain": "A news media company uses an ad-hoc Kinesis Firehose based solution to ingest raw data in JSON format and then deliver it to an Amazon S3 bucket. The data analytics team at the company uses Apache Spark SQL to analyze this data via Amazon EMR, which is configured to use AWS Glue Data Catalog as the metastore. An AWS Glue crawler runs every four hours to update the schema of the data catalog. The team has noticed that it sometimes obtains outdated data. You have been hired by the company as an AWS Certified Data Analytics Specialist to build a solution for ensuring that the team always has access to the current data.\n\nWhich of the following represents the best solution to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 54888242, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare company uses an Amazon Redshift database cluster to store sensitive user data. The regulatory guidelines mandate logging so that any database authentication attempts as well as the connections/disconnections are recorded. Also, the logs must include a record of each query executed against the database along with the database user who executed that query.</p>\n\n<p>Which of the following options represent the best solution for these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable audit logging for Amazon Redshift</strong></p>\n\n<p>Amazon Redshift logs information about connections and user activities in your database. These logs help you to monitor the database for security and troubleshooting purposes, a process called database auditing. The logs are stored in Amazon S3 buckets.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q4-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a></p>\n\n<p>Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable audit trail for Amazon Redshift on Amazon CloudTrail</strong> - Amazon Redshift is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Redshift. CloudTrail captures all API calls for Amazon Redshift as events. These include calls from the Amazon Redshift console and from code calls to the Amazon Redshift API operations. For example, you can have a CloudTrail log entry for a CreateCluster call or a DeleteCluster call. CloudTrail cannot capture details such as database authentication attempts, any connections/disconnections to the database or capturing the queries executed by Redshift users. So this option is incorrect.</p>\n\n<p><strong>Enable and download audit reports from AWS Artifact</strong> - AWS Artifact is a self-service audit artifact retrieval portal that provides AWS' customers with on-demand access to AWS\u2019 compliance documentation and AWS agreements. You cannot use Artifact for capturing Redshift details such as database authentication attempts, any connections/disconnections to the database or capturing the queries executed by Redshift users. So this option is incorrect.</p>\n\n<p><strong>Enable audit metrics on Amazon CloudWatch</strong> - You can use CloudWatch to monitor the performance of your Redshift cluster and not for audit purposes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/metrics-listing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/metrics-listing.html</a></p>\n", "answers": ["<p>Enable audit trail for Amazon Redshift on Amazon CloudTrail</p>", "<p>Enable audit logging for Amazon Redshift</p>", "<p>Enable and download audit reports from AWS Artifact</p>", "<p>Enable audit metrics on Amazon CloudWatch</p>"]}, "correct_response": ["b"], "section": "Domain 5: Security", "question_plain": "A healthcare company uses an Amazon Redshift database cluster to store sensitive user data. The regulatory guidelines mandate logging so that any database authentication attempts as well as the connections/disconnections are recorded. Also, the logs must include a record of each query executed against the database along with the database user who executed that query.\n\nWhich of the following options represent the best solution for these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 54888244, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A credit card company is looking for a solution that detects anomalies in order to identify fraudulent transactions. The company utilizes Amazon Kinesis to transfer JSON-formatted transaction records from its on-premises database to Amazon S3. The existing dataset comprises 100-column wide records for each transaction. To identify fraudulent transactions, the solution needs to analyze just ten of these columns.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following would you suggest as the lowest-cost solution that needs the least development work and offers out-of-the-box anomaly detection functionality?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</strong></p>\n\n<p>For the given use case, you can use an AWS Glue job to extract, transform, and load (ETL) data from the data source (in JSON format) to the data target (in Parquet format). You can then use an AWS Glue crawler, which is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run, thereby making this solution really low cost. You can also use Athena to build a table with only the subset of columns that are required for downstream analysis.</p>\n\n<p>Finally, you can read the data in the given Athena table via Amazon QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection. QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis. With ML-powered anomaly detection, you can find outliers in your data without the need for manual analysis, custom development, or ML domain expertise.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries which compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</strong> - Using Kinesis Data Analytics involves some custom query development to analyze the incoming data to compute an anomaly score for all transactions. In addition, this solution processes all columns of the data instead of just the subset of columns required for the analysis. Therefore, this option is not the best fit for the given use case.</p>\n\n<p><img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Amazon-Kinesis-Data-Analytics_HIW.82e3aa53a5c87db03c766218b3d51f1a110c60eb.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. Using SageMaker involves custom code development to build, develop, test, and deploy the anomaly detection model that is relevant to the given scenario. Instead, you can directly use QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-based anomaly detection functionality. Therefore, this option is not the right fit for the given use case.</p>\n\n<p><strong>Leverage Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</strong> - This option involves significant custom code development on a Lambda function to examine the incoming stream from Firehose and then compute an anomaly score for all transactions. In addition, the lambda looks at all the fields in the data instead of just the subset of fields required for the analysis. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\">https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/\">https://aws.amazon.com/blogs/big-data/accessing-and-visualizing-data-from-multiple-data-sources-with-amazon-athena-and-amazon-quicksight/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html\">https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sagemaker/faqs/\">https://aws.amazon.com/sagemaker/faqs/</a></p>\n", "answers": ["<p>Leverage Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries which compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</p>", "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</p>", "<p>Leverage Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</p>", "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</p>"]}, "correct_response": ["d"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A credit card company is looking for a solution that detects anomalies in order to identify fraudulent transactions. The company utilizes Amazon Kinesis to transfer JSON-formatted transaction records from its on-premises database to Amazon S3. The existing dataset comprises 100-column wide records for each transaction. To identify fraudulent transactions, the solution needs to analyze just ten of these columns.\n\nAs an AWS Certified Data Analytics Specialist, which of the following would you suggest as the lowest-cost solution that needs the least development work and offers out-of-the-box anomaly detection functionality?", "related_lectures": []}, {"_class": "assessment", "id": 54888246, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data analytics team at a retail company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL-based data sanity checks on the raw zone of the data lake.</p>\n\n<p>What AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Athena to run SQL based analytics against S3 data</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>AWS Athena Benefits:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q6-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL-based sanity checks</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.\nAs the development team would have to maintain and monitor the Redshift cluster size and would require significant development time to set up the processes to consume the data periodically, so this option is ruled out.</p>\n\n<p><strong>Load the incremental raw zone data into an EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it\u2019s ruled out because the correct solution for the given use case should require the least amount of development effort and ongoing maintenance.</p>\n\n<p><strong>Load the incremental raw zone data into RDS on an hourly basis and run the SQL-based sanity checks</strong> - Loading the incremental data into RDS implies data migration jobs will have to be written via a Lambda function or an EC2 based process. This goes against the requirement that the solution should involve the least amount of development effort and ongoing maintenance. Hence this option is not correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n", "answers": ["<p>Load the incremental raw zone data into Redshift on an hourly basis and run the SQL based sanity checks</p>", "<p>Load the incremental raw zone data into RDS on an hourly basis and run the SQL based sanity checks</p>", "<p>Load the incremental raw zone data into an EMR based Spark Cluster on an hourly basis and use SparkSQL to run the SQL based sanity checks</p>", "<p>Use Athena to run SQL based analytics against S3 data</p>"]}, "correct_response": ["d"], "section": "Domain 4: Analysis and Visualization", "question_plain": "The data analytics team at a retail company has set up a workflow to ingest the clickstream data into the raw zone of the S3 data lake. The team wants to run some SQL-based data sanity checks on the raw zone of the data lake.\n\nWhat AWS services would you recommend for this use-case such that the solution is cost-effective and easy to maintain?", "related_lectures": []}, {"_class": "assessment", "id": 54888248, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are a data analyst at an IT company. The company has multiple enterprise customers that use the company's mobile app to capture and send data to Amazon Kinesis Data Streams. The customers have been getting a <code>ProvisionedThroughputExceededException</code> exception. Upon analysis, you notice that messages are being sent one by one at a high rate.</p>\n\n<p>Which of the following options will help with the exception while keeping costs at a minimum?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Use batch messages</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Exponential Backoff</strong>: While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short-term fix but will substantially increase the cost, so this option is ruled out.</p>\n\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n", "answers": ["<p>Use batch messages</p>", "<p>Decrease the Stream retention duration</p>", "<p>Increase the number of shards</p>", "<p>Use Exponential Backoff</p>"]}, "correct_response": ["a"], "section": "Domain 1: Collection", "question_plain": "You are a data analyst at an IT company. The company has multiple enterprise customers that use the company's mobile app to capture and send data to Amazon Kinesis Data Streams. The customers have been getting a ProvisionedThroughputExceededException exception. Upon analysis, you notice that messages are being sent one by one at a high rate.\n\nWhich of the following options will help with the exception while keeping costs at a minimum?", "related_lectures": []}, {"_class": "assessment", "id": 54888250, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs an analytics workload with heavy reads and writes through the workload lifecycle. The data analytics team at the company is interested in using Amazon S3 as the data lake to support this workload. The team has hired you to advise them on the S3 data consistency model.</p>\n\n<p>Which of the following statements would you identify as correct?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</strong> - After a successful write of a new object, or an overwrite or delete of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what\u2019s in the bucket.</p>\n\n<p>This improvement is great for data lakes, but other types of applications will also benefit. Because S3 now has strong consistency, migration of on-premises workloads and storage to AWS should now be easier than ever before.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</strong></p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</strong></p>\n\n<p>As mentioned in the explanation above, Amazon S3 is strongly consistent for all GET, PUT and LIST operations, so these two options are incorrect</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</strong> - For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/consistency/\">https://aws.amazon.com/s3/consistency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/\">https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/</a></p>\n", "answers": ["<p>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</p>", "<p>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</p>", "<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</p>", "<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</p>"]}, "correct_response": ["c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A company runs an analytics workload with heavy reads and writes through the workload lifecycle. The data analytics team at the company is interested in using Amazon S3 as the data lake to support this workload. The team has hired you to advise them on the S3 data consistency model.\n\nWhich of the following statements would you identify as correct?", "related_lectures": []}, {"_class": "assessment", "id": 54888252, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A subscription streaming service company uses AWS Cloud for analytics, recommendation engines and video transcoding. To monitor and optimize this network, the analytics team at the company has developed a solution for ingesting, augmenting, and analyzing the multiple terabytes of data its network generates daily in the form of virtual private cloud (VPC) flow logs. This would enable the company to identify performance-improvement opportunities such as identifying apps that are communicating across regions and collocating them. The VPC flow logs data is funneled into Kinesis Data Streams which further acts as the source of a delivery stream for Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send the VPC flow logs data from another set of network devices to the same Firehose delivery stream. They noticed that data is not reaching Firehose as expected.</p>\n\n<p>Which of the following options would you identify as the MOST plausible root cause behind this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>When a Kinesis data stream is configured as the source of a Firehose delivery stream, Firehose\u2019s PutRecord and PutRecordBatch operations are disabled and Kinesis Agent cannot write to Firehose delivery stream directly. Data needs to be added to the Kinesis data stream through the Kinesis Data Streams PutRecord and PutRecords operations instead. Therefore, this option is correct.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</strong> - Kinesis Agent is a stand-alone Java-based software application that offers an easy way to collect and send data to Kinesis Data Streams or Kinesis Firehose. So this option is incorrect.</p>\n\n<p><strong>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</strong> - Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. Therefore this option is not correct.</p>\n\n<p>How Kinesis Firehose works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><strong>The data sent by Kinesis Agent is lost because of a configuration error</strong> - This is a made-up option and has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n", "answers": ["<p>Kinesis Agent can only write to Kinesis Data Streams, not to Kinesis Firehose</p>", "<p>Kinesis Firehose delivery stream has reached its limit and needs to be scaled manually</p>", "<p>The data sent by Kinesis Agent is lost because of a configuration error</p>", "<p>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</p>"]}, "correct_response": ["d"], "section": "Domain 1: Collection", "question_plain": "A subscription streaming service company uses AWS Cloud for analytics, recommendation engines and video transcoding. To monitor and optimize this network, the analytics team at the company has developed a solution for ingesting, augmenting, and analyzing the multiple terabytes of data its network generates daily in the form of virtual private cloud (VPC) flow logs. This would enable the company to identify performance-improvement opportunities such as identifying apps that are communicating across regions and collocating them. The VPC flow logs data is funneled into Kinesis Data Streams which further acts as the source of a delivery stream for Kinesis Firehose. The engineering team has now configured a Kinesis Agent to send the VPC flow logs data from another set of network devices to the same Firehose delivery stream. They noticed that data is not reaching Firehose as expected.\n\nWhich of the following options would you identify as the MOST plausible root cause behind this issue?", "related_lectures": []}, {"_class": "assessment", "id": 54888254, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is looking at developing an Internet-of-Things (IoT) solution that would analyze real-time clickstream events from embedded sensors in consumer electronic devices. The company has hired you as an AWS Certified Data Analytics Specialist to consult the engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications.</p>\n\n<p>Which solution would provide a highly available and fault-tolerant solution to capture the clickstream events from the source and also provide a simultaneous feed of the data stream to the downstream applications?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Kinesis Data Streams to facilitate multiple applications to consume the same streaming data concurrently and independently</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q10-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you\u2019re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p><strong>Use AWS Kinesis Data Analytics to facilitate multiple applications to consume and analyze the same streaming data concurrently and independently</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries on streaming data, therefore this option is incorrect.</p>\n\n<p><strong>Use Amazon SQS to facilitate multiple applications to process the same streaming data concurrently and independently</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n", "answers": ["<p>Use AWS Kinesis Data Streams to facilitate multiple applications to consume the same streaming data concurrently and independently</p>", "<p>Use AWS Kinesis Data Firehose to allow applications to consume the same streaming data concurrently and independently</p>", "<p>Use AWS Kinesis Data Analytics to facilitate multiple applications to consume and analyze the same streaming data concurrently and independently</p>", "<p>Use Amazon SQS to facilitate multiple applications to process the same streaming data concurrently and independently</p>"]}, "correct_response": ["a"], "section": "Domain 1: Collection", "question_plain": "A company is looking at developing an Internet-of-Things (IoT) solution that would analyze real-time clickstream events from embedded sensors in consumer electronic devices. The company has hired you as an AWS Certified Data Analytics Specialist to consult the engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications.\n\nWhich solution would provide a highly available and fault-tolerant solution to capture the clickstream events from the source and also provide a simultaneous feed of the data stream to the downstream applications?", "related_lectures": []}, {"_class": "assessment", "id": 54888256, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has developed an end-to-end AWS cloud-based Internet-of-Things (IoT) solution that provides customers with integrated IoT functionality in devices including baby monitors, security cameras and entertainment systems. The company is using Kinesis Data Streams (KDS) to process IoT data from these devices. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.</p>\n\n<p>Which of the following would you suggest to improve the performance for the given use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream.</p>\n\n<p>You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2018/08/16/enhanced_fan-out-1024x504.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications</strong></p>\n\n<p><strong>Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q11-i2.jpg\">\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n", "answers": ["<p>Swap out Kinesis Data Streams with SQS Standard queues to support the desired read throughput for the downstream applications</p>", "<p>Swap out Kinesis Data Streams with SQS FIFO queues to support the desired read throughput for the downstream applications</p>", "<p>Use Enhanced Fanout feature of Kinesis Data Streams to support the desired read throughput for the downstream applications</p>", "<p>Swap out Kinesis Data Streams with Kinesis Data Firehose to support the desired read throughput for the downstream applications</p>"]}, "correct_response": ["c"], "section": "Domain 1: Collection", "question_plain": "A company has developed an end-to-end AWS cloud-based Internet-of-Things (IoT) solution that provides customers with integrated IoT functionality in devices including baby monitors, security cameras and entertainment systems. The company is using Kinesis Data Streams (KDS) to process IoT data from these devices. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.\n\nWhich of the following would you suggest to improve the performance for the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 54888258, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company has built a custom data warehousing solution for a large shipping company by using Amazon Redshift. The solution helps the shipping company to analyze the international/domestic cargo transportation details and operational records for the ships. As part of the cost optimizations, the shipping company now wants to move any historical data (any data older than a year) into S3, as the daily analytical reports consume data for just the last year. However, the analysts at multiple divisions of the shipping company want to retain the ability to cross-reference this historical data along with the daily reports. The shipping company wants to develop a solution with the LEAST amount of effort and MINIMUM cost.</p>\n\n<p>Which option would you recommend for this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</strong></p>\n\n<p>Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis.</p>\n\n<p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables.</p>\n\n<p>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use less of your cluster's processing capacity than other queries.</p>\n\n<p>Redshift Spectrum Overview\n<img src=\"https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/18/redshift_spectrum-1.gif\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Setup access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Providing access to historical data via Athena would mean that historical data reconciliation would become difficult as the daily report would still be produced via Redshift. Such a setup is cumbersome to maintain on a day-to-day basis. Hence the option to use Athena is ruled out.</p>\n\n<p><strong>Use the Redshift COPY command to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p><strong>Use Glue ETL job to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</strong></p>\n\n<p>Loading historical data into Redshift via COPY command or via Glue ETL job would be cost-heavy for a one-time ad-hoc process. The same result can be achieved more cost-efficiently by using Redshift Spectrum. Therefore both these options to load historical data into Redshift are also incorrect for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a></p>\n", "answers": ["<p>Set up access to the historical data via Athena. The analytics team can run historical data queries on Athena and continue the daily reporting on Redshift. In case the reports need to be cross-referenced, the analytics team needs to export these in flat files and then do further analysis</p>", "<p>Use the Redshift COPY command to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>", "<p>Use Redshift Spectrum to create Redshift cluster tables pointing to the underlying historical data in S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift</p>", "<p>Use Glue ETL job to load the S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Redshift</p>"]}, "correct_response": ["c"], "section": "Domain 2: Storage and Data Management", "question_plain": "An analytics company has built a custom data warehousing solution for a large shipping company by using Amazon Redshift. The solution helps the shipping company to analyze the international/domestic cargo transportation details and operational records for the ships. As part of the cost optimizations, the shipping company now wants to move any historical data (any data older than a year) into S3, as the daily analytical reports consume data for just the last year. However, the analysts at multiple divisions of the shipping company want to retain the ability to cross-reference this historical data along with the daily reports. The shipping company wants to develop a solution with the LEAST amount of effort and MINIMUM cost.\n\nWhich option would you recommend for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 54888260, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company recently decided to go all-in on AWS and use the platform to host its website, order and stock management systems and fulfillment applications. The company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired you to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and you must validate that the data was migrated accurately from the source to the target before the cutover.</p>\n\n<p>Which of the following solutions will MOST effectively address this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</strong></p>\n\n<p>You can use AWS DMS data validation to ensure that your data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches. In addition, for a CDC-enabled task, AWS DMS compares the incremental changes and reports any mismatches. As part of data validation, DMS compares each row in the source with its corresponding row at the target and verifies that those rows contain the same data. For this comparison, DMS issues appropriate queries to retrieve the data. These queries consume additional resources at the source and the target as well as additional network resources.</p>\n\n<p>DMS data validation overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</strong> - You can use table metrics to capture statistics such as insert, update, delete, and DDL statements completed for the tables being migrated. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q13-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n\n<p><strong>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</strong> - A premigration assessment evaluates specified components of a database migration task to help identify any problems that might prevent a migration task from running as expected. This assessment gives you a chance to identify issues before you run a new or modified task. You can then fix problems before they occur while running the migration task itself. This can avoid delays in completing a given database migration needed to repair data and your database environment. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p><strong>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</strong> - You can use the AWS Schema Conversion Tool (AWS SCT) to convert your existing database schema from one database engine to another. You can convert relational OLTP schema or data warehouse schema. Your converted schema is suitable for an Amazon Relational Database Service (Amazon RDS) MySQL, MariaDB, Oracle, SQL Server, PostgreSQL DB, an Amazon Aurora DB cluster, or an Amazon Redshift cluster. This option will not help you compare the source and target data for the DMS task and report any mismatches.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics</a></p>\n", "answers": ["<p>Use the table metrics of the DMS task to verify the statistics for tables being migrated including the DDL statements completed</p>", "<p>Configure DMS data validation on the migration task so it can compare the source and target data for the DMS task and report any mismatches</p>", "<p>Configure DMS premigration assessment on the migration task so the assessment can compare the source and target data and report any mismatches</p>", "<p>Use AWS Schema Conversion Tool for the migration task so it can compare the source and target data and report any mismatches</p>"]}, "correct_response": ["b"], "section": "Domain 2: Storage and Data Management", "question_plain": "A company recently decided to go all-in on AWS and use the platform to host its website, order and stock management systems and fulfillment applications. The company wants to migrate its on-premises Oracle database to Aurora MySQL. The company has hired you to carry out the migration with minimal downtime using AWS DMS. The company has mandated that the migration must have minimal impact on the performance of the source database and you must validate that the data was migrated accurately from the source to the target before the cutover.\n\nWhich of the following solutions will MOST effectively address this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 54888262, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their data analysts with near real-time analysis of millions of rows of health data having 2,000 data points per row. The data analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The development team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the data analytics reporting, the development team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p>\n\n<p>Which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</strong></p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\">\nvia - https://aws.amazon.com/dms/</p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.</p>\n\n<p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue to replicate the data from the databases into Amazon Redshift</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.</p>\n\n<p>Using AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift, so this option is not correct.</p>\n\n<p><strong>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>Using EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally, this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>\n\n<p>However, this option involves a major development effort to write custom migration job to process the incoming data streams from the source database and reliably write into Redshift via S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n", "answers": ["<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>", "<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>", "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift</p>", "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>"]}, "correct_response": ["a"], "section": "Domain 2: Storage and Data Management", "question_plain": "A healthcare company has significant investments in running Oracle and PostgreSQL services on Amazon RDS which provide their data analysts with near real-time analysis of millions of rows of health data having 2,000 data points per row. The data analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The development team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the data analytics reporting, the development team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.\n\nWhich of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time?", "related_lectures": []}, {"_class": "assessment", "id": 54888264, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company provides solutions for diagnostic, treatment and preventative healthcare in the US. The company uses data to drive decisions, and when its on-premises database system couldn\u2019t handle the amount of data in 147 million records, the company migrated to Amazon Redshift. The technology team at the company is now working on the Disaster Recovery (DR) plans for the Redshift cluster deployed in the eu-west-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.</p>\n\n<p>Which of the following solutions would you recommend to meet the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</strong></p>\n\n<p>To copy snapshots for AWS KMS\u2013encrypted clusters to another AWS Region, you need to create a grant for Redshift to use a KMS customer master key (CMK) in the destination AWS Region. Then choose that grant when you enable copying of snapshots in the source AWS Region. You cannot use a KMS key from the source Region as AWS KMS keys are specific to an AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</strong> - As described above, you need to configure the Redshift cross-Region snapshot in the source Region and not the destination Region. Also, the snapshot copy grant must be set up in the destination Region for a KMS key in the destination Region.</p>\n\n<p><strong>Create an IAM role in destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</strong> - This has been added as a distractor as AWS KMS keys are specific to an AWS Region. You cannot create a snapshot copy grant in the destination Region for a KMS key in the source Region.</p>\n\n<p><strong>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</strong> - This has been added as a distractor as there is no such thing as cross-Region replication for Redshift. The concept of cross-Region replication (CRR) applies to Amazon S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant</a></p>\n", "answers": ["<p>Create a snapshot copy grant in the source Region for a KMS key in the source Region. Configure Redshift cross-Region snapshots in the destination Region</p>", "<p>Create an IAM role in destination Region with access to the KMS key in the source Region. Create a snapshot copy grant in the destination Region for this KMS key in the source Region. Configure Redshift cross-Region snapshots in the source Region</p>", "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region snapshots in the source Region</p>", "<p>Create a snapshot copy grant in the destination Region for a KMS key in the destination Region. Configure Redshift cross-Region replication in the source Region</p>"]}, "correct_response": ["c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A company provides solutions for diagnostic, treatment and preventative healthcare in the US. The company uses data to drive decisions, and when its on-premises database system couldn\u2019t handle the amount of data in 147 million records, the company migrated to Amazon Redshift. The technology team at the company is now working on the Disaster Recovery (DR) plans for the Redshift cluster deployed in the eu-west-1 Region. The existing cluster is encrypted via AWS KMS and the team wants to copy the Redshift snapshots to another Region to meet the DR requirements.\n\nWhich of the following solutions would you recommend to meet the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 54888266, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A digital media company has hired you to improve the data backup solution for applications running on the AWS Cloud. Currently, all of the applications running on AWS use at least two Availability Zones (AZs). The updated backup policy at the company mandates that all nightly backups for its data are durably stored in at least two geographically distinct Regions for Production and Disaster Recovery (DR) and the backup processes for both Regions must be fully automated. The new backup solution must ensure that the backup is available to be restored immediately for the Production Region and should be restored within 24 hours in the DR Region.</p>\n\n<p>Which of the following represents the MOST cost-effective solution that will address the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a backup process to persist all the data to an S3 bucket A using S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier</strong></p>\n\n<p>S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. There are two types of Replications:</p>\n\n<p>Cross-Region Replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.</p>\n\n<p>Same-Region replication (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p>\n\n<p>For the given use-case, you can set up cross-Region replication from S3 bucket A using S3 standard storage class in the production Region to S3 bucket B using S3 standard storage class in the DR Region and further create a lifecycle policy to transition this data in bucket B from standard storage class to Amazon Glacier.</p>\n\n<p>Please note the allowed transitions for S3 Lifecycle Policy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q16-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>By default, Amazon S3 stores object replicas using the same storage class as the source object. You can also specify a different storage class for the replicas. This allows you to use something like an S3 Standard-IA for the replica bucket, however, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario because the data would be moved to Glacier via a Lifecycle policy immediately.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q16-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a backup process to persist all the data to an S3 bucket A using S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier</strong> - As mentioned in the explanation above, S3 standard IA has a minimum storage duration charge of 30 days thereby making it costlier than using S3 Standard storage class for the given scenario, so this option is incorrect.</p>\n\n<p><strong>Create a backup process to persist all the data to Amazon Glacier in the Production Region. Set up cross-Region replication of this data to Amazon Glacier in the DR Region to ensure minimum possible costs in both Regions</strong> - One of the key requirements of the given scenario is to ensure that the backup is available to be restored immediately for the Production Region. However, Glacier has a first-byte latency of minutes to hours while restoring data, hence this option is not correct for the given use-case.</p>\n\n<p><strong>Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region</strong> - One of the key requirements of the given scenario is to ensure that the backup is durable but the data in an EBS volume is only replicated within its Availability Zone so it is not highly durable. However, the EBS snapshots are stored on S3 which are durable. The issue with this option is that it introduces the additional cost of an EBS volume and also does not optimize the storage cost in the DR Region as it does not leverage Glacier for the backup data storage.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n", "answers": ["<p>Create a backup process to persist all the data to Amazon Glacier in the Production Region. Set up cross-Region replication of this data to Amazon Glacier in the DR Region to ensure minimum possible costs in both Regions</p>", "<p>Create a backup process to persist all the data to an S3 bucket A using S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier</p>", "<p>Create a backup process to persist all the data to an S3 bucket A using S3 standard storage class in the Production Region. Set up cross-Region replication of this S3 bucket A to an S3 bucket B using S3 standard-IA storage class in the DR Region and set up a lifecycle policy in the DR Region to immediately move this data to Amazon Glacier</p>", "<p>Create a backup process to persist all the data to a large Amazon EBS volume attached to the backup server in the Production Region. Run nightly cron jobs to snapshot these volumes and then copy these snapshots to the DR Region</p>"]}, "correct_response": ["b"], "section": "Domain 2: Storage and Data Management", "question_plain": "A digital media company has hired you to improve the data backup solution for applications running on the AWS Cloud. Currently, all of the applications running on AWS use at least two Availability Zones (AZs). The updated backup policy at the company mandates that all nightly backups for its data are durably stored in at least two geographically distinct Regions for Production and Disaster Recovery (DR) and the backup processes for both Regions must be fully automated. The new backup solution must ensure that the backup is available to be restored immediately for the Production Region and should be restored within 24 hours in the DR Region.\n\nWhich of the following represents the MOST cost-effective solution that will address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 54888268, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data-storage service uses Amazon S3 under the hood to power its storage offerings which allow the customers to upload and view the files immediately. Currently, all the customer files are uploaded directly under a single S3 bucket. The data analytics team has started seeing scalability issues where customer file uploads are failing during the peak access hours with more than 5000 requests per second.</p>\n\n<p>Which of the following represents the MOST resource-efficient and cost-optimal way of resolving this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.</p>\n\n<p>There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second. Please see this example for more clarity on prefixes:\nif you have a file f1 stored in an S3 object path like so <code>s3://your_bucket_name/folder1/sub_folder_1/f1</code>, then <code>/folder1/sub_folder_1/</code> becomes the prefix for file f1.</p>\n\n<p>Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface used for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second. Therefore creating customer-specific custom prefixes within the single bucket and then uploading the daily files into those prefixed locations is the BEST solution for the given constraints.</p>\n\n<p>Optimizing Amazon S3 Performance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</strong> - Creating a new S3 bucket for each new customer is an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some customers may use the service sparingly but the bucket name is locked for them forever. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</strong> - Creating a new S3 bucket for each new day's data is also an inefficient way of handling resource availability (S3 buckets need to be globally unique) as some of the bucket names may not be available for daily data processing. Moreover, this is really not required as we can use S3 prefixes to improve the performance.</p>\n\n<p><strong>Change the application architecture to use S3 Glacier Deep Archive storage class</strong> - Glacier Deep Archive is designed to provide durable and secure long-term storage for large amounts of data at a price that is competitive with off-premises tape archival services. Data is stored across 3 or more AWS Availability Zones and can be retrieved in 12 hours or less. This option is a distractor because Glacier Deep Archive would not allow customers to retrieve the objects immediately.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p>\n", "answers": ["<p>Change the application architecture to create a new S3 bucket for each customer and then upload each customer's files directly under the respective buckets</p>", "<p>Change the application architecture to create a new S3 bucket for each day's data and then upload the daily files directly under that day's bucket</p>", "<p>Change the application architecture to use S3 Glacier Deep Archive storage class</p>", "<p>Change the application architecture to create customer-specific custom prefixes within the single bucket and then upload the daily files into those prefixed locations</p>"]}, "correct_response": ["d"], "section": "Domain 2: Storage and Data Management", "question_plain": "A data-storage service uses Amazon S3 under the hood to power its storage offerings which allow the customers to upload and view the files immediately. Currently, all the customer files are uploaded directly under a single S3 bucket. The data analytics team has started seeing scalability issues where customer file uploads are failing during the peak access hours with more than 5000 requests per second.\n\nWhich of the following represents the MOST resource-efficient and cost-optimal way of resolving this issue?", "related_lectures": []}, {"_class": "assessment", "id": 54888270, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has an S3 bucket that contains files in two different folders - <code>s3://my-bucket/images</code> and <code>s3://my-bucket/thumbnails</code>. When an image is first uploaded and new, it is viewed several times. Post a detailed analysis, the data analytics team has noticed that after 45 days those image files are rarely requested, but the thumbnails still are. After 180 days, the company would like to archive the image files and the thumbnails. Overall, the company would like the solution to remain highly available to prevent disasters happening against a whole AZ.</p>\n\n<p>Which of the following represent the best-fit solutions for an efficient cost strategy for the given S3 bucket? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>To manage your S3 objects, so they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p>\n\n<p>Transition actions \u2014 Define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p>\n\n<p>Expiration actions \u2014 Define when objects expire. Amazon S3 deletes expired objects on your behalf.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</strong></p>\n\n<p>S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>As the use-case mentions, after 45 days, image files are rarely requested, but the thumbnails still are. So you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to Glacier after 180 days</strong></p>\n\n<p>Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</strong> - As discussed above, you need to use a prefix while configuring the Lifecycle Policy so that only objects in the <code>s3://my-bucket/images</code> are transitioned to Standard IA and not all the objects in the bucket.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to Glacier using a prefix after 180 days</strong> - After 180 days, you can move all the objects to Glacier storage as per the use case. Glacier doesn't need prefixes for the given use-case.</p>\n\n<p><strong>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p>S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. The minimum storage duration charge is 30 days.</p>\n\n<p>Finally, S3 One Zone IA will not offer the required availability in case an AZ goes down.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n", "answers": ["<p>Create a Lifecycle Policy to transition objects to S3 One Zone IA using a prefix after 45 days</p>", "<p>Create a Lifecycle Policy to transition objects to S3 Standard IA using a prefix after 45 days</p>", "<p>Create a Lifecycle Policy to transition all objects to Glacier after 180 days</p>", "<p>Create a Lifecycle Policy to transition all objects to S3 Standard IA after 45 days</p>", "<p>Create a Lifecycle Policy to transition objects to Glacier using a prefix after 180 days</p>"]}, "correct_response": ["b", "c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A company has an S3 bucket that contains files in two different folders - s3://my-bucket/images and s3://my-bucket/thumbnails. When an image is first uploaded and new, it is viewed several times. Post a detailed analysis, the data analytics team has noticed that after 45 days those image files are rarely requested, but the thumbnails still are. After 180 days, the company would like to archive the image files and the thumbnails. Overall, the company would like the solution to remain highly available to prevent disasters happening against a whole AZ.\n\nWhich of the following represent the best-fit solutions for an efficient cost strategy for the given S3 bucket? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888272, "assessment_type": "multi-select", "prompt": {"question": "<p>A trading firm wants to migrate its on-premises Apache Hadoop cluster to an Amazon Elastic Map Reduce (EMR) cluster. The cluster is only operational during normal business hours. The EMR cluster must be highly available to prevent intraday cluster failures. The data must survive when the cluster is terminated at the end of each business day.</p>\n\n<p>Which of the following options would you recommend to address these requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>EMR File System (EMRFS) for storage</strong></p>\n\n<p>The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption. This ensures that the data persists even when the cluster is terminated at the end of each business day.</p>\n\n<p><strong>Multiple master nodes in a single Availability Zone</strong></p>\n\n<p>An EMR cluster with multiple master nodes ensures that the master node is no longer a single point of failure. If one of the master nodes fails, the cluster uses the other two master nodes and runs without interruption. This would ensure high availability to prevent intraday cluster failures.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html</a></p>\n\n<p><strong>AWS Glue Data Catalog as the metastore for Apache Hive</strong> - The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. You use the information in the Data Catalog to create and monitor your ETL jobs. The catalog would persist even when the cluster is terminated at the end of each business day.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q19-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html\">https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Hadoop Distributed File System (HDFS) for storage</strong> - Hadoop Distributed File System (HDFS) is a distributed, scalable file system for Hadoop. HDFS distributes the data it stores across instances in the cluster, storing multiple copies of data on different instances to ensure that no data is lost if an individual instance fails. HDFS is ephemeral storage that is reclaimed when you terminate a cluster. So this option is incorrect for the given use case.</p>\n\n<p><strong>MySQL database on the master node as the metastore for Apache Hive</strong> - Since the cluster is terminated at the end of each business day, the MySQL database running on the master node would also terminate, hence this option is incorrect.</p>\n\n<p><strong>Multiple master nodes in multiple Availability Zones</strong> - An EMR cluster can only reside in a single Availability Zone, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html\">https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html</a></p>\n", "answers": ["<p>Hadoop Distributed File System (HDFS) for storage</p>", "<p>EMR File System (EMRFS) for storage</p>", "<p>MySQL database on the master node as the metastore for Apache Hive</p>", "<p>AWS Glue Data Catalog as the metastore for Apache Hive</p>", "<p>Multiple master nodes in multiple Availability Zones</p>", "<p>Multiple master nodes in a single Availability Zone</p>"]}, "correct_response": ["b", "d", "f"], "section": "Domain 2: Storage and Data Management", "question_plain": "A trading firm wants to migrate its on-premises Apache Hadoop cluster to an Amazon Elastic Map Reduce (EMR) cluster. The cluster is only operational during normal business hours. The EMR cluster must be highly available to prevent intraday cluster failures. The data must survive when the cluster is terminated at the end of each business day.\n\nWhich of the following options would you recommend to address these requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 54888274, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company recently migrated to a data lake design based on Amazon S3. Amazon Redshift and Amazon QuickSight are being used by the company's data analytics team to analyze data for better insights. To ensure access to the most up-to-date actionable data, the team has now shifted to a nightly Amazon Redshift refresh utilizing terabytes of the previous day's changes. The team has noticed that post the switchover to nightly refresh, several popular dashboards that had good performance earlier, are now seeing degraded performance during the business hours as well. Amazon CloudWatch shows no notifications regarding the performance metrics.</p>\n\n<p>Which of the following represents the MOST LIKELY cause for this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads</strong></p>\n\n<p>The Redshift vacuum operation re-sorts rows and reclaims space in either a specified table or all tables in the current database. Amazon Redshift does not reclaim free space automatically. Such available space is created whenever you delete or update rows on a table. This process is a design choice inherited from PostgreSQL and a routine maintenance process that you need to follow for your tables to maximize the performance of your Redshift cluster. By running a vacuum command on one of your tables, you can reclaim any free space that is the result of delete and update operations. At the same time, the data of the table gets sorted. Thus, you end up with a compact and sorted table, which is useful for the performance of your cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a></p>\n\n<p>You should note that the automatic vacuum operations can pause if the cluster experiences a period of high load. So, in case the cluster continues to see consistently high loads during the nightly refresh, the vacuum operations won't complete, thereby causing persistent performance degradation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q20-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The nightly data refreshes are causing the queries to hang during the business hours</strong> - The nightly data refreshes cannot cause the queries to hang during the business hours since these refreshes are carried out via ETL steps that could use long-running queries such as a COPY command. This can only cause the queries to hang for the duration of the nightly refresh and not during business hours.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q20-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs\">https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs</a></p>\n\n<p><strong>Inefficient SQL queries are being run by the dashboards</strong></p>\n\n<p><strong>The Redshift cluster is undersized for the queries being executed by the dashboards</strong></p>\n\n<p>These two options have been added as distractors. Had inefficient SQL queries or an undersized Redshift cluster been the reasons behind the cluster's performance deterioration, these would have impacted the cluster even before the switchover to the nightly refresh process. Therefore, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\">https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/\">https://aws.amazon.com/about-aws/whats-new/2018/12/amazon-redshift-automatic-vacuum/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs\">https://docs.aws.amazon.com/redshift/latest/dg/queries-troubleshooting.html#queries-troubleshooting-query-hangs</a></p>\n", "answers": ["<p>Inefficient SQL queries are being run by the dashboards</p>", "<p>The Redshift cluster is undersized for the queries being executed by the dashboards</p>", "<p>The nightly data refreshes left the dashboard tables in need of a vacuum operation that could not be automatically performed by Amazon Redshift due to ongoing user workloads</p>", "<p>The nightly data refreshes are causing the queries to hang during the business hours</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A retail company recently migrated to a data lake design based on Amazon S3. Amazon Redshift and Amazon QuickSight are being used by the company's data analytics team to analyze data for better insights. To ensure access to the most up-to-date actionable data, the team has now shifted to a nightly Amazon Redshift refresh utilizing terabytes of the previous day's changes. The team has noticed that post the switchover to nightly refresh, several popular dashboards that had good performance earlier, are now seeing degraded performance during the business hours as well. Amazon CloudWatch shows no notifications regarding the performance metrics.\n\nWhich of the following represents the MOST LIKELY cause for this issue?", "related_lectures": []}, {"_class": "assessment", "id": 54888276, "assessment_type": "multi-select", "prompt": {"question": "<p>An IT company is revamping its ETL process and wants to transfer data from Amazon S3 to an Amazon Redshift cluster. The company wants to load the data in bulk onto Amazon Redshift using the best possible high-performance solution.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following steps would you recommend for this requirement? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>When loading multiple files into a single table, use a single COPY command</strong></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Amazon Redshift is an MPP (massively parallel processing) database, where all the compute nodes divide and parallelize the work of ingesting data. Each node is further subdivided into slices, with each slice having one or more dedicated cores, equally dividing the processing capacity. When you load data into Amazon Redshift, you should aim to have each slice do an equal amount of work. When splitting your data files, ensure that they are of approximately equal size \u2013 between 1 MB and 1 GB after compression. The number of files should be a multiple of the number of slices in your cluster.</p>\n\n<p>When loading multiple files into a single table, use a single COPY command for the table, rather than multiple COPY commands. Amazon Redshift automatically parallelizes the data ingestion. Using a single COPY command to bulk load data into a table ensures optimal use of cluster resources and the quickest possible throughput.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/</a></p>\n\n<p><strong>Leverage temporary staging tables during the data loading process</strong></p>\n\n<p>When you are doing bulk ETL for Redshift, AWS recommends that you use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. This allows efficient and fast transfer of these bulk datasets into Amazon Redshift.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When loading multiple files into a single table, use a single S3DistCp command</strong> - Using S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster. S3DistCp cannot be used with a Redshift cluster, so this option is incorrect.</p>\n\n<p><strong>When loading multiple files into a single table, use multiple COPY commands</strong> - As mentioned in the explanation above, when loading multiple files into a single table, use a single COPY command for the table.</p>\n\n<p><strong>Use Amazon Redshift Spectrum to upload data from multiple files in Amazon S3 into a single Amazon Redshift table</strong> - Amazon Redshift Spectrum enables you to run Amazon Redshift SQL queries on data that is stored in Amazon S3. With Redshift Spectrum, data is not uploaded into Redshift tables. Instead, the native Amazon Redshift cluster makes the invocation to Amazon Redshift Spectrum when the SQL query requests data from an external table stored in Amazon S3. You can handle multiple requests in parallel by using Amazon Redshift Spectrum on external tables to scan, filter, aggregate, and return rows from Amazon S3 into the Amazon Redshift cluster. All these operations are performed outside of Amazon Redshift, which reduces the computational load on the Amazon Redshift cluster and improves concurrency.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/top-8-best-practices-for-high-performance-etl-processing-using-amazon-redshift/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/\">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a></p>\n", "answers": ["<p>When loading multiple files into a single table, use a single S3DistCp command</p>", "<p>When loading multiple files into a single table, use a single COPY command</p>", "<p>Leverage temporary staging tables during the data loading process</p>", "<p>When loading multiple files into a single table, use multiple COPY commands</p>", "<p>Use Amazon Redshift Spectrum to upload data from multiple files in Amazon S3 into a single Amazon Redshift table</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3: Processing", "question_plain": "An IT company is revamping its ETL process and wants to transfer data from Amazon S3 to an Amazon Redshift cluster. The company wants to load the data in bulk onto Amazon Redshift using the best possible high-performance solution.\n\nAs an AWS Certified Data Analytics Specialist, which of the following steps would you recommend for this requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888278, "assessment_type": "multi-select", "prompt": {"question": "<p>An e-commerce company runs its workloads on Amazon EMR clusters. The data analytics team at the company manually installs third-party libraries on the newly launched clusters by logging onto the master nodes. The team wants to develop an automated solution that will replace this human intervention.</p>\n\n<p>Which of the following options would you recommend for the given requirement? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Upload the required installation scripts in Amazon S3 and execute them using custom bootstrap actions</strong></p>\n\n<p>You can use a bootstrap action to install additional software or customize the configuration of the EMR cluster instances. Bootstrap actions are scripts that run on the cluster after Amazon EMR launches the instance using the Amazon Linux Amazon Machine Image (AMI). Bootstrap actions run before Amazon EMR installs the applications that you specify when you create the cluster and before cluster nodes begin processing data.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html</a></p>\n\n<p><strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI using this EC2 instance and then use this AMI to launch the EMR cluster</strong></p>\n\n<p>You can create Amazon EMR clusters that have custom Amazon Machine Images (AMI) running Amazon Linux. You can create the AMI from an EC2 instance running Amazon Linux. Make sure that you have installed all the required third-party libraries on this EC2 instance. This allows you to preload additional software on your AMI and use these AMIs to launch your EMR clusters.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upload the required installation scripts in DynamoDB and use a Lambda function to execute these scripts for installing the third-party libraries on the EMR cluster</strong> - This option has been added as a distractor. You can only load installation scripts from Amazon S3 for custom bootstrap actions on the EMR cluster.</p>\n\n<p><strong>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance and then use this EC2 instance to launch the EMR cluster</strong> - You need to use an AMI to launch the EMR cluster. You cannot directly use an EC2 instance to launch an EMR cluster.</p>\n\n<p><strong>Upload the required installation scripts in Amazon S3 and execute them using AWS EMR CLI</strong> - You can automate the installation of libraries by executing the installation scripts on S3 via custom bootstrap actions. You cannot replace custom bootstrap actions with AWS EMR CLI for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/\">https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-emr-now-supports-launching-clusters-with-custom-amazon-linux-amis/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html</a></p>\n", "answers": ["<p>Upload the required installation scripts in DynamoDB and use a Lambda function to execute these scripts for installing the third-party libraries on the EMR cluster</p>", "<p>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance and then use this EC2 instance to launch the EMR cluster</p>", "<p>Upload the required installation scripts in Amazon S3 and execute them using custom bootstrap actions</p>", "<p>Provision an Amazon EC2 instance with Amazon Linux and install the required third-party libraries on the instance. Create an AMI using this EC2 instance and then use this AMI to launch the EMR cluster</p>", "<p>Upload the required installation scripts in Amazon S3 and execute them using AWS EMR CLI</p>"]}, "correct_response": ["c", "d"], "section": "Domain 3: Processing", "question_plain": "An e-commerce company runs its workloads on Amazon EMR clusters. The data analytics team at the company manually installs third-party libraries on the newly launched clusters by logging onto the master nodes. The team wants to develop an automated solution that will replace this human intervention.\n\nWhich of the following options would you recommend for the given requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888280, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A subscription streaming service delivers billions of hours of content from Amazon S3 to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.</p>\n\n<p>Which of the following is the MOST cost-effective option to store this intermediary query data?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the intermediary query results in S3 Standard storage class</strong></p>\n\n<p>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAs there is no minimum storage duration charge and no retrieval fee (remember that intermediary query results are heavily referenced by other parts of the analytics pipeline), this is the MOST cost-effective storage class amongst the given options.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the intermediary query results in S3 Glacier Instant Retrieval storage class</strong> - S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as the S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is ideal for archive data that needs immediate access, such as medical images, news media assets, or user-generated content archives.</p>\n\n<p>The minimum storage duration charge is 90 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in S3 Standard-Infrequent Access storage class</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p><strong>Store the intermediary query results in S3 One Zone-Infrequent Access storage class</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.\nThe minimum storage duration charge is 30 days, so this option is NOT cost-effective because intermediary query results need to be kept only for 24 hours. Hence this option is not correct.</p>\n\n<p>To summarize again, S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days (so instead of 24 hours, you end up paying for 30 days). S3 Standard-IA and S3 One Zone-IA also have retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, so the retrieval costs would be pretty high). Therefore, these storage classes are not cost-optimal for the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n", "answers": ["<p>Store the intermediary query results in S3 Standard storage class</p>", "<p>Store the intermediary query results in S3 Glacier Instant Retrieval storage class</p>", "<p>Store the intermediary query results in S3 Standard-Infrequent Access storage class</p>", "<p>Store the intermediary query results in S3 One Zone-Infrequent Access storage class</p>"]}, "correct_response": ["a"], "section": "Domain 2: Storage and Data Management", "question_plain": "A subscription streaming service delivers billions of hours of content from Amazon S3 to customers around the world. Amazon S3 also serves as the data lake for its big data analytics solution. The data lake has a staging zone where intermediary query results are kept only for 24 hours. These results are also heavily referenced by other parts of the analytics pipeline.\n\nWhich of the following is the MOST cost-effective option to store this intermediary query data?", "related_lectures": []}, {"_class": "assessment", "id": 54888282, "assessment_type": "multi-select", "prompt": {"question": "<p>A data analytics company needs to set up a data lake on Amazon S3 for a healthcare client. The data lake is split into raw and curated zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue-based ETL job into the curated zone. The data analysts run ad-hoc queries only on the data in the curated zone using Athena. The team is concerned about the cost of data storage in both the raw and curated zones as the data is increasing at a rate of 2 TB daily in each zone.</p>\n\n<p>Which of the following options would you implement together as the MOST cost-optimal solution? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</strong></p>\n\n<p>You can manage your objects so that they are stored cost-effectively throughout their lifecycle by configuring their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p>\n\n<p>For the given use-case, the raw zone consists of the source data, so it cannot be deleted due to compliance reasons. Therefore, you should use a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation.</p>\n\n<p>Please read more about S3 Object Lifecycle Management:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the curated zone using a compressed file format</strong></p>\n\n<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p>\n\n<p>You cannot transition the curated zone data into Glacier Deep Archive because it is used by business analysts for ad-hoc querying. Therefore, the best optimization is to have the curated zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the curated zone.</p>\n\n<p>Please see this example for a Glue ETL Pipeline:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Glue_Event-driven-ETL-Pipelines.e24d59bb79a9e24cdba7f43ffd234ec0482a60e2.png\">\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function-based job to delete the raw zone data after 1 day</strong> - As mentioned in the use case, the source data needs to be kept for a minimum of 5 years for compliance reasons. Therefore the data in the raw zone cannot be deleted after 1 day.</p>\n\n<p><strong>Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation</strong> - You cannot transition the curated zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Hence this option is incorrect.</p>\n\n<p><strong>Use Glue ETL job to write the transformed data in the curated zone using CSV format</strong> - It is cost-optimal to write the data in the curated zone using a compressed format instead of CSV format. The compressed data would reduce the storage cost incurred on the data in the curated zone. So, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n", "answers": ["<p>Setup a lifecycle policy to transition the raw zone data into Glacier Deep Archive after 1 day of object creation</p>", "<p>Create a Lambda function based job to delete the raw zone data after 1 day</p>", "<p>Setup a lifecycle policy to transition the curated zone data into Glacier Deep Archive after 1 day of object creation</p>", "<p>Use Glue ETL job to write the transformed data in the curated zone using CSV format</p>", "<p>Use Glue ETL job to write the transformed data in the curated zone using a compressed file format</p>"]}, "correct_response": ["a", "e"], "section": "Domain 2: Storage and Data Management", "question_plain": "A data analytics company needs to set up a data lake on Amazon S3 for a healthcare client. The data lake is split into raw and curated zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue-based ETL job into the curated zone. The data analysts run ad-hoc queries only on the data in the curated zone using Athena. The team is concerned about the cost of data storage in both the raw and curated zones as the data is increasing at a rate of 2 TB daily in each zone.\n\nWhich of the following options would you implement together as the MOST cost-optimal solution? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888284, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A digital media company does not want to own and manage its own IT infrastructure so it can redeploy resources toward innovation in Artificial Intelligence and related areas to create a better customer experience. As part of this digital transformation, the media company wants to archive about 9 PB of data in its on-premises data center to durable long-term storage on AWS cloud.</p>\n\n<p>What would you recommend for migrating and storing this data in the quickest and the MOST cost-optimal way?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</strong></p>\n\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large-scale data transfer and pre-processing use cases.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html</a></p>\n\n<p>The data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into AWS Glacier via a lifecycle policy. You can't directly copy data from Snowball Edge devices into AWS Glacier.</p>\n\n<p>Glacier Lifecycle Rule:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q25-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/\">https://aws.amazon.com/blogs/storage/using-aws-snowball-to-migrate-data-to-amazon-s3-glacier-for-long-term-storage/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</strong> - AWS Snowmobile is an exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100 PB per Snowmobile. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration.</p>\n\n<p>AWS recommends that you should use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q25-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a></p>\n\n<p><strong>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into AWS Glacier</strong> - As mentioned above, AWS recommends that you should use Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. So, this option is not the best fit for the given use case.</p>\n\n<p>Although you should note that for Snowmobile, you can import your data directly into Glacier.</p>\n\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data directly into AWS Glacier</strong> - As mentioned earlier, you can't directly copy data from Snowball Edge devices into AWS Glacier. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/snowball/latest/ug/how-it-works.html\">https://docs.aws.amazon.com/snowball/latest/ug/how-it-works.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/faqs/\">https://aws.amazon.com/snowmobile/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html</a></p>\n", "answers": ["<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data directly into AWS Glacier</p>", "<p>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data directly into AWS Glacier</p>", "<p>Transfer the on-premises data into a Snowmobile device. Copy the Snowmobile data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</p>", "<p>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</p>"]}, "correct_response": ["d"], "section": "Domain 2: Storage and Data Management", "question_plain": "A digital media company does not want to own and manage its own IT infrastructure so it can redeploy resources toward innovation in Artificial Intelligence and related areas to create a better customer experience. As part of this digital transformation, the media company wants to archive about 9 PB of data in its on-premises data center to durable long-term storage on AWS cloud.\n\nWhat would you recommend for migrating and storing this data in the quickest and the MOST cost-optimal way?", "related_lectures": []}, {"_class": "assessment", "id": 54888286, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services firm is modernizing its message queuing system by migrating from self-managed message-oriented middleware systems to Amazon SQS. The firm is using SQS to migrate several applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The data analytics team at the firm expects a peak rate of about 2,400 transactions per second to be processed via SQS. The messages must be processed in the order they are received.</p>\n\n<p>Which of the following options can be used to implement this system most cost-effectively?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues - Standard queues vs FIFO queues.</p>\n\n<p>For FIFO queues, the order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out). On the other hand, the standard SQS queues offer best-effort ordering. This means that occasionally, messages might be delivered in an order different from which they were sent.</p>\n\n<p>By default, FIFO queues support up to 300 transactions (API calls) per second (300 send, receive, or delete operations per second). When you batch 10 transactions per operation (maximum), FIFO queues can support up to 3,000 (300<em>10) transactions per second. Therefore, you need to process 8 transactions per operation so that the FIFO queue can support up to 2,400 (300</em>8) transactions per second, which satisfies the peak rate constraint.</p>\n\n<p>FIFO Queues Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q26-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/\">https://aws.amazon.com/about-aws/whats-new/2016/11/amazon-sqs-introduces-fifo-queues-with-exactly-once-processing-and-lower-prices-for-standard-queues/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon SQS standard queue to process the messages</strong> - As messages need to be processed in order, therefore standard queues are ruled out.</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate</strong> - This option has been added as a distractor, as SQS FIFO only supports a maximum of 10 transactions per operation in batch mode.</p>\n\n<p><strong>Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate</strong> - As mentioned earlier in the explanation, you need to use FIFO queues in batch mode and process 8 transactions per operation, so that the FIFO queue can support up to 2,400 transactions per second. With 4 transactions per operation, you can only support up to 1,200 transactions per second.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n", "answers": ["<p>Use Amazon SQS standard queue to process the messages</p>", "<p>Use Amazon SQS FIFO queue in batch mode of 12 transactions per operation to process the transactions at the peak rate</p>", "<p>Use Amazon SQS FIFO queue in batch mode of 8 transactions per operation to process the transactions at the peak rate</p>", "<p>Use Amazon SQS FIFO queue in batch mode of 4 transactions per operation to process the transactions at the peak rate</p>"]}, "correct_response": ["c"], "section": "Domain 3: Processing", "question_plain": "A financial services firm is modernizing its message queuing system by migrating from self-managed message-oriented middleware systems to Amazon SQS. The firm is using SQS to migrate several applications to the cloud to ensure high availability and cost efficiency while simplifying administrative complexity and overhead. The data analytics team at the firm expects a peak rate of about 2,400 transactions per second to be processed via SQS. The messages must be processed in the order they are received.\n\nWhich of the following options can be used to implement this system most cost-effectively?", "related_lectures": []}, {"_class": "assessment", "id": 54888288, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data analytics company leverages AWS Cloud to process Internet of Things (IoT) sensor data from the field devices of a logistics company. The analytics company stores the sensor data in Amazon DynamoDB tables. To detect anomalous behaviors and respond quickly, all changes to the items stored in the DynamoDB tables must be logged in near real-time.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following solutions would you suggest to meet the requirements of the given use-case so that it requires minimal custom development and infrastructure maintenance?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS</strong></p>\n\n<p>A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table for up to 24 hours.</p>\n\n<p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table.</p>\n\n<p>DynamoDB Streams supports the following stream record views:</p>\n\n<p>KEYS_ONLY \u2014 Only the key attributes of the modified item\nNEW_IMAGE \u2014 The entire item, as it appears after it was modified\nOLD_IMAGE \u2014 The entire item, as it appears before it was modified\nNEW_AND_OLD_IMAGES \u2014 Both the new and the old images of the item</p>\n\n<p>You can process DynamoDB streams in multiple ways. The most common approaches use AWS Lambda or a standalone application that uses the Kinesis Client Library (KCL) with the DynamoDB Streams Kinesis Adapter. The KCL is a client-side library that provides an interface to process DynamoDB stream changes. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p>\n\n<p>Please review this excellent reference architecture for DynamoDB streams design patterns:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2017/06/26/DesignPatternReference.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/</a></p>\n\n<p>For the given use-case, you can use a Lambda function to capture updates from DynamoDB Streams and send those records to KDA via KDS. You can then detect and analyze anomalies in KDA and send notifications via SNS.</p>\n\n<p>How KDS Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>How KDA Works:\n<img src=\"https://d1.awsstatic.com/architecture-diagrams/Product-Page-Diagram_Kinesis-Data-Analytics-How-it-Works@2x-updated.7340988926f37d36097e2f9099483e7e67692deb.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>\n\n<p>It is important to note that Kinesis Data Analytics (KDA) only supports the following streaming sources for an application:</p>\n\n<p>A Kinesis data stream (KDS)</p>\n\n<p>A Kinesis Data Firehose (KDF) delivery stream</p>\n\n<p>Therefore, you cannot directly write the output of the records from a Lambda function to KDA, although you can certainly use a Lambda function to pre-process the incoming stream from either KDS or KDF.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up CloudTrail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected</strong> - You can use CloudTrail to capture API calls for DynamoDB as events. The calls captured include calls from the DynamoDB console and code calls to the DynamoDB API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for DynamoDB. The CloudTrail does not support the GetRecords API for DynamoDB Streams so you cannot use it to capture the actual records. Moreover, you cannot use CloudTrail event filtering to analyze anomalous behaviors as it is just a simple filtering mechanism based on certain event attributes such as Read Only, Event Source, Event Time, etc.</p>\n\n<p><strong>Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected</strong> - CloudWatch Events service does not offer event type for DynamoDB as it's dependent on ClodTrail to get the relevant API call information. As explained above, CloudTrail itself cannot capture the DynamoDB streams records as CloudTrail does not support the GetRecords API for DynamoDB Streams. Therefore this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q27-i1.jpg\"></p>\n\n<p><strong>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS</strong> - As mentioned earlier, KDA only supports KDS and KDF as the streaming sources for an application, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-input.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-input.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/logging-using-cloudtrail.html</a></p>\n", "answers": ["<p>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records to Kinesis Data Analytics (KDA) via Kinesis Data Streams (KDS). Detect and analyze anomalies in KDA and send notifications via SNS</p>", "<p>Configure event patterns in CloudWatch Events to capture DynamoDB API call events and set up Lambda function as a target to analyze anomalous behavior. Send SNS notifications when anomalous behaviors are detected</p>", "<p>Set up DynamoDB Streams to capture and send updates to a Lambda function that outputs records directly to Kinesis Data Analytics (KDA). Detect and analyze anomalies in KDA and send notifications via SNS</p>", "<p>Set up CloudTrail to capture all API calls that update the DynamoDB tables. Leverage CloudTrail event filtering to analyze anomalous behaviors and send SNS notifications in case anomalies are detected</p>"]}, "correct_response": ["a"], "section": "Domain 3: Processing", "question_plain": "A data analytics company leverages AWS Cloud to process Internet of Things (IoT) sensor data from the field devices of a logistics company. The analytics company stores the sensor data in Amazon DynamoDB tables. To detect anomalous behaviors and respond quickly, all changes to the items stored in the DynamoDB tables must be logged in near real-time.\n\nAs an AWS Certified Data Analytics Specialist, which of the following solutions would you suggest to meet the requirements of the given use-case so that it requires minimal custom development and infrastructure maintenance?", "related_lectures": []}, {"_class": "assessment", "id": 54888290, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A logistics company has multiple AWS accounts hosting its portfolio of IT applications that serve the company's retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts.</p>\n\n<p>Which of the following solutions would you recommend to address these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</strong></p>\n\n<p>You can configure Amazon Kinesis Data Firehose to aggregate and collate CloudWatch Logs from different AWS accounts and receive their log events in a centralized logging AWS Account (this is known as cross-account data sharing) by using a CloudWatch Logs destination and then creating a Subscription Filter. This log event data can be read from a centralized Amazon Kinesis Firehose delivery stream to perform downstream processing and analysis.</p>\n\n<p>You can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. Logs that are sent to a receiving service through a subscription filter are Base64 encoded and compressed with the gzip format.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</strong> - As the Lambda function is performing an hourly export, so it's not a near-real-time solution. In addition, Lambda is not the right choice to build a high volume and high-velocity streaming solution which is better handled by using the Kinesis Family of services.</p>\n\n<p><strong>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</strong> - The CloudWatch Logs agent (on the path to deprecation) supports the collection of logs from only servers running Linux. It is recommended to use the unified CloudWatch agent. It enables you to collect both logs and advanced metrics with one agent. It offers support across operating systems, including servers running Windows Server. This agent also provides better performance. CloudWatch Logs agent cannot publish data to a Kinesis Data Firehose stream, so this option is incorrect.</p>\n\n<p><strong>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3</strong> - You can use a subscription filter with Kinesis Streams, Lambda, or Kinesis Data Firehose. So you cannot just forward events directly to CloudWatch Logs in another account. In addition, the Kinesis Data Firehose stream cannot subscribe to CloudWatch Events, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/\">https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/</a></p>\n", "answers": ["<p>Set up a new IAM role in each application AWS account with permissions to view CloudWatch Logs. Create a Lambda function to assume this new role and perform an hourly export of each AWS account's CloudWatch Logs data to an S3 bucket in the centralized logging AWS account</p>", "<p>Set up CloudWatch Logs agents to publish data to a Kinesis Data Firehose stream in the centralized logging AWS account. Create a Lambda function to read messages from the stream and push messages to Kinesis Data Firehose and then store the data in S3</p>", "<p>Set up Kinesis Data Firehose in the logging account and then subscribe the delivery stream to CloudWatch Logs streams in each application AWS account via subscription filters. Persist the log data in an Amazon S3 bucket inside the logging AWS account</p>", "<p>Set up CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the centralized logging AWS account. In the centralized logging AWS account, subscribe a Kinesis Data Firehose stream to Amazon CloudWatch Events and further use the Firehose stream to store the log data in S3</p>"]}, "correct_response": ["c"], "section": "Domain 3: Processing", "question_plain": "A logistics company has multiple AWS accounts hosting its portfolio of IT applications that serve the company's retail and enterprise customers. A CloudWatch Logs agent is installed on each of the EC2 instances running these IT applications. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage. The centralized operations team at the company needs to perform near-real-time gathering and collating events across multiple AWS accounts.\n\nWhich of the following solutions would you recommend to address these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 54888292, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A research agency stores and manages the global seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes.</p>\n\n<p>Which of the following solutions would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</strong></p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you\u2019re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p>\n\n<p>Kinesis Data Firehose Overview\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p>The correct option is to ingest the data in Kinesis Data Firehose and use a Lambda function to filter and transform the incoming data before the output is dumped on S3. This way you only need to store a sliced version of the data with only the relevant data attributes required for your model. Also, it should be noted that this solution is entirely serverless and requires no infrastructure maintenance.</p>\n\n<p>Kinesis Data Firehose to S3:\n<img src=\"https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png\">\nvia - <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: set up your streaming data sources, write your queries or streaming applications and set up your destination for processed data. Kinesis Data Analytics cannot directly ingest data from the source as it ingests data either from Kinesis Data Streams or Kinesis Data Firehose, so this option is ruled out.</p>\n\n<p><strong>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. Amazon Kinesis Data Streams is integrated with many AWS services, including Amazon Kinesis Data Firehose for near real-time transformation.</p>\n\n<p>Kinesis Data Streams cannot directly write the output to S3. In addition, KDS does not offer a plug-and-play integration with an intermediary Lambda function like Firehose does. You will need to do a lot of custom coding to get the Lambda function to process the incoming stream and then reliably dump the transformed output to S3. So this option is incorrect.</p>\n\n<p><strong>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing an EMR cluster would imply managing the underlying infrastructure so it\u2019s ruled out because the correct solution for the given use case should require the least amount of infrastructure maintenance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p>\n", "answers": ["<p>Ingest the data in Kinesis Data Firehose and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</p>", "<p>Ingest the data in Kinesis Data Analytics and use SQL queries to filter and transform the data before writing to S3</p>", "<p>Ingest the data in Kinesis Data Streams and use an intermediary Lambda function to filter and transform the incoming stream before the output is dumped on S3</p>", "<p>Ingest the data in a Spark Streaming Cluster on EMR use Spark Streaming transformations before writing to S3</p>"]}, "correct_response": ["a"], "section": "Domain 3: Processing", "question_plain": "A research agency stores and manages the global seismological data for the last 100 years. The data has a velocity of 1GB per minute. You would like to store the data with only the most relevant attributes to build a predictive model for earthquakes.\n\nWhich of the following solutions would you use to build the most cost-effective solution with the LEAST amount of infrastructure maintenance?", "related_lectures": []}, {"_class": "assessment", "id": 54888294, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Data Analytics Specialist to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.</p>\n\n<p>Which of the following solutions will you suggest to address these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</strong></p>\n\n<p>To help ingest real-time data or streaming data at large scales, you can use Amazon Kinesis Data Streams (KDS). KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources. The data collected is available in milliseconds, enabling real-time analytics. KDS provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications.</p>\n\n<p>Lambda integrates natively with Kinesis Data Streams. The polling, checkpointing, and error handling complexities are abstracted when you use this native integration. The processed data can then be configured to be saved in DynamoDB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</strong></p>\n\n<p><strong>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</strong></p>\n\n<p><strong>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates, and then store these processed updates in a SQL database running on Amazon EC2</strong></p>\n\n<p>These three options use EC2 instances as part of the solution architecture. The use-case seeks to minimize the management overhead required to maintain the solution. However, EC2 instances involve several maintenance activities such as managing the guest operating system and software deployed to the guest operating system, including updates and security patches, etc. Hence these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/\">https://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/</a></p>\n", "answers": ["<p>Push score updates to an SQS queue which uses a fleet of EC2 instances (with Auto Scaling) to process these updates in the SQS queue and then store these processed updates in an RDS MySQL database</p>", "<p>Push score updates to Kinesis Data Streams which uses a Lambda function to process these updates and then store these processed updates in DynamoDB</p>", "<p>Push score updates to Kinesis Data Streams which uses a fleet of EC2 instances (with Auto Scaling) to process the updates in Kinesis Data Streams and then store these processed updates in DynamoDB</p>", "<p>Push score updates to an SNS topic, subscribe a Lambda function to this SNS topic to process the updates and then store these processed updates in a SQL database running on Amazon EC2</p>"]}, "correct_response": ["b"], "section": "Domain 3: Processing", "question_plain": "A gaming company is developing a mobile game that streams score updates to a backend processor and then publishes results on a leaderboard. The company has hired you as an AWS Certified Data Analytics Specialist to design a solution that can handle major traffic spikes, process the mobile game updates in the order of receipt, and store the processed updates in a highly available database. The company wants to minimize the management overhead required to maintain the solution.\n\nWhich of the following solutions will you suggest to address these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 54888296, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage patterns, the data analytics team has detected that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. The team would like to preserve the SQL querying capability on the data and get the queries started immediately. Also, the team wants to adopt a pricing model that allows the company to save the maximum amount of cost on Redshift.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, which of the following options would you recommend? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Move the data to S3 Standard IA after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p><strong>Analyze the cold data with Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Moving the data to S3 Glacier will prevent us from being able to query it. Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the Redshift underlying storage to S3 IA</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large-scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p>Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.</p>\n\n<p><strong>Create a smaller Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out.</p>\n\n<p><strong>Move the data to S3 Glacier after 30 days</strong> - Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Moving the data to  S3 Glacier will prevent us from being able to query it.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html</a></p>\n", "answers": ["<p>Migrate the Redshift underlying storage to S3 IA</p>", "<p>Create a smaller Redshift Cluster with the cold data</p>", "<p>Move the data to S3 Glacier after 30 days</p>", "<p>Move the data to S3 Standard IA after 30 days</p>", "<p>Analyze the cold data with Athena</p>"]}, "correct_response": ["d", "e"], "section": "Domain 3: Processing", "question_plain": "A company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage patterns, the data analytics team has detected that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. The team would like to preserve the SQL querying capability on the data and get the queries started immediately. Also, the team wants to adopt a pricing model that allows the company to save the maximum amount of cost on Redshift.\n\nAs an AWS Certified Data Analytics Specialist, which of the following options would you recommend? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888298, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce application provides a visual search feature by letting the customer take a picture of any item and provide a wide selection of similar items that the customer can buy with just a few clicks on the app. Creating the best user experience requires that the machine learning framework of the application should identify objects and their attributes from the given set of images and return visually and contextually similar recommendations. With over a million users, the company is looking at the most cost-effective solution to store these images as well as run the underlying machine learning engine.</p>\n\n<p>Which of the following represents the best solution for this use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong> - Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. S3 offers the most cost-effective storage for storing the images of objects, thereby making it the right fit for this use case. In addition, you can use EC2 instances to deploy and host the machine learning framework of the application at the most cost-effective rate.</p>\n\n<p>S3 storage advantages:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q32-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong> - Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. If cost is the primary optimization criteria, then SageMaker does not represent the best option, as the SageMaker instances are estimated to be 40% costlier than Amazon EC2 instances.\nvia - <a href=\"https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9\">https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9</a></p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</strong></p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system that lets you share file data without provisioning or managing storage. EFS is well suited to support a broad spectrum of use cases from home directories to business-critical applications. Use cases include storage for containerized and serverless applications, big data analytics, web serving and content management, application development, and testing, media and entertainment workflows, and database backups.</p>\n\n<p>Amazon EFS offers a wide range of features but it is a costlier option than Amazon S3. Hence, both these options are incorrect for this use case.</p>\n\n<p>via - <a href=\"https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system\">https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/industries/aws-is-how-pinterest-lens-helps-pinners-find-and-buy-the-perfect-item/\">https://aws.amazon.com/blogs/industries/aws-is-how-pinterest-lens-helps-pinners-find-and-buy-the-perfect-item/</a></p>\n\n<p><a href=\"https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9\">https://medium.com/radix-ai-blog/is-sagemaker-worth-it-4b78a2082ca9</a></p>\n\n<p><a href=\"https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system\">https://cloud.netapp.com/blog/ebs-efs-amazons3-best-cloud-storage-system</a></p>\n", "answers": ["<p>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</p>", "<p>Use Amazon S3 to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</p>", "<p>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon SageMaker</p>", "<p>Use Amazon Elastic File System (Amazon EFS) to store the images of objects. The machine learning framework of the application should be hosted on Amazon EC2 instances</p>"]}, "correct_response": ["b"], "section": "Domain 2: Storage and Data Management", "question_plain": "An e-commerce application provides a visual search feature by letting the customer take a picture of any item and provide a wide selection of similar items that the customer can buy with just a few clicks on the app. Creating the best user experience requires that the machine learning framework of the application should identify objects and their attributes from the given set of images and return visually and contextually similar recommendations. With over a million users, the company is looking at the most cost-effective solution to store these images as well as run the underlying machine learning engine.\n\nWhich of the following represents the best solution for this use case?", "related_lectures": []}, {"_class": "assessment", "id": 54888300, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A legacy application is migrating its large dataset from the on-premises JDBC database to Amazon S3 for easy analysis and cost-effective storage. While migrating, the team noticed that the AWS Glue job runs for a long time and eventually fails with lost nodes.</p>\n\n<p>As a Data Analyst, which of the following options would you suggest as a corrective measure to ensure the successful completion of the Glue job?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Read the JDBC table in parallel</strong></p>\n\n<p>AWS Glue uses a single connection to read the entire dataset. If you're migrating a large JDBC table, the ETL job might run for a long time without signs of progress on the AWS Glue side. Then, the job might eventually fail because of disk space issues (lost nodes). To resolve this issue, read the JDBC table in parallel. If the job still fails with lost nodes, use an SQL expression as a pushdown predicate.</p>\n\n<p>If the table doesn't have numeric columns (INT or BIGINT), then use the <code>hashfield</code> option to partition the data. Set <code>hashfield</code> to the name of a column in the JDBC table. For best results, choose a column with an even distribution of values.</p>\n\n<p>If the table has numeric columns, set the <code>hashpartitions</code> and <code>hashexpression</code> options in the table or while creating the DynamicFrame:</p>\n\n<ol>\n<li>hashpartitions: defines the number of tasks that AWS Glue creates to read the data</li>\n<li>hashexpression: divides rows evenly among tasks</li>\n</ol>\n\n<p>If you're not partitioning the data appropriately, the data isn't distributed to the new nodes, and then you will receive the lost node errors.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The worker type configured on the Glue job should be changed to <code>Standard</code> worker type to prevent the Glue job from failing</strong></p>\n\n<p><strong>For large data sets, the worker type of the Glue job should be set to <code>G.2X</code> worker type to prevent job failure</strong></p>\n\n<p>These two options have been added as distractors. The worker types can be changed to G.1X or G.2X for memory-intensive jobs. Worker types have no impact on resolving this issue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html</a></p>\n\n<p><strong>Add more data processing units (DPUs) to process the job faster, minimizing the chances of job failure</strong> - This option has been added as a distractor. With AWS Glue, You are charged an hourly rate based on the number of Data Processing Units (or DPUs) used to run your ETL job. A single Data Processing Unit (DPU) provides 4 vCPU and 16 GB of memory. Adding more DPUs helps only if the job workload is already parallelized.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/container-released-lost-node-100-glue/\">https://aws.amazon.com/premiumsupport/knowledge-center/container-released-lost-node-100-glue/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/glue-lost-nodes-rds-s3-migration/\">https://aws.amazon.com/premiumsupport/knowledge-center/glue-lost-nodes-rds-s3-migration/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html</a></p>\n", "answers": ["<p>The worker type configured on the Glue job should be changed to <code>Standard</code> worker type to prevent the Glue job from failing</p>", "<p>Add more data processing units (DPUs) to process the job faster, minimizing the chances of job failure</p>", "<p>For large data sets, the worker type of the Glue job should be set to <code>G.2X</code> worker type to prevent job failure</p>", "<p>Read the JDBC table in parallel</p>"]}, "correct_response": ["d"], "section": "Domain 3: Processing", "question_plain": "A legacy application is migrating its large dataset from the on-premises JDBC database to Amazon S3 for easy analysis and cost-effective storage. While migrating, the team noticed that the AWS Glue job runs for a long time and eventually fails with lost nodes.\n\nAs a Data Analyst, which of the following options would you suggest as a corrective measure to ensure the successful completion of the Glue job?", "related_lectures": []}, {"_class": "assessment", "id": 54888302, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A government healthcare agency receives multiple compressed (gzip) CSV files containing data about contagious diseases for the past month aggregated from all government-managed hospitals. The files are about ~300GB and are stored in Amazon S3 Glacier. As per the government guidelines, the agency needs to query a portion of this data to prepare a report every month.</p>\n\n<p>Which of the following is the most cost-effective way to query this data?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Load the data into Amazon S3 from S3 Glacier and query the required data with Amazon S3 Select</strong> - S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster.</p>\n\n<p>You can use S3 Select to retrieve a subset of data using SQL clauses, like SELECT and WHERE, from objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only) and server-side encrypted objects.</p>\n\n<p>You can use S3 Select with AWS Lambda to build serverless applications that use S3 Select to efficiently and easily retrieve data from Amazon S3 instead of retrieving and processing entire objects. You can also use S3 Select with Big Data frameworks, such as Presto, Apache Hive, and Apache Spark to scan and filter the data in Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Glacier Select to query data from S3 Glacier directly</strong> - Using S3 Glacier Select, you can use SQL commands to query S3 Glacier archive objects that are in uncompressed CSV format. With this restriction, you can perform simple query operations only on your text-based data in S3 Glacier. Glacier Select cannot be used on compressed data.</p>\n\n<p><strong>Load the data to Amazon S3 and query it with Amazon Redshift Spectrum</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. This is a doable solution but is not cost-effective because of the costs involved for Redshift.</p>\n\n<p><strong>Load the data into Amazon S3 from S3 Glacier and query the required data with Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in any S3 storage class. Though viable, this option is costlier than S3 Select.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/\">https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/s3-glacier-select/\">https://aws.amazon.com/blogs/aws/s3-glacier-select/</a></p>\n", "answers": ["<p>Use Amazon Glacier Select to query data from S3 Glacier directly</p>", "<p>Load the data to Amazon S3 and query it with Amazon Redshift Spectrum</p>", "<p>Load the data into Amazon S3 from S3 Glacier and query the required data with Amazon S3 Select</p>", "<p>Load the data into Amazon S3 from S3 Glacier and query the required data with Amazon Athena</p>"]}, "correct_response": ["c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A government healthcare agency receives multiple compressed (gzip) CSV files containing data about contagious diseases for the past month aggregated from all government-managed hospitals. The files are about ~300GB and are stored in Amazon S3 Glacier. As per the government guidelines, the agency needs to query a portion of this data to prepare a report every month.\n\nWhich of the following is the most cost-effective way to query this data?", "related_lectures": []}, {"_class": "assessment", "id": 54888304, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A popular healthcare brand sells drugs through an online portal as well as their physical stores. The data from all these transactions is fed into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL). The Data Streams are managed via Auto Scaling configuration. On the other hand, Kinesis Client Library (KCL) ingests the incoming data into the company's warehousing system to be used for downstream analytics. Lately, the support team has come across issues arising out of duplicate records.</p>\n\n<p>Which of the following would you identify as the most likely reason for this behavior?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The producer is experiencing network-related timeouts, forcing duplicate entries into the Kinesis Data Stream</strong> - There are two primary reasons why records may be delivered more than once to your Amazon Kinesis Data Streams application: producer retries and consumer retries.</p>\n\n<p>Consider a producer that experiences a network-related timeout after it makes a call to <code>PutRecord</code>, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have been written to retry the call with the same data. If both <code>PutRecord</code> calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records.</p>\n\n<p>Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Kinesis Producer Library (KPL) is aggregating smaller records into larger records of up to 1 MB, sometimes resulting in duplicate records</strong> - The Kinesis Producer Library (KPL) aggregates small user-formatted records into larger records up to 1 MB to make better use of Amazon Kinesis Data Streams throughput. This has no bearing on the duplicate records issue.</p>\n\n<p><strong>If the <code>GetRecord</code> call fails without an acknowledgment from Amazon Kinesis Data Streams, the Kinesis Producer Library (KPL) will write the same data again</strong> - It's the <code>PutRecord</code> call that results in writing duplicate records if the acknowledgment from Amazon Kinesis Data Streams is not received.</p>\n\n<p><strong>If <code>PutRecords.Bytes</code> metric exceeds the provisioned write capacity, throttling for the stream kicks in, which results in record failures leading to re-writing of data by Kinesis Producer Library (KPL)</strong> - Kinesis Data Streams sends these stream-level metrics to CloudWatch every minute. The PutRecords.Bytes` metric depicts the number of bytes put to the Kinesis stream using the PutRecords operation over the specified period. This metric has no relation to duplicate records.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html</a></p>\n", "answers": ["<p>The Kinesis Producer Library (KPL) is aggregating smaller records into larger records of up to 1 MB, sometimes resulting in duplicate records</p>", "<p>The producer is experiencing network-related timeouts, forcing duplicate entries into the Kinesis Data Streams</p>", "<p>If the <code>GetRecord</code> call fails without an acknowledgment from Amazon Kinesis Data Streams, the Kinesis Producer Library (KPL) will write the same data again</p>", "<p>If <code>PutRecords.Bytes</code> metric exceeds the provisioned write capacity, throttling for the stream kicks-in, which results in record failures leading to re-writing of data by Kinesis Producer Library (KPL)</p>"]}, "correct_response": ["b"], "section": "Domain 1: Collection", "question_plain": "A popular healthcare brand sells drugs through an online portal as well as their physical stores. The data from all these transactions is fed into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL). The Data Streams are managed via Auto Scaling configuration. On the other hand, Kinesis Client Library (KCL) ingests the incoming data into the company's warehousing system to be used for downstream analytics. Lately, the support team has come across issues arising out of duplicate records.\n\nWhich of the following would you identify as the most likely reason for this behavior?", "related_lectures": []}, {"_class": "assessment", "id": 54888306, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of application functionality, data is pushed from Amazon Kinesis Data Firehose to Amazon Simple Storage Service (Amazon S3). However, the development team noticed that Kinesis Data Firehose is creating several small files in the Amazon S3 bucket, as opposed to a much lower expected number of files.</p>\n\n<p>Which of the following would you attribute as the most likely cause behind this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Kinesis Data Firehose delivery stream has scaled</strong></p>\n\n<p>If a limit increase was requested or Kinesis Data Firehose has automatically scaled, then the Data Firehose delivery stream can scale. By default, Kinesis Data Firehose automatically scales delivery streams up to a certain limit. Amazon Kinesis' automatic scaling behavior reduces the likelihood of throttling without requiring a limit increase.</p>\n\n<p>When Kinesis Data Firehose's delivery stream scales, it can cause an effect on the buffering hints of Data Firehose. The overall buffer size (SizeInMBs) of the delivery stream scales proportionally but inversely. For example, if the capacity of Kinesis Data Firehose increases by two times the original buffer size limit, the buffer size is halved. If Kinesis Data Firehose scales up to four times, the buffer size reduces to one-quarter of the overall buffer size.</p>\n\n<p>There is also a proportional number of parallel buffering within the Kinesis Data Firehose delivery stream, where data is delivered simultaneously from all these buffers. For example, Kinesis Data Firehose can buffer the data and create a single file based on the buffer size limit. If Kinesis Data Firehose scales to double the buffer limit, then two separate channels will create the files within the same time interval. If Kinesis Data Firehose scales up to four times, there will be four different channels creating four files in S3 during the same time interval.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When data delivery to destination is falling behind data writing to a delivery stream, Firehose starts creating multiple smaller files for storing them faster to Amazon S3</strong> - When data delivery to destination is falling behind data writing to a delivery stream, Firehose raises buffer size dynamically to catch up and make sure that all data is delivered to the destination. In these circumstances, the size of delivered S3 objects might be larger than the specified buffer size.</p>\n\n<p><strong>This behavior is noted if compression is disabled on the Kinesis Data Firehose delivery stream</strong> - This statement is incorrect. Kinesis Data Firehose delivers smaller records than specified (in the BufferingHints API) if compression is enabled on your Kinesis Data Firehose delivery stream.</p>\n\n<p><strong>If a single delivery stream is configured to deliver data to multiple Amazon S3 buckets, the parallel processing results in smaller files stored on Amazon S3</strong> - This statement is incorrect. A single delivery stream can only deliver data to one Amazon S3 bucket currently.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-small-files-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-small-files-s3/</a></p>\n", "answers": ["<p>This behavior is noted if compression is disabled on the Kinesis Data Firehose delivery stream</p>", "<p>When data delivery to destination is falling behind data writing to a delivery stream, Firehose starts creating multiple smaller files for storing them faster to Amazon S3</p>", "<p>If a single delivery stream is configured to deliver data to multiple Amazon S3 buckets, the parallel processing results in smaller files stored on Amazon S3</p>", "<p>Kinesis Data Firehose delivery stream has scaled</p>"]}, "correct_response": ["d"], "section": "Domain 3: Processing", "question_plain": "As part of application functionality, data is pushed from Amazon Kinesis Data Firehose to Amazon Simple Storage Service (Amazon S3). However, the development team noticed that Kinesis Data Firehose is creating several small files in the Amazon S3 bucket, as opposed to a much lower expected number of files.\n\nWhich of the following would you attribute as the most likely cause behind this issue?", "related_lectures": []}, {"_class": "assessment", "id": 54888308, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company operates a chain of pet-care shops and the company saves all of its data in an Amazon S3 bucket. The company uses Amazon Athena to analyze customer preferences to launch new products for the different pet types. The company's data is of the order of tens of terabytes and the company is looking at reducing the costs of running these queries.</p>\n\n<p>As an Analyst, suggest a way to reduce the cost of the Athena queries?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Partition the data by pet type</strong></p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>While analyzing the new product launches for the different pet types, most of the queries will be related to the pet type, so partitioning data by pet type will reduce the amount of data scanned by Athena and hence reduce the associated costs.</p>\n\n<p>More on partitioning Data in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><em>*Migrate the data to Redshift tables for a more cost-efficient analysis</em> - Migrating data to Redshift will increase the costs compared to directly querying data on S3 via Athena.</p>\n\n<p><strong>Partition the data by customer</strong></p>\n\n<p><strong>Partition the data by store region</strong></p>\n\n<p>Partitioning data by customers or by store region will not help in optimizing the queries relevant to the given use case, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n", "answers": ["<p>Partition the data by pet type</p>", "<p>Partition the data by customer</p>", "<p>Migrate the data to Redshift tables for a more cost-efficient analysis</p>", "<p>Partition the data by store region</p>"]}, "correct_response": ["a"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A company operates a chain of pet-care shops and the company saves all of its data in an Amazon S3 bucket. The company uses Amazon Athena to analyze customer preferences to launch new products for the different pet types. The company's data is of the order of tens of terabytes and the company is looking at reducing the costs of running these queries.\n\nAs an Analyst, suggest a way to reduce the cost of the Athena queries?", "related_lectures": []}, {"_class": "assessment", "id": 54888310, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An AWS Database Migration Service (AWS DMS) task is migrating data to Amazon Redshift as the target. The task has changed to one-by-one mode even though the configuration was set to bulk operation mode.</p>\n\n<p>What is the reason for this change and how can it be fixed?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The bulk operation task of AWS DMS failed, forcing the task to change to one-by-one mode. When all the transactions from this failed batch are applied, the AWS DMS will switch back to Batch Apply mode automatically</strong></p>\n\n<p>AWS Database Migration Service (AWS DMS) helps you migrate databases to AWS. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.</p>\n\n<p>AWS Database Migration Service:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q38-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n\n<p>When a migration task that is replicating data to Amazon Redshift has an issue applying a batch, AWS DMS doesn't fail the whole batch. AWS DMS breaks the batch down and switches to a one-by-one mode to apply transactions. When AWS DMS encounters the transaction that caused the batch to fail, AWS DMS logs the transaction to the awsdms_apply_exceptions table on the Amazon Redshift target. Then, AWS DMS applies the other transactions in the batch one by one until all transactions from that batch are applied onto the target. Finally, AWS DMS switches back to Batch Apply mode for a new batch and continues to use Batch Apply unless another batch fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>An error in Amazon Redshift storage can cause the migration task to fail</strong> - Generally, if the replication instance storage is full, a migration task can fail silently with no errors.</p>\n\n<p><strong>The failed transaction logged in the awsdms_apply_exceptions table for the Amazon Redshift target should be reset after an error so that AWS DMS can continue in batch mode</strong> - This statement is incorrect and given only as a distractor. When AWS DMS encounters the transaction that caused the batch to fail, AWS DMS logs the transaction to the awsdms_apply_exceptions table on the Amazon Redshift target. Then, AWS DMS applies the other transactions in the batch one by one until all transactions from that batch are applied onto the target. Finally, AWS DMS switches back to Batch Apply mode for a new batch and continues to use Batch Apply unless another batch fails.</p>\n\n<p><strong>The bulk operation task of AWS DMS failed, forcing the task to change to one-by-one mode. The transactions are applied in one-by-one mode till the AWS DMS encounters the transaction that caused the batch to fail. After logging this failed record, the rest of the batch will be processed in Batch mode automatically</strong> - As discussed above, the failed task will continue in one-by-one mode till all the records in the batch are processed and then switches back to Batch Apply mode for a new batch.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dms-task-redshift-bulk-operation/\">https://aws.amazon.com/premiumsupport/knowledge-center/dms-task-redshift-bulk-operation/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dms-task-failed-no-errors/\">https://aws.amazon.com/premiumsupport/knowledge-center/dms-task-failed-no-errors/</a></p>\n", "answers": ["<p>An error in Amazon Redshift storage can cause the migration task to fail</p>", "<p>The bulk operation task of AWS DMS failed, forcing the task to change to one-by-one mode. When all the transactions from this failed batch are applied, the AWS DMS will switch back to Batch Apply mode automatically</p>", "<p>The failed transaction logged in the awsdms_apply_exceptions table for the Amazon Redshift target should be reset after an error so that AWS DMS can continue in batch mode</p>", "<p>The bulk operation task of AWS DMS failed, forcing the task to change to one-by-one mode. The transactions are applied in one-by-one mode till the AWS DMS encounters the transaction that caused the batch to fail. After logging this failed record, the rest of the batch will be processed in Batch mode automatically</p>"]}, "correct_response": ["b"], "section": "Domain 3: Processing", "question_plain": "An AWS Database Migration Service (AWS DMS) task is migrating data to Amazon Redshift as the target. The task has changed to one-by-one mode even though the configuration was set to bulk operation mode.\n\nWhat is the reason for this change and how can it be fixed?", "related_lectures": []}, {"_class": "assessment", "id": 54888312, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company wants to develop a user click analytics dashboard to see near-real-time user click patterns. The user clicks are currently ingested from various devices through Amazon Kinesis Data Streams. The dashboard must be refreshed automatically every ten seconds to display the most updated data. The company is looking for an easy-to-implement solution that can be put into production as soon as possible.</p>\n\n<p>Which solution would you recommend for the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service. Visualize the data by using OpenSearch (Kibana) dashboards</strong></p>\n\n<p>You can load streaming data into your Amazon OpenSearch Service domain from many different sources. Some sources, like Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, have built-in support for OpenSearch Service. Others, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers.</p>\n\n<p>Amazon Kinesis Data Firehose manages all underlying infrastructure, storage, networking, and configuration needed to capture and load your data into Amazon S3, Amazon Redshift, or Amazon OpenSearch Service. OpenSearch (Kibana) dashboards support auto-refresh of data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Glue streaming ETL to store the data into Amazon S3. Use S3 Analytics for analyzing the data</strong> - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. S3 Analytics helps monitor S3 storage patterns and is not useful for running real-time analytics on data stored on S3.</p>\n\n<p><strong>Use Amazon Managed Streaming for Apache Kafka (MSK) to read the data in near-real-time. Develop a custom application for the dashboard by using D3.js</strong> - While Apache Kafka has several advantages, it is not easy to set up and takes more development time. Hence, is not an option for this use case.</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to push data into Amazon S3. Use Amazon QuickSight to build the dashboards from S3 data</strong> - Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. Amazon QuickSight connects to your data in the cloud and combines data from many different sources.</p>\n\n<p>QuickSight has options to refresh data on a Daily, Weekly, or Monthly basis. For enterprise edition only, you have an option to choose Hourly. Hence, QuickSight is an incorrect option since the expected refresh rate cannot be met.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.amazonaws.cn/en/kinesis/data-firehose/faqs/\">https://www.amazonaws.cn/en/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html\">https://docs.aws.amazon.com/quicksight/latest/user/refreshing-imported-data.html</a></p>\n", "answers": ["<p>Use AWS Glue streaming ETL to store the data into Amazon S3. Use S3 Analytics for analyzing the data</p>", "<p>Use Amazon Managed Streaming for Apache Kafka (MSK) to read the data in near-real-time. Develop a custom application for the dashboard by using D3.js</p>", "<p>Use Amazon Kinesis Data Firehose to push data into Amazon S3. Use Amazon QuickSight to build the dashboards from S3 data</p>", "<p>Use Amazon Kinesis Data Firehose to push the data into an Amazon OpenSearch Service. Visualize the data by using OpenSearch (Kibana) dashboards</p>"]}, "correct_response": ["d"], "section": "Domain 4: Analysis and Visualization", "question_plain": "An e-commerce company wants to develop a user click analytics dashboard to see near-real-time user click patterns. The user clicks are currently ingested from various devices through Amazon Kinesis Data Streams. The dashboard must be refreshed automatically every ten seconds to display the most updated data. The company is looking for an easy-to-implement solution that can be put into production as soon as possible.\n\nWhich solution would you recommend for the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 54888314, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Once a month, the sales team at a luxury car company presents an Amazon QuickSight graphic visualization of the different models of cars sold that should illustrate hierarchically the total quantity as well as the total sales for each model.</p>\n\n<p>Which QuickSight visual best fits the requirements of the sales team?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Tree Maps</strong></p>\n\n<p>To visualize one or two measures for a dimension, you can use tree maps.</p>\n\n<p>Each rectangle on the tree map represents one item in the dimension. Rectangle size represents the proportion of the value for the selected measure that the item represents compared to the whole for the dimension. You can optionally use rectangle color to represent another measure for the item. Rectangle color represents where the value for the item falls in the range for the measure, with darker colors indicating higher values and lighter colors indicating lower ones.</p>\n\n<p>Color gradient customization on heat and tree maps allows you to select colors for lower, intermediate, and upper limits so that the gradient is applied within these colors. You can configure this under Color in the visual settings.</p>\n\n<p>Tree maps can be used to represent hierarchical data in the form of nested rectangles. The nodes in this type of chart are categorized with two dimensions of size and color, giving us a very compact and data-rich visualization.</p>\n\n<p>Visualization via Tree Maps:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://towardsdatascience.com/awesome-aws-quicksight-visuals-every-data-analyst-should-know-e4e9302b2711\">https://towardsdatascience.com/awesome-aws-quicksight-visuals-every-data-analyst-should-know-e4e9302b2711</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Waterfall Charts</strong> - Use a waterfall chart to visualize a sequential summation as values are added or subtracted. In a waterfall chart, the initial value goes through a (positive or negative) change, with each change represented as a bar. The final total is represented by the last bar. Waterfall charts are also known as bridges because the connectors between the bars bridge the bars together, showing that they visually belong to the same story.</p>\n\n<p><strong>Funnel Charts</strong> - Use a funnel chart to visualize data that moves across multiple stages in a linear process. In a funnel chart, each stage of a process is represented in blocks of different shapes and colors. The first stage, known as the head, is the largest block and is followed by the smaller stages, known as the neck, in a funnel shape. The size of the block representing each stage in a funnel chart is a percentage of the total and is proportionate to its value. The bigger the size of the block, the bigger its value.</p>\n\n<p>Funnel charts are often useful in business contexts because you can view trends or potential problem areas in each stage, such as bottlenecks. For example, they can help you visualize the amount of the potential revenue in each stage of a sale, from first contact to final sale and on through maintenance.</p>\n\n<p><strong>Heat Maps</strong> - Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range. Heat maps can also be used to show the count of values for the intersection of the two dimensions.</p>\n\n<p>Each rectangle on a heat map represents the value for the specified measure for the intersection of the selected dimensions. Rectangle color represents where the value falls in the range for the measure, with darker colors indicating higher values and lighter colors indicating lower ones.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/tree-map.html\">https://docs.aws.amazon.com/quicksight/latest/user/tree-map.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/waterfall-chart.html\">https://docs.aws.amazon.com/quicksight/latest/user/waterfall-chart.html</a></p>\n\n<p><a href=\"https://towardsdatascience.com/awesome-aws-quicksight-visuals-every-data-analyst-should-know-e4e9302b2711\">https://towardsdatascience.com/awesome-aws-quicksight-visuals-every-data-analyst-should-know-e4e9302b2711</a></p>\n", "answers": ["<p>Waterfall Charts</p>", "<p>Funnel Charts</p>", "<p>Tree Maps</p>", "<p>Heat Maps</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "Once a month, the sales team at a luxury car company presents an Amazon QuickSight graphic visualization of the different models of cars sold that should illustrate hierarchically the total quantity as well as the total sales for each model.\n\nWhich QuickSight visual best fits the requirements of the sales team?", "related_lectures": []}, {"_class": "assessment", "id": 54888316, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application writes real-time streaming data of chats into Amazon Kinesis Data Streams partitioned by user id. Before writing this data into an Amazon Elasticsearch Service cluster (now Amazon OpenSearch Service), an AWS Lambda function checks the content for validation. The validation procedure must receive the data of a specific user in the sequence in which the Kinesis data stream received it without changing the order. But, during peak hours, the lag between data received in Kinesis Data Streams to the data reaching OpenSearch Service is very high, thereby resulting in data anomalies.</p>\n\n<p>Which of the following is the best way to fix this issue with the least amount of operational overhead?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Increase the number of shards in the Kinesis data stream to accommodate the increased data during peak hours</strong></p>\n\n<p>Amazon Kinesis Data Streams supports resharding, which lets you adjust the number of shards in your stream to adapt to changes in the rate of data flow through the stream. Resharding is considered an advanced operation.</p>\n\n<p>Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis, splitting increases the cost of your stream.</p>\n\n<p>The capacity limits of a Kinesis stream are defined by the number of shards within the stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a <code>ProvisionedThroughputExceeded</code> exception. If this is due to a temporary rise of the stream\u2019s input data rate, retry by the data producer will eventually lead to the completion of the requests. If this is due to a sustained rise of the stream\u2019s input data rate, you should increase the number of shards within your stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the stream\u2019s input data rate and the occurrence of <code>ProvisionedThroughputExceeded</code> exceptions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q41-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The validation process should be moved from AWS Lambda to Amazon Firehose to accommodate the high volumes of data</strong> - AWS Lambda is a highly scalable compute service. You can also increase the concurrency by processing multiple batches from each shard in parallel with Lambda. Lambda can process up to 10 batches in each shard simultaneously. If you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level. Hence, replacing Amazon Lambda with Amazon Firehose is not a correct option.</p>\n\n<p><strong>Replace Amazon Data Streams functionality with Apache Kafka to deal with the high volume of data</strong> - Shifting to Apache Kafka is not an option here since the user wants a solution with the least amount of operational overhead. Adding Kafka into the existing solution would involve significant development effort.</p>\n\n<p><strong>Multiple consumer applications must be reading from the Data Stream exceeding the per-shard limits. Define different Data Streams for different consumer applications</strong> - The use case does not talk about multiple consumers. Hence, this option is irrelevant to the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n", "answers": ["<p>The validation process should be moved from AWS Lambda to Amazon Firehose to accommodate the high volumes of data</p>", "<p>Replace Amazon Data Streams functionality with Apache Kafka to deal with the high volume of data</p>", "<p>Multiple consumer applications must be reading from the Data Stream exceeding the per-shard limits. Define different Data Streams for different consumer applications</p>", "<p>Increase the number of shards in the Kinesis data stream to accommodate the increased data during peak hours</p>"]}, "correct_response": ["d"], "section": "Domain 3: Processing", "question_plain": "An application writes real-time streaming data of chats into Amazon Kinesis Data Streams partitioned by user id. Before writing this data into an Amazon Elasticsearch Service cluster (now Amazon OpenSearch Service), an AWS Lambda function checks the content for validation. The validation procedure must receive the data of a specific user in the sequence in which the Kinesis data stream received it without changing the order. But, during peak hours, the lag between data received in Kinesis Data Streams to the data reaching OpenSearch Service is very high, thereby resulting in data anomalies.\n\nWhich of the following is the best way to fix this issue with the least amount of operational overhead?", "related_lectures": []}, {"_class": "assessment", "id": 54888318, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has set a strategic aim to boost employee diversity. The HR department at the company has hired a data analytics team to develop a dashboard with data visualizations that will allow stakeholders to see the historical hiring patterns of the employees. Access to all dashboards should be through Microsoft Active Directory that should cater to the company's security policy of encrypting data-in-transit and at-rest.</p>\n\n<p>Which option would you identify as the right solution that satisfies the criteria given by the company?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 (and the default encryption settings)</strong></p>\n\n<p>Amazon QuickSight Enterprise edition integrates with your existing directories, using either Microsoft Active Directory or single sign-on (SSO) using Security Assertion Markup Language (SAML).</p>\n\n<p>Amazon QuickSight supports identity federation in both Standard and Enterprise editions. When you use federated identities, you can manage users with your enterprise identity provider (IdP) and use AWS Identity and Access Management (IAM) to authenticate users when they sign in to Amazon QuickSight.</p>\n\n<p>You can use a third-party identity provider that supports through Security Assertion Markup Language 2.0 (SAML 2.0) to provide a simple onboarding flow for your Amazon QuickSight users. Such identity providers include Microsoft Active Directory Federation Services, Okta, and Ping One Federation Server.</p>\n\n<p>With identity federation, your users get one-click access to their Amazon QuickSight applications using their existing identity credentials. You also have the security benefit of identity authentication by your identity provider. You can control which users have access to Amazon QuickSight using your existing identity provider.</p>\n\n<p>Enterprise edition additionally offers encryption at rest and Microsoft Active Directory integration. In the Enterprise Edition, you select a Microsoft Active Directory directory in AWS Directory Service. You use that active directory to identify and manage your Amazon QuickSight users and administrators</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure QuickSight to use customer-provided keys imported into AWS KMS</strong></p>\n\n<p><strong>Use Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0. (and the default encryption settings)</strong></p>\n\n<p>Amazon QuickSight Standard edition is not integrated with Microsoft Active Directory. Also, the encryption at rest feature is not available for the standard edition. Therefore, both these options are incorrect.</p>\n\n<p><strong>Use Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS</strong> - This option is incorrect since all keys associated with Amazon QuickSight are managed by AWS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/editions.html\">https://docs.aws.amazon.com/quicksight/latest/user/editions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp\">https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers-setting-up-saml.html#external-identity-providers-config-idp</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html\">https://docs.aws.amazon.com/quicksight/latest/user/aws-directory-service.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html\">https://docs.aws.amazon.com/quicksight/latest/user/external-identity-providers.html</a></p>\n", "answers": ["<p>Use Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0. (and the default encryption settings)</p>", "<p>Use Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory. Configure QuickSight to use customer-provided keys imported into AWS KMS</p>", "<p>Use Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 (and the default encryption settings)</p>", "<p>Use Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory. Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A company has set a strategic aim to boost employee diversity. The HR department at the company has hired a data analytics team to develop a dashboard with data visualizations that will allow stakeholders to see the historical hiring patterns of the employees. Access to all dashboards should be through Microsoft Active Directory that should cater to the company's security policy of encrypting data-in-transit and at-rest.\n\nWhich option would you identify as the right solution that satisfies the criteria given by the company?", "related_lectures": []}, {"_class": "assessment", "id": 54888320, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to store all of its consumer data on Amazon S3. Before storing the data, the company must clean it by standardizing the formats of a few of the data columns. A single data record might range in size from 500 KB to 10 MB.</p>\n\n<p>Which of these options represents the right solution?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Managed Streaming for Apache Kafka. Create a topic for the initial raw data. Use a Kafka producer to write data on this topic. Use the Apache Kafka consumer API to create a consumer application (that can be hosted on Amazon EC2 instance) that reads data from this topic, transforms the data as needed, and writes it to Amazon S3 for final storage</strong></p>\n\n<p>Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Apache Kafka is an open-source platform for building real-time streaming data pipelines and applications. With Amazon MSK, you can use native Apache Kafka APIs to populate data lakes, stream changes to and from databases, and power machine learning and analytics applications.</p>\n\n<p>Apache Kafka stores records in topics. Data producers write records to topics and consumers read records from topics. Each record in Apache Kafka consists of a key, a value, and a timestamp.</p>\n\n<p>Amazon MSK is preferred over Amazon Kinesis Data Streams and Kinesis Firehose because of the laters limitations on record size. The maximum record size of an Amazon MSK is 100 MB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Kinesis Data Firehose to ingest data. Configure an AWS Lambda function to cleanse/transform the data written into the Firehose delivery stream which is then delivered to Amazon S3</strong> - The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. Hence, Firehose cannot be used for this requirement where data can be of the order of 10MB.</p>\n\n<p><strong>Use Amazon Kinesis Data Streams. Configure a stream for incoming raw data. Kinesis Agent can be used to write data to the stream. Configure an Amazon Kinesis Data Analytics application to read the raw data and transform it to the necessary format before writing it to Amazon S3</strong> - For Amazon Kinesis Data Streams, the maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB). Hence, Kinesis Data Streams cannot be used for this requirement where data can be of the order of 10MB.</p>\n\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) to ingest incoming data. Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3</strong> - Amazon SQS offers a reliable, highly-scalable hosted queue for storing messages as they travel between applications or microservices. It moves data between distributed application components and helps you decouple these components. Amazon SQS provides common middleware constructs such as dead-letter queues and poison-pill management. It is not the best fit as a real-time data stream processing solution.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/msk/latest/developerguide/limits.html\">https://docs.aws.amazon.com/msk/latest/developerguide/limits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/limits.html\">https://docs.aws.amazon.com/firehose/latest/dev/limits.html</a></p>\n\n<p><a href=\"https://www.amazonaws.cn/en/kinesis/data-streams/faqs/\">https://www.amazonaws.cn/en/kinesis/data-streams/faqs/</a></p>\n", "answers": ["<p>Use Amazon Managed Streaming for Apache Kafka. Create a topic for the initial raw data. Use a Kafka producer to write data on this topic. Use the Apache Kafka consumer API to create a consumer application (that can be hosted on Amazon EC2 instance) that reads data from this topic, transforms the data as needed, and writes it to Amazon S3 for final storage</p>", "<p>Use Amazon Kinesis Data Firehose to ingest data. Configure an AWS Lambda function to cleanse/transform the data written into the Firehose delivery stream which is then delivered to Amazon S3</p>", "<p>Use Amazon Simple Queue Service (Amazon SQS) to ingest incoming data. Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3</p>", "<p>Use Amazon Kinesis Data Streams. Configure a stream for incoming raw data. Kinesis Agent can be used to write data to the stream. Configure an Amazon Kinesis Data Analytics application to read the raw data and transform it to the necessary format before writing it to Amazon S3</p>"]}, "correct_response": ["a"], "section": "Domain 1: Collection", "question_plain": "A company wants to store all of its consumer data on Amazon S3. Before storing the data, the company must clean it by standardizing the formats of a few of the data columns. A single data record might range in size from 500 KB to 10 MB.\n\nWhich of these options represents the right solution?", "related_lectures": []}, {"_class": "assessment", "id": 54888322, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data analyst is setting up Amazon QuickSight to create dashboards for the senior management. The data warehousing service is handled by the Amazon Redshift clusters, hosted in a public subnet of a VPC. Currently, data is fetched using SQL tools. When trying to launch QuickSight for the first time, QuickSight is failing with an error indicating that the connection to the data source is failing.</p>\n\n<p>What configuration changes are needed to connect QuickSight to the Redshift clusters?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add the QuickSight IP address range of the AWS Region to Amazon Redshift security group</strong></p>\n\n<p>For Amazon QuickSight to connect to an Amazon Redshift instance, you must create a new security group for that instance. This security group contains an inbound rule authorizing access from the appropriate IP address range for the Amazon QuickSight servers in that AWS Region. To create and assign a security group for an Amazon Redshift cluster, you must have AWS credentials that permit access to that cluster.</p>\n\n<p>If you activated Amazon QuickSight in multiple AWS Regions, you can create inbound rules for each Amazon QuickSight endpoint CIDR. Doing this allows Amazon QuickSight to have access to the Amazon RDS DB instance from any AWS Region defined in the inbound rules.</p>\n\n<p>An Amazon QuickSight user or administrator who uses Amazon QuickSight in multiple AWS Regions is treated as a single user. In other words, even if you are using Amazon QuickSight in every AWS Region, both your Amazon QuickSight account and your users are global.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Grant the SELECT permissions on Amazon Redshift system tables</strong> - When you connect to a data source that requires a user name, the user name must have SELECT permissions on some system tables. These permissions allow Amazon QuickSight to do things such as discover table schemas and estimate table size. So this option is not relevant to the given use case.</p>\n\n<p><strong>Use a QuickSight admin user for creating the dataset</strong> - This statement is incorrect and just used as a distractor.</p>\n\n<p><strong>Create an IAM role for QuickSight to access Amazon Redshift</strong> - In case of an incorrect IAM role being assigned to QuickSight, you would see an UnauthorizedOperation error or AccessDenied error. The given use case mentions an error where the connection has timed out, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html\">https://docs.aws.amazon.com/quicksight/latest/user/enabling-access-redshift.html</a></p>\n", "answers": ["<p>Add the QuickSight IP address range of the AWS Region to Amazon Redshift security group</p>", "<p>Grant the SELECT permissions on Amazon Redshift system tables</p>", "<p>Use a QuickSight admin user for creating the dataset</p>", "<p>Create an IAM role for QuickSight to access Amazon Redshift</p>"]}, "correct_response": ["a"], "section": "Domain 5: Security", "question_plain": "A data analyst is setting up Amazon QuickSight to create dashboards for the senior management. The data warehousing service is handled by the Amazon Redshift clusters, hosted in a public subnet of a VPC. Currently, data is fetched using SQL tools. When trying to launch QuickSight for the first time, QuickSight is failing with an error indicating that the connection to the data source is failing.\n\nWhat configuration changes are needed to connect QuickSight to the Redshift clusters?", "related_lectures": []}, {"_class": "assessment", "id": 54888324, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company produces a huge volume of data on a daily basis and it is stored in the form of .csv files on Amazon S3. The company also needs to run queries on historical data on a regular basis for reporting purposes. Currently, the company uses Amazon Athena to run SQL queries for analysis. Although Athena has worked well for the company, the volume of data fed into Amazon S3 has risen drastically leading to query lags as well as performance deterioration.</p>\n\n<p>As a Data Analyst, what will you recommend to boost the query performance and reduce lags?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a daily AWS Glue ETL job to convert the data files to Apache Parquet format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong></p>\n\n<p>AWS states that you can improve the performance of your query by compressing, partitioning, or converting your data into columnar formats. Amazon Athena supports open-source columnar data formats such as Apache Parquet and Apache ORC. Converting your data into a compressed, columnar format lowers your cost and improves query performance by enabling Athena to scan fewer data from S3 when executing your query.</p>\n\n<p>Therefore, converting the CSV files to Parquet format and partitioning will help improve the query performance in Amazon Athena.</p>\n\n<p>Also, you can use AWS Glue crawlers to automatically infer database and table schema from your data in Amazon S3 and store the associated metadata in the AWS Glue Data Catalog. Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query.</p>\n\n<p>Partitioning divides your table into parts and keeps the related data together based on column values such as date, country, region, etc. Partitions act as virtual columns. You define them at table creation, and they can help reduce the amount of data scanned per query, thereby improving performance. You can restrict the amount of data scanned by a query by specifying filters based on the partition.</p>\n\n<p>Highly recommend the following blog on performance tuning tips for Amazon Athena:\n<a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a daily AWS Glue ETL job to convert the data files to ZIP format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</strong> - The ZIP file format is not supported by Amazon Athena.</p>\n\n<p><strong>When joining two tables in Athena, specify the smaller table on the left side of the join and the larger table on the right side of the join to consume less memory and run queries faster</strong> - This statement is incorrect. When you join two tables, specify the larger table on the left side of the join and the smaller table on the right side of the join. Presto distributes the table on the right to worker nodes and then streams the table on the left to do the join. If the table on the right is smaller, then there is less memory used and the query runs faster.</p>\n\n<p><strong>Use Athena to extract the data and store it in Apache Parquet format daily. Query the extracted data</strong> - You cannot use Athena to store the existing .csv data in Apache Parquet format, as it is not an ETL tool.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-types-for-converting-data-to-apache-parquet.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html\">https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html</a></p>\n", "answers": ["<p>Configure a daily AWS Glue ETL job to convert the data files to ZIP format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</p>", "<p>When joining two tables in Athena, specify the smaller table on the left side of the join and the larger table on the right side of the join to consume less memory and run queries faster</p>", "<p>Configure a daily AWS Glue ETL job to convert the data files to Apache Parquet format and partition these converted files. Create a periodic AWS Glue crawler to automatically crawl the partitioned data on a daily basis</p>", "<p>Use Athena to extract the data and store it in Apache Parquet format daily. Query the extracted data</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A company produces a huge volume of data on a daily basis and it is stored in the form of .csv files on Amazon S3. The company also needs to run queries on historical data on a regular basis for reporting purposes. Currently, the company uses Amazon Athena to run SQL queries for analysis. Although Athena has worked well for the company, the volume of data fed into Amazon S3 has risen drastically leading to query lags as well as performance deterioration.\n\nAs a Data Analyst, what will you recommend to boost the query performance and reduce lags?", "related_lectures": []}, {"_class": "assessment", "id": 54888326, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses AWS services such as Amazon Redshift and Amazon S3 as well as their on-premises SQL Server database to store the consumer data. The company also uses Salesforce as its SaaS application. The company wants to build a dashboard that will help the managers visualize the data points from all these systems.</p>\n\n<p>Which of the following represents a simple and easy way to build the dashboard in the least possible time?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon QuickSight to connect to the data sources and generate the visualizations needed for the dashboard</strong></p>\n\n<p>Amazon QuickSight is a very fast, easy-to-use, cloud-powered business analytics service that makes it easy for all employees within an organization to build visualizations, perform ad-hoc analysis, and quickly get business insights from their data, anytime, on any device.</p>\n\n<p>QuickSight can connect to AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena, and Amazon S3. You can also upload Excel spreadsheets or flat files (CSV, TSV, CLF, and ELF), connect to on-premises databases like SQL Server, MySQL, and PostgreSQL, and import data from SaaS applications like Salesforce.</p>\n\n<p>QuickSight Key Features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/quicksight/\">https://aws.amazon.com/quicksight/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use federated queries feature of Amazon Athena to join the different data sources and visualize the data using Amazon QuickSight</strong> - Amazon Athena supports federated query, a feature that allows you to query data in sources other than Amazon Simple Storage Service (Amazon S3). You can use federated queries in Athena to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3.</p>\n\n<p>You can use different connectors to run federated queries with complex joins across different data sources with Athena and visualize the data with Amazon QuickSight. This option requires a lot of query development effort to join the data sources, so it's not the right fit for the given requirement.</p>\n\n<p><strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the visualizations needed for the dashboards</strong></p>\n\n<p><strong>Use AWS Lake Formation to migrate the data sources into Amazon S3. Configure AWS Glue Data Catalog to connect the S3 data to Amazon Athena for further analysis and visualizations. Use a Glue Crawler to automate the process</strong></p>\n\n<p>Both these options require a lot of heavy lifting to migrate data from the different sources into S3. These options are unnecessarily complex and hence not the best fit for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/quicksight/resources/faqs/\">https://aws.amazon.com/quicksight/resources/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html\">https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html</a></p>\n", "answers": ["<p>Use federated queries feature of Amazon Athena to join the different data sources and visualize the data using Amazon QuickSight</p>", "<p>Use AWS Lake Formation to migrate the data sources into Amazon S3. Configure AWS Glue Data Catalog to connect the S3 data to Amazon Athena for further analysis and visualizations. Use a Glue Crawler to automate the process</p>", "<p>Configure Amazon QuickSight to connect to the data sources and generate the visualizations needed for the dashboard</p>", "<p>Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the visualizations needed for the dashboards</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A company uses AWS services such as Amazon Redshift and Amazon S3 as well as their on-premises SQL Server database to store the consumer data. The company also uses Salesforce as its SaaS application. The company wants to build a dashboard that will help the managers visualize the data points from all these systems.\n\nWhich of the following represents a simple and easy way to build the dashboard in the least possible time?", "related_lectures": []}, {"_class": "assessment", "id": 54888328, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A healthcare company collects user health information on a daily basis and stores this data on Amazon S3 which is then queried by Amazon Athena for analysis. Any user health information older than a week is never used in the queries. The data analytics team at the company has now set up a Glue crawler to automate the process but the crawler has been running for several hours and is still unable to identify the schema of the data store.</p>\n\n<p>Which of the following options can be used to fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an exclude pattern for the Glue crawler to filter out the unwanted files</strong></p>\n\n<p>An exclude pattern tells the crawler to skip certain files or paths. Exclude patterns reduce the number of files that the crawler must list, making the crawler run faster. For example, use an exclude pattern to exclude metafiles and files that have already been crawled.</p>\n\n<p>More on crawler exclude patterns:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude\">https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Compressed files, like Apache Parquet, take longer to crawl. Instead, use uncompressed files</strong> - Compressed files take longer to crawl. That's because the crawler must download the file and decompress it before reading the first megabyte or listing the file. For Apache Parquet, Apache Avro, and Apache Orc files, the crawler doesn't crawl the first megabyte. Instead, the crawler reads the metadata stored in each file. So this option is not relevant for the given use case.</p>\n\n<p><strong>Change the <code>Scanning rate</code> parameter of Glue Crawler to a higher value for the Crawler to complete the scan faster</strong> - Scanning rate specifies the percentage of the configured read capacity units to use by the AWS Glue crawler. This parameter is available only for DynamoDB data stores.</p>\n\n<p><strong>Split larger files into smaller ones, to reduce the overhead of reading large files for the Glue Crawler</strong> - This statement is incorrect. It takes more time to crawl a large number of small files than a small number of large files. That's because the crawler must list each file and must read the first megabyte of each new file.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/long-running-glue-crawler/\">https://aws.amazon.com/premiumsupport/knowledge-center/long-running-glue-crawler/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html</a></p>\n", "answers": ["<p>Use an exclude pattern for the Glue crawler to filter out the unwanted files</p>", "<p>Change the <code>Scanning rate</code> parameter of Glue Crawler to a higher value for the Crawler to complete the scan faster</p>", "<p>Split larger files into smaller ones, to reduce the overhead of reading large files for the Glue Crawler</p>", "<p>Compressed files, like Apache Parquet, take longer to crawl. Instead, use uncompressed files</p>"]}, "correct_response": ["a"], "section": "Domain 5: Security", "question_plain": "A healthcare company collects user health information on a daily basis and stores this data on Amazon S3 which is then queried by Amazon Athena for analysis. Any user health information older than a week is never used in the queries. The data analytics team at the company has now set up a Glue crawler to automate the process but the crawler has been running for several hours and is still unable to identify the schema of the data store.\n\nWhich of the following options can be used to fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 54888330, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce application runs on a single EC2 instance and processes one Kinesis data stream that has four shards. The instance has one KCL worker configured on it. As part of application scaling, another EC2 instance has been added to this configuration.</p>\n\n<p>What is the outcome of this change?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process two shards</strong> - Resharding enables you to increase or decrease the number of shards in a stream to adapt to changes in the rate of data flowing through the stream. Resharding is typically performed by an administrative application that monitors shard data-handling metrics. Although the KCL itself doesn't initiate resharding operations, it is designed to adapt to changes in the number of shards that result from resharding.</p>\n\n<p>KCL tracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the new shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data from them. The KCL also distributes the shards in the stream across all the available workers and record processors.</p>\n\n<p>The following example illustrates how the KCL helps you handle scaling and resharding:</p>\n\n<ol>\n<li><p>For example, if your application is running on one EC2 instance, and is processing one Kinesis data stream that has four shards. This one instance has one KCL worker and four record processors (one record processor for every shard). These four record processors run in parallel within the same process.</p></li>\n<li><p>Next, if you scale the application to use another instance, you have two instances processing one stream that has four shards. When the KCL worker starts up on the second instance, it load-balances with the first instance, so that each instance now processes two shards.</p></li>\n<li><p>If you then decide to split the four shards into five shards. The KCL again coordinates the processing across instances: one instance processes three shards, and the other processes two shards. Similar coordination occurs when you merge shards.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process four shards</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case.</p>\n\n<p><strong>When the KCL worker starts up on the second instance, it can randomly pick between one to four shards depending on the load experienced by the second instance</strong> - As mentioned in the explanation above, each instance will process two shards for the given use case.</p>\n\n<p><strong>When you use the KCL, you should ensure that the number of instances exceeds the number of shards for better performance and scaling abilities, so the data processing will be paused</strong> - This option has been added as a distractor. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html</a></p>\n", "answers": ["<p>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process four shards</p>", "<p>When the KCL worker starts up on the second instance, it load-balances with the first instance, and each instance will now process two shards</p>", "<p>When you use the KCL, you should ensure that the number of instances exceeds the number of shards for better performance and scaling abilities, so the data processing will be paused</p>", "<p>When the KCL worker starts up on the second instance, it can randomly pick between one to four shards depending on the load experienced by the second instance</p>"]}, "correct_response": ["b"], "section": "Domain 3: Processing", "question_plain": "An e-commerce application runs on a single EC2 instance and processes one Kinesis data stream that has four shards. The instance has one KCL worker configured on it. As part of application scaling, another EC2 instance has been added to this configuration.\n\nWhat is the outcome of this change?", "related_lectures": []}, {"_class": "assessment", "id": 54888332, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application uses Kinesis Data Streams to process real-time data for business analytics. Monitoring this incoming and outgoing data stream from the Kinesis Data Streams is important for the performance of the system as well as the downstream applications. For a read-intensive requirement, the age for the last record in the data stream for all the  <code>GetRecords</code> requests needs to be tracked.</p>\n\n<p>Which stream-level metric will help address this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong><code>GetRecords.IteratorAgeMilliseconds</code></strong> - <code>GetRecords.IteratorAgeMilliseconds</code> measures the age in milliseconds of the last record in the stream for all GetRecords requests. A value of zero for this metric indicates that the records are current within the stream. A lower value is preferred. To monitor any performance issues, increase the number of consumers for your stream so that the data is processed more quickly. To optimize your application code, increase the number of consumers to reduce the delay in processing records.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong><code>GetRecords.Latency</code></strong> - <code>GetRecords.Latency</code> measures the time taken for each GetRecords operation on the stream over a specified time period. Confirms sufficient physical resources or record processing logic for increased stream throughput. Processes larger batches of data to reduce network and other downstream latencies in your application. The <code>GetRecords.Latency</code> metric confirms that the IDLE_TIME_BETWEEN_READS_IN_MILLIS setting is set to keep up with stream processing.</p>\n\n<p><strong><code>PutRecords.Latency</code></strong> - <code>PutRecords.Latency</code> measures the time taken for each PutRecords operation on the stream over a specified time period. If the PutRecords.Latency value is high, aggregate records into a larger file to put batch data into the Kinesis data stream.</p>\n\n<p><strong><code>ReadProvisionedThroughputExceeded</code></strong> - <code>ReadProvisionedThroughputExceeded</code> measures the count of GetRecords calls that throttled during a given time period, exceeding the service or shard limits for Kinesis Data Streams. A value of zero indicates that the data consumers aren't exceeding service quotas. Any other value indicates that the throughput limit is exceeded, requiring additional shards.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-troubleshoot/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-troubleshoot/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring.html</a></p>\n", "answers": ["<p><code>GetRecords.Latency</code></p>", "<p><code>PutRecords.Latency</code></p>", "<p><code>GetRecords.IteratorAgeMilliseconds</code></p>", "<p><code>ReadProvisionedThroughputExceeded</code></p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "An application uses Kinesis Data Streams to process real-time data for business analytics. Monitoring this incoming and outgoing data stream from the Kinesis Data Streams is important for the performance of the system as well as the downstream applications. For a read-intensive requirement, the age for the last record in the data stream for all the  GetRecords requests needs to be tracked.\n\nWhich stream-level metric will help address this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 54888334, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Amazon Redshift cluster is used to store sensitive information of a business-critical application. The regulatory guidelines mandate tracking audit logs of the Redshift cluster. The business needs to store the audit logs securely by encrypting the logs at rest. The logs are to be stored for a year at least and audits need to be conducted on the audit logs on a monthly basis.</p>\n\n<p>Which of the following is a cost-effective solution that fulfills the requirement of storing the logs securely while having access to the logs for monthly audits?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong></p>\n\n<p>Audit logging is not turned on by default in Amazon Redshift. When you turn on logging on your cluster, Amazon Redshift creates and uploads logs to Amazon S3 that capture data from the time audit logging is enabled to the present time. Each logging update is a continuation of the information that was already logged.</p>\n\n<p>Audit logging to Amazon S3 is an optional, manual process. When you enable logging on your cluster, you are enabling logging to Amazon S3 only. Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging.</p>\n\n<p>Amazon Redshift Spectrum is a feature of Amazon Redshift that lets you run queries against your data lake in Amazon S3, with no data loading or ETL required. When you issue an SQL query, it goes to the Amazon Redshift endpoint, which generates and optimizes a query plan. Amazon Redshift determines what data is local and what is in Amazon S3, generates a plan to minimize the amount of S3 data that needs to be read, and requests Amazon Redshift Spectrum workers out of a shared resource pool to read and process data from S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q50-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Copy the data into the Amazon Redshift cluster from Amazon S3 when data needs to be queried for monthly audits</strong> - Copying data into the Redshift cluster for enabling monthly audits would turn out to be a costly solution. Using Redshift Spectrum is the right fit for the given scenario.</p>\n\n<p><strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon QuickSight to query the data for monthly audits</strong></p>\n\n<p><strong>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</strong></p>\n\n<p>Currently, you can only use Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Hence, both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n", "answers": ["<p>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</p>", "<p>Enable default encryption on the Amazon S3 bucket that uses Amazon S3-managed keys (SSE-S3) encryption (AES-256) for audit logging. Copy the data into the Amazon Redshift cluster from Amazon S3 when data needs to be queried for monthly audits</p>", "<p>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon QuickSight to query the data for monthly audits</p>", "<p>Enable encryption on the Amazon S3 bucket that uses Amazon Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS) for audit logging. Use Amazon Redshift Spectrum to query the data for monthly audits</p>"]}, "correct_response": ["a"], "section": "Domain 5: Security", "question_plain": "An Amazon Redshift cluster is used to store sensitive information of a business-critical application. The regulatory guidelines mandate tracking audit logs of the Redshift cluster. The business needs to store the audit logs securely by encrypting the logs at rest. The logs are to be stored for a year at least and audits need to be conducted on the audit logs on a monthly basis.\n\nWhich of the following is a cost-effective solution that fulfills the requirement of storing the logs securely while having access to the logs for monthly audits?", "related_lectures": []}, {"_class": "assessment", "id": 54888336, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Internet-of-Things (IoT) devices company uses Amazon S3 as the data lake to store the input data that is ingested from the field devices on an hourly basis. The ingested data has attributes such as the device type, ID of the device, the status of the device, the timestamp of the event, the source IP address, etc. The data runs into millions of records per day and the company wants to run complex analytical queries on this data on a daily basis for product improvements for each device type.</p>\n\n<p>Which is the most optimal way to save this data to get the best performance from the millions of data points saved on a daily basis?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</strong> - Apache Parquet and ORC are columnar storage formats that are optimized for fast retrieval of data and used in AWS analytical applications.</p>\n\n<p>By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. You can partition your data by any key. A common practice is to partition the data based on time, often leading to a multi-level partitioning scheme. For example, a customer who has data coming in every hour might decide to partition by year, month, date, and hour. Another customer, who has data coming from many different sources but that is loaded only once per day, might partition by a data source identifier and date.</p>\n\n<p>For the given use case, as the company does daily analysis, so it only needs to look at the data generated for a given date. Hence partitioning by date offers significant performance and cost advantages. Since the company also wants to do analysis for product improvements for each device type, it is better to keep the data sorted by device type, so it allows faster query execution.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the data in Apache Parquet, partitioned by device type and sorted by date</strong> -  Apache Parquet is a columnar storage format that is optimized for fast retrieval of data and used in AWS analytical applications. However, partitioning by device type is incorrect for this use case, and partitioning by date is optimal.</p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</strong></p>\n\n<p><strong>Store the data in compressed .csv, partitioned by date and sorted by device type</strong></p>\n\n<p>Both the above options are not columnar storage formats, they are row-based formats that are not optimal for big data retrievals for complex analytical queries.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a></p>\n", "answers": ["<p>Store the data in compressed .csv, partitioned by date and sorted by the status of the device</p>", "<p>Store the data in compressed .csv, partitioned by date and sorted by device type</p>", "<p>Store the data in Apache Parquet, partitioned by device type and sorted by date</p>", "<p>Store the data in Apache ORC, partitioned by date and sorted by device type of the device</p>"]}, "correct_response": ["d"], "section": "Domain 3: Processing", "question_plain": "An Internet-of-Things (IoT) devices company uses Amazon S3 as the data lake to store the input data that is ingested from the field devices on an hourly basis. The ingested data has attributes such as the device type, ID of the device, the status of the device, the timestamp of the event, the source IP address, etc. The data runs into millions of records per day and the company wants to run complex analytical queries on this data on a daily basis for product improvements for each device type.\n\nWhich is the most optimal way to save this data to get the best performance from the millions of data points saved on a daily basis?", "related_lectures": []}, {"_class": "assessment", "id": 54888338, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A consulting firm uses Amazon Athena to analyze data in Amazon S3 using SQL. A rapid expansion plan has resulted in the company doubling its data analysts in a year resulting in high usage costs for Athena. Preliminary investigation suggests that most of the day-to-day queries run for only a few seconds fetching limited data. The firm wants a cap on the amount of data each query can fetch while also defining different thresholds on hourly or daily aggregates on data scanned by the queries.</p>\n\n<p>As an AWS Certified Data Analytics Specialist, how will you configure this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure multiple per-workgroup limits by utilizing the workgroup-wide data usage control limit on Athena</strong> - Athena allows you to set two types of cost controls: per-query limit and per-workgroup limit. For each workgroup, you can set only one per-query limit and multiple per-workgroup limits.</p>\n\n<p>The workgroup-wide data usage control limit specifies the total amount of data scanned for all queries that run in this workgroup during the specified time period. You can create multiple limits per workgroup. The workgroup-wide query limit allows you to set multiple thresholds on hourly or daily aggregates on data scanned by queries running in the workgroup.</p>\n\n<p>If the aggregate amount of data scanned exceeds the threshold, you can push a notification to an Amazon SNS topic. You can also create an alarm and an action on any metric that Athena publishes from the CloudWatch console.</p>\n\n<p>Data Usage Control Limits in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q52-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query\">https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure multiple per-query limits by utilizing the per-query cost control limit feature on Athena</strong> - The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled. You can create only one per-query control limit in a workgroup and it applies to each query that runs in it.</p>\n\n<p><strong>Configure a single per-query limit by configuring all the mentioned thresholds into a single limit configuration</strong> - This is an invalid statement, given only as a distractor.</p>\n\n<p><strong>Configure a single per-workgroup limit by configuring all the mentioned thresholds into a single limit configuration</strong> - As discussed above, multiple per-workgroup limits have to be defined to cater to hourly and daily aggregate thresholds of data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query\">https://docs.aws.amazon.com/athena/latest/ug/workgroups-setting-control-limits-cloudwatch.html#configure-control-limit-per-query</a></p>\n", "answers": ["<p>Configure multiple per-workgroup limits by utilizing the workgroup-wide data usage control limit on Athena</p>", "<p>Configure multiple per-query limits by utilizing the per-query cost control limit feature on Athena</p>", "<p>Configure a single per-query limit by configuring all the mentioned thresholds into a single limit configuration</p>", "<p>Configure a single per-workgroup limit by configuring all the mentioned thresholds into a single limit configuration</p>"]}, "correct_response": ["a"], "section": "Domain 5: Security", "question_plain": "A consulting firm uses Amazon Athena to analyze data in Amazon S3 using SQL. A rapid expansion plan has resulted in the company doubling its data analysts in a year resulting in high usage costs for Athena. Preliminary investigation suggests that most of the day-to-day queries run for only a few seconds fetching limited data. The firm wants a cap on the amount of data each query can fetch while also defining different thresholds on hourly or daily aggregates on data scanned by the queries.\n\nAs an AWS Certified Data Analytics Specialist, how will you configure this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 54888340, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IT company stores a huge volume of data on Amazon S3 buckets. For a new business requirement, the company needs to copy this data from Amazon S3 into HDFS hosted on the Amazon EMR cluster.</p>\n\n<p>Which is the most optimal way of copying this data from S3 to EMR cluster?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use <code>S3DistCp</code> to copy data between Amazon S3 and Amazon EMR clusters</strong> - Use <code>S3DistCp</code> to copy data between Amazon S3 and Amazon EMR clusters. <code>S3DistCp</code> is installed on Amazon EMR clusters by default.</p>\n\n<p><code>S3DistCp</code> is similar to <code>DistCp</code>, but optimized to work with AWS, particularly Amazon S3. The command for S3DistCp in Amazon EMR version 4.0 and later is <code>s3-dist-cp</code>, which you add as a step in a cluster or at the command line. Using <code>S3DistCp</code>, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster. You can also use <code>S3DistCp</code> to copy data between Amazon S3 buckets or from HDFS to Amazon S3. <code>S3DistCp</code> is more scalable and efficient for parallel copying large numbers of objects across buckets and across AWS accounts.</p>\n\n<p>Like <code>DistCp</code>, <code>S3DistCp</code> uses MapReduce to copy in a distributed manner. It shares the copy, error handling, recovery, and reporting tasks across several servers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Apache <code>DistCp</code> to copy data between Amazon S3 and Amazon EMR clusters</strong> - Apache DistCp is an open-source tool you can use to copy large amounts of data. S3DistCp is similar to DistCp, but optimized to work with AWS, particularly Amazon S3.</p>\n\n<p><strong>Use Database Migration Service (DMS) to copy data between Amazon S3 and Amazon EMR clusters</strong> - AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud or between combinations of cloud and on-premises setups. AWS DMS does not support copying to Amazon EMR.</p>\n\n<p><strong>Use AWS Glue job to copy data between Amazon S3 and Amazon EMR clusters</strong> - Though AWS Glue can be used for this purpose, it requires custom coding, so it is not optimal from a resource and cost-efficiency perspective.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-hdfs-emr/\">https://aws.amazon.com/premiumsupport/knowledge-center/copy-s3-hdfs-emr/</a></p>\n", "answers": ["<p>Use Apache <code>DistCp</code> to copy data between Amazon S3 and Amazon EMR clusters</p>", "<p>Use <code>S3DistCp</code> to copy data between Amazon S3 and Amazon EMR clusters</p>", "<p>Use AWS Glue job to copy data between Amazon S3 and Amazon EMR clusters</p>", "<p>Use Database Migration Service (DMS) to copy data between Amazon S3 and Amazon EMR clusters</p>"]}, "correct_response": ["b"], "section": "Domain 2: Storage and Data Management", "question_plain": "An IT company stores a huge volume of data on Amazon S3 buckets. For a new business requirement, the company needs to copy this data from Amazon S3 into HDFS hosted on the Amazon EMR cluster.\n\nWhich is the most optimal way of copying this data from S3 to EMR cluster?", "related_lectures": []}, {"_class": "assessment", "id": 54888342, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Amazon S3 is used by an organization to store the regional data in the us-west-1, us-east-1 and us-west-2 regions. To date, Amazon Athena has been used by the business as an interactive query service for all the regions to query data from the same region. With business regulations restricting query access, now only managerial users from California (us-west-1) are to be given access to run a few critical Athena queries. These queries would need data from all three regions.</p>\n\n<p>Which is the most cost-effective way of querying the entire data from the California region?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Run the AWS Glue crawler from the us-west-1 region to catalog datasets from all three regions. Once all the data is crawled, run the needed Athena queries from us-west-1</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p>An AWS Glue crawler is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog. You can use a Lambda function that is triggered by an S3:ObjectCreated:* event notification on the S3 bucket to invoke the AWS Glue crawler on-demand. This obviates the need to periodically run the crawler on a schedule to update the new data into the existing data catalog tables.</p>\n\n<p>You can use a crawler to populate the AWS Glue Data Catalog with tables. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable cross-Region replication for the S3 buckets in us-east-1, us-west-2 to replicate data to us-west-1. Once data from all the regions are replicated to us-west-1, run the AWS Glue crawler in us-west-1 to update the AWS Glue Data Catalog and run the needed Athena queries from us-west-1</strong> - Replicating data across regions is a possible solution but not a cost-effective one. Hence, this option is ruled out.</p>\n\n<p><strong>Use AWS DMS to migrate the AWS Glue Data Catalog from us-east-1 and us-west-2 to us-west-1. Run Athena queries in us-west-1 region</strong> - You can set up a DMS task for either one-time migration or ongoing replication. An ongoing replication task keeps your source and target databases in sync. But, DMS is not cost-effective for this purpose.</p>\n\n<p><strong>Use <code>S3DistCp</code> to copy data between Amazon S3 buckets present in different regions</strong> - Apache <code>DistCp</code> is an open-source tool you can use to copy large amounts of data. <code>S3DistCp</code> is similar to <code>DistCp</code>, but optimized to work with AWS, particularly Amazon S3. Using S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS. This option is not relevant to the given use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html\">https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html</a></p>\n", "answers": ["<p>Enable cross-Region replication for the S3 buckets in us-east-1, us-west-2 to replicate data to us-west-1. Once data from all the regions are replicated to us-west-1, run the AWS Glue crawler in us-west-1 to update the AWS Glue Data Catalog and run the needed Athena queries from us-west-1</p>", "<p>Use AWS DMS to migrate the AWS Glue Data Catalog from us-east-1 and us-west-2 to us-west-1. Run Athena queries in the us-west-1 region</p>", "<p>Run the AWS Glue crawler from us-west-1 region to catalog datasets from all three regions. Once all the data is crawled, run the needed Athena queries from us-west-1</p>", "<p>Use <code>S3DistCp</code> to copy data between Amazon S3 buckets present in different regions</p>"]}, "correct_response": ["c"], "section": "Domain 4: Analysis and Visualization", "question_plain": "Amazon S3 is used by an organization to store the regional data in the us-west-1, us-east-1 and us-west-2 regions. To date, Amazon Athena has been used by the business as an interactive query service for all the regions to query data from the same region. With business regulations restricting query access, now only managerial users from California (us-west-1) are to be given access to run a few critical Athena queries. These queries would need data from all three regions.\n\nWhich is the most cost-effective way of querying the entire data from the California region?", "related_lectures": []}, {"_class": "assessment", "id": 54888344, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses Amazon Simple Storage Service (Amazon S3) as a storage service for storing various media files, log files, audit files, etc. The company has hired you as an AWS Certified Data Analytics Specialist to also configure Amazon EMR to use Amazon S3 as the Hadoop storage layer instead of the Hadoop Distributed File System (HDFS).</p>\n\n<p>How will you configure this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>You can't configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer</strong></p>\n\n<p>You can't configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer. HDFS and the EMR File System (EMRFS), which uses Amazon S3, are both compatible with Amazon EMR, but they're not interchangeable. HDFS is an implementation of the Hadoop FileSystem API, which models POSIX file system behavior. EMRFS is an object store, not a file system.</p>\n\n<p>The EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You can configure Amazon EMR to use the Amazon S3 block file system for this requirement</strong> - The Amazon S3 block file system is a legacy file system that was used to support uploads to Amazon S3 that were larger than 5 GB in size. With the multipart upload functionality Amazon EMR provides through the AWS Java SDK, you can upload files of up to 5 TB in size to the Amazon S3 native file system, and the Amazon S3 block file system is deprecated. Since this legacy file system can create race conditions that can corrupt the file system, you should avoid this format and use EMRFS instead. This option is not relevant to the given use case.</p>\n\n<p><strong>You can configure Amazon EMR to use Amazon S3 as the Hadoop storage layer while launching the EMR cluster</strong> - This statement is incorrect. It is not possible to replace HDFS with Amazon S3 Glacier.</p>\n\n<p><strong>You can configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer by launching the cluster as a long-running cluster</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-emr-s3-hadoop-storage/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-file-systems.html</a></p>\n", "answers": ["<p>You can configure Amazon EMR to use Amazon S3 block file system for this requirement</p>", "<p>You can configure Amazon EMR to use Amazon S3 as the Hadoop storage layer while launching the EMR cluster</p>", "<p>You can configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer by launching the cluster as a long-running cluster</p>", "<p>You can't configure Amazon EMR to use Amazon S3 instead of HDFS for the Hadoop storage layer</p>"]}, "correct_response": ["d"], "section": "Domain 5: Security", "question_plain": "A company uses Amazon Simple Storage Service (Amazon S3) as a storage service for storing various media files, log files, audit files, etc. The company has hired you as an AWS Certified Data Analytics Specialist to also configure Amazon EMR to use Amazon S3 as the Hadoop storage layer instead of the Hadoop Distributed File System (HDFS).\n\nHow will you configure this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 54888346, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A business is moving their data to Amazon Redshift. A core table with billions of rows needs to be moved to Redshift. This table contains certain columns that have sensitive data that can only be accessed by the finance team. Once the data is moved to Redshift, queries will be run on this table by multiple teams.</p>\n\n<p>How will you configure the requirement such that the columns holding sensitive data are only accessible to members of the finance team?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Grant the finance team (defined as a group) permissions to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns having non-sensitive data to the other users</strong></p>\n\n<p>Since March 2020, Amazon Redshift supports column-level access control for data in Redshift. Customers can use column-level GRANT and REVOKE statements to help meet their security and compliance needs.</p>\n\n<p>Redshift's table-level access controls for the data in Redshift are already in use by many customers, but they also want the ability to control access in more detail. You can now control access to columns without having to implement view-based access control or use another system. Column-level access control is available in all Amazon Redshift regions.</p>\n\n<p>GRANT command defines access privileges for a user or user group. Privileges include access options such as being able to read data in tables and views, write data, create tables, and drop tables. Use this command to give specific privileges for a table, database, schema, function, procedure, language, or column.</p>\n\n<p>The syntax for column-level privileges on Amazon Redshift tables and views looks like the below:</p>\n\n<p>GRANT { { SELECT | UPDATE } ( column_name [, ...] ) [, ...] | ALL [ PRIVILEGES ] ( column_name [,...] ) }\n     ON { [ TABLE ] table_name [, ...] }\n     TO { username | GROUP group_name | PUBLIC } [, ...]</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Grant all users read-only permissions to the non-sensitive columns. Add the finance team to the administrator group so they have complete access to the table</strong> - This is a security concern as the finance team do not need access privileges as an administrator.</p>\n\n<p><strong>Grant the finance team (defined as a group) permission to read from the table. Create a second table having data only for columns with non-sensitive data. Grant read-only permissions to the second table for the rest of the users</strong> - This is not an efficient solution as the storage space would be wasted to create the second table. It is better to use column level access controls on the original table.</p>\n\n<p><strong>Grant the finance group permission to read from the table. Create a view of the new table with only those columns having non-sensitive data. Grant the rest of the users read-only permissions to this view</strong> - AWS recommends that you should use column-level access control, instead of views, to manage access to sensitive columns within a table.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html#r_GRANT-usage-notes-clp\">https://docs.aws.amazon.com/redshift/latest/dg/r_GRANT.html#r_GRANT-usage-notes-clp</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/\">https://aws.amazon.com/about-aws/whats-new/2020/03/announcing-column-level-access-control-for-amazon-redshift/</a></p>\n", "answers": ["<p>Grant all users read-only permissions to the non-sensitive columns. Add the finance team to the administrator group so they have complete access to the table</p>", "<p>Grant the finance team (defined as a group) permission to read from the table. Create a second table having data only for columns with non-sensitive data. Grant read-only permissions to the second table for the rest of the users</p>", "<p>Grant the finance team (defined as a group) permissions to read from the table. Use the GRANT SQL command to allow read-only access to a subset of columns having non-sensitive data to the other users</p>", "<p>Grant the finance group permission to read from the table. Create a view of the new table with only those columns having non-sensitive data. Grant the rest of the users read-only permissions to this view</p>"]}, "correct_response": ["c"], "section": "Domain 5: Security", "question_plain": "A business is moving their data to Amazon Redshift. A core table with billions of rows needs to be moved to Redshift. This table contains certain columns that have sensitive data that can only be accessed by the finance team. Once the data is moved to Redshift, queries will be run on this table by multiple teams.\n\nHow will you configure the requirement such that the columns holding sensitive data are only accessible to members of the finance team?", "related_lectures": []}, {"_class": "assessment", "id": 54888348, "assessment_type": "multi-select", "prompt": {"question": "<p>A company runs a real-time data processing application that uses Kinesis Client Library (KCL) to help consume and process data from the real-time data streams. The development team has raised a query on the viability of using the same DynamoDB table for different KCL applications.</p>\n\n<p>Which of the following are correct statements for KCL while consuming Kinesis Data Streams? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Each KCL application must use its own DynamoDB table</strong></p>\n\n<p>Users can't use different KCL applications with the same DynamoDB table for the following reasons:</p>\n\n<ol>\n<li><p>Scan operations are used to obtain leases from a DynamoDB table. Therefore, if a table contains leases of different KCL applications, each application could receive a lease that isn't related to the application itself.</p></li>\n<li><p>Shard IDs in streams are used as primary keys in DynamoDB tables during checkpointing. When different KCL applications use the same DynamoDB table and the same shard IDs are used in the streams, inconsistencies in checkpoints can occur.</p></li>\n</ol>\n\n<p><strong>You can only use DynamoDB for checkpointing KCL</strong> - Users can only use DynamoDB as a checkpointing table for the KCL. A DynamoDB table is required as a checkpointing table for the KCL because the KCL behavior and implementation are interconnected with DynamoDB in the following ways:</p>\n\n<ol>\n<li><p>The KCL includes <code>ShardSyncTask.java</code>, which guarantees that shard leases in a stream are included in the DynamoDB table. This check is conducted periodically in the KCL.</p></li>\n<li><p>The KCL includes <code>DynamoDBLeaseTaker.java</code> and <code>DynamoDBLeaseRenewer.java</code>, which are components that manage and update leases in the KCL. <code>DynamoDBLeaseTaker.java</code> and <code>DynamoDBLeaseRenewer.java</code> work with <code>DynamoDBLeaseRefresher.java</code> to make frequent API requests to DynamoDB.</p></li>\n<li><p>When the KCL makes checkpoints, requests from <code>DynamoDBCheckpointer.java</code> and <code>DynamoDBLeaseCoordinator.java</code> are made to DynamoDB.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Multiple KCL applications can share a DynamoDB table</strong></p>\n\n<p><strong>Amazon Relational Database Service (Amazon RDS) can also be used for checkpointing KCL</strong></p>\n\n<p><strong>Multiple KCL applications can share a DynamoDB table if Amazon Amazon Simple Storage Service (Amazon S3) is configured to save transit data and metadata</strong></p>\n\n<p>These three options are incorrect. Since each KCL application must use its own DynamoDB table and only DynamoDB can be used for checkpointing KCL.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-kcl-apps-dynamodb-table/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-kcl-apps-dynamodb-table/</a></p>\n", "answers": ["<p>Multiple KCL applications can share a DynamoDB table</p>", "<p>Each KCL application must use its own DynamoDB table</p>", "<p>You can only use DynamoDB for checkpointing KCL</p>", "<p>Amazon Relational Database Service (Amazon RDS) can also be used for checkpointing KCL</p>", "<p>Multiple KCL applications can share a DynamoDB table if Amazon Amazon Simple Storage Service (Amazon S3) is configured to save transit data and metadata</p>"]}, "correct_response": ["b", "c"], "section": "Domain 3: Processing", "question_plain": "A company runs a real-time data processing application that uses Kinesis Client Library (KCL) to help consume and process data from the real-time data streams. The development team has raised a query on the viability of using the same DynamoDB table for different KCL applications.\n\nWhich of the following are correct statements for KCL while consuming Kinesis Data Streams? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888350, "assessment_type": "multi-select", "prompt": {"question": "<p>An e-commerce company performs analytics on the company's data using the Amazon Redshift cluster. The Redshift cluster has two important tables: the orders table and the product table that have millions of rows each. A few small tables with supporting data are also present. The team is looking for the right distribution patterns for the tables, to optimize query speed.</p>\n\n<p>Which of the following are the key points to consider while planning for the best distribution style for your data? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Data should be distributed in such a way that the rows that participate in joins are already collocated on the nodes with their joining rows in other tables</strong></p>\n\n<p>A prime goal of data distribution is to minimize data movement when queries run. Therefore, if the rows that participate in joins or aggregates are already collocated on the nodes with their joining rows in other tables, the optimizer doesn't need to redistribute as much data when queries run.</p>\n\n<p><strong>If a dimension table cannot be collocated with the fact table or other important joining tables, use ALL distribution style for such tables</strong></p>\n\n<p>When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. The goal in selecting a table distribution style is to minimize the impact of the redistribution step by locating the data where it needs to be before the query is run.</p>\n\n<p>One of the best approaches is to change some dimension tables to use ALL distribution. If a dimension table cannot be collocated with the fact table or other important joining tables, you can improve query performance significantly by distributing the entire table to all of the nodes. Using ALL distribution multiplies storage space requirements and increases load times and maintenance operations, so you should weigh all factors before choosing ALL distribution.</p>\n\n<p>Choosing the best distribution style:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a column with low cardinality in the filtered result set</strong> - This is incorrect. A column with high cardinality should be considered in the filtered result set. If you distribute a sales table on a date column, for example, you should probably get fairly even data distribution, unless most of your sales are seasonal. However, if you commonly use a range-restricted predicate to filter for a narrow date period, most of the filtered rows occur on a limited set of slices and the query workload is skewed.</p>\n\n<p><strong>A fact table with multiple distribution keys is useful when multiple dimension tables have to be joined to it</strong> - Your fact table can have only one distribution key. Any tables that join on another key aren't collocated with the fact table. Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows.</p>\n\n<p><strong>Small Dimension tables should be marked to use KEY distribution style, which will cause them to be replicated to each physical node in the cluster</strong> - In a KEY distribution style, the rows are distributed according to the values in one column. The leader node places matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns. This way, matching values from the common columns are physically stored together.</p>\n\n<p>A copy of the entire table is distributed to every node in ALL distribution styles.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html</a></p>\n", "answers": ["<p>Choose a column with low cardinality in the filtered result set</p>", "<p>A fact table with multiple distribution keys is useful when multiple dimension tables have to be joined to it</p>", "<p>Small Dimension tables should be marked to use KEY distribution style, which will cause them to be replicated to each physical node in the cluster</p>", "<p>Data should be distributed in such a way that the rows that participate in joins are already collocated on the nodes with their joining rows in other tables</p>", "<p>If a dimension table cannot be collocated with the fact table or other important joining tables, use ALL distribution style for such tables</p>"]}, "correct_response": ["d", "e"], "section": "Domain 4: Analysis and Visualization", "question_plain": "An e-commerce company performs analytics on the company's data using the Amazon Redshift cluster. The Redshift cluster has two important tables: the orders table and the product table that have millions of rows each. A few small tables with supporting data are also present. The team is looking for the right distribution patterns for the tables, to optimize query speed.\n\nWhich of the following are the key points to consider while planning for the best distribution style for your data? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888352, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Consider the following scenario on Amazon S3: A folder INPUT-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with SCH_B, and another folder INPUT-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B. The schemas are defined as follows:</p>\n\n<pre><code>SCH_A:\n{ \"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\"}\n{ \"id\": 2, \"first_name\": \"Li\", \"last_name\": \"Juan\"}\n</code></pre>\n\n<pre><code>SCH_B:\n{\"city\":\"Dublin\",\"country\":\"Ireland\"}\n{\"city\":\"Paris\",\"country\":\"France\"}\n</code></pre>\n\n<p>What is the outcome, when the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2 separately?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>For S3 path s3://INPUT-FOLDER1, the crawler creates one table with columns of both the schemas. And for S3 path s3://INPUT-FOLDER2, the crawler creates two tables, each table having columns of one schema respectively</strong></p>\n\n<p>For schemas to be considered similar, the following conditions must be true:\n1. The partition threshold is higher than 0.7 (70%).\n2. The maximum number of different schemas (also referred to as \"clusters\" in this context) doesn't exceed 5.</p>\n\n<p>The crawler infers the schema at the folder level and compares the schemas across all folders. If the schemas that are compared match, that is, if the partition threshold is higher than 70%, then the schemas are denoted as partitions of a table. If they don\u2019t match, then the crawler creates a table for each folder, resulting in a higher number of tables.</p>\n\n<p>Suppose that the folder DOC-EXAMPLE-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with SCH_B.</p>\n\n<p>Suppose that the files with the schema SHC_A are similar to the following:</p>\n\n<pre><code>{ \"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\"}\n{ \"id\": 2, \"first_name\": \"Li\", \"last_name\": \"Juan\"}\n</code></pre>\n\n<p>Suppose that the files with the schema SCH_B are similar to the following:</p>\n\n<pre><code>{\"city\":\"Dublin\",\"country\":\"Ireland\"}\n{\"city\":\"Paris\",\"country\":\"France\"}\n</code></pre>\n\n<p>When the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://DOC-EXAMPLE-FOLDER1, the crawler creates one table. The table comprises columns of both schema SCH_A and SCH_B. This is because 80% of the files in the path belong to the SCH_A schema, and 20% of the files belong to the SCH_B schema. Therefore, the partition threshold value is met. Also, the number of different schemas hasn't exceeded the number of clusters, and the cluster size limit isn't exceeded.</p>\n\n<p>Suppose that the folder DOC-EXAMPLE-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B.</p>\n\n<p>When the crawler crawls the Amazon S3 path s3://DOC-EXAMPLE-FOLDER2, the crawler creates one table for each file. This is because 70% of the files belong to the schema SCH_A and 30% of the files belong to the schema SCH_B. This means that the partition threshold isn't met. You can check the crawler logs in Amazon CloudWatch to get information on the created tables.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates two tables each, each table having columns of one schema respectively</strong></p>\n\n<p><strong>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas</strong></p>\n\n<p><strong>For S3 path s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas. And for S3 path s3://INPUT-FOLDER1, the crawler creates two tables, each table having columns of one schema respectively</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/glue-crawler-detect-schema/\">https://aws.amazon.com/premiumsupport/knowledge-center/glue-crawler-detect-schema/</a></p>\n", "answers": ["<p>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates two tables each, each table having columns of one schema respectively</p>", "<p>For S3 path s3://INPUT-FOLDER1, the crawler creates one table with columns of both the schemas. And for S3 path s3://INPUT-FOLDER2, the crawler creates two tables, each table having columns of one schema respectively</p>", "<p>For both the S3 paths s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas</p>", "<p>For S3 path s3://INPUT-FOLDER2, the crawler creates one table with columns of both the schemas. And for S3 path s3://INPUT-FOLDER1, the crawler creates two tables, each table having columns of one schema respectively</p>"]}, "correct_response": ["b"], "section": "Domain 4: Analysis and Visualization", "question_plain": "Consider the following scenario on Amazon S3: A folder INPUT-FOLDER1 has 10 files, 8 files with schema SCH_A and 2 files with SCH_B, and another folder INPUT-FOLDER2 has 10 files, 7 files with the schema SCH_A and 3 files with the schema SCH_B. The schemas are defined as follows:\n\nSCH_A:\n{ \"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\"}\n{ \"id\": 2, \"first_name\": \"Li\", \"last_name\": \"Juan\"}\n\n\nSCH_B:\n{\"city\":\"Dublin\",\"country\":\"Ireland\"}\n{\"city\":\"Paris\",\"country\":\"France\"}\n\n\nWhat is the outcome, when the crawler crawls the Amazon Simple Storage Service (Amazon S3) path s3://INPUT-FOLDER1 and s3://INPUT-FOLDER2 separately?", "related_lectures": []}, {"_class": "assessment", "id": 54888354, "assessment_type": "multi-select", "prompt": {"question": "<p>A Data Analyst is troubleshooting a failed AWS Glue ETL job that transforms data to be used for downstream query processing via Athena. While troubleshooting, the analyst noticed that some of the tables that are visible from the AWS Glue console aren't visible on the console of Amazon Athena.</p>\n\n<p>Which of the following would you identify as the most likely reasons for this behavior? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Glue lists tables that are created from Amazon DynamoDB tables</strong></p>\n\n<p><strong>AWS Glue Data Catalog has a list of tables created in XML format</strong></p>\n\n<p>You might see more tables in the AWS Glue console than in the Athena console for the following reasons:</p>\n\n<ol>\n<li><p>You've created tables that point to different data sources. The Athena console displays tables that point to Amazon Simple Storage Service (Amazon S3) paths only. AWS Glue lists tables that point to different data sources, such as Amazon Relational Database Service (Amazon RDS) DB instances and Amazon DynamoDB tables.</p></li>\n<li><p>You've created tables in formats that aren't supported by Athena, such as XML. These tables appear in the AWS Glue Data Catalog, but not in the Athena console.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>MSCK REPAIR TABLE has to be run to sync the tables of AWS Glue with Athena</strong> - The MSCK REPAIR TABLE command scans a file system such as Amazon S3 for Hive compatible partitions that were added to the file system after the table was created. MSCK REPAIR TABLE compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table.</p>\n\n<p><strong>Tables created in JSON format are not supported by a few versions of Athena</strong> - This statement is incorrect. Athena supports tables created in JSON format.</p>\n\n<p><strong>Athena query processing engine has not been restarted after new tables have been added to AWS Glue</strong> - Athena is serverless, so the user cannot restart the underlying query processing engine. This option has been added as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/athena-glue-tables-not-visible/\">https://aws.amazon.com/premiumsupport/knowledge-center/athena-glue-tables-not-visible/</a></p>\n", "answers": ["<p>AWS Glue lists tables that are created from Amazon DynamoDB tables</p>", "<p>MSCK REPAIR TABLE has to be run to sync the tables of AWS Glue with Athena</p>", "<p>Athena query processing engine has not been restarted after new tables have been added to AWS Glue</p>", "<p>AWS Glue Data Catalog has a list of tables created in XML format</p>", "<p>Tables created in JSON format are not supported by few versions of Athena</p>"]}, "correct_response": ["a", "d"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A Data Analyst is troubleshooting a failed AWS Glue ETL job that transforms data to be used for downstream query processing via Athena. While troubleshooting, the analyst noticed that some of the tables that are visible from the AWS Glue console aren't visible on the console of Amazon Athena.\n\nWhich of the following would you identify as the most likely reasons for this behavior? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888356, "assessment_type": "multi-select", "prompt": {"question": "<p>An AWS Glue job is scheduled to be run on the Sunday of every week. The Glue job copies data from certain folders in an S3 bucket to Redshift. To prevent reprocessing of old data, job bookmarks have been enabled on the AWS Glue job. However, the ETL job is reprocessing data that was already processed in an earlier run.</p>\n\n<p>What could be the underlying issue and how should it be fixed to stop reprocessing of data? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>You have multiple concurrent jobs with job bookmarks, and the max concurrency isn't set to 1</strong> - Ensure that the maximum number of concurrent runs for the job is 1. When you have multiple concurrent jobs with job bookmarks and the maximum concurrency is set to 1, the job bookmark doesn't work correctly.</p>\n\n<p><strong>The job.commit() object is missing</strong> - Ensure that your job run script ends with the following commit: job.commit(). When you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store.</p>\n\n<p>Troubleshooting reprocessing error in job bookmarks:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q61-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data\">https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Job bookmarks do not work for Amazon S3 input sources</strong> - This statement is incorrect. Job bookmarks can be configured even when the input source is Amazon S3.</p>\n\n<p><strong>You have added transformation context parameter to the DynamicFrame referenced within the job, which is causing the job to crash</strong> - Transformation context is an optional parameter in the GlueContext class, but job bookmarks don't work if you don't include it. So this option is incorrect.</p>\n\n<p><strong>AWS Glue keeps track of job bookmarks by storing the metadata in Amazon S3 configured during job creation. Deleting the S3 bucket can result in job reprocessing</strong> - This statement is incorrect. AWS Glue keeps track of job bookmarks with the job itself. If you delete a job, the job bookmark is deleted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data\">https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html\">https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html</a></p>\n", "answers": ["<p>Job bookmarks do not work for Amazon S3 input sources</p>", "<p>You have multiple concurrent jobs with job bookmarks, and the max concurrency isn't set to 1</p>", "<p>You have added transformation context parameter to the DynamicFrame referenced within the job, which is causing the job to crash</p>", "<p>The job.commit() object is missing</p>", "<p>AWS Glue keeps track of job bookmarks by storing the metadata in Amazon S3 configured during job creation. Deleting the S3 bucket can result in job reprocessing</p>"]}, "correct_response": ["b", "d"], "section": "Domain 3: Processing", "question_plain": "An AWS Glue job is scheduled to be run on the Sunday of every week. The Glue job copies data from certain folders in an S3 bucket to Redshift. To prevent reprocessing of old data, job bookmarks have been enabled on the AWS Glue job. However, the ETL job is reprocessing data that was already processed in an earlier run.\n\nWhat could be the underlying issue and how should it be fixed to stop reprocessing of data? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 54888358, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Data Analyst is configuring Amazon Athena to create a table for each file stored under the same prefix in Amazon S3. After running CREATE TABLE statement in Athena with expected columns and their data types, the analyst has issued a SELECT query. However, the query has returned zero records.</p>\n\n<p>Which of the following is the right way to configure the Amazon S3 location path?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create individual S3 prefixes for each table similar to the following - <code>s3://doc-example-bucket/table1/table1.csv</code>, <code>s3://doc-example-bucket/table2/table2.csv</code></strong> - Athena query returns zero records if your table location is similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv</p>\n\n<p>To resolve this issue, create individual S3 prefixes for each table similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/\">https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>S3 table location should be similar to the following - <code>s3://doc-example-bucket/table1.csv</code>, <code>s3://doc-example-bucket/table2.csv</code></strong> - Glue crawlers create separate tables for data that's stored in the same S3 prefix. However, when you query those tables in Athena, you get zero records.</p>\n\n<p>For example, your Athena query returns zero records if your table location is similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1.csv\ns3://doc-example-bucket/table2.csv</p>\n\n<p>To resolve this issue, create individual S3 prefixes for each table similar to the following:</p>\n\n<p>s3://doc-example-bucket/table1/table1.csv\ns3://doc-example-bucket/table2/table2.csv</p>\n\n<p><strong>S3 table location path should be similar to this: <code>s3://doc-example-bucket/myprefix//input//</code></strong> - Athena doesn't support table location paths that include a double slash (//). For example, the following LOCATION path returns empty results:</p>\n\n<p>s3://doc-example-bucket/myprefix//input//</p>\n\n<p>To resolve this issue, copy the files to a location that doesn't have double slashes.</p>\n\n<p><strong>S3 file names should be similar to the following - <code>s3://doc-example-bucket/athena/inputdata/_file1</code></strong> - If the files in your S3 path have names that start with an underscore or a dot, then Athena considers these files as placeholders. Athena ignores these files when processing a query. If all the files in your S3 path have names that start with an underscore or a dot, then you get zero records.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/\">https://aws.amazon.com/premiumsupport/knowledge-center/athena-empty-results/</a></p>\n", "answers": ["<p>S3 table location should be similar to the following - <code>s3://doc-example-bucket/table1.csv</code>, <code>s3://doc-example-bucket/table2.csv</code></p>", "<p>S3 table location path should be similar to this: <code>s3://doc-example-bucket/myprefix//input//</code></p>", "<p>Create individual S3 prefixes for each table similar to the following - <code>s3://doc-example-bucket/table1/table1.csv</code>, <code>s3://doc-example-bucket/table2/table2.csv</code></p>", "<p>S3 file names should be similar to the following - <code>s3://doc-example-bucket/athena/inputdata/_file1</code></p>"]}, "correct_response": ["c"], "section": "Domain 2: Storage and Data Management", "question_plain": "A Data Analyst is configuring Amazon Athena to create a table for each file stored under the same prefix in Amazon S3. After running CREATE TABLE statement in Athena with expected columns and their data types, the analyst has issued a SELECT query. However, the query has returned zero records.\n\nWhich of the following is the right way to configure the Amazon S3 location path?", "related_lectures": []}, {"_class": "assessment", "id": 54888360, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A university is carrying out research on multiple economic parameters such as student loan, student credit card usage, monthly spends, etc for its students from different countries. The university wants to identify the trends and outliers present in this data. The entire data is stored in Parquet format on Amazon S3.</p>\n\n<p>Which of the following would you recommend as the best option to visualize this data with the least effort?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use heat map visualizations in Amazon QuickSight with Amazon Athena as the data source</strong></p>\n\n<p>Amazon QuickSight allows everyone in your organization to understand your data by asking questions in natural language, exploring through interactive dashboards, or automatically looking for patterns and outliers powered by machine learning.</p>\n\n<p>Use heat maps to show a measure for the intersection of two dimensions, with color-coding to easily differentiate where values fall in the range. Heat maps can also be used to show the count of values for the intersection of the two dimensions.</p>\n\n<p>Each rectangle on a heat map represents the value for the specified measure for the intersection of the selected dimensions. Rectangle color represents where the value falls in the range for the measure, with darker colors indicating higher values and lighter colors indicating lower ones.</p>\n\n<p>Heat maps and pivot tables display data in a similar tabular fashion. Use a heat map if you want to identify trends and outliers because the use of color makes these easier to spot. Use a pivot table if you want to further analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q63-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\">https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html</a></p>\n\n<p>Amazon Athena can process unstructured, semi-structured, and structured data sets. Examples include CSV, JSON, Avro or columnar data formats such as Apache Parquet and Apache ORC. Amazon Athena integrates with Amazon QuickSight for easy visualizations. For the given use case, you need to configure an Athena Table that points to the parquet data in S3. Further, use this Athena table as the data source for visualizations in QuickSight.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use pivot tables in Amazon QuickSight with Amazon Athena as the data source</strong></p>\n\n<p><strong>Use pivot tables in Amazon QuickSight with Amazon S3 as the data source</strong></p>\n\n<p>Heat maps and pivot tables display data in a similar tabular fashion. Use a pivot table if you want to analyze data on the visual, for example by changing column sort order or applying aggregate functions across rows or columns. You cannot use pivot tables to identify trends and outliers. Therefore, both these options are incorrect.</p>\n\n<p><strong>Use heat map visualizations in Amazon QuickSight with Amazon S3 as the data source</strong> - You use JSON manifest files to specify files in Amazon S3 to import into Amazon QuickSight. You should note that QuickSight does not support parquet format while reading the data from Amazon S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q63-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\">https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html\">https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html\">https://docs.aws.amazon.com/quicksight/latest/user/heat-map.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/supported-manifest-file-format.html\">https://docs.aws.amazon.com/quicksight/latest/user/supported-manifest-file-format.html</a></p>\n", "answers": ["<p>Use heat map visualizations in Amazon QuickSight with Amazon S3 as the data source</p>", "<p>Use pivot tables in Amazon QuickSight with Amazon Athena as the data source</p>", "<p>Use pivot tables in Amazon QuickSight with Amazon S3 as the data source</p>", "<p>Use heat map visualizations in Amazon QuickSight with Amazon Athena as the data source</p>"]}, "correct_response": ["d"], "section": "Domain 4: Analysis and Visualization", "question_plain": "A university is carrying out research on multiple economic parameters such as student loan, student credit card usage, monthly spends, etc for its students from different countries. The university wants to identify the trends and outliers present in this data. The entire data is stored in Parquet format on Amazon S3.\n\nWhich of the following would you recommend as the best option to visualize this data with the least effort?", "related_lectures": []}, {"_class": "assessment", "id": 54888362, "assessment_type": "multi-select", "prompt": {"question": "<p>A stock trading company uses Amazon Redshift to power the Business Intelligence (BI) specific queries which are run on Redshift. The data analytics team at the company needs to provide the sales team access to a historical trades table whose data is stored in Apache Parquet format in an S3 bucket of the company's data lake. The data analytics team should provide access to only a few specific columns in the historical trades table so that the access does not violate the compliance regulations.</p>\n\n<p>Which of the following options should be combined together to build a solution for the given use case? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation</strong></p>\n\n<p><strong>Create an external schema in Amazon Redshift by using the Amazon Redshift IAM role</strong></p>\n\n<p><strong>Grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong></p>\n\n<p>Amazon Redshift is a fully managed, scalable data warehouse that enables secure analytics at scale. Using Redshift Spectrum, Amazon Redshift customers can easily query their data in Amazon S3. Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the hood, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent\u2014you can access your Amazon S3 data from any number of Amazon Redshift clusters. To access this data on S3 via Redshift Spectrum, you need to create an external schema in Amazon Redshift.</p>\n\n<p>Amazon Redshift Spectrum supports column level access control for data stored in Amazon S3 and managed by AWS Lake Formation. Column level access control can be used to limit access to only the specific columns of a table rather than allowing access to all columns of a table. To use this feature, an administrator needs to create an IAM role for Amazon Redshift and create the policy to allow Redshift to access AWS Lake Formation. The administrator can then use the Lake Formation console to specify the tables and columns that the role is allowed access to. The column level access control policies can also be created and managed by the SQL grant statements.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an internal schema in Amazon Redshift by using the Amazon Redshift IAM role</strong> - As mentioned in the explanation above, you need to create an external schema in Amazon Redshift to access the data on S3 via Redshift spectrum.</p>\n\n<p><strong>Grant permissions in the S3 bucket policy to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</strong> - You need to grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table.</p>\n\n<p><strong>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access the S3 bucket having data for the historical trades table</strong> - You need to create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/\">https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-redshift-spectrum-now-supports-column-level-access-control-with-aws-lake-formation/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/\">https://aws.amazon.com/blogs/big-data/achieve-finer-grained-data-security-with-column-level-access-control-in-amazon-redshift/</a></p>\n", "answers": ["<p>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access AWS Lake Formation</p>", "<p>Create an external schema in Amazon Redshift by using the Amazon Redshift IAM role</p>", "<p>Grant permissions in Lake Formation to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</p>", "<p>Create an internal schema in Amazon Redshift by using the Amazon Redshift IAM role</p>", "<p>Grant permissions in the S3 bucket policy to allow the Amazon Redshift IAM role to access the specific columns of the historical trades table</p>", "<p>Create an IAM role for Amazon Redshift which has a policy to allow Redshift to access the S3 bucket having data for the historical trades table</p>"]}, "correct_response": ["a", "b", "c"], "section": "Domain 5: Security", "question_plain": "A stock trading company uses Amazon Redshift to power the Business Intelligence (BI) specific queries which are run on Redshift. The data analytics team at the company needs to provide the sales team access to a historical trades table whose data is stored in Apache Parquet format in an S3 bucket of the company's data lake. The data analytics team should provide access to only a few specific columns in the historical trades table so that the access does not violate the compliance regulations.\n\nWhich of the following options should be combined together to build a solution for the given use case? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 54888364, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An investment firm uses AWS Cloud for its IT infrastructure. The firm runs several investment-related risk-simulation applications and develops complex algorithms to simulate multiple scenarios to build a financial roadmap for its customers. The firm stores customers' financial data on Amazon S3. The data analytics team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.</p>\n\n<p>Which of the following solutions would you suggest for this use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</strong></p>\n\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>An S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as \u201cwrite once read many\u201d (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</strong> - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p><strong>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</strong> - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\">https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</a></p>\n", "answers": ["<p>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</p>", "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</p>", "<p>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>", "<p>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>"]}, "correct_response": ["a"], "section": "Domain 5: Security", "question_plain": "An investment firm uses AWS Cloud for its IT infrastructure. The firm runs several investment-related risk-simulation applications and develops complex algorithms to simulate multiple scenarios to build a financial roadmap for its customers. The firm stores customers' financial data on Amazon S3. The data analytics team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access.\n\nWhich of the following solutions would you suggest for this use case?", "related_lectures": []}]}
