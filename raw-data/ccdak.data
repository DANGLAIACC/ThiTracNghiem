4581118
~~~
{'count': 50, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 20172940, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Two brokers can go down, and one replica will still be able to receive and serve data</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>0</p>', '<p>1</p>', '<p>2</p>', '<p>3</p>'], 'question': '<p>A kafka topic has a replication factor of 3 and min.insync.replicas setting of 1. What is the maximum number of brokers that can be down so that a producer with acks=all can still produce to the topic?</p>\n'}, 'correct_response': ['c'], 'section': 'Topic', 'question_plain': 'A kafka topic has a replication factor of 3 and min.insync.replicas setting of 1. What is the maximum number of brokers that can be down so that a producer with acks=all can still produce to the topic?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172942, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>clients can interact with the schema registry using the HTTP or HTTPS interface</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>HTTP</p>', '<p>HTTPS</p>', '<p>SASL</p>', '<p>Websocket</p>', '<p>JDBC</p>'], 'question': '<p>What client protocol is supported for the schema registry? (select two)</p>\n'}, 'correct_response': ['a', 'b'], 'section': 'Schema Registry', 'question_plain': 'What client protocol is supported for the schema registry? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172944, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>2181 - client port, 2888 - peer port, 3888 - leader port</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>2181</p>', '<p>443</p>', '<p>9092</p>', '<p>80</p>', '<p>2888</p>', '<p>3888</p>'], 'question': '<p>A Zookeeper ensemble contains 3 servers. Over which ports the members of the ensemble should be able to communicate in default configuration? (select three)</p>\n'}, 'correct_response': ['a', 'e', 'f'], 'section': 'Topic', 'question_plain': 'A Zookeeper ensemble contains 3 servers. Over which ports the members of the ensemble should be able to communicate in default configuration? (select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172946, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>each consumer can process only 50 MB/s, so we need at least 20 consumers consuming one partition so that 50 * 20 = 1000 MB target is achieved.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>10</p>', '<p>20</p>', '<p>1</p>', '<p>50</p>'], 'question': '<p>A customer has many consumer applications that process messages from a Kafka topic. Each consumer application can only process 50 MB/s. Your customer wants to achieve a target throughput of 1 GB/s. What is the minimum number of partitions will you suggest to the customer for that particular topic?</p>\n'}, 'correct_response': ['b'], 'section': 'Producer', 'question_plain': 'A customer has many consumer applications that process messages from a Kafka topic. Each consumer application can only process 50 MB/s. Your customer wants to achieve a target throughput of 1 GB/s. What is the minimum number of partitions will you suggest to the customer for that particular topic?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172948, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>Both key and value serializer are mandatory.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>partition</p>', '<p>bootstrap.servers</p>', '<p>key</p>', '<p>value</p>', '<p>key.serializer</p>', '<p>value.serializer</p>'], 'question': '<p>A Kafka producer application wants to send log messages to a topic that does not include any key. What are the properties that are mandatory to configure for the producer configuration? (select three)</p>\n'}, 'correct_response': ['b', 'e', 'f'], 'section': 'Producer', 'question_plain': 'A Kafka producer application wants to send log messages to a topic that does not include any key. What are the properties that are mandatory to configure for the producer configuration? (select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172950, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>linger.ms forces the producer to wait to send messages, hence increasing the chance of creating batches</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Increase batch.size</p>', '<p>Increase the number of producer threads</p>', '<p>Increase linger.ms</p>', '<p>Increase message.max.bytes</p>'], 'question': '<p>Which of the following setting increases the chance of batching for a Kafka Producer?</p>\n'}, 'correct_response': ['c'], 'section': 'Producer', 'question_plain': 'Which of the following setting increases the chance of batching for a Kafka Producer?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172952, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>two partitions groups - one for each application so that all messages are delivered to both the application. 4 consumers in each as there are 4 partitions of the topic, and you cannot have more consumers per groups than the number of partitions (otherwise they will be inactive and wasting resources)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Create four consumers in the same group, one for each partition - two for fulfilment and two for monitoring</p>', '<p>Create two consumer groups for two applications with 4 consumers in each</p>', '<p>Create 8 consumers in the same group with 4 consumers for each application</p>', '<p>Create two consumers groups for two applications with 8 consumers in each</p>'], 'question': '<p>A topic receives all the orders for the products that are available on a commerce site. Two applications want to process all the messages independently - order fulfilment and monitoring. The topic has 4 partitions, how would you organise the consumers for optimal performance and resource usage?</p>\n'}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A topic receives all the orders for the products that are available on a commerce site. Two applications want to process all the messages independently - order fulfilment and monitoring. The topic has 4 partitions, how would you organise the consumers for optimal performance and resource usage?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172954, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>The correct option is the only one where the two consumers share an equal number of partitions amongst the two topics of three partitions. An interesting article to read is: <a href="https://medium.com/@anyili0928/what-i-have-learned-from-kafka-partition-assignment-strategy-799fdf15d3ab">https://medium.com/@anyili0928/what-i-have-learned-from-kafka-partition-assignment-strategy-799fdf15d3ab</a></p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>C1 will be assigned partitions 0 and 1 from T1 and T2, C2 will be assigned partition 2 from T1 and T2.</p>', '<p>C1 will be assigned partitions 0 and 2 from T1 and partition 1 from T2. C2 will have partition 1 from T1 and partitions 0 and 2 from T2.</p>', '<p>Two consumers cannot read from two topics at the same time</p>', '<p>All consumers will read from all partitions</p>'], 'question': '<p>There are two consumers C1 and C2 belonging to the same group G subscribed to topics T1 and T2. Each of the topics has 3 partitions. How will the partitions be assigned to consumers with PartitionAssignor being RoundRobinAssignor?</p>\n'}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'There are two consumers C1 and C2 belonging to the same group G subscribed to topics T1 and T2. Each of the topics has 3 partitions. How will the partitions be assigned to consumers with PartitionAssignor being RoundRobinAssignor?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172956, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'relatedLectureIds': '', 'explanation': '<p>Here, we must commit the offsets right after receiving a batch from a call to .poll()</p>\n', 'question': '<p>We would like to be in an at-most once consuming scenario. Which offset commit strategy would you recommend?</p>\n', 'answers': ['<p>Commit the offsets on disk, after processing the data</p>', '<p>Commit the offsets in Kafka, after processing the data</p>', '<p>Commit the offsets in Kafka, before processing the data</p>', '<p>Do not commit any offsets and read from beginning</p>']}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'We would like to be in an at-most once consuming scenario. Which offset commit strategy would you recommend?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172958, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>assign() can be used for manual assignment of a partition to a consumer, in which case subscribe() must not be used. Assign() takes a collection of TopicPartition object as an argument <a href="https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign-java.util.Collection-">https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign-java.util.Collection-</a></p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>Call subscribe(String topic, int partition) passing the topic and partition number as the arguments</p>', '<p>Call assign() passing a Collection of TopicPartitions as the argument</p>', '<p>Call subscribe() passing TopicPartition as the argument</p>'], 'question': '<p>A consumer wants to read messages from a specific partition of a topic. How can this be achieved?</p>\n'}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer wants to read messages from a specific partition of a topic. How can this be achieved?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172960, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>In case the consumer has the wrong leader of a partition, it will issue a metadata request. The Metadata request can be handled by any node, so clients know afterwards which broker are the designated leader for the topic partitions. Produce and consume requests can only be sent to the node hosting partition leader.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Send fetch request to each Broker in the cluster</p>', '<p>Get the Broker id from Zookeeper that is hosting the leader replica and send request to it</p>', '<p>Send metadata request to Zookeeper for the topic and select the broker hosting the leader replica</p>', '<p>Send metadata request to the same broker for the topic and select the broker hosting the leader replica</p>'], 'question': '<p>A client connects to a broker in the cluster and sends a fetch request for a partition in a topic. It gets an exception NotLeaderForPartitionException in the response. How does client handle this situation?</p>\n'}, 'correct_response': ['d'], 'section': 'Clients', 'question_plain': 'A client connects to a broker in the cluster and sends a fetch request for a partition in a topic. It gets an exception NotLeaderForPartitionException in the response. How does client handle this situation?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172962, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>The offsets are already committed for this consumer group and topic partition, so the property auto.offset.reset is ignored</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>offset 45</p>', '<p>offset 643</p>', '<p>offset 2311</p>', '<p>it will crash</p>'], 'question': '<p>A consumer starts and has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 643 for the topic before. Where will the consumer read from?</p>\n'}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer starts and has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 643 for the topic before. Where will the consumer read from?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172964, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>Kafka transfers data with zero-copy and sends the raw bytes it receives from the producer straight to the consumer, leveraging the RAM available as page cache</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>It buffers the messages on disk, and sends messages from the disk reads</p>', '<p>It compresses the messages as it writes to the disk</p>', '<p>It does not transform the messages</p>', '<p>It transforms the messages into a binary format</p>', '<p>It leverages zero-copy optimisations to send data straight from the page-cache</p>'], 'question': '<p>How do Kafka brokers ensure great performance between the producers and consumers? (select two)</p>\n'}, 'correct_response': ['c', 'e'], 'section': 'Broker', 'question_plain': 'How do Kafka brokers ensure great performance between the producers and consumers? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172966, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Setting unclean.leader.election.enable to true means we allow out-of-sync replicas to become leaders, we will lose messages when this occurs, effectively losing credit card payments and making our customers very angry.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', ''], 'answers': ['<p>TRUE</p>', '<p>FALSE</p>'], 'question': '<p>A bank uses a Kafka cluster for credit card payments. What should be the value of the property unclean.leader.election.enable?</p>\n'}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'A bank uses a Kafka cluster for credit card payments. What should be the value of the property unclean.leader.election.enable?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172968, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>acks is a producer setting\nmin.insync.replicas is a topic or broker setting and is only effective when acks=all</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>acks is a topic setting</p>', '<p>acks is a producer setting</p>', '<p>min.insync.replicas is a topic setting</p>', '<p>min.insync.replicas is a producer setting</p>', '<p>min.insync.replicas only matters if acks=all</p>', '<p>min.insync.replicas matters regardless of the values of acks</p>'], 'question': '<p>Select all that applies (select THREE)</p>\n'}, 'correct_response': ['b', 'c', 'e'], 'section': 'Producer', 'question_plain': 'Select all that applies (select THREE)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172970, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Kafka Streams can only guarantee exactly once processing if you have a Kafka to Kafka topology. </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>Kafka =&gt; Kafka</p>', '<p>External =&gt; Kafka</p>', '<p>Kafka =&gt; External</p>'], 'question': '<p>The exactly once guarantee in the Kafka Streams is for which flow of data?</p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Streams', 'question_plain': 'The exactly once guarantee in the Kafka Streams is for which flow of data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172972, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>we have two tables, so the max number of tasks is 2</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>6</p>'], 'question': '<p>You are using JDBC source connector to copy data from 2 tables to two Kafka topics. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?</p>\n'}, 'correct_response': ['b'], 'section': 'Kafka Connect', 'question_plain': 'You are using JDBC source connector to copy data from 2 tables to two Kafka topics. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172974, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>ACLs are stored in Zookeeper node /kafka-acls/ by default.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>In Kafka topic __kafka_acls</p>', '<p>Under Zookeeper node /kafka-acl/</p>', "<p>Inside the broker's data directory</p>", "<p>Inside the Zookeeper's data directory</p>"], 'question': '<p>Where are the ACLs stored in a Kafka cluster by default?</p>\n'}, 'correct_response': ['b'], 'section': 'Security', 'question_plain': 'Where are the ACLs stored in a Kafka cluster by default?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172976, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>There is only one controller in a cluster at all times.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>3</p>', '<p>5</p>', '<p>1</p>', '<p>2</p>'], 'question': '<p>There are 3 producers writing to a topic with 5 partitions. There are 5 consumers consuming from the topic. How many Controllers will be present in the cluster?</p>\n'}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'There are 3 producers writing to a topic with 5 partitions. There are 5 consumers consuming from the topic. How many Controllers will be present in the cluster?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172978, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>KSQL is not ANSI SQL compliant, for now there are no defined standards on streaming SQL languages</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', ''], 'answers': ['<p>Yes</p>', '<p>No</p>'], 'question': '<p>is KSQL ANSI SQL compliant?</p>\n'}, 'correct_response': ['b'], 'section': 'KSQL', 'question_plain': 'is KSQL ANSI SQL compliant?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172980, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Some messages may require multiple retries. If there are more than 1 requests in flight, it may result in messages received out of order. Note an exception to this rule is if you enable the producer setting: enable.idempotence=true which takes care of the out of ordering case on its own. See: <a href="https://issues.apache.org/jira/browse/KAFKA-5494">https://issues.apache.org/jira/browse/KAFKA-5494</a></p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Reduce throughput</p>', '<p>At least once delivery is not guaranteed</p>', '<p>Message order not preserved</p>', '<p>Less resilient</p>'], 'question': '<p>What is the risk of increasing max.in.flight.requests.per.connection while also enabling retries in a producer?</p>\n'}, 'correct_response': ['c'], 'section': 'Producer', 'question_plain': 'What is the risk of increasing max.in.flight.requests.per.connection while also enabling retries in a producer?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172982, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>You cannot have more sink tasks (= consumers) than the number of partitions, so 2. </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>3</p>', '<p>2</p>', '<p>6</p>', '<p>10</p>'], 'question': '<p>You want to sink data from a Kafka topic to S3 using Kafka Connect. There are 10 brokers in the cluster, the topic has 2 partitions with replication factor of 3. How many tasks will you configure for the S3 connector?</p>\n'}, 'correct_response': ['b'], 'section': 'Kafka Connect', 'question_plain': 'You want to sink data from a Kafka topic to S3 using Kafka Connect. There are 10 brokers in the cluster, the topic has 2 partitions with replication factor of 3. How many tasks will you configure for the S3 connector?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172984, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>Consumer offsets are stored in a Kafka topic __consumer_offsets, and the Schema Registry stored schemas in the _schemas topic.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Broker registration info</p>', '<p>ACL inforomation</p>', '<p>Consumer offset</p>', '<p>Controller registration</p>', '<p>Schema Registry schemas</p>'], 'question': "<p>What information isn't stored inside of Zookeeper? (select two)</p>\n"}, 'correct_response': ['c', 'e'], 'section': 'Zookeeper', 'question_plain': "What information isn't stored inside of Zookeeper? (select two)", 'related_lectures': []}, {'_class': 'assessment', 'id': 20172986, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>Both of these are retriable errors, others non-retriable errors. See the full list of errors and their "retriable" status here: <a href="https://kafka.apache.org/protocol#protocol_error_codes">https://kafka.apache.org/protocol#protocol_error_codes</a></p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>INVALID_REQUIRED_ACKS</p>', '<p>NOT_LEADER_FOR_PARTITION</p>', '<p>MESSAGE_TOO_LARGE</p>', '<p>NOT_ENOUGH_REPLICAS</p>', '<p>TOPIC_AUTHORIZATION_FAILED</p>'], 'question': '<p>Which of the following errors are retriable from a producer perspective? (select two)</p>\n'}, 'correct_response': ['b', 'd'], 'section': 'Producer', 'question_plain': 'Which of the following errors are retriable from a producer perspective? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172988, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>The Schema Registry stores all the schemas in the _schemas Kafka topic</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>In the Schema Registry embedded SQL database</p>', '<p>In the _schemas topic</p>', '<p>In the message bytes themselves</p>', '<p>In the Zookeeper node /schemas</p>'], 'question': '<p>Using the Confluent Schema Registry, where are Avro schema stored?</p>\n'}, 'correct_response': ['b'], 'section': 'Schema Registry', 'question_plain': 'Using the Confluent Schema Registry, where are Avro schema stored?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172990, 'assessment_type': 'multi-select', 'prompt': {'feedbacks': ['', '', '', ''], 'answers': ['<p><code>consumer.subscribe(Arrays.asList("topic.history", "topic.sports", "topic.politics"));</code></p>', '<p><code>consumer.subscribe(Pattern.compile("topic\\..*"));</code></p>', '<p><code>consumer.subscribe("topic.history");\nconsumer.subscribe("topic.sports");\nconsumer.subscribe("topic.politics");\n</code></p>', '<p><code>consumer.subscribePrefix("topic.");</code></p>'], 'relatedLectureIds': '', 'question': '<p>Select all the way for one consumer to subscribe simultaneously to the following topics - topic.history, topic.sports, topic.politics? (select two)</p>\n', 'explanation': '<p>Multiple topics can be passed as a list or regex pattern.</p>\n'}, 'correct_response': ['a', 'b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'Select all the way for one consumer to subscribe simultaneously to the following topics - topic.history, topic.sports, topic.politics? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172992, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>First local cache is checked for the message schema. In case of cache miss, schema is pulled from the schema registry. An exception will be thrown in the Schema Registry does not have the schema (which should never happen if you set it up properly)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Throws DeserializationException</p>', '<p>Fetches schema from Schema Registry</p>', '<p>Fails silently</p>', '<p>Throws SerializationException</p>'], 'question': '<p>A consumer application is using KafkaAvroDeserializer to deserialize Avro messages. What happens if message schema is not present in AvroDeserializer local cache?</p>\n'}, 'correct_response': ['b'], 'section': 'Schema Registry', 'question_plain': 'A consumer application is using KafkaAvroDeserializer to deserialize Avro messages. What happens if message schema is not present in AvroDeserializer local cache?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172994, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>batch.size controls how many bytes of data to collect before sending messages to the Kafka broker. Set this as high as possible, without exceeding available memory. Enabling compression can also help make more compact batches and increase the throughput of your producer. Linger.ms will have no effect as the batches are already full</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>Enable compression</p>', '<p>Increase batch.size</p>', '<p>Decrease batch.size</p>', '<p>Increase linger.ms</p>', '<p>Disable compression</p>', '<p>Decrease linger.ms</p>'], 'question': '<p>Your producer is producing at a very high rate and the batches are completely full each time. How can you improve the producer throughput? (select two)</p>\n'}, 'correct_response': ['a', 'b'], 'section': 'Producer', 'question_plain': 'Your producer is producing at a very high rate and the batches are completely full each time. How can you improve the producer throughput? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172996, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p></p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>kafka-topics.sh --zookeeper localhost:2181 --describe --under-replicated-partitions</p>', '<p>kafka-topics.sh --zookeeper localhost:2181 --describe --unavailable- partitions</p>', '<p>kafka-topics.sh --broker-list localhost:9092 --describe --under-replicated-partitions</p>', '<p>kafka-topics.sh --bootstrap-server localhost:9092 --describe --unavailable- partitions</p>'], 'question': '<p>How will you find out all the partitions where one or more of the replicas for the partition are not in-sync with the leader?</p>\n'}, 'correct_response': ['a'], 'section': 'CLI', 'question_plain': 'How will you find out all the partitions where one or more of the replicas for the partition are not in-sync with the leader?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20172998, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Dynamic topic configurations are maintained in Zookeeper.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>In an internal Kafka topic __topic_configuratins</p>', '<p>In Zookeeper</p>', '<p>On the Kafka broker file system</p>', '<p>In server.properties</p>'], 'question': '<p>Where are the dynamic configurations for a topic stored?</p>\n'}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'Where are the dynamic configurations for a topic stored?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173000, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>Stateless means processing of each message depends only on the message, so converting from JSON to Avro or filtering a stream are both stateless operations</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Read log messages from a stream and writes ERROR events into a high-priority stream and the rest of the events into a low-priority stream</p>', '<p>Find the minimum and maximum stock prices for each day of trading</p>', '<p>Read events from a stream and modifies them from JSON to Avro</p>', '<p>Publish the top 10 stocks each day</p>'], 'question': '<p>Which of the following event processing application is stateless? (select two)</p>\n'}, 'correct_response': ['a', 'c'], 'section': 'Kafka Streams', 'question_plain': 'Which of the following event processing application is stateless? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173002, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Here KStream is being processed to create another KStream.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Kstream</p>', '<p>KTable</p>', '<p>GlobalKTable</p>', '<p>You choose between KStream or KTable</p>'], 'question': '<p>You want to perform table lookups against a KTable everytime a new record is received from the KStream. What is the output of KStream-KTable join?</p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Streams', 'question_plain': 'You want to perform table lookups against a KTable everytime a new record is received from the KStream. What is the output of KStream-KTable join?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173004, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>doc represents optional description of message</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>name</p>', '<p>namespace</p>', '<p>fields</p>', '<p>doc</p>'], 'question': '<p>Which is an optional field in an Avro record?</p>\n'}, 'correct_response': ['d'], 'section': 'Schema Registry', 'question_plain': 'Which is an optional field in an Avro record?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173006, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Clients with old schema will be able to read records saved with new schema.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>backward</p>', '<p>forward</p>', '<p>breaking</p>', '<p>full</p>'], 'question': '<p>In Avro, adding a field to a record without default is a __ schema evolution</p>\n'}, 'correct_response': ['b'], 'section': 'Schema Registry', 'question_plain': 'In Avro, adding a field to a record without default is a __ schema evolution', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173008, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Data is stored on brokers.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>Store schemas</p>', '<p>Enforce compatibility rules</p>', '<p>Store avro data</p>'], 'question': "<p>What isn't a feature of the Confluent schema registry?</p>\n"}, 'correct_response': ['c'], 'section': 'Schema Registry', 'question_plain': "What isn't a feature of the Confluent schema registry?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20173010, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>On the producer side, after receiving base64 data, the REST Proxy will convert it into bytes and then send that bytes payload to Kafka. Therefore consumers reading directly from Kafka will receive binary data.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>base64 encoded data, it will need to decode it</p>', '<p>binary data</p>', '<p>json data</p>', '<p>avro data</p>'], 'question': '<p>If I want to send binary data through the REST proxy to topic "test_binary", it needs to be base64 encoded. A consumer connecting directly into the Kafka topic "test_binary" will receive</p>\n'}, 'correct_response': ['b'], 'section': 'REST Proxy', 'question_plain': 'If I want to send binary data through the REST proxy to topic "test_binary", it needs to be base64 encoded. A consumer connecting directly into the Kafka topic "test_binary" will receive', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173012, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Replicas are spread across available brokers, and each replica = one broker. RF 3 = 3 brokers</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Each partition will live on 3 different brokers</p>', '<p>Each partition will live on 2 different brokers</p>', '<p>Each partition will live on 4 different brokers</p>', '<p>3 replicas of the same data will live on 1 broker</p>'], 'question': '<p>If a topic has a replication factor of 3...</p>\n'}, 'correct_response': ['a'], 'section': 'Topic', 'question_plain': 'If a topic has a replication factor of 3...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173014, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Producers can set acks=1 to get acknowledgement from partition leader only. </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>acks=1</p>', '<p>acks=0</p>', '<p>acks=all</p>'], 'question': '<p>To get acknowledgement of writes to only the leader partition, we need to use the config...</p>\n'}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'To get acknowledgement of writes to only the leader partition, we need to use the config...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173016, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>The broker settings comes into play when a topic is auto created</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Kafka will automatically create the topic with the broker settings num.partitions and default.replication.factor</p>', '<p>Kafka will automatically create the topic with 1 partition and 1 replication factor</p>', '<p>Kafka will automatically create the topic with num.partitions=#of brokers and replication.factor=3</p>', '<p>Kafka will automatically create the topic with the indicated producer settings num.partitions and default.replication.factor</p>'], 'question': '<p>If I produce to a topic that does not exist, and the broker setting auto.create.topic.enable=true, what will happen?</p>\n'}, 'correct_response': ['a'], 'section': 'Broker', 'question_plain': 'If I produce to a topic that does not exist, and the broker setting auto.create.topic.enable=true, what will happen?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173018, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Keys are necessary if you require strong ordering or grouping for messages that share the same key. If you require that messages with the same key are always seen in the correct order, attaching a key to messages will ensure messages with the same key always go to the same partition in a topic. Kafka guarantees order within a partition, but not across partitions in a topic, so alternatively not providing a key - which will result in round-robin distribution across partitions - will not maintain such order.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Add more information to my message</p>', '<p>Influence partitioning of the producer messages</p>', '<p>Allow a Kafka Consumer to subscribe to a (topic,key) pair and only receive that data</p>', '<p>Ensure per-record level security</p>'], 'question': '<p>Producing with a key allows to...</p>\n'}, 'correct_response': ['b'], 'section': 'Producer', 'question_plain': 'Producing with a key allows to...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173020, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>acks=all means the leader will wait for all in-sync replicas to acknowledge the record. Also the min in-sync replica setting specifies the minimum number of replicas that need to be in-sync for the partition to remain available for writes. </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>acks=1, replication factor=3, min.insync.replicas=2</p>', '<p>acks=all, replication factor=2, min.insync.replicas=1</p>', '<p>acks=all, replication factor=3, min.insync.replicas=1</p>', '<p>acks=all, replication factor=3, min.insync.replicas=2</p>'], 'question': '<p>If I want to have an extremely high confidence that leaders and replicas have my data, I should use</p>\n'}, 'correct_response': ['d'], 'section': 'Producer', 'question_plain': 'If I want to have an extremely high confidence that leaders and replicas have my data, I should use', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173022, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>linger.ms forces the producer to wait before sending messages, hence increasing the chance of creating batches that can be heavily compressed. </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>linger.ms=20</p>', '<p>batch.size=65536</p>', '<p>acks=all</p>', '<p>max.message.size=10MB</p>'], 'question': '<p>To enhance compression, I can increase the chances of batching by using</p>\n'}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'To enhance compression, I can increase the chances of batching by using', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173024, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Kafka Connect Sink is used to export data from Kafka to external databases and Kafka Connect Source is used to import from external databases into Kafka.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Kafka Connect Source</p>', '<p>Kafka Connect Sink</p>', '<p>Kafka Streams</p>', '<p>Confluent REST Proxy</p>'], 'question': '<p>To import data from external databases, I should use</p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Connect', 'question_plain': 'To import data from external databases, I should use', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173026, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>The Confluent Schema Registry is your safeguard against incompatible schema changes and will be the component that ensures no breaking schema evolution will be possible. Kafka Brokers do not look at your payload and your payload schema, and therefore will not reject data</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>The Kafka Producer itself</p>', '<p>The Confluent Schema Registry</p>', '<p>The Kafka Broker</p>', '<p>Zookeeper</p>'], 'question': '<p>I am producing Avro data on my Kafka cluster that is integrated with the Confluent Schema Registry. After a schema change that is incompatible, I know my data will be rejected. Which component will reject the data?</p>\n'}, 'correct_response': ['b'], 'section': 'Schema Registry', 'question_plain': 'I am producing Avro data on my Kafka cluster that is integrated with the Confluent Schema Registry. After a schema change that is incompatible, I know my data will be rejected. Which component will reject the data?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173028, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': "<p>Consumers can set auto.offset.reset property to earliest to start consuming from beginning. For KSQL, SET 'auto.offset.reset'='earliest';</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>KSQL reads from the end of a topic. This cannot be changed.</p>', '<p>KSQL reads from the beginning of a topic, by default.</p>', '<p>Use KSQL CLI to set auto.offset.reset property to earliest</p>'], 'question': '<p>How will you read all the messages from a topic in your KSQL query?</p>\n'}, 'correct_response': ['c'], 'section': 'KSQL', 'question_plain': 'How will you read all the messages from a topic in your KSQL query?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173030, 'assessment_type': 'multi-select', 'prompt': {'explanation': "<p>SHOW STREAMS and EXPLAIN &lt;query&gt; statements run against the KSQL server that the KSQL client is connected to. They don't communicate directly with Kafka. CREATE STREAM WITH &lt;topic&gt; and CREATE TABLE WITH &lt;topic&gt; write metadata to the KSQL command topic. Persistent queries based on CREATE STREAM AS SELECT and CREATE TABLE AS SELECT read and write to Kafka topics. Non-persistent queries based on SELECT that are stateless only read from Kafka topics, for example SELECT ‚Ä¶ FROM foo WHERE ‚Ä¶. Non-persistent queries that are stateful read and write to Kafka, for example, COUNT and JOIN. The data in Kafka is deleted automatically when you terminate the query with CTRL-C.</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>SHOW STREAMS and EXPLAIN &lt;query&gt; statements</p>', '<p>CREATE STREAM WITH &lt;topic&gt; and CREATE TABLE WITH &lt;topic&gt;</p>', '<p>CREATE STREAM AS SELECT and CREATE TABLE AS SELECT</p>', '<p>COUNT and JOIN</p>'], 'question': '<p>Which KSQL queries write to Kafka?</p>\n'}, 'correct_response': ['b', 'c', 'e'], 'section': 'KSQL', 'question_plain': 'Which KSQL queries write to Kafka?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173032, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Although any Kafka Streams application is stateless as the state is stored in Kafka, it can take a while and lots of resources to recover the state from Kafka. In order to speed up recovery, it is advised to store the Kafka Streams state on a persistent volume, so that only the missing part of the state needs to be recovered.  </p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Mount a persistent volume for your RocksDB</p>', '<p>Increase the number of partitions in your inputs topic</p>', '<p>Increase the number of Streams threads</p>', '<p>Reduce the Streams caching property</p>'], 'question': '<p>You are running a Kafka Streams application in a Docker container managed by Kubernetes, and upon application restart, it takes a long time for the docker container to replicate the state and get back to processing the data. How can you improve dramatically the application restart?</p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Streams', 'question_plain': 'You are running a Kafka Streams application in a Docker container managed by Kubernetes, and upon application restart, it takes a long time for the docker container to replicate the state and get back to processing the data. How can you improve dramatically the application restart?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173034, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Kafka replication mechanism makes it resilient to the scenarios where the broker lose data on disk, but can recover from replicating from other brokers. This makes Kafka amazing!</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>The broker will crash</p>', '<p>The broker will start, and other topics will also be deleted as the broker data on the disk got deleted</p>', "<p>The broker will start, and won't have any data. If the broker comes leader, we have a data loss</p>", "<p>The broker will start, and won't be online until all the data it needs to have is replicated from other leaders</p>"], 'question': '<p>You have a Kafka cluster and all the topics have a replication factor of 3. One intern at your company stopped a broker, and accidentally deleted all the data of that broker on the disk. What will happen if the broker is restarted?</p>\n'}, 'correct_response': ['d'], 'section': 'Broker', 'question_plain': 'You have a Kafka cluster and all the topics have a replication factor of 3. One intern at your company stopped a broker, and accidentally deleted all the data of that broker on the disk. What will happen if the broker is restarted?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20173036, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>connect-configs stores configurations, connect-status helps to elect leaders for connect, and connect-offsets store source offsets for source connectors</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>connect-configs</p>', '<p>connect-status</p>', '<p>connect-offsets</p>', '<p>connect-jars</p>'], 'question': "<p>What isn't an internal Kafka Connect topic?</p>\n"}, 'correct_response': ['d'], 'section': 'Kafka Connect', 'question_plain': "What isn't an internal Kafka Connect topic?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20173038, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>See: <a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#stateful-transformations">https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#stateful-transformations</a></p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>peek</p>', '<p>aggregate</p>', '<p>flatmap</p>', '<p>count</p>', '<p>reduce</p>', '<p>joining</p>'], 'question': '<p>Which of the following Kafka Streams operators are stateful? (select all that apply)</p>\n'}, 'correct_response': ['b', 'd', 'e', 'f'], 'section': 'Kafka Streams', 'question_plain': 'Which of the following Kafka Streams operators are stateful? (select all that apply)', 'related_lectures': []}]}
4581122
~~~
{'count': 50, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 20842240, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=all can't produce?</p>\n", 'feedbacks': ['', '', '', ''], 'answers': ['<p>0</p>', '<p>1</p>', '<p>2</p>', '<p>3</p>'], 'explanation': '<p>acks=all and min.insync.replicas=2 means we must have at least 2 brokers up for the partition to be available </p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Producer', 'question_plain': "A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=all can't produce?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20842242, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A Zookeeper ensemble contains 5 servers. What is the maximum number of servers that can go missing and the ensemble still run?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>4</p>'], 'explanation': '<p>majority consists of 3 zk nodes for 5 nodes zk cluster, so 2 can fail</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Zookeeper', 'question_plain': 'A Zookeeper ensemble contains 5 servers. What is the maximum number of servers that can go missing and the ensemble still run?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842244, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>When auto.create.topics.enable is set to true in Kafka configuration, what are the circumstances under which a Kafka broker automatically creates a topic? (select three)</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Producer sends message to a topic</p>', '<p>Client alters number of partitions of a topic</p>', '<p>Consumer reads message from a topic</p>', '<p>Client requests metadata for a topic</p>'], 'explanation': '<p>A kafka broker automatically creates a topic under the following circumstances:\n- When a producer starts writing messages to the topic\n- When a consumer starts reading messages from the topic\n- When any client requests metadata for the topic</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a', 'c', 'd'], 'section': 'Topic', 'question_plain': 'When auto.create.topics.enable is set to true in Kafka configuration, what are the circumstances under which a Kafka broker automatically creates a topic? (select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842246, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Kafka is configured with following parameters - \n    log.retention.hours = 168\n    log.retention.minutes = 168\n    log.retention.ms = 168 \nHow long will the messages be retained for?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>168 minutes</p>', '<p>168 ms</p>', '<p>Broker will not start due to bad configuration</p>', '<p>168 hours</p>'], 'relatedLectureIds': '', 'explanation': '<p>If more than one similar config is specified, the smaller unit size will take precedence.</p>\n'}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'Kafka is configured with following parameters - \n    log.retention.hours = 168\n    log.retention.minutes = 168\n    log.retention.ms = 168 \nHow long will the messages be retained for?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842248, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is returned by a producer.send() call in the Java API?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>A Boolean indicating if the call succeeded</p>', '<p>Unit</p>', '<p>Future&lt;RecordMetadata&gt; object</p>', '<p>Future&lt;ProducerRecord&gt; object</p>'], 'relatedLectureIds': '', 'explanation': '<p>See: <a href="https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html">https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html</a></p>\n'}, 'correct_response': ['c'], 'section': 'Producer', 'question_plain': 'What is returned by a producer.send() call in the Java API?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842250, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>You are sending messages with keys to a topic. To increase throughput, you decide to increase the number of partitions of the topic. Select all that apply.</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>All the existing records will get rebalanced among the partitions to balance load</p>', '<p>Old records will stay in their partitions</p>', '<p>New records with the same key will get written to the partition where old records with that key were written</p>', '<p>New records may get written to a different partition</p>'], 'explanation': '<p>Increasing the number of partition causes new messages keys to get hashed differently, and breaks the guarantee &quot;same keys goes to the same partition&quot;. Kafka logs are immutable and the previous messages are not re-shuffled</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b', 'd'], 'section': 'Producer', 'question_plain': 'You are sending messages with keys to a topic. To increase throughput, you decide to increase the number of partitions of the topic. Select all that apply.', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842252, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>Which actions will trigger partition rebalance for a consumer group? (select three)</p>\n', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Add a new consumer to consumer group</p>', '<p>A consumer in a consumer group shuts down</p>', '<p>Increase partitions of a topic</p>', '<p>Add a broker to the cluster</p>', '<p>Remove a broker from the cluster</p>'], 'explanation': '<p>Rebalance occurs when a new consumer is added, removed or consumer dies or paritions increased.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a', 'b', 'c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'Which actions will trigger partition rebalance for a consumer group? (select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842254, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A consumer sends a request to commit offset 2000. There is a temporary communication problem, so the broker never gets the request and therefore never responds. Meanwhile, the consumer processed another batch and successfully committed offset 3000. What should you do?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Nothing</p>', '<p>Use the kafka-consumer-group command to manually commit the offsets 2000 for the consumer group</p>', '<p>Restart the consumer</p>', '<p>Add a new consumer to the group</p>'], 'relatedLectureIds': '', 'explanation': '<p>In this case, because the offset 3000 has been committed and all the messages between 0 and 3000 have all been processed, it is okay not to have committed offset 2000. The right answer is to do &quot;nothing&quot;, this behaviour is acceptable</p>\n'}, 'correct_response': ['a'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer sends a request to commit offset 2000. There is a temporary communication problem, so the broker never gets the request and therefore never responds. Meanwhile, the consumer processed another batch and successfully committed offset 3000. What should you do?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842256, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How does a consumer commit offsets in Kafka?</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>It directly sends a message to the __consumer_offsets topic</p>', '<p>It interacts with the Group Coordinator broker</p>', '<p>It directly commits the offsets in Zookeeper</p>'], 'explanation': '<p>Consumers do not directly write to the __consumer_offsets topic, they instead interact with a broker that has been elected to manage that topic, which is the Group Coordinator broker</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'How does a consumer commit offsets in Kafka?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842258, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A consumer wants to read messages from partitions 0 and 1 of a topic topic1. Code snippet is shown below.</p>\n\n<pre><code>consumer.subscribe(Arrays.asList("topic1"));\nList&lt;TopicPartition&gt; pc = new ArrayList&lt;&gt;();\npc.add(new PartitionTopic("topic1", 0));\npc.add(new PartitionTopic("topic1", 1));\nconsumer.assign(pc);\n</code></pre>\n', 'feedbacks': ['', ''], 'answers': ['<p>This works fine. subscribe() will subscribe to the topic and assign() will assign partitions to the consumer.</p>', '<p>Throws IllegalStateException</p>'], 'explanation': '<p>subscribe() and assign() cannot be called by the same consumer, subscribe() is used to leverage the consumer group mechanism, while assign() is used to manually control partition assignment and reads assignment</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer wants to read messages from partitions 0 and 1 of a topic topic1. Code snippet is shown below.\n\nconsumer.subscribe(Arrays.asList("topic1"));\nList&lt;TopicPartition&gt; pc = new ArrayList&lt;&gt;();\npc.add(new PartitionTopic("topic1", 0));\npc.add(new PartitionTopic("topic1", 1));\nconsumer.assign(pc);', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842260, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>Right away</p>', '<p>When the message has been fully replicated to all replicas</p>', '<p>When the high watermark has advanced</p>', '<p>Never, the produce request will fail</p>'], 'explanation': '<p>The high watermark is an advanced Kafka concept, and is advanced once all the ISR replicates the latest offsets.  A consumer can only read up to the value of the High Watermark (which can be less than the highest offset, in the case of <code>acks=1</code>)</p>\n', 'question': '<p>A producer just sent a message to the leader broker for a topic partition. The producer used <code>acks=1</code> and therefore the data has not yet been replicated to followers. Under which conditions will the consumer see the message?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '']}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A producer just sent a message to the leader broker for a topic partition. The producer used acks=1 and therefore the data has not yet been replicated to followers. Under which conditions will the consumer see the message?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842262, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A consumer starts and has auto.offset.reset=none, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 10 for the topic before. Where will the consumer read from?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>offset 45</p>', '<p>offset 10</p>', '<p>offset 2311</p>', '<p>it will crash</p>'], 'explanation': "<p>auto.offset.reset=none means that the consumer will crash if the offsets it's recovering from have been deleted from Kafka, which is the case here, as 10 &lt; 45</p>\n", 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer starts and has auto.offset.reset=none, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 10 for the topic before. Where will the consumer read from?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842264, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Compaction is enabled for a topic in Kafka by setting log.cleanup.policy=compact. What is true about log compaction?</p>\n', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Each message stored in the topic is compressed</p>', '<p>After cleanup, only one message per key is retained with the latest value</p>', '<p>After cleanup, only one message per key is retained with the first value</p>', '<p>Compaction changes the offset of messages</p>', '<p>Kafka automatically de-duplicates incoming messages based on key hashes</p>'], 'relatedLectureIds': '', 'explanation': '<p>Log compaction retains at least the last known value for each record key for a single topic partition. All compacted log offsets remain valid, even if record at offset has been compacted away as a consumer will get the next highest offset.</p>\n'}, 'correct_response': ['b'], 'section': 'Topic', 'question_plain': 'Compaction is enabled for a topic in Kafka by setting log.cleanup.policy=compact. What is true about log compaction?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842266, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a consume request is sent to broker?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>NotEnoughReplicasException will be returned</p>', '<p>Data will be returned from the remaining in-sync replica</p>', '<p>A new leader for the partition will be elected</p>', '<p>An empty message will be returned</p>'], 'relatedLectureIds': '', 'explanation': '<p>With this configuration, a single in-sync replica is still readable, but not writeable if the producer using acks=all</p>\n'}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a consume request is sent to broker?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842268, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>You are doing complex calculations using a machine learning framework on records fetched from a Kafka topic. It takes more about 6 minutes to process a record batch, and the consumer enters rebalances even though it's still running. How can you improve this scenario?</p>\n", 'feedbacks': ['', '', '', ''], 'answers': ['<p>Increase session.timeout.ms to 600000</p>', '<p>Increase heartbeat.interval.ms to 600000</p>', '<p>Increase max.poll.interval.ms to 600000</p>', '<p>Add consumers to the consumer group and kill them right away</p>'], 'relatedLectureIds': '', 'explanation': "<p>Here, we need to change the setting max.poll.interval.ms (default 300000) to its double in order to tell Kafka a consumer should be considered dead if the consumer only if it hasn't called the .poll() method in 10 minutes instead of 5.</p>\n"}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': "You are doing complex calculations using a machine learning framework on records fetched from a Kafka topic. It takes more about 6 minutes to process a record batch, and the consumer enters rebalances even though it's still running. How can you improve this scenario?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20842270, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Your topic is log compacted and you are sending a message with the key K and value null. What will happen?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>The message will get ignored by the Kafka broker</p>', '<p>The producer will throw a Runtime exception</p>', '<p>The broker will delete all messages with the key K upon cleanup</p>', '<p>The broker will delete the message with the key K and null value only upon cleanup</p>'], 'explanation': '<p>Sending a message with the null value is called a tombstone in Kafka and will ensure the log compacted topic does not contain any messages with the key K upon compaction</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Topic', 'question_plain': 'Your topic is log compacted and you are sending a message with the key K and value null. What will happen?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842272, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are using JDBC source connector to copy data from 3 tables to three Kafka topics. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>6</p>'], 'explanation': "<p>here, we have three tables, but the max.tasks is 2, so that's the maximum number of tasks that will be created</p>\n", 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Kafka Connect', 'question_plain': 'You are using JDBC source connector to copy data from 3 tables to three Kafka topics. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842274, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A producer application in a developer machine was able to send messages to a Kafka topic. After copying the producer application into another developer's machine, the producer is able to connect to Kafka but unable to produce to the same Kafka topic because of an authorization issue. What is the likely issue?</p>\n", 'feedbacks': ['', '', '', ''], 'answers': ['<p>You cannot copy a producer application from one machine to another</p>', '<p>Broker configuration needs to be changed to allow a different producer</p>', '<p>The Kafka ACL does not allow another machine IP</p>', '<p>The Kafka Broker needs to be rebooted</p>'], 'relatedLectureIds': '', 'explanation': "<p>ACLs take &quot;Host&quot; as a parameter, which represents an IP. It can be * (all IP), or a specific IP. Here, it's a specific IP as moving a producer to a different machine breaks the consumer, so the ACL needs to be updated</p>\n"}, 'correct_response': ['c'], 'section': 'Security', 'question_plain': "A producer application in a developer machine was able to send messages to a Kafka topic. After copying the producer application into another developer's machine, the producer is able to connect to Kafka but unable to produce to the same Kafka topic because of an authorization issue. What is the likely issue?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20842276, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>There are 3 producers writing to a topic with 5 partitions. There are 10 consumers consuming from the topic as part of the same group. How many consumers will remain idle?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>3</p>', '<p>5</p>', '<p>10</p>', '<p>None</p>'], 'explanation': '<p>One consumer per partition assignment will keep 5 consumers idle.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'There are 3 producers writing to a topic with 5 partitions. There are 10 consumers consuming from the topic as part of the same group. How many consumers will remain idle?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842278, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An ecommerce website maintains two topics - a high volume &quot;purchase&quot; topic with 5 partitions and low volume &quot;customer&quot; topic with 3 partitions. You would like to do a stream-table join of these topics. How should you proceed?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Repartition the purchase topic to have 3 partitions</p>', '<p>Repartition customer topic to have 5 partitions</p>', '<p>Do a KStream / KTable join after a repartition step</p>', '<p>Model customer as a GlobalKTable</p>'], 'explanation': '<p>In case of KStream-KStream join, both need to be co-partitioned. This restriction is not applicable in case of join with GlobalKTable, which is the most efficient here. </p>\n', 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Kafka Streams', 'question_plain': 'An ecommerce website maintains two topics - a high volume &quot;purchase&quot; topic with 5 partitions and low volume &quot;customer&quot; topic with 3 partitions. You would like to do a stream-table join of these topics. How should you proceed?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842280, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are building a consumer application that processes events from a Kafka topic. What is the most important metric to monitor to ensure real-time processing?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>UnderReplicatedPartitions</p>', '<p>BytesInPerSec</p>', '<p>MessagesInPerSec</p>', '<p>records-lag-max</p>'], 'relatedLectureIds': '', 'explanation': '<p>This metric shows the current lag (number of messages behind the broker)</p>\n'}, 'correct_response': ['d'], 'section': 'Monitoring', 'question_plain': 'You are building a consumer application that processes events from a Kafka topic. What is the most important metric to monitor to ensure real-time processing?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842282, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A producer application was sending messages to a partition with a replication factor of 2 by connecting to Broker 1 that was hosting partition leader. If the Broker 1 goes down, what will happen?</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>The producer will stop working</p>', '<p>The topic will be unavailable</p>', '<p>The producer will automatically produce to the broker that has been elected leader</p>'], 'explanation': '<p>Once the client connects to any broker, it is connected to the entire cluster and in case of leadership changes, the clients automatically do a Metadata Request to an available broker to find out who is the new leader for the topic. Hence the producer will automatically keep on producing to the correct Kafka Broker</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'A producer application was sending messages to a partition with a replication factor of 2 by connecting to Broker 1 that was hosting partition leader. If the Broker 1 goes down, what will happen?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842284, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Which Kafka CLI should you use to consume from a topic?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>kafka-console</p>', '<p>kafka-console-consumer</p>', '<p>kafka-consumer-groups</p>', '<p>kafka-topics</p>'], 'explanation': '<p>Example: kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic test --from-beginning</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'CLI', 'question_plain': 'Which Kafka CLI should you use to consume from a topic?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842286, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When is the onCompletion() method called?</p>\n\n<pre><code>private class ProducerCallback implements Callback {\n         @Override\n        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         if (e != null) {\n             e.printStackTrace();\n            }\n        } \n}\n\n    ProducerRecord&lt;String, String&gt; record =\n            new ProducerRecord&lt;&gt;("topic1", "key1", "value1");\n    producer.send(record, new ProducerCallback()); \n</code></pre>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>When send() method is called</p>', '<p>When message is serialized successfully</p>', '<p>When the message is partitioned and batched successfully</p>', '<p>When the broker response is received</p>'], 'explanation': '<p>Callback is invoked when a broker response is received.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Producer', 'question_plain': 'When is the onCompletion() method called?\n\nprivate class ProducerCallback implements Callback {\n         @Override\n        public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         if (e != null) {\n             e.printStackTrace();\n            }\n        } \n}\n\n    ProducerRecord&lt;String, String&gt; record =\n            new ProducerRecord&lt;&gt;("topic1", "key1", "value1");\n    producer.send(record, new ProducerCallback());', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842288, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A producer is sending messages with null key to a topic with 6 partitions using the DefaultPartitioner. Where will the messages be stored?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Any of the topic partitions</p>', '<p>The partition for the null key</p>', '<p>Partition 0</p>', '<p>Partition 5</p>'], 'relatedLectureIds': '', 'explanation': '<p>Message with no keys will be stored with round-robin strategy among partitions.</p>\n'}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'A producer is sending messages with null key to a topic with 6 partitions using the DefaultPartitioner. Where will the messages be stored?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842290, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<pre><code>while (true) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);\n         try {\n          consumer.commitSync();\n        } catch (CommitFailedException e) {\n            log.error("commit failed", e)\n        }\n        for (ConsumerRecord&lt;String, String&gt; record : records)\n        {\n            System.out.printf("topic = %s, partition = %s, offset =\n              %d, customer = %s, country = %s\n",\n                 record.topic(), record.partition(),\n                 record.offset(), record.key(), record.value());\n        }\n}\n</code></pre>\n\n<p>What kind of delivery guarantee this consumer offers?</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>Exactly-once</p>', '<p>At-most-once</p>', '<p>At-least-once</p>'], 'explanation': '<p>Here offset is committed before processing the message. If consumer crashes before processing the message, message will be lost when it comes back up.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'while (true) {\n        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);\n         try {\n          consumer.commitSync();\n        } catch (CommitFailedException e) {\n            log.error("commit failed", e)\n        }\n        for (ConsumerRecord&lt;String, String&gt; record : records)\n        {\n            System.out.printf("topic = %s, partition = %s, offset =\n              %d, customer = %s, country = %s\n",\n                 record.topic(), record.partition(),\n                 record.offset(), record.key(), record.value());\n        }\n}\n\n\nWhat kind of delivery guarantee this consumer offers?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842292, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>The Controller is a broker that is... (select two)</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>elected by broker majority</p>', '<p>elected by Zookeeper ensemble</p>', '<p>is responsible for consumer group rebalances</p>', '<p>is responsible for partition leader election</p>'], 'explanation': '<p>Controller is a broker that in addition to usual broker functions is responsible for partition leader election. The election of that broker happens thanks to Zookeeper and at any time only one broker can be a controller</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b', 'd'], 'section': 'Broker', 'question_plain': 'The Controller is a broker that is... (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842294, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>In the Kafka consumer metrics it is observed that fetch-rate is very high and each fetch is small. What steps will you take to increase throughput?</p>\n', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>Increase <code>fetch.max.bytes</code></p>', '<p>Decrease <code>fetch.max.bytes</code></p>', '<p>Increase <code>fetch.min.bytes</code></p>', '<p>Decrease <code>fetch.min.bytes</code></p>', '<p>Increase <code>fetch.max.wait</code></p>'], 'explanation': '<p>This will allow consumers to wait and receive more bytes in each fetch request.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'In the Kafka consumer metrics it is observed that fetch-rate is very high and each fetch is small. What steps will you take to increase throughput?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842296, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How will you find out all the partitions without a leader?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p><code>kafka-topics.sh --zookeeper localhost:2181 --describe --under-replicated-partitions</code></p>', '<p><code>kafka-topics.sh --zookeeper localhost:2181 --describe --unavailable-partitions</code></p>', '<p><code>kafka-topics.sh --broker-list localhost:9092 --describe --under-replicated-partitions</code></p>', '<p><code>kafka-topics.sh --bootstrap-server localhost:2181 --describe --unavailable-partitions</code></p>'], 'explanation': '<p>Please note that as of Kafka 2.2, the --zookeeper option is deprecated and you can now use: <code>kafka-topics.sh --bootstrap-server localhost:9092 --describe --unavailable-partitions</code></p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'CLI', 'question_plain': 'How will you find out all the partitions without a leader?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842298, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>We have a store selling shoes. What dataset is a great candidate to be modeled as a KTable in Kafka Streams?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>The transaction stream</p>', '<p>Inventory contents right now</p>', '<p>Money made until now</p>', '<p>Items returned</p>'], 'explanation': '<p>Aggregations of stream are stored in table, whereas Streams must be modeled as a KStream to avoid data explosion</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b', 'c'], 'section': 'Kafka Streams', 'question_plain': 'We have a store selling shoes. What dataset is a great candidate to be modeled as a KTable in Kafka Streams?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842300, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Select the Kafka Streams joins that are always windowed joins.</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>KStream-KStream join</p>', '<p>KTable-KTable join</p>', '<p>KStream-KTable join</p>', '<p>KStream-GlobalKTable</p>'], 'relatedLectureIds': '', 'explanation': '<p>See: <a href="https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#joining">https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#joining</a></p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Streams', 'question_plain': 'Select the Kafka Streams joins that are always windowed joins.', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842302, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<pre><code>StreamsBuilder builder = new StreamsBuilder();\n        KStream&lt;String, String&gt; textLines = builder.stream("word-count-input");\n        KTable&lt;String, Long&gt; wordCounts = textLines\n                .mapValues(textLine -&gt; textLine.toLowerCase())\n                .flatMapValues(textLine -&gt; Arrays.asList(textLine.split("\\W+")))\n                .selectKey((key, word) -&gt; word)\n                .groupByKey()\n                .count(Materialized.as("Counts"));\n        wordCounts.toStream().to("word-count-output", Produced.with(Serdes.String(), Serdes.Long()));\n        builder.build();\n</code></pre>\n\n<p>What is an adequate topic configuration for the topic word-count-output?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>compression.type=lz4</p>', '<p>cleanup.policy=compact</p>', '<p>cleanup.policy=delete</p>', '<p>max.message.bytes=10000000</p>'], 'relatedLectureIds': '', 'explanation': "<p>Result is aggregated into a table with key as the unique word and value its frequency. We have to enable log compaction for this topic to align the topic's cleanup policy with KTable semantics.</p>\n"}, 'correct_response': ['b'], 'section': 'Kafka Streams', 'question_plain': 'StreamsBuilder builder = new StreamsBuilder();\n        KStream&lt;String, String&gt; textLines = builder.stream("word-count-input");\n        KTable&lt;String, Long&gt; wordCounts = textLines\n                .mapValues(textLine -&gt; textLine.toLowerCase())\n                .flatMapValues(textLine -&gt; Arrays.asList(textLine.split("\\W+")))\n                .selectKey((key, word) -&gt; word)\n                .groupByKey()\n                .count(Materialized.as("Counts"));\n        wordCounts.toStream().to("word-count-output", Produced.with(Serdes.String(), Serdes.Long()));\n        builder.build();\n\n\nWhat is an adequate topic configuration for the topic word-count-output?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842304, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>In Java, Avro SpecificRecords classes are </p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>automatically generated from an Avro Schema + a Maven / Gradle Plugin</p>', '<p>automatically generated from an Avro Schema</p>', '<p>written manually by the programmer</p>'], 'explanation': '<p>SpecificRecord is created from generated record classes</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a'], 'section': 'Schema Registry', 'question_plain': 'In Java, Avro SpecificRecords classes are', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842306, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>In Avro, removing a field that does not have a default is a __ schema evolution</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>backward</p>', '<p>forward</p>', '<p>breaking</p>', '<p>full</p>'], 'relatedLectureIds': '', 'explanation': '<p>Clients with new schema will be able to read records saved with old schema.</p>\n'}, 'correct_response': ['a'], 'section': 'Schema Registry', 'question_plain': 'In Avro, removing a field that does not have a default is a __ schema evolution', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842308, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>What data format isn't natively available with the Confluent REST Proxy?</p>\n", 'feedbacks': ['', '', '', ''], 'answers': ['<p>json</p>', '<p>avro</p>', '<p>protobuf</p>', '<p>binary</p>'], 'explanation': "<p>Protocol buffers isn't a natively supported type for the Confluent REST Proxy, but you may use the binary format instead</p>\n", 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'REST Proxy', 'question_plain': "What data format isn't natively available with the Confluent REST Proxy?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20842310, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Once sent to a topic, a message can be modified</p>\n', 'feedbacks': ['', ''], 'answers': ['<p>Yes</p>', '<p>No</p>'], 'explanation': '<p>Kafka logs are append-only and the data is immutable</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'Once sent to a topic, a message can be modified', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842312, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>To read data from a topic, the following configuration is needed for the consumers</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>any broker to connect to, and the topic name</p>', '<p>all brokers of the cluster, and the topic name</p>', '<p>any broker, and the list of topic partitions</p>', '<p>the list of brokers that have the data, the topic name and the partitions list</p>'], 'explanation': '<p>All brokers can respond to Metadata request, so a client can connect to any broker in the cluster.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'To read data from a topic, the following configuration is needed for the consumers', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842314, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The kafka-console-consumer CLI, when used with the default options</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>does not use a group id</p>', '<p>always uses the same group id</p>', '<p>uses a random group id</p>'], 'explanation': '<p>If a group is not specified, the kafka-console-consumer generates a random consumer group.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'CLI', 'question_plain': 'The kafka-console-consumer CLI, when used with the default options', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842316, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>To allow consumers in a group to resume at the previously committed offset, I need to set the proper value for...</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>auto.offset.resets</p>', '<p>group.id</p>', '<p>value.deserializer</p>', '<p>enable.auto.commit</p>'], 'relatedLectureIds': '', 'explanation': "<p>Setting a group.id that's consistent across restarts will allow your consumers part of the same group to resume reading from where offsets were last committed for that group</p>\n"}, 'correct_response': ['b'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'To allow consumers in a group to resume at the previously committed offset, I need to set the proper value for...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842318, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>To prevent network-induced duplicates when producing to Kafka, I should use</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>retries=200000</p>', '<p>enable.idempotence=true</p>', '<p>max.in.flight.requests.per.connection=1</p>', '<p>batch.size=1</p>'], 'explanation': '<p>Producer idempotence helps prevent the network introduced duplicates. More details here: <a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent Producer">https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer</a></p>\n', 'relatedLectureIds': ''}, 'correct_response': ['b'], 'section': 'Producer', 'question_plain': 'To prevent network-induced duplicates when producing to Kafka, I should use', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842320, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The rule &quot;same key goes to the same partition&quot; is true unless...</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>the number of partition changes</p>', '<p>the number of producer changes</p>', '<p>the replication factor changes</p>', '<p>the number of kafka broker changes</p>'], 'explanation': '<p>Increasing the number of partition causes new messages keys to get hashed differently, and breaks the guarantee &quot;same keys goes to the same partition&quot;. Kafka logs are immutable and the previous messages are not re-shuffled.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'The rule &quot;same key goes to the same partition&quot; is true unless...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842322, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>To transform data from a Kafka topic to another one, I should use</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Kafka Connect Source</p>', '<p>Kafka Connect Sink</p>', '<p>Kafka Streams</p>', '<p>Consumer + Producer</p>'], 'explanation': '<p>Kafka Streams is a library for building streaming applications, specifically applications that transform input Kafka topics into output Kafka topics</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Kafka Streams', 'question_plain': 'To transform data from a Kafka topic to another one, I should use', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842324, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When using the Confluent Kafka Distribution, where does the schema registry reside?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>As an in-memory plugin on your Kafka Brokers</p>', '<p>As an in-memory plugin on your Zookeeper cluster</p>', '<p>As an in-memory plugin on your Kafka Connect Workers</p>', '<p>As a separate JVM component</p>'], 'explanation': '<p>Schema registry is a separate application that provides RESTful interface for storing and retrieving Avro schemas.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Schema Registry', 'question_plain': 'When using the Confluent Kafka Distribution, where does the schema registry reside?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842326, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What Java library is KSQL based on?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Kafka Streams</p>', '<p>Kafka Connect</p>', '<p>Schema Registry</p>', '<p>REST Proxy</p>'], 'relatedLectureIds': '', 'explanation': '<p>KSQL is based on Kafka Streams and allows you to express transformations in the SQL language that get automatically converted to a Kafka Streams program in the backend</p>\n'}, 'correct_response': ['a'], 'section': 'KSQL', 'question_plain': 'What Java library is KSQL based on?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842328, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>When using plain JSON data with Connect, you see the following error message: org.apache.kafka.connect.errors.DataException: JsonDeserializer with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. How will you fix the error?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Set key.converter, value.converter to AvroConverter and the schema registry url</p>', '<p>Use Single Message Transforms to add schema and payload fields in the message</p>', '<p>Set key.converter.schemas.enable and value.converter.schemas.enable to false</p>', '<p>Set key.converter, value.converter to JsonConverter and the schema registry url</p>'], 'explanation': '<p>You will need to set the schemas.enable parameters for the converter to false for plain text with no schema.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Kafka Connect', 'question_plain': 'When using plain JSON data with Connect, you see the following error message: org.apache.kafka.connect.errors.DataException: JsonDeserializer with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. How will you fix the error?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842330, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How can you gracefully make a Kafka consumer to stop immediately polling data from Kafka and gracefully shut down a consumer application?</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>Call consumer.wakeUp() and catch a WakeUpException</p>', '<p>Kill the consumer thread</p>', '<p>Call consumer.poll() in another thread</p>'], 'explanation': '<p>See <a href="https://stackoverflow.com/a/37748336/3019499">https://stackoverflow.com/a/37748336/3019499</a></p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a'], 'section': 'Kafka Consumer', 'question_plain': 'How can you gracefully make a Kafka consumer to stop immediately polling data from Kafka and gracefully shut down a consumer application?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842332, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>How often is log compaction evaluated?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>Every time a message is sent to Kafka</p>', '<p>Every time a message is flushed to disk</p>', '<p>Every time a segment is closed</p>', '<p>Every time a new partition is created</p>'], 'explanation': '<p>Log compaction is evaluated every time a segment is closed. It will be triggered if enough data is &quot;dirty&quot; (see dirty ratio config)</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'How often is log compaction evaluated?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842334, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>In Kafka Streams, by what value are internal topics prefixed by?</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>kafka-streams-</p>', '<p>application.id</p>', '<p>group.id</p>', '<p>tasks-&lt;number&gt;</p>'], 'relatedLectureIds': '', 'explanation': '<p>In Kafka Streams, the application.id is also the underlying group.id for your consumers, and the prefix for all internal topics (repartition and state)</p>\n'}, 'correct_response': ['b'], 'section': 'Kafka Streams', 'question_plain': 'In Kafka Streams, by what value are internal topics prefixed by?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842336, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You have a Zookeeper cluster that needs to be able to withstand the loss of 2 servers and still be able to function. What size should your Zookeeper cluster have?</p>\n', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>2</p>', '<p>3</p>', '<p>4</p>', '<p>5</p>', '<p>6</p>'], 'relatedLectureIds': '', 'explanation': '<p>Your Zookeeper cluster needs to have an odd number of servers, and must maintain a majority of servers up to be able to vote. Therefore, a 2N+1 zookeeper cluster can survive to N zookeeper being down, so here the right answer is N=2, 2*N+1=5</p>\n'}, 'correct_response': ['d'], 'section': 'Zookeeper', 'question_plain': 'You have a Zookeeper cluster that needs to be able to withstand the loss of 2 servers and still be able to function. What size should your Zookeeper cluster have?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20842338, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>What is true about partitions? (select two)</p>\n', 'feedbacks': ['', '', '', '', ''], 'answers': ['<p>A partition has one replica that is a leader, while the other replicas are followers</p>', '<p>You cannot have more partitions than the number of brokers in your cluster</p>', '<p>A broker can have a partition and its replica on its disk</p>', '<p>A broker can have different partitions numbers for the same topic on its disk</p>', '<p>Only out of sync replicas are replicas, the remaining partitions that are in sync are also leader</p>'], 'explanation': '<p>Only one of the replicas is elected as partition leader. And a broker can definitely hold many partitions from the same topic on its disk, try creating a topic with 12 partitions on one broker!</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['a', 'd'], 'section': 'Topic', 'question_plain': 'What is true about partitions? (select two)', 'related_lectures': []}]}
4581124
~~~
{'count': 50, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 20534554, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>0</p>', '<p>1</p>', '<p>2</p>', '<p>3</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=1 can't produce?</p>\n", 'explanation': '<p>min.insync.replicas does not impact producers when acks=1 (only when acks=all)</p>\n'}, 'correct_response': ['c'], 'section': 'Producer', 'question_plain': "A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=1 can't produce?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20534556, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>40 sec</p>', '<p>20 sec</p>', '<p>2000 ms</p>', '<p>10 sec</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>A Zookeeper configuration has tickTime of 2000, initLimit of 20 and syncLimit of 5. What's the timeout value for followers to connect to Zookeeper?</p>\n", 'explanation': '<p>tick time is 2000 ms, and initLimit is the config taken into account when establishing a connection to Zookeeper, so the answer is 2000 * 20 = 40000 ms = 40s</p>\n'}, 'correct_response': ['a'], 'section': 'Zookeeper', 'question_plain': "A Zookeeper configuration has tickTime of 2000, initLimit of 20 and syncLimit of 5. What's the timeout value for followers to connect to Zookeeper?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20534558, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>We can remove partitions in a topic using the kafka-topics.sh command</p>', '<p>The number of partitions in a topic cannot be altered</p>', '<p>We can remove partitions in a topic by removing a broker</p>', '<p>We can add partitions in a topic using the kafka-topics.sh command</p>', '<p>We can add partitions in a topic by adding a broker to the cluster</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>Which of the following statements are true regarding the number of partitions of a topic?</p>\n', 'explanation': '<p>We can only add partitions to an existing topic, and it must be done using the kafka-topics.sh command</p>\n'}, 'correct_response': ['d'], 'section': 'Topic', 'question_plain': 'Which of the following statements are true regarding the number of partitions of a topic?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534560, 'assessment_type': 'multi-select', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>All the brokers must share the same broker.id</p>', "<p>Unique values for each broker's broker.id parameter</p>", '<p>All the brokers must share the same zookeeper.connect parameter</p>', "<p>Unique value for each broker's zookeeper.connect parameter</p>"], 'feedbacks': ['', '', '', ''], 'question': '<p>What are the requirements for a Kafka broker to connect to a Zookeeper ensemble? (select two)</p>\n', 'explanation': '<p>Each broker must have a unique broker id and connect to the same zk ensemble and root zNode</p>\n'}, 'correct_response': ['b', 'c'], 'section': 'Broker', 'question_plain': 'What are the requirements for a Kafka broker to connect to a Zookeeper ensemble? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534562, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>KafkaProducer divides messages into sizes of message.max.bytes and sends them in order</p>', '<p>KafkaProducer divides messages into sizes of max.request.size and sends them in order</p>', '<p>MessageSizeTooLarge exception will be thrown, KafkaProducer retries until the number of retries are exhausted</p>', '<p>MessageSizeTooLarge exception will be thrown, KafkaProducer will not retry and return exception immediately</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You want to send a message of size 3 MB to a topic with default message size configuration. How does KafkaProducer handle large messages?</p>\n', 'explanation': '<p>MessageSizeTooLarge is not a retryable exception.</p>\n'}, 'correct_response': ['d'], 'section': 'Producer', 'question_plain': 'You want to send a message of size 3 MB to a topic with default message size configuration. How does KafkaProducer handle large messages?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534564, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>All messages with the same key will go the same partition, but the same partition may have messages with different keys. It is not possible to reserve</p>', '<p>Create a custom partitioner</p>', '<p>Add metadata to the producer record</p>', '<p>Define a Kafka Broker routing rule</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are receiving orders from different customer in an &quot;orders&quot; topic with multiple partitions. Each message has the customer name as the key. There is a special customer named ABC that generates a lot of orders and you would like to reserve a partition exclusively for ABC. The rest of the message should be distributed among other partitions. How can this be achieved?</p>\n', 'explanation': '<p>A Custom Partitioner allows you to easily customise how the partition number gets computed from a source message.</p>\n'}, 'correct_response': ['b'], 'section': 'Producer', 'question_plain': 'You are receiving orders from different customer in an &quot;orders&quot; topic with multiple partitions. Each message has the customer name as the key. There is a special customer named ABC that generates a lot of orders and you would like to reserve a partition exclusively for ABC. The rest of the message should be distributed among other partitions. How can this be achieved?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534566, 'assessment_type': 'multi-select', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>One Producer needs to be run in one thread</p>', '<p>One Consumer can be safely used in multiple threads</p>', '<p>One Producer can be safely used in multiple threads</p>', '<p>One Consumer needs to run in one thread</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Which of the following is true regarding thread safety in the Java Kafka Clients?</p>\n', 'explanation': '<p>KafkaConsumer is not thread-safe, KafkaProducer is thread safe.</p>\n'}, 'correct_response': ['c', 'd'], 'section': 'Clients', 'question_plain': 'Which of the following is true regarding thread safety in the Java Kafka Clients?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534568, 'assessment_type': 'multi-select', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Increase session.timeout.ms</p>', '<p>Decrease session.timeout.ms</p>', '<p>Increase heartbeat.interval.ms</p>', '<p>Decrease heartbeat.interval.ms</p>', '<p>increase max.poll.interval.ms</p>', '<p>decrease max.poll.interval.ms</p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>You have a consumer group of 12 consumers and when a consumer gets killed by the process management system, rather abruptly, it does not trigger a graceful shutdown of your consumer. Therefore, it takes up to 10 seconds for a rebalance to happen. The business would like to have a 3 seconds rebalance time. What should you do? (select two)</p>\n', 'explanation': '<p>session.timeout.ms must be decreased to 3 seconds to allow for a faster rebalance, and the heartbeat thread must be quicker, so we also need to decrease heartbeat.interval.ms</p>\n'}, 'correct_response': ['b', 'd'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'You have a consumer group of 12 consumers and when a consumer gets killed by the process management system, rather abruptly, it does not trigger a graceful shutdown of your consumer. Therefore, it takes up to 10 seconds for a rebalance to happen. The business would like to have a 3 seconds rebalance time. What should you do? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534570, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Zookeeper</p>', '<p>Vote amongst the brokers</p>', '<p>The Kafka Broker that is the Controller</p>', '<p>The consumers</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Partition leader election is done by</p>\n', 'explanation': '<p>The Controller is a broker that is responsible for electing partition leaders</p>\n'}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'Partition leader election is done by', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534572, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Produce requests can be done to the replicas that are followers</p>', '<p>Follower replica handles all consume requests</p>', '<p>Produce and consume requests are load-balanced between Leader and Follower replicas</p>', '<p>Leader replica handles all produce and consume requests</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>What is true about replicas ?</p>\n', 'explanation': "<p>Replicas are passive - they don't handle produce or consume request. Produce and consume requests get sent to the node hosting partition leader.</p>\n"}, 'correct_response': ['d'], 'section': 'Topic', 'question_plain': 'What is true about replicas ?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534574, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>offset 45</p>', '<p>offset 0</p>', '<p>offset 2311</p>', '<p>it will crash</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A consumer has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group never committed offsets for the topic before. Where will the consumer read from?</p>\n', 'explanation': '<p>Latest means that data retrievals will start from where the offsets currently end</p>\n'}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group never committed offsets for the topic before. Where will the consumer read from?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534576, 'assessment_type': 'multi-select', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>0</p>', '<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>5</p>', '<p>6</p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>Suppose you have 6 brokers and you decide to create a topic with 10 partitions and a replication factor of 3. The brokers 0 and 1 are on rack A, the brokers 2 and 3 are on rack B, and the brokers 4 and 5 are on rack C. If the leader for partition 0 is on broker 4, and the first replica is on broker 2, which broker can host the last replica? (select two)</p>\n', 'explanation': '<p>When you create a new topic, partitions replicas are spreads across racks to maintain availability. Hence, the Rack A, which currently does not hold the topic partition, will be selected for the last replica</p>\n'}, 'correct_response': ['a', 'b'], 'section': 'Broker', 'question_plain': 'Suppose you have 6 brokers and you decide to create a topic with 10 partitions and a replication factor of 3. The brokers 0 and 1 are on rack A, the brokers 2 and 3 are on rack B, and the brokers 4 and 5 are on rack C. If the leader for partition 0 is on broker 4, and the first replica is on broker 2, which broker can host the last replica? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534578, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>You can use the same broker.id as long as they have different broker.rack configuration</p>', '<p>Replicas for a partition are spread across different racks</p>', '<p>Replicas for a partition are placed in the same rack</p>', '<p>Each rack contains all the topics and partitions, effectively making Kafka highly available</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>What happens when broker.rack configuration is provided in broker configuration in Kafka cluster?</p>\n', 'explanation': '<p>Partitions for newly created topics are assigned in a rack alternating manner, this is the only change broker.rack does</p>\n'}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'What happens when broker.rack configuration is provided in broker configuration in Kafka cluster?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534580, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>NotEnoughReplicasException will be returned</p>', '<p>Produce request is honored with single in-sync replica</p>', '<p>Produce request will block till one of the two unavailable partition is available again.</p>'], 'feedbacks': ['', '', ''], 'question': '<p>A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a produce request with acks=all is sent to broker?</p>\n', 'explanation': '<p>With this configuration, a single in-sync replica becomes read-only. Produce request will receive NotEnoughReplicasException. </p>\n'}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a produce request with acks=all is sent to broker?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534582, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Commit offsets at 11</p>', '<p>Commit offsets at 10</p>', '<p>Do not commit until successfully processing the record #10</p>'], 'feedbacks': ['', '', ''], 'question': '<p>Consumer failed to process record # 10 and succeeded in processing record # 11. Select the course of action that you should choose to guarantee at least once processing</p>\n', 'explanation': "<p>Here, you shouldn't commit offsets 11 or 10 as it would indicate that the message #10 has been processed successfully. </p>\n"}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'Consumer failed to process record # 10 and succeeded in processing record # 11. Select the course of action that you should choose to guarantee at least once processing', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534584, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>1</p>', '<p>2</p>', '<p>3</p>', '<p>6</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are using JDBC source connector to copy data from a table to Kafka topic. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?</p>\n', 'explanation': '<p>JDBC connector allows one task per table.</p>\n'}, 'correct_response': ['a'], 'section': 'Kafka Connect', 'question_plain': 'You are using JDBC source connector to copy data from a table to Kafka topic. There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers. How many tasks are launched?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534586, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>Exactly-once delivery</p>', '<p>Zero copy</p>', '<p>Cross-cluster mirroring</p>', '<p>Support for Avro format</p>'], 'explanation': '<p>With SSL, messages will need to be encrypted and decrypted, by being first loaded into the JVM, so you lose the zero copy optimization. See more information here: <a href="https://twitter.com/ijuma/status/1161303431501324293?s=09">https://twitter.com/ijuma/status/1161303431501324293?s=09</a> </p>\n', 'feedbacks': ['', '', '', ''], 'relatedLectureIds': '', 'question': '<p>If you enable an SSL endpoint in Kafka, what feature of Kafka will be lost?</p>\n'}, 'correct_response': ['b'], 'section': 'Security', 'question_plain': 'If you enable an SSL endpoint in Kafka, what feature of Kafka will be lost?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534588, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>HTTP</p>', '<p>HTTPS (SSL/TLS)</p>', '<p>SASL</p>', '<p>Kerberos</p>'], 'relatedLectureIds': '', 'question': '<p>What is the protocol used by Kafka clients to securely connect to the Confluent REST Proxy?</p>\n', 'explanation': '<p>TLS - but it is still called SSL.</p>\n', 'feedbacks': ['', '', '', '']}, 'correct_response': ['b'], 'section': 'Security', 'question_plain': 'What is the protocol used by Kafka clients to securely connect to the Confluent REST Proxy?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534590, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>Point-to-point (request-response) style will couple client to the server.</p>\n', 'answers': ['<p>Reliability</p>', '<p>Cost</p>', '<p>Coupling</p>', '<p>Scalability</p>'], 'question': '<p>What is the disadvantage of request/response communication?</p>\n', 'feedbacks': ['', '', '', '']}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'What is the disadvantage of request/response communication?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534592, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'answers': ['<p>Purchase as stream, Product as stream, Customer as stream</p>', '<p>Purchase as stream, Product as table, Customer as stream</p>', '<p>Purchase as stream, Product as table, Customer as table</p>', '<p>Purchase as table, Product as table, Customer as table</p>'], 'relatedLectureIds': '', 'question': "<p>An ecommerce wesbite sells some custom made goods. What's the natural way of modeling this data in Kafka streams?</p>\n", 'explanation': '<p>Mostly-static data is modeled as a table whereas business transactions should be modeled as a stream.</p>\n'}, 'correct_response': ['c'], 'section': 'Kafka Streams', 'question_plain': "An ecommerce wesbite sells some custom made goods. What's the natural way of modeling this data in Kafka streams?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20534594, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>2</p>', '<p>1</p>', '<p>3</p>', '<p>6</p>'], 'relatedLectureIds': '', 'explanation': "<p>1 is not possible as it doesn't provide resilience to failure, 2 is not enough as if we take a broker down for maintenance, we cannot tolerate a broker failure, and 6 is impossible as we only have 3 brokers (RF cannot be greater than the number of brokers). Here the correct answer is 3</p>\n", 'feedbacks': ['', '', '', ''], 'question': '<p>There are 3 brokers in the cluster. You want to create a topic with a single partition that is resilient to one broker failure and one broker maintenance. What is the replication factor will you specify while creating the topic?</p>\n'}, 'correct_response': ['c'], 'section': 'Topic', 'question_plain': 'There are 3 brokers in the cluster. You want to create a topic with a single partition that is resilient to one broker failure and one broker maintenance. What is the replication factor will you specify while creating the topic?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534596, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>As of Kafka 2.3, the <code>kafka-topics.sh</code> command can take --bootstrap-server localhost:9092 as an argument. You could also use the (now deprecated)\xa0option of --zookeeper localhost:2181.</p>\n', 'answers': ['<p>bin/kafka-topics.sh --create --broker-list localhost:9092 --replication-factor 3 --partitions 3 --topic test</p>', '<p>bin/kafka-topics-create.sh --zookeeper localhost:9092 --replication-factor 3 --partitions 3 --topic test</p>', '<p>bin/kafka-topics.sh --create --bootstrap-server localhost:2181 --replication-factor 3 --partitions 3 --topic test</p>', '<p>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 3 --topic test</p>'], 'feedbacks': ['', '', '', ''], 'relatedLectureIds': '', 'question': '<p>How do you create a topic named test with 3 partitions and 3 replicas using the Kafka CLI?</p>\n'}, 'correct_response': ['d'], 'section': 'CLI', 'question_plain': 'How do you create a topic named test with 3 partitions and 3 replicas using the Kafka CLI?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534598, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>16 GB</p>', '<p>4 GB</p>', '<p>128 GB</p>', '<p>512 MB</p>'], 'question': '<p>How much should be the heap size of a broker in a production setup on a machine with 256 GB of RAM, in PLAINTEXT mode?</p>\n', 'explanation': '<p>In Kafka, a small heap size is needed, while the rest of the RAM goes automatically to the page cache (managed by the OS). The heap size goes slightly up if you need to enable SSL</p>\n', 'feedbacks': ['', '', '', '']}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'How much should be the heap size of a broker in a production setup on a machine with 256 GB of RAM, in PLAINTEXT mode?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534600, 'assessment_type': 'multi-select', 'prompt': {'explanation': '<p>These are the client side exceptions that may be encountered before message is sent to the broker, and before a future is returned by the .send() method.</p>\n', 'answers': ['<p>SerializationException</p>', '<p>BrokerNotAvailableException</p>', '<p>InvalidPartitionsException</p>', '<p>BufferExhaustedException</p>'], 'question': '<p>What exceptions may be caught by the following producer? (select two) </p>\n\n<pre><code>ProducerRecord&lt;String, String&gt; record =\n            new ProducerRecord&lt;&gt;("topic1", "key1", "value1");\n    try {\n      producer.send(record);\n    } catch (Exception e) {\n            e.printStackTrace();\n}\n</code></pre>\n', 'feedbacks': ['', '', '', ''], 'relatedLectureIds': ''}, 'correct_response': ['a', 'd'], 'section': 'Producer', 'question_plain': 'What exceptions may be caught by the following producer? (select two) \n\nProducerRecord&lt;String, String&gt; record =\n            new ProducerRecord&lt;&gt;("topic1", "key1", "value1");\n    try {\n      producer.send(record);\n    } catch (Exception e) {\n            e.printStackTrace();\n}', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534602, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Calling close() on consumer immediately triggers a partition rebalance as the consumer will not be available anymore.</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', ''], 'answers': ['<p>A rebalance in the consumer group will happen immediately</p>', '<p>The group coordinator will discover that the consumer stopped sending heartbeats. It will cause rebalance after session.timeout.ms</p>', '<p>The uncommitted offsets are committed</p>'], 'question': '<p>A consumer is configured with enable.auto.commit=false. What happens when close() is called on the consumer object?</p>\n'}, 'correct_response': ['a'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'A consumer is configured with enable.auto.commit=false. What happens when close() is called on the consumer object?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534604, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': "<p>retention.ms can be configured at topic level while creating topic or by altering topic. It shouldn't be set at the broker level (log.retention.ms) as this would impact all the topics in the cluster, not just the one we are interested in</p>\n", 'feedbacks': ['', '', '', ''], 'question': '<p>How will you set the retention for the topic named ‚Äúmy-topic‚Äù to 1 hour?</p>\n', 'answers': ['<p>Set the topic config retention.ms to 3600000</p>', '<p>Set the broker config log.retention.ms to 3600000</p>', '<p>Set the producer config retention.ms to 3600000</p>', '<p>Set the consumer config retention.ms to 3600000</p>'], 'relatedLectureIds': ''}, 'correct_response': ['a'], 'section': 'Topic', 'question_plain': 'How will you set the retention for the topic named ‚Äúmy-topic‚Äù to 1 hour?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534606, 'assessment_type': 'multi-select', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>Preferred leader is a broker that was leader when topic was created. It is preferred because when partitions are first created, the leaders are balanced between brokers. Otherwise, any of the in-sync replicas (ISR) will be elected leader, as long as unclean.leader.election=false (by default)</p>\n', 'answers': ['<p>Any of the replicas</p>', '<p>An in-sync replica</p>', '<p>Preferred leader broker if it is in-sync and auto.leader.rebalance.enable=true</p>', '<p>Preferred leader broker if it is in-sync and auto.leader.rebalance.enable=false</p>'], 'question': '<p>By default, which replica will be elected as a partition leader? (select two)</p>\n', 'feedbacks': ['', '', '', '']}, 'correct_response': ['b', 'c'], 'section': 'Topic', 'question_plain': 'By default, which replica will be elected as a partition leader? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534608, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Active-Active</p>', '<p>Active-Passive</p>', '<p>Passive-Passive</p>'], 'question': '<p>A topic &quot;sales&quot; is being produced to in the Americas region. You are mirroring this topic using Mirror Maker to the European region. From there, you are only reading the topic for analytics purposes. What kind of mirroring is this?</p>\n', 'explanation': '<p>This is active-passing as the replicated topic is used for read-only purposes only</p>\n', 'feedbacks': ['', '', '']}, 'correct_response': ['b'], 'section': 'Broker', 'question_plain': 'A topic &quot;sales&quot; is being produced to in the Americas region. You are mirroring this topic using Mirror Maker to the European region. From there, you are only reading the topic for analytics purposes. What kind of mirroring is this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534610, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>Each producer is allowed to produce @ 1MB/s to a broker. Max throughput 5 * 1MB, because we have 5 brokers.</p>\n', 'relatedLectureIds': '', 'question': '<p>There are five brokers in a cluster, a topic with 10 partitions and replication factor of 3, and a quota of producer_bytes_rate of 1 MB/sec has been specified for the client. What is the maximum throughput allowed for the client?</p>\n', 'answers': ['<p>0.33 MB/s</p>', '<p>1 MB/s</p>', '<p>5 MB/s</p>', '<p>10 MB/s</p>'], 'feedbacks': ['', '', '', '']}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'There are five brokers in a cluster, a topic with 10 partitions and replication factor of 3, and a quota of producer_bytes_rate of 1 MB/sec has been specified for the client. What is the maximum throughput allowed for the client?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534612, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': "<p>A hopping window is defined by two properties: the window's size and its advance interval (aka &quot;hop&quot;), e.g., a hopping window with a size 5 minutes and an advance interval of 1 minute.</p>\n", 'feedbacks': ['', '', '', ''], 'question': '<p>We want the average of all events in every five-minute window updated every minute. What kind of Kafka Streams window will be required on the stream?</p>\n', 'answers': ['<p>Tumbling window</p>', '<p>Sliding window</p>', '<p>Hopping window</p>', '<p>Session window</p>'], 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Kafka Streams', 'question_plain': 'We want the average of all events in every five-minute window updated every minute. What kind of Kafka Streams window will be required on the stream?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534614, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>GlobalKTables have their datasets replicated on each Kafka Streams instance and therefore no repartitioning is required</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>KStream-KStream join</p>', '<p>KTable-KTable join</p>', '<p>KStream-KTable join</p>', '<p>KStream-GlobalKTable</p>'], 'question': '<p>Which of these joins does not require input topics to be sharing the same number of partitions?</p>\n'}, 'correct_response': ['d'], 'section': 'Kafka Streams', 'question_plain': 'Which of these joins does not require input topics to be sharing the same number of partitions?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534616, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>date is a logical type</p>\n', 'feedbacks': ['', '', '', '', ''], 'question': '<p>Which of the following is not an Avro primitive type?</p>\n', 'answers': ['<p>null</p>', '<p>string</p>', '<p>date</p>', '<p>long</p>', '<p>int</p>']}, 'correct_response': ['c'], 'section': 'Schema Registry', 'question_plain': 'Which of the following is not an Avro primitive type?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534618, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': "<p>Since Confluent 5.4.0, Avro 1.9.1 is used. Since default value was added to enum complex type , the schema resolution changed from:</p>\n\n<p><strong>(&lt;1.9.1) if both are enums:**\n\nif the writer's symbol is not present in the reader's enum, then an error is signalled.\n\n**(&gt;=1.9.1) if both are enums:</strong></p>\n\n<p>if the writer's symbol is not present in the reader's enum and the reader has a default value, then that value is used, otherwise an error is signalled.</p>\n", 'answers': ['<p>backward</p>', '<p>forward</p>', '<p>breaking</p>', '<p>full</p>'], 'question': '<p>In Avro, adding an element to an enum without a default is a __ schema evolution</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['c'], 'section': 'Schema Registry', 'question_plain': 'In Avro, adding an element to an enum without a default is a __ schema evolution', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534620, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>backward</p>', '<p>forward</p>', '<p>breaking</p>', '<p>full</p>'], 'question': '<p>In Avro, removing or adding a field that has a default is a __ schema evolution</p>\n', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Clients with new schema will be able to read records saved with old schema and clients with old schema will be able to read records saved with new schema.</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Schema Registry', 'question_plain': 'In Avro, removing or adding a field that has a default is a __ schema evolution', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534622, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>The REST Proxy requires to receive data over REST that is already base64 encoded, hence it is the responsibility of the producer</p>\n', 'feedbacks': ['', '', '', ''], 'answers': ['<p>The Producer</p>', '<p>The REST Proxy</p>', '<p>The Kafka Broker</p>', '<p>Zookeeper</p>'], 'relatedLectureIds': '', 'question': '<p>If I want to send binary data through the REST proxy, it needs to be base64 encoded. Which component needs to encode the binary data into base 64?</p>\n'}, 'correct_response': ['a'], 'section': 'REST Proxy', 'question_plain': 'If I want to send binary data through the REST proxy, it needs to be base64 encoded. Which component needs to encode the binary data into base 64?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534624, 'assessment_type': 'multi-select', 'prompt': {'feedbacks': ['', '', '', '', '', ''], 'answers': ['<p>contains all the topics and all the partitions</p>', '<p>contains only a subset of the topics and the partitions</p>', '<p>knows all the metadata for all topics and partitions</p>', '<p>knows the metadata for the topics and partitions it has on its disk</p>', '<p>is a bootstrap broker</p>', '<p>is a controller</p>'], 'relatedLectureIds': '', 'question': '<p>In Kafka, every broker... (select three)</p>\n', 'explanation': '<p>Kafka topics are divided into partitions and spread across brokers. Each brokers knows about all the metadata and each broker is a bootstrap broker, but only one of them is elected controller</p>\n'}, 'correct_response': ['b', 'c', 'e'], 'section': 'Broker', 'question_plain': 'In Kafka, every broker... (select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534626, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'answers': ['<p>any broker from the cluster and the topic name</p>', '<p>any broker from the cluster and the topic name and the partitions list</p>', '<p>all the brokers from the cluster and the topic name</p>', '<p>the list of brokers that have the data, the topic name and the partitions list</p>'], 'explanation': '<p>All brokers can respond to a Metadata request, so a client can connect to any broker in the cluster and then figure out on its own which brokers to send data to.</p>\n', 'question': '<p>To produce data to a topic, a producer must provide the Kafka client with...</p>\n'}, 'correct_response': ['a'], 'section': 'Producer', 'question_plain': 'To produce data to a topic, a producer must provide the Kafka client with...', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534628, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', ''], 'question': '<p>Two consumers share the same group.id (consumer group id). Each consumer will</p>\n', 'relatedLectureIds': '', 'explanation': '<p>Each consumer is assigned a different partition of the topic to consume.</p>\n', 'answers': ['<p>Read all data from all partitions</p>', '<p>Read mutually exclusive offsets blocks on all the partitions</p>', '<p>Read all the data on mutual exclusive partitions</p>']}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'Two consumers share the same group.id (consumer group id). Each consumer will', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534630, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Using Future.get() to wait for a reply from Kafka will limit throughput.</p>\n', 'relatedLectureIds': '', 'question': '<p>What happens if you write the following code in your producer? producer.send(producerRecord).get()</p>\n', 'answers': ['<p>Compression will be increased</p>', '<p>Batching will be increased</p>', '<p>Throughput will be decreased</p>', '<p>It will force all brokers in Kafka to acknowledge the producerRecord</p>']}, 'correct_response': ['c'], 'section': 'Producer', 'question_plain': 'What happens if you write the following code in your producer? producer.send(producerRecord).get()', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534632, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'answers': ['<p>A newer client can talk to a newer broker, but an older client cannot talk to a newer broker</p>', "<p>A newer client can't talk to a newer broker, but an older client can talk to a newer broker</p>", '<p>A newer client can talk to a newer broker, and an older client can talk to a newer broker</p>', '<p>Clients and brokers must have the exact same version to be able to communicate</p>'], 'relatedLectureIds': '', 'question': "<p>What's is true about Kafka brokers and clients from version 0.10.2 onwards?</p>\n", 'explanation': '<p>Kafka\'s new bidirectional client compatibility introduced in 0.10.2 allows this. Read more here: <a href="https://www.confluent.io/blog/upgrading-apache-kafka-clients-just-got-easier/">https://www.confluent.io/blog/upgrading-apache-kafka-clients-just-got-easier/</a></p>\n'}, 'correct_response': ['c'], 'section': 'Clients', 'question_plain': "What's is true about Kafka brokers and clients from version 0.10.2 onwards?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20534634, 'assessment_type': 'multi-select', 'prompt': {'feedbacks': ['', '', '', '', ''], 'question': '<p>If I supply the setting compression.type=snappy to my producer, what will happen? (select two)</p>\n', 'relatedLectureIds': '', 'answers': ['<p>The Kafka brokers have to de-compress the data</p>', '<p>The Consumers have to de-compress the data</p>', '<p>The Kafka brokers have to compress the data</p>', '<p>The Producers have to compress the data</p>', '<p>The Consumers have to compress the data</p>'], 'explanation': '<p>Kafka transfers data with zero copy and no transformation. Any transformation (including compression) is the responsibility of clients.</p>\n'}, 'correct_response': ['b', 'd'], 'section': 'Producer', 'question_plain': 'If I supply the setting compression.type=snappy to my producer, what will happen? (select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534636, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'explanation': '<p>(Topic,Partition,Offset) uniquely identifies a message in Kafka</p>\n', 'feedbacks': ['', '', ''], 'answers': ['<p>topic + timestamp</p>', '<p>topic + partition + timestamp</p>', '<p>topic + partition + offset</p>'], 'question': '<p>What is a generic unique id that I can use for messages I receive from a consumer?</p>\n'}, 'correct_response': ['c'], 'section': 'Consumer / Consumer Groups', 'question_plain': 'What is a generic unique id that I can use for messages I receive from a consumer?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534638, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>Kafka Connect Source</p>', '<p>Kafka Connect Sink</p>', '<p>Kafka Streams</p>', '<p>Kafka Producer</p>'], 'question': '<p>To continuously export data from Kafka into a target database, I should use</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Kafka Connect Sink is used to export data from Kafka to external databases and Kafka Connect Source is used to import from external databases into Kafka.</p>\n'}, 'correct_response': ['b'], 'section': 'Kafka Connect', 'question_plain': 'To continuously export data from Kafka into a target database, I should use', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534640, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>What is the default port that the KSQL server listens on?</p>\n', 'answers': ['<p>2181</p>', '<p>9092</p>', '<p>8083</p>', '<p>8088</p>'], 'feedbacks': ['', '', '', ''], 'relatedLectureIds': '', 'explanation': '<p>Default port of KSQL server is 8088</p>\n'}, 'correct_response': ['d'], 'section': 'KSQL', 'question_plain': 'What is the default port that the KSQL server listens on?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534642, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Where are KSQL-related data and metadata stored?</p>\n', 'feedbacks': ['', '', '', ''], 'relatedLectureIds': '', 'answers': ['<p>Zookeeper</p>', '<p>Kafka Topics</p>', '<p>PostgreSQL database</p>', '<p>Schema Registry</p>'], 'explanation': '<p>metadata is stored in and built from the KSQL command topic. Each KSQL server has its own in-memory version of the metastore. </p>\n'}, 'correct_response': ['b'], 'section': 'KSQL', 'question_plain': 'Where are KSQL-related data and metadata stored?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534644, 'assessment_type': 'multiple-choice', 'prompt': {'explanation': '<p>One partition is assigned a thread, so only 5 will be active, and 25 threads (i.e. tasks) will be created</p>\n', 'feedbacks': ['', '', '', ''], 'question': '<p>Your streams application is reading from an input topic that has 5 partitions. You run 5 instances of your application, each with num.streams.threads set to 5. How many stream tasks will be created and how many will be active?</p>\n', 'relatedLectureIds': '', 'answers': ['<p>25 created, 25 active</p>', '<p>25 created, 5 active</p>', '<p>5 created, 5 active</p>', '<p>5 created, 1 active</p>']}, 'correct_response': ['b'], 'section': 'Kafka Streams', 'question_plain': 'Your streams application is reading from an input topic that has 5 partitions. You run 5 instances of your application, each with num.streams.threads set to 5. How many stream tasks will be created and how many will be active?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534646, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>What's a Kafka partition made of?</p>\n", 'answers': ['<p>One file</p>', '<p>One file and one index</p>', '<p>One file and two indexes</p>', '<p>One file and two indexes per segment</p>'], 'explanation': '<p>Kafka partitions are made of segments (usually each segment is 1GB), and each segment has two corresponding indexes (offset index and time index)</p>\n'}, 'correct_response': ['d'], 'section': 'Topic', 'question_plain': "What's a Kafka partition made of?", 'related_lectures': []}, {'_class': 'assessment', 'id': 20534648, 'assessment_type': 'multiple-choice', 'prompt': {'relatedLectureIds': '', 'question': '<p>Your manager would like to have topic availability over consistency. Which setting do you need to change in order to enable that?</p>\n', 'answers': ['<p>min.insync.replicas</p>', '<p>compression.type</p>', '<p>unclean.leader.election.enable</p>'], 'explanation': '<p>unclean.leader.election.enable=true allows non ISR replicas to become leader, ensuring availability but losing consistency as data loss will occur</p>\n', 'feedbacks': ['', '', '']}, 'correct_response': ['c'], 'section': 'Broker', 'question_plain': 'Your manager would like to have topic availability over consistency. Which setting do you need to change in order to enable that?', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534650, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>Which of the following Kafka Streams operators are stateless? (select all that apply)</p>\n', 'explanation': '<p>See: <a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#stateless-transformations">https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#stateless-transformations</a></p>\n', 'answers': ['<p>branch</p>', '<p>map</p>', '<p>flatmap</p>', '<p>aggregate</p>', '<p>filter</p>', '<p>groupBy</p>'], 'feedbacks': ['', '', '', '', '', ''], 'relatedLectureIds': ''}, 'correct_response': ['a', 'b', 'c', 'e', 'f'], 'section': 'Kafka Streams', 'question_plain': 'Which of the following Kafka Streams operators are stateless? (select all that apply)', 'related_lectures': []}, {'_class': 'assessment', 'id': 20534652, 'assessment_type': 'multiple-choice', 'prompt': {'answers': ['<p>SSL</p>', '<p>SASL/GSSAPI</p>', '<p>SASL/SCRAM</p>', '<p>SAML</p>'], 'explanation': '<p>Learn more about security here: <a href="https://kafka.apache.org/documentation/#security">https://kafka.apache.org/documentation/#security</a></p>\n', 'feedbacks': ['', '', '', ''], 'question': '<p>What is not a valid authentication mechanism in Kafka?</p>\n', 'relatedLectureIds': ''}, 'correct_response': ['d'], 'section': 'Security', 'question_plain': 'What is not a valid authentication mechanism in Kafka?', 'related_lectures': []}]}
