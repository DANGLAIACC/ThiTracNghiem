5731990
~~~
{"count": 45, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 66801918, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands can a data engineer use to compact small data files of a Delta table into larger ones ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake can improve the speed of read queries from a table. One way to improve this speed is by compacting small files into larger ones. You trigger compaction by running the <code>OPTIMIZE</code> command</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-optimize.html\">https://docs.databricks.com/sql/language-manual/delta-optimize.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>PARTITION BY</p>", "<p>ZORDER BY</p>", "<p>COMPACT</p>", "<p>VACUUM</p>", "<p>OPTIMIZE</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following commands can a data engineer use to compact small data files of a Delta table into larger ones ?", "related_lectures": []}, {"_class": "assessment", "id": 66801920, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is trying to use Delta time travel to rollback a table to a previous version, but the data engineer received an error that the data files are no longer present.</p><p><br></p><p>Which of the following commands was run on the table that caused deleting the data files?</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Running the VACUUM command on a Delta table deletes the unused data files older than a specified data retention period. As a result, you lose the ability to time travel back to any version older than that retention threshold.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-vacuum.html\">https://docs.databricks.com/sql/language-manual/delta-vacuum.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture</a> </p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on </a></p></li></ul><p><br></p>", "answers": ["<p>VACUUM</p>", "<p>OPTIMIZE</p>", "<p>ZORDER BY</p>", "<p>DEEP CLONE</p>", "<p>DELETE</p>"]}, "correct_response": ["a"], "section": "Databricks Lakehouse Platform", "question_plain": "A data engineer is trying to use Delta time travel to rollback a table to a previous version, but the data engineer received an error that the data files are no longer present.Which of the following commands was run on the table that caused deleting the data files?", "related_lectures": []}, {"_class": "assessment", "id": 66801922, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Delta Lake tables, which of the following is the primary format for the data files?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake builds upon standard data formats. Delta lake table gets stored on the storage in one or more data files in Parquet format, along with transaction logs in JSON format.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/index.html\">https://docs.databricks.com/delta/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664674/?referralCode=F0FA48E9A0546C975F14\">Lecture</a> </p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665592/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Delta</p>", "<p>Parquet</p>", "<p>JSON</p>", "<p>Hive-specific format</p>", "<p>Both, Parquet and JSON</p>"]}, "correct_response": ["b"], "section": "Databricks Lakehouse Platform", "question_plain": "In Delta Lake tables, which of the following is the primary format for the data files?", "related_lectures": []}, {"_class": "assessment", "id": 66801924, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following locations hosts the Databricks web application ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>According to the Databricks Lakehouse architecture, Databricks workspace is deployed in the control plane along with Databricks services like Databricks web application (UI), Cluster manager, workflow service, and notebooks.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/getting-started/overview.html\">https://docs.databricks.com/getting-started/overview.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664668/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li></ul><p><br></p>", "answers": ["<p>Data plane</p>", "<p>Control plane</p>", "<p>Databricks Filesystem</p>", "<p>Databricks-managed cluster</p>", "<p>Customer Cloud Account</p>"]}, "correct_response": ["b"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following locations hosts the Databricks web application ?", "related_lectures": []}, {"_class": "assessment", "id": 66801926, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Databricks Repos, which of the following operations a data engineer can use to update the local version of a repo from its remote Git repository ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The git Pull operation is used to fetch and download content from a remote repository and immediately update the local repository to match that content.</p><p><br></p><p>References:</p><ul><li><p><a href=\"https://docs.databricks.com/repos/index.html\">https://docs.databricks.com/repos/index.html</a></p></li><li><p><a href=\"https://github.com/git-guides/git-pull\">https://github.com/git-guides/git-pull</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34844752/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Clone</p>", "<p>Commit</p>", "<p>Merge</p>", "<p>Push</p>", "<p>Pull</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "In Databricks Repos, which of the following operations a data engineer can use to update the local version of a repo from its remote Git repository ?", "related_lectures": []}, {"_class": "assessment", "id": 66801928, "assessment_type": "multiple-choice", "prompt": {"question": "<p>According to the Databricks Lakehouse architecture, which of the following is located in the customer's cloud account?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>When the customer sets up a Spark cluster, the cluster virtual machines are deployed in the data plane in the customer's cloud account.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/getting-started/overview.html\">https://docs.databricks.com/getting-started/overview.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664668/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li></ul>", "answers": ["<p>Databricks web application</p>", "<p>Notebooks</p>", "<p>Repos</p>", "<p>Cluster virtual machines</p>", "<p>Workflows</p>"]}, "correct_response": ["d"], "section": "Databricks Lakehouse Platform", "question_plain": "According to the Databricks Lakehouse architecture, which of the following is located in the customer's cloud account?", "related_lectures": []}, {"_class": "assessment", "id": 66801930, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following best describes Databricks Lakehouse?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Lakehouse is a unified analytics platform that combines the best elements of data lakes and data warehouses. So, in the Lakehouse, you can work on data engineering, analytics, and AI, all in one platform.</p><p><br></p><p>Reference: <a href=\"https://www.databricks.com/glossary/data-lakehouse\">https://www.databricks.com/glossary/data-lakehouse</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664668/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li></ul>", "answers": ["<p>Single, flexible, high-performance system that supports data, analytics, and machine learning workloads.</p>", "<p>Reliable data management system with transactional guarantees for organization\u2019s structured data.</p>", "<p>Platform that helps reduce the costs of storing organization\u2019s open-format data files in the cloud.</p>", "<p>Platform for developing increasingly complex machine learning workloads using a simple, SQL-based solution.</p>", "<p>Platform that scales data lake workloads for organizations without investing on-premises hardware.</p>"]}, "correct_response": ["a"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following best describes Databricks Lakehouse?", "related_lectures": []}, {"_class": "assessment", "id": 66801932, "assessment_type": "multiple-choice", "prompt": {"question": "<p>If \u200b\u200bthe default notebook language is SQL, which of the following options a data engineer can use to run a Python code in this SQL Notebook ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, cells use the default language of the notebook. You can override the default language in a cell by using the language magic command at the beginning of a cell. The supported magic commands are: <code>%python</code>, <code>%sql</code>, <code>%scala</code>, and <code>%r</code>.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/notebooks/notebooks-code.html\">https://docs.databricks.com/notebooks/notebooks-code.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>They need first to import the python module in a cell</p>", "<p>This is not possible! They need to change the default language of the notebook to Python</p>", "<p>Databricks detects cells language automatically, so they can write Python syntax in any cell</p>", "<p>They can add <code>%language</code> magic command at the start of a cell to force language detection.</p>", "<p>They can add <code>%python</code> at the start of a cell.</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "If \u200b\u200bthe default notebook language is SQL, which of the following options a data engineer can use to run a Python code in this SQL Notebook ?", "related_lectures": []}, {"_class": "assessment", "id": 66801934, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following tasks is not supported by Databricks Repos, and must be performed in your Git provider ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The following tasks are not supported by Databricks Repos, and must be performed in your Git provider:</p><ul><li><p>Create a pull request</p></li><li><p>Delete branches</p></li><li><p>Merge and rebase branches <strong>*</strong></p></li></ul><p><br></p><p><strong>* NOTE:</strong> Recently, merge and rebase branches have become supported in Databricks Repos. However, this may still not be updated in the current exam version.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-29_23-30-13-596b4c38180bcdc4c82aa93eab565273.png\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/repos/index.html\">https://docs.databricks.com/repos/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34844752/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>Clone, push to, or pull from a remote Git repository.</p>", "<p>Create and manage branches for development work.</p>", "<p>Create notebooks, and edit notebooks and other files.</p>", "<p>Visually compare differences upon commit.</p>", "<p>Delete branches</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following tasks is not supported by Databricks Repos, and must be performed in your Git provider ?", "related_lectures": []}, {"_class": "assessment", "id": 66801936, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements is <strong>Not</strong> true about Delta Lake ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>It is not true that Delta Lake builds upon XML format. It builds upon Parquet and JSON formats</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/index.html\">https://docs.databricks.com/delta/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664674/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665592/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Delta Lake provides ACID transaction guarantees</p>", "<p>Delta Lake provides scalable data and metadata handling</p>", "<p>Delta Lake provides audit history and time travel</p>", "<p>Delta Lake builds upon standard data formats: Parquet + XML</p>", "<p>Delta Lake supports unified streaming and batch data processing</p>"]}, "correct_response": ["d"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following statements is Not true about Delta Lake ?", "related_lectures": []}, {"_class": "assessment", "id": 66801938, "assessment_type": "multiple-choice", "prompt": {"question": "<p>How long is the default retention period of the VACUUM command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, the retention threshold of the VACUUM command is 7 days. This means that VACUUM operation will prevent you from deleting files less than 7 days old, just to ensure that no long-running operations are still referencing any of the files to be deleted.<br><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-vacuum.html\">https://docs.databricks.com/sql/language-manual/delta-vacuum.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>0 days</p>", "<p>7 days</p>", "<p>30 days</p>", "<p>90 days</p>", "<p>365 days</p>"]}, "correct_response": ["b"], "section": "Databricks Lakehouse Platform", "question_plain": "How long is the default retention period of the VACUUM command ?", "related_lectures": []}, {"_class": "assessment", "id": 66801916, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta table called <strong>employees</strong> that contains the employees personal information including their gross salaries.</p><p><br></p><p>Which of the following code blocks will keep in the table only the employees having a salary greater than 3000 ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In order to keep only the employees having a salary greater than 3000, we must delete the employees having salary less than or equal 3000. To do so, use the DELETE statement:</p><p><code>DELETE FROM table_name WHERE condition;</code></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-delete-from.html\">https://docs.databricks.com/sql/language-manual/delta-delete-from.html</a></p>", "answers": ["<p>DELETE FROM employees WHERE salary &gt; 3000; </p>", "<p>SELECT CASE WHEN salary &lt;= 3000 THEN DELETE ELSE UPDATE END FROM employees; </p>", "<p>UPDATE employees WHERE salary &gt; 3000 WHEN MATCHED SELECT; </p>", "<p>UPDATE employees WHERE salary &lt;= 3000 WHEN MATCHED DELETE; </p>", "<p>DELETE FROM employees WHERE salary &lt;= 3000; </p>"]}, "correct_response": ["e"], "section": "ELT with Spark SQL and Python", "question_plain": "The data engineering team has a Delta table called employees that contains the employees personal information including their gross salaries.Which of the following code blocks will keep in the table only the employees having a salary greater than 3000 ?", "related_lectures": []}, {"_class": "assessment", "id": 66801940, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to create a relational object by pulling data from two tables. The relational object must be used by other data engineers in other sessions on the same cluster only. In order to save on storage costs, the date engineer wants to avoid copying and storing physical data.</p><p><br></p><p>Which of the following relational objects should the data engineer create?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In order to avoid copying and storing physical data, the data engineer must create a view object. A view in databricks is a virtual table that has no physical data. It\u2019s just a saved SQL query against actual tables.</p><p>The view type should be Global Temporary view that can be accessed in other sessions on the same cluster. Global Temporary views are tied to a cluster temporary database called global_temp.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672154/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672162/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Temporary view</p>", "<p>External table</p>", "<p>Managed table</p>", "<p>Global Temporary view</p>", "<p>View</p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "A data engineer wants to create a relational object by pulling data from two tables. The relational object must be used by other data engineers in other sessions on the same cluster only. In order to save on storage costs, the date engineer wants to avoid copying and storing physical data.Which of the following relational objects should the data engineer create?", "related_lectures": []}, {"_class": "assessment", "id": 66801942, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has developed a code block to completely reprocess data based on the following if-condition in Python:</p><p><br></p><pre class=\"prettyprint linenums\">if process_mode = \"init\" and not is_table_exist:\n   print(\"Start processing ...\")</pre><p><br></p><p>This if-condition is returning an invalid syntax error.</p><p><br></p><p>Which of the following changes should be made to the code block to fix this error ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Python if statement looks like this in its simplest form:</p><pre class=\"prettyprint linenums\">if &lt;expr&gt;:\n    &lt;statement&gt;</pre><p><br></p><p>Python supports the usual logical conditions from mathematics:</p><ul><li><p>Equals: <code>a == b</code></p></li><li><p>Not Equals: <code>a != b</code></p></li><li><p><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code></p></li></ul><p><br></p><p>To combine conditional statements, you can use the following logical operators:</p><ul><li><p><code>and</code></p></li><li><p><code>or</code></p></li></ul><p>The negation operator in Python is: <code>not</code></p><p><br></p><p>Reference: <a href=\"https://www.w3schools.com/python/python_conditions.asp\">https://www.w3schools.com/python/python_conditions.asp</a></p>", "answers": ["<pre class=\"prettyprint linenums\">if process_mode = \"init\" &amp; not is_table_exist:\n    print(\"Start processing ...\")</pre>", "<pre class=\"prettyprint linenums\">if process_mode = \"init\" and not is_table_exist = True:\n    print(\"Start processing ...\")</pre>", "<pre class=\"prettyprint linenums\">if process_mode = \"init\" and is_table_exist = False:\n    print(\"Start processing ...\")</pre>", "<pre class=\"prettyprint linenums\">if (process_mode = \"init\") and (not is_table_exist):\n    print(\"Start processing ...\")</pre>", "<pre class=\"prettyprint linenums\">if process_mode == \"init\" and not is_table_exist:\n    print(\"Start processing ...\")</pre>"]}, "correct_response": ["e"], "section": "ELT with Spark SQL and Python", "question_plain": "A data engineer has developed a code block to completely reprocess data based on the following if-condition in Python:if process_mode = \"init\" and not is_table_exist:\n   print(\"Start processing ...\")This if-condition is returning an invalid syntax error.Which of the following changes should be made to the code block to fix this error ?", "related_lectures": []}, {"_class": "assessment", "id": 66801944, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Fill in the below blank to successfully create a table in Databricks using data from an existing PostgreSQL database:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE employees\n  USING ____________\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"employees\"\n  )</pre>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Using the JDBC library, Spark SQL can extract data from any existing relational database that supports JDBC. Examples include mysql, postgres, SQLite, and more.</p><p><br></p><p>Reference: <a href=\"https://learn.microsoft.com/en-us/azure/databricks/external-data/jdbc\">https://learn.microsoft.com/en-us/azure/databricks/external-data/jdbc</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34673300/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p>", "answers": ["<p>org.apache.spark.sql.jdbc </p>", "<p>postgresql </p>", "<p>DELTA </p>", "<p>dbserver </p>", "<p>cloudfiles </p>"]}, "correct_response": ["a"], "section": "ELT with Spark SQL and Python", "question_plain": "Fill in the below blank to successfully create a table in Databricks using data from an existing PostgreSQL database:CREATE TABLE employees\n  USING ____________\n  OPTIONS (\n    url \"jdbc:postgresql:dbserver\",\n    dbtable \"employees\"\n  )", "related_lectures": []}, {"_class": "assessment", "id": 66801946, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands can a data engineer use to create a new table along with a comment ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The <code>CREATE TABLE</code> clause supports adding a descriptive comment for the table. This allows for easier discovery of table contents.</p><p><br></p><p>Syntax:</p><pre class=\"prettyprint linenums\">CREATE TABLE table_name\nCOMMENT \"here is a comment\"\nAS query</pre><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p>", "answers": ["<pre class=\"prettyprint linenums\">CREATE TABLE payments\nCOMMENT \"This table contains sensitive information\"\nAS SELECT * FROM bank_transactions</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE payments\nCOMMENT(\"This table contains sensitive information\")\nAS SELECT * FROM bank_transactions</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE payments\nAS SELECT * FROM bank_transactions\nCOMMENT \"This table contains sensitive information\"</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE payments\nAS SELECT * FROM bank_transactions\nCOMMENT(\"This table contains sensitive information\")</pre>", "<pre class=\"prettyprint linenums\">COMMENT(\"This table contains sensitive information\")\nCREATE TABLE payments\nAS SELECT * FROM bank_transactions</pre>"]}, "correct_response": ["a"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following commands can a data engineer use to create a new table along with a comment ?", "related_lectures": []}, {"_class": "assessment", "id": 66801948, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer usually uses <code>INSERT INTO</code> command to write data into a Delta table. A senior data engineer suggested using another command that avoids writing of duplicate records.</p><p><br></p><p>Which of the following commands is the one suggested by the senior data engineer ?</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>MERGE INTO allows to merge a set of updates, insertions, and deletions based on a source table into a target Delta table. With MERGE INTO, you can avoid inserting the duplicate records when writing into Delta tables.</p><p><br></p><p>References:</p><ul><li><p><a href=\"https://docs.databricks.com/sql/language-manual/delta-merge-into.html\">https://docs.databricks.com/sql/language-manual/delta-merge-into.html</a></p></li><li><p><a href=\"https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables\">https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664814/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>MERGE INTO </p>", "<p>APPLY CHANGES INTO </p>", "<p>UPDATE </p>", "<p>COPY INTO </p>", "<p>INSERT OR OVERWRITE </p>"]}, "correct_response": ["a"], "section": "ELT with Spark SQL and Python", "question_plain": "A junior data engineer usually uses INSERT INTO command to write data into a Delta table. A senior data engineer suggested using another command that avoids writing of duplicate records.Which of the following commands is the one suggested by the senior data engineer ?", "related_lectures": []}, {"_class": "assessment", "id": 66801950, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer is designing a Delta Live Tables pipeline. The source system generates files containing changes captured in the source data. Each change event has metadata indicating whether the specified record was inserted, updated, or deleted. In addition to a timestamp column indicating the order in which the changes happened. The data engineer needs to update a target table based on these change events.</p><p><br></p><p>Which of the following commands can the data engineer use to best solve this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The events described in the question represent Change Data Capture (CDC) feed. CDC is logged at the source as events that contain both the data of the records along with metadata information:</p><ol><li><p>Operation column indicating whether the specified record was inserted, updated, or deleted</p></li><li><p>Sequence column that is usually a timestamp indicating the order in which the changes happened</p></li></ol><p>You can use the <code>APPLY CHANGES INTO</code> statement to use Delta Live Tables CDC functionality</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34886032/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34903750/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>MERGE INTO </p>", "<p>APPLY CHANGES INTO </p>", "<p>UPDATE </p>", "<p>COPY INTO </p>", "<p>cloud_files </p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "A data engineer is designing a Delta Live Tables pipeline. The source system generates files containing changes captured in the source data. Each change event has metadata indicating whether the specified record was inserted, updated, or deleted. In addition to a timestamp column indicating the order in which the changes happened. The data engineer needs to update a target table based on these change events.Which of the following commands can the data engineer use to best solve this problem?", "related_lectures": []}, {"_class": "assessment", "id": 66801952, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In PySpark, which of the following commands can you use to query the Delta table <strong>employees</strong> created in Spark SQL?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>spark.table()</code> function returns the specified Spark SQL table as a PySpark DataFrame</p><p><br></p><p>Reference:</p><p><a href=\"https://spark.apache.org/docs/2.4.0/api/python/_modules/pyspark/sql/session.html#SparkSession.table\">https://spark.apache.org/docs/2.4.0/api/python/_modules/pyspark/sql/session.html#SparkSession.table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>pyspark.sql.read(SELECT * FROM employees) </p>", "<p>spark.sql(\"employees\") </p>", "<p>spark.format(\u201csql\u201d).read(\"employees\") </p>", "<p>spark.table(\"employees\") </p>", "<p>Spark SQL tables can not be accessed from PySpark </p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "In PySpark, which of the following commands can you use to query the Delta table employees created in Spark SQL?", "related_lectures": []}, {"_class": "assessment", "id": 66801954, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following code blocks can a data engineer use to create a user defined function (UDF) ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The correct syntax to create a UDF is:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE [OR REPLACE] FUNCTION function_name ( [ parameter_name data_type [, ...] ] )\nRETURNS data_type\nRETURN { expression | query }</pre><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/udf/index.html\">https://docs.databricks.com/udf/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34823118/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>CREATE FUNCTION plus_one(value INTEGER)</p><p>RETURN value +1</p>", "<p>CREATE UDF plus_one(value INTEGER)</p><p>RETURNS INTEGER</p><p>RETURN value +1;</p>", "<p>CREATE UDF plus_one(value INTEGER)</p><p>RETURN value +1;</p>", "<p>CREATE FUNCTION plus_one(value INTEGER)</p><p>RETURNS INTEGER</p><p>RETURN value +1;</p>", "<p>CREATE FUNCTION plus_one(value INTEGER)</p><p>RETURNS INTEGER</p><p>value +1;</p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following code blocks can a data engineer use to create a user defined function (UDF) ?", "related_lectures": []}, {"_class": "assessment", "id": 66801956, "assessment_type": "multiple-choice", "prompt": {"question": "<p>When dropping a Delta table, which of the following explains why only the table's metadata will be deleted, while the data files will be kept in the storage ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>External (unmanaged) tables are tables whose data is stored in an external storage path by using a <code>LOCATION</code> clause.</p><p>When you run <code>DROP TABLE</code> on an external table, only the table's metadata is deleted, while the underlying data files are kept.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table\">https://docs.databricks.com/lakehouse/data-objects.html#what-is-an-unmanaged-table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>The table is deep cloned</p>", "<p>The table is external</p>", "<p>The user running the command has no permission to delete the data files</p>", "<p>The table is managed</p>", "<p>Delta prevents deleting files less than retention threshold, just to ensure that no long-running operations are still referencing any of the files to be deleted</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "When dropping a Delta table, which of the following explains why only the table's metadata will be deleted, while the data files will be kept in the storage ?", "related_lectures": []}, {"_class": "assessment", "id": 66801958, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the two tables <strong>students_course_1</strong> and <strong>students_course_2</strong>. Which of the following commands can a data engineer use to get all the students from the above two tables without duplicate records ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With <code>UNION</code>, you can return the result of subquery1 plus the rows of subquery2</p><p><br></p><p>Syntax:</p><pre class=\"prettyprint linenums\">subquery1\nUNION [ ALL | DISTINCT ]\nsubquery2</pre><p><br></p><ul><li><p>If <code>ALL</code> is specified duplicate rows are preserved.</p></li><li><p>If <code>DISTINCT</code> is specified the result does not contain any duplicate rows. This is the default.</p></li></ul><p><br></p><p>Note that both subqueries must have the same number of columns and share a least common type for each respective column.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-setops.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-setops.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34673444/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<pre class=\"prettyprint linenums\">SELECT * FROM students_course_1\nCROSS JOIN\nSELECT * FROM students_course_2</pre>", "<pre class=\"prettyprint linenums\">SELECT * FROM students_course_1\nUNION\nSELECT * FROM students_course_2</pre>", "<pre class=\"prettyprint linenums\">SELECT * FROM students_course_1\nINTERSECT\nSELECT * FROM students_course_2</pre>", "<pre class=\"prettyprint linenums\">SELECT * FROM students_course_1\nOUTER JOIN\nSELECT * FROM students_course_2</pre>", "<pre class=\"prettyprint linenums\">SELECT * FROM students_course_1\nINNER JOIN\nSELECT * FROM students_course_2</pre>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Given the two tables students_course_1 and students_course_2. Which of the following commands can a data engineer use to get all the students from the above two tables without duplicate records ?", "related_lectures": []}, {"_class": "assessment", "id": 66801960, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following command:</p><p><br></p><p><code>CREATE DATABASE IF NOT EXISTS hr_db ;</code></p><p><br></p><p>In which of the following locations will the <strong>hr_db</strong> database be located?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Since we are creating the database here without specifying a <code>LOCATION</code> clause, the database will be created in the default warehouse directory under dbfs:/user/hive/warehouse</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>dbfs:/user/hive/warehouse </p>", "<p>dbfs:/user/hive/db_hr </p>", "<p>dbfs:/user/hive/databases/db_hr.db </p>", "<p>dbfs:/user/hive/databases </p>", "<p>dbfs:/user/hive </p>"]}, "correct_response": ["a"], "section": "ELT with Spark SQL and Python", "question_plain": "Given the following command:CREATE DATABASE IF NOT EXISTS hr_db ;In which of the following locations will the hr_db database be located?", "related_lectures": []}, {"_class": "assessment", "id": 66801962, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Fill in the below blank to get the students enrolled in less than 3 courses from array column <strong>students</strong></p><p><br></p><pre class=\"prettyprint linenums\">SELECT\n  faculty_id,\n  students,\n  ___________ AS few_courses_students\nFROM faculties</pre><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>filter(input_array, lamda_function)</code> is a higher order function that returns an output array from an input array by extracting elements for which the predicate of a lambda function holds.</p><p><br></p><p><strong>Example:</strong></p><p>Extracting odd numbers from an input array of integers:</p><p><code>SELECT filter(array(1, 2, 3, 4), i -&gt; i % 2 == 1);</code></p><p>output: [1, 3]</p><p><br></p><p><strong>References:</strong></p><ul><li><p><a href=\"https://docs.databricks.com/sql/language-manual/functions/filter.html\">https://docs.databricks.com/sql/language-manual/functions/filter.html</a></p></li><li><p><a href=\"https://docs.databricks.com/optimizations/higher-order-lambda-functions.html\">https://docs.databricks.com/optimizations/higher-order-lambda-functions.html</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34823118/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>TRANSFORM (students, total_courses &lt; 3) </p>", "<p>TRANSFORM (students, i -&gt; i.total_courses &lt; 3) </p>", "<p>FILTER (students, total_courses &lt; 3) </p>", "<p>FILTER (students, i -&gt; i.total_courses &lt; 3) </p>", "<p>CASE WHEN students.total_courses &lt; 3 THEN students</p><p>ELSE NULL</p><p>END</p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "Fill in the below blank to get the students enrolled in less than 3 courses from array column studentsSELECT\n  faculty_id,\n  students,\n  ___________ AS few_courses_students\nFROM faculties", "related_lectures": []}, {"_class": "assessment", "id": 66801964, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n         .______________ \n        .table(\"new_orders\")\n)</pre><p><br></p><p>Fill in the blank to make the query executes a micro-batch to process data every 2 minutes</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Spark Structured Streaming, in order to process data in micro-batches at the user-specified intervals, you can use <code>processingTime</code> keyword. It allows to specify a time duration as a string.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/triggers.html#configure-structured-streaming-trigger-intervals\">https://docs.databricks.com/structured-streaming/triggers.html#configure-structured-streaming-trigger-intervals</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>trigger(once=\u201d2 minutes\u201d)&nbsp; </p>", "<p>trigger(processingTime=\u201d2 minutes\") </p>", "<p>processingTime(\u201d2 minutes\")&nbsp; </p>", "<p>trigger(\u201d2 minutes\") </p>", "<p>trigger() </p>"]}, "correct_response": ["b"], "section": "Incremental Data Processing", "question_plain": "Given the following Structured Streaming query:(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n         .______________ \n        .table(\"new_orders\")\n)Fill in the blank to make the query executes a micro-batch to process data every 2 minutes", "related_lectures": []}, {"_class": "assessment", "id": 66801966, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is used by Auto Loader to load data incrementally?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Loader is based on Spark Structured Streaming. It provides a Structured Streaming source called cloudFiles.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html\">https://docs.databricks.com/ingestion/auto-loader/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>DEEP CLONE </p>", "<p>Multi-hop architecture</p>", "<p>COPY INTO </p>", "<p>Spark Structured Streaming</p>", "<p>Databricks SQL</p>"]}, "correct_response": ["d"], "section": "Incremental Data Processing", "question_plain": "Which of the following is used by Auto Loader to load data incrementally?", "related_lectures": []}, {"_class": "assessment", "id": 66801968, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes Auto Loader ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html\">https://docs.databricks.com/ingestion/auto-loader/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Auto loader allows applying Change Data Capture (CDC) feed to update tables based on changes captured in source data.</p>", "<p>Auto loader monitors a source location, in which files accumulate, to identify and ingest only new arriving files with each command run. While the files that have already been ingested in previous runs are skipped.</p>", "<p>Auto loader allows cloning a source Delta table to a target destination at a specific version.</p>", "<p>Auto loader defines data quality expectations on the contents of a dataset, and reports the records that violate these expectations in metrics.</p>", "<p>Auto loader enables efficient insert, update, deletes, and rollback capabilities by adding a storage layer that provides better data reliability to data lakes.</p>"]}, "correct_response": ["b"], "section": "Incremental Data Processing", "question_plain": "Which of the following statements best describes Auto Loader ?", "related_lectures": []}, {"_class": "assessment", "id": 66801984, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:</p><p><br></p><p><code>CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ </code></p><p><br></p><p>Fill in the above blank so records violating this constraint will be added to the target table, and reported in metrics</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, records that violate the expectation are added to the target dataset along with valid records, but violations will be reported in the event log</p><p><br></p><p>Reference:</p><p><a href=\"https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations\">https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>ON VIOLATION ADD ROW</p>", "<p>ON VIOLATION FAIL UPDATE</p>", "<p>ON VIOLATION SUCCESS UPDATE</p>", "<p>ON VIOLATION NULL</p>", "<p>There is no need to add ON VIOLATION clause. By default, records violating the constraint will be kept, and reported as invalid in the event log </p>"]}, "correct_response": ["e"], "section": "Incremental Data Processing", "question_plain": "A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ Fill in the above blank so records violating this constraint will be added to the target table, and reported in metrics", "related_lectures": []}, {"_class": "assessment", "id": 66801982, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineer team has a DLT pipeline that updates all the tables once and then stops. The compute resources of the pipeline continue running to allow for quick testing.</p><p><br></p><p>Which of the following best describes the execution modes of this DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Triggered pipelines update each table with whatever data is currently available and then they shut down.</p><p><br></p><p>In Development mode, the Delta Live Tables system ease the development process by</p><ul><li><p>Reusing a cluster to avoid the overhead of restarts. The cluster runs for two hours when development mode is enabled.</p></li><li><p>Disabling pipeline retries so you can immediately detect and fix errors.</p></li></ul><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The DLT pipeline executes in Continuous Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Continuous Pipeline mode under Development mode.</p><p><br></p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Development mode.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["d"], "section": "Incremental Data Processing", "question_plain": "The data engineer team has a DLT pipeline that updates all the tables once and then stops. The compute resources of the pipeline continue running to allow for quick testing.Which of the following best describes the execution modes of this DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 66801972, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following will utilize Gold tables as their source?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Gold tables provide business level aggregates often used for reporting and dashboarding, or even for Machine learning</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742310/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li></ul>", "answers": ["<p>Silver tables </p>", "<p>Auto loader</p>", "<p>Bronze tables</p>", "<p>Dashboards</p>", "<p>Streaming jobs</p>"]}, "correct_response": ["d"], "section": "Incremental Data Processing", "question_plain": "Which of the following will utilize Gold tables as their source?", "related_lectures": []}, {"_class": "assessment", "id": 66801974, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following code blocks can a data engineer use to query the existing streaming table <strong>events</strong> ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake is deeply integrated with Spark Structured Streaming. You can load tables as a stream using:</p><p><code>spark.readStream.table(table_name)</code></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/delta-lake.html\">https://docs.databricks.com/structured-streaming/delta-lake.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>spark.readStream(\"events\") </p>", "<p>spark.read</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .table(\"events\")</p>", "<p>spark.readStream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .table(\"events\")</p>", "<p>spark.readStream()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .table(\"events\")</p>", "<p>spark.stream</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .read(\"events\")</p>"]}, "correct_response": ["c"], "section": "Incremental Data Processing", "question_plain": "Which of the following code blocks can a data engineer use to query the existing streaming table events ?", "related_lectures": []}, {"_class": "assessment", "id": 66801976, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In multi-hop architecture, which of the following statements best describes the Bronze layer ? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Bronze tables contain data in its rawest format ingested from various sources (e.g., JSON files, Operational Databaes, Kakfa stream, ...)</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742310/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>It maintains data that powers analytics, machine learning, and production applications</p>", "<p>It maintains raw data ingested from various sources</p>", "<p>It represents a filtered, cleaned, and enriched version of data</p>", "<p>It provides business-level aggregated version of data</p>", "<p>It provides a more refined view of the data. </p>"]}, "correct_response": ["b"], "section": "Incremental Data Processing", "question_plain": "In multi-hop architecture, which of the following statements best describes the Bronze layer ?", "related_lectures": []}, {"_class": "assessment", "id": 66801978, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query</p><p><br></p><pre class=\"prettyprint linenums\">(spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(ordersLocation)\n     .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"uncleanedOrders\")\n)</pre><p><br></p><p>Which of the following best describe the purpose of this query in a multi-hop architecture?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The query here is using Autoloader (<strong>cloudFiles</strong>) to load raw <strong>json</strong> data from <strong>ordersLocation</strong> into the Bronze table <strong>uncleanedOrders</strong></p><p><br></p><p>References:</p><ul><li><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p></li><li><p> <a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html\">https://docs.databricks.com/ingestion/auto-loader/index.html</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The query is performing raw data ingestion into a Bronze table</p>", "<p>The query is performing a hop from a Bronze table to a Silver table</p>", "<p>The query is performing a hop from Silver table to a Gold table</p>", "<p>The query is performing data transfer from a Gold table into a production application </p>", "<p>This query is performing data quality controls prior to Silver layer</p>"]}, "correct_response": ["a"], "section": "Incremental Data Processing", "question_plain": "Given the following Structured Streaming query(spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(ordersLocation)\n     .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"uncleanedOrders\")\n)Which of the following best describe the purpose of this query in a multi-hop architecture?", "related_lectures": []}, {"_class": "assessment", "id": 66801980, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has the following query in a Delta Live Tables pipeline:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE LIVE TABLE aggregated_sales\nAS\n  SELECT store_id, sum(total)\n  FROM cleaned_sales\n  GROUP BY store_id</pre><p><br></p><p>The pipeline is failing to start due to an error in this query</p><p><br></p><p>Which of the following changes should be made to this query to successfully start the DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In DLT pipelines, we use the <code><strong>CREATE LIVE TABLE</strong></code> syntax to create a table with SQL. To query another live table, prepend the <code><strong>LIVE.</strong></code> keyword to the table name.</p><p><br></p><p><strong>CREATE LIVE TABLE</strong> aggregated_sales</p><p>AS</p><p>SELECT store_id, sum(total)</p><p>FROM <strong>LIVE.</strong>cleaned_sales</p><p>GROUP BY store_id</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-sql-ref.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-sql-ref.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<pre class=\"prettyprint linenums\">CREATE STREAMING TABLE aggregated_sales\nAS\n&nbsp; SELECT store_id, sum(total)\n&nbsp; FROM LIVE.cleaned_sales\n&nbsp; GROUP BY store_id</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE aggregated_sales\nAS\n&nbsp; SELECT store_id, sum(total)\n&nbsp; FROM LIVE.cleaned_sales\n&nbsp; GROUP BY store_id</pre>", "<pre class=\"prettyprint linenums\">CREATE LIVE TABLE aggregated_sales\nAS\n&nbsp; SELECT store_id, sum(total)\n&nbsp; FROM LIVE.cleaned_sales\n&nbsp; GROUP BY store_id</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE aggregated_sales\nAS\n&nbsp; SELECT store_id, sum(total)\n&nbsp; FROM cleaned_sales\n&nbsp; GROUP BY store_id</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE aggregated_sales\nAS\n&nbsp; SELECT store_id, sum(total)\n&nbsp; FROM STREAM(cleaned_sales)\n&nbsp; GROUP BY store_id</pre>"]}, "correct_response": ["c"], "section": "Incremental Data Processing", "question_plain": "A data engineer has the following query in a Delta Live Tables pipeline:CREATE LIVE TABLE aggregated_sales\nAS\n  SELECT store_id, sum(total)\n  FROM cleaned_sales\n  GROUP BY store_idThe pipeline is failing to start due to an error in this queryWhich of the following changes should be made to this query to successfully start the DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 66801970, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:</p><p><br></p><p><code>CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ </code></p><p><br></p><p>Fill in the above blank so records violating this constraint will be dropped, and reported in metrics</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With <code>ON VIOLATION DROP ROW</code>, records that violate the expectation are dropped, and violations are reported in the event log</p><p><br></p><p>Reference:</p><p><a href=\"https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations\">https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>ON VIOLATION DROP ROW </p>", "<p>ON VIOLATION FAIL UPDATE </p>", "<p>ON VIOLATION DELETE ROW </p>", "<p>ON VIOLATION DISCARD ROW </p>", "<p>There is no need to add ON VIOLATION clause. By default, records violating the constraint will be discarded, and reported as invalid in the event log</p>"]}, "correct_response": ["a"], "section": "Incremental Data Processing", "question_plain": "A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ Fill in the above blank so records violating this constraint will be dropped, and reported in metrics", "related_lectures": []}, {"_class": "assessment", "id": 66801994, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following compute resources is available in Databricks SQL ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Compute resources are infrastructure resources that provide processing capabilities in the cloud. A SQL warehouse is a compute resource that lets you run SQL commands on data objects within Databricks SQL. </p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/admin/sql-endpoints.html\">https://docs.databricks.com/sql/admin/sql-endpoints.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>Single-node clusters</p>", "<p>Multi-nodes clusters</p>", "<p>On-premises clusters</p>", "<p>SQL warehouses</p>", "<p>SQL engines</p>"]}, "correct_response": ["d"], "section": "Production Pipelines", "question_plain": "Which of the following compute resources is available in Databricks SQL ?", "related_lectures": []}, {"_class": "assessment", "id": 66801986, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following is the benefit of using the Auto Stop feature of Databricks SQL warehouses ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The Auto Stop feature stops the warehouse if it\u2019s idle for a specified number of minutes.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/admin/sql-endpoints.html\">https://docs.databricks.com/sql/admin/sql-endpoints.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Improves the performance of the warehouse by automatically stopping ideal services</p>", "<p>Minimizes the total running time of the warehouse</p>", "<p>Provides higher security by automatically stopping unused ports of the warehouse</p>", "<p>Increases the availability of the warehouse by automatically stopping long-running SQL queries</p>", "<p>Databricks SQL does not have Auto Stop feature</p>"]}, "correct_response": ["b"], "section": "Production Pipelines", "question_plain": "Which of the following is the benefit of using the Auto Stop feature of Databricks SQL warehouses ?", "related_lectures": []}, {"_class": "assessment", "id": 66801988, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following alert destinations is <strong>Not</strong> supported in Databricks SQL ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>SMS is not supported as an alert destination in Databricks SQL . While, email, webhook, Slack, and Microsoft Teams are supported alert destinations in Databricks SQL.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/admin/alert-destinations.html\">https://docs.databricks.com/sql/admin/alert-destinations.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Slack</p>", "<p>Webhook</p>", "<p>SMS</p>", "<p>Microsoft Teams</p>", "<p>Email</p>"]}, "correct_response": ["c"], "section": "Production Pipelines", "question_plain": "Which of the following alert destinations is Not supported in Databricks SQL ?", "related_lectures": []}, {"_class": "assessment", "id": 66801990, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineering team has a long-running multi-tasks Job. The team members need to be notified when the run of this job completes.</p><p><br></p><p>Which of the following approaches can be used to send emails to the team members when the job completes ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Jobs supports email notifications to be notified in the case of job start, success, or failure. Simply, click <strong>Edit email notifications</strong> from the details panel in the Job page. From there, you can add one or more email addresses.</p><p><br></p><p><img src=\"https://lh4.googleusercontent.com/jUhThvOnKPec0Lecb7a08Unino7uV-hFgUSYpd2WEEt-w5YEw-vcT-WZMRFirKd9SJ0Wpj9j6QezFGKu7PuxEys5IJu6G4DjxH5HlVxBCUluk5ORmaRfYt6UUEo_m8d5Hbxx3Y4PQ3rYJF6cdRxRPRytq7UA_8RIq1f_QK2ZNzoKV31x46_nyByEaXw8-w\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#alerts-job\">https://docs.databricks.com/workflows/jobs/jobs.html#alerts-job</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>They can use Job API to programmatically send emails according to each task status</p>", "<p>They can configure email notifications settings in the job page</p>", "<p>There is no way to notify users when the job completes</p>", "<p>Only Job owner can be configured to be notified when the job completes</p>", "<p>They can configure email notifications settings per notebook in the task page</p>"]}, "correct_response": ["b"], "section": "Production Pipelines", "question_plain": "A data engineering team has a long-running multi-tasks Job. The team members need to be notified when the run of this job completes.Which of the following approaches can be used to send emails to the team members when the job completes ?", "related_lectures": []}, {"_class": "assessment", "id": 66801992, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to increase the cluster size of an existing Databricks SQL warehouse.</p><p><br></p><p>Which of the following is the benefit of increasing the cluster size of Databricks SQL warehouses ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Cluster Size represents the number of cluster workers and size of compute resources available to run your queries and dashboards. To reduce query latency, you can increase the cluster size.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/admin/sql-endpoints.html#cluster-size-1\">https://docs.databricks.com/sql/admin/sql-endpoints.html#cluster-size-1</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>Improves the latency of the queries execution</p>", "<p>Speeds up the start up time of the SQL warehouse</p>", "<p>Reduces cost since large clusters use Spot instances </p>", "<p>The cluster size of SQL warehouses is not configurable. Instead, they can increase the number of clusters</p>", "<p>The cluster size can not be changed for existing SQL warehouses. Instead, they can enable the auto-scaling option.</p>"]}, "correct_response": ["a"], "section": "Production Pipelines", "question_plain": "A data engineer wants to increase the cluster size of an existing Databricks SQL warehouse.Which of the following is the benefit of increasing the cluster size of Databricks SQL warehouses ?", "related_lectures": []}, {"_class": "assessment", "id": 66801996, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following describes Cron syntax in Databricks Jobs ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To define a schedule for a Databricks job, you can either interactively specify the period and starting time, or write a Cron Syntax expression. The Cron Syntax allows to represent complex job schedule that can be defined programmatically</p><p><br></p><p><img src=\"https://lh5.googleusercontent.com/s-0glgQIU7_nvv4BaNGNAgITBEZztolAkIyCsE7P6YavKFH2yiwuevM5r2RBpUbCqrtOZfoi-NzxQDhc1xOmdz3JJs4dNsEHd_ZbhbFUeypyCw9NeCasCXCrRWvCADGhpXfNDPyrSeVgSPxA1ZGACicCpvudQhleXS2gTaRzZVsIzguFQ3MDrMBiecHToA\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#schedule-a-job\">https://docs.databricks.com/workflows/jobs/jobs.html#schedule-a-job</a></p><p><br></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>It\u2019s an expression to represent the maximum concurrent runs of a job</p>", "<p>It\u2019s an expression to represent complex job schedule that can be defined programmatically</p>", "<p>It\u2019s an expression to represent the retry policy of a job</p>", "<p>It\u2019s an expression to describe the email notification events (start, success, failure)</p>", "<p>It\u2019s an expression to represent the run timeout of a job</p>"]}, "correct_response": ["b"], "section": "Production Pipelines", "question_plain": "Which of the following describes Cron syntax in Databricks Jobs ?", "related_lectures": []}, {"_class": "assessment", "id": 66801998, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineer team has a DLT pipeline that updates all the tables at defined intervals until manually stopped. The compute resources terminate when the pipeline is stopped.</p><p><br></p><p>Which of the following best describes the execution modes of this DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run until the pipeline is shut down.</p><p><br></p><p>In Production mode, the Delta Live Tables system:</p><ul><li><p>Terminates the cluster immediately when the pipeline is stopped.</p></li><li><p>Restarts the cluster for recoverable errors (e.g., memory leak or stale credentials). </p></li><li><p>Retries execution in case of specific errors (e.g., a failure to start a cluster)</p></li></ul><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The DLT pipeline executes in Continuous Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Continuous Pipeline mode under Development mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Development mode.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["a"], "section": "Production Pipelines", "question_plain": "The data engineer team has a DLT pipeline that updates all the tables at defined intervals until manually stopped. The compute resources terminate when the pipeline is stopped.Which of the following best describes the execution modes of this DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 66802004, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which part of the Databricks Platform can a data engineer use to grant permissions on tables to users ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Data Explorer in Databricks SQL allows you to manage data object permissions. This includes granting privileges on tables and databases to users or groups of users.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/data-acl.html#data-explorer\">https://docs.databricks.com/security/access-control/data-acl.html#data-explorer</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Data Studio</p>", "<p>Cluster event log</p>", "<p>Workflows</p>", "<p>DBFS</p>", "<p>Data Explorer</p>"]}, "correct_response": ["e"], "section": "Data Governance", "question_plain": "Which part of the Databricks Platform can a data engineer use to grant permissions on tables to users ?", "related_lectures": []}, {"_class": "assessment", "id": 66802000, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands can a data engineer use to grant full permissions to the HR team on the table <strong>employees</strong> ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>ALL PRIVILEGES</code> is used to grant full permissions on an object to a user or group of users. It is translated into all the below privileges:</p><ul><li><p><code>SELECT</code></p></li><li><p><code>CREATE</code></p></li><li><p><code>MODIFY</code></p></li><li><p><code>USAGE</code></p></li><li><p><code>READ_METADATA</code></p></li></ul><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>GRANT FULL PRIVILEGES ON TABLE employees TO hr_team </p>", "<p>GRANT FULL PRIVILEGES ON TABLE hr_team TO employees </p>", "<p>GRANT ALL PRIVILEGES ON TABLE employees TO hr_team </p>", "<p>GRANT ALL PRIVILEGES ON TABLE hr_team TO employees </p>", "<p>GRANT SELECT, MODIFY, CREATE, READ_METADATA ON TABLE employees TO hr_team </p>"]}, "correct_response": ["c"], "section": "Data Governance", "question_plain": "Which of the following commands can a data engineer use to grant full permissions to the HR team on the table employees ?", "related_lectures": []}, {"_class": "assessment", "id": 66802002, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer uses the following SQL query:</p><p><br></p><p><code>GRANT MODIFY ON TABLE employees TO hr_team</code></p><p><br></p><p>Which of the following describes the ability given by the <code>MODIFY</code> privilege ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The <code>MODIFY</code> privilege gives the ability to add, delete, and modify data to or from an object.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>It gives the ability to add data from the table</p>", "<p>It gives the ability to delete data from the table</p>", "<p>It gives the ability to modify data in the table</p>", "<p>All the above abilities are given by the <code>MODIFY</code> privilege</p>", "<p>None of these options correctly describe the ability given by the <code>MODIFY</code> privilege</p>"]}, "correct_response": ["d"], "section": "Data Governance", "question_plain": "A data engineer uses the following SQL query:GRANT MODIFY ON TABLE employees TO hr_teamWhich of the following describes the ability given by the MODIFY privilege ?", "related_lectures": []}]}
5732132
~~~
{"count": 45, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 67214726, "assessment_type": "multiple-choice", "prompt": {"question": "<p>One of the foundational technologies provided by the Databricks Lakehouse Platform is an open-source, file-based storage format that brings reliability to data lakes.</p><p><br></p><p>Which of the following technologies is being described in the above statement? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake is an open source technology that extends Parquet data files with a file-based transaction log for ACID transactions that brings reliability to data lakes.</p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/index.html\">https://docs.databricks.com/delta/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664674/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665592/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>Delta Lives Tables (DLT)</p>", "<p>Delta Lake</p>", "<p>Apache Spark</p>", "<p>Unity Catalog</p>", "<p>Photon</p>"]}, "correct_response": ["b"], "section": "Databricks Lakehouse Platform", "question_plain": "One of the foundational technologies provided by the Databricks Lakehouse Platform is an open-source, file-based storage format that brings reliability to data lakes.Which of the following technologies is being described in the above statement?", "related_lectures": []}, {"_class": "assessment", "id": 67214714, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands can a data engineer use to purge stale data files of a Delta table?</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The VACUUM command deletes the unused data files older than a specified data retention period.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-vacuum.html\">https://docs.databricks.com/sql/language-manual/delta-vacuum.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>DELETE</p>", "<p>GARBAGE COLLECTION</p>", "<p>CLEAN</p>", "<p>VACUUM</p>", "<p>OPTIMIZE</p>"]}, "correct_response": ["d"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following commands can a data engineer use to purge stale data files of a Delta table?", "related_lectures": []}, {"_class": "assessment", "id": 67214716, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Databricks Repos, which of the following operations a data engineer can use to save local changes of a repo to its remote repository ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Commit &amp; Push is used to save the changes on a local repo, then uploads this local repo content to the remote repository.</p><p><br></p><p>References:</p><ul><li><p><a href=\"https://docs.databricks.com/repos/index.html\">https://docs.databricks.com/repos/index.html</a></p></li><li><p><a href=\"https://github.com/git-guides/git-push\">https://github.com/git-guides/git-push</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34844752/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Create Pull Request</p>", "<p>Commit &amp; Pull</p>", "<p>Commit &amp; Push</p>", "<p>Merge &amp; Push</p>", "<p>Merge &amp; Pull</p>"]}, "correct_response": ["c"], "section": "Databricks Lakehouse Platform", "question_plain": "In Databricks Repos, which of the following operations a data engineer can use to save local changes of a repo to its remote repository ?", "related_lectures": []}, {"_class": "assessment", "id": 67214718, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Delta Lake tables, which of the following is the primary format for the transaction log files?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Lake builds upon standard data formats. Delta lake table gets stored on the storage in one or more data files in Parquet format, along with transaction logs in JSON format.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/delta/index.html\">https://docs.databricks.com/delta/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664674/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665592/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Delta</p>", "<p>Parquet</p>", "<p>JSON</p>", "<p>Hive-specific format</p>", "<p>XML</p>"]}, "correct_response": ["c"], "section": "Databricks Lakehouse Platform", "question_plain": "In Delta Lake tables, which of the following is the primary format for the transaction log files?", "related_lectures": []}, {"_class": "assessment", "id": 67214720, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following functionalities can be performed in Databricks Repos ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Repos supports git Pull operation. It is used to fetch and download content from a remote repository and immediately update the local repo to match that content.</p><p><br></p><p>References:</p><ul><li><p><a href=\"https://docs.databricks.com/repos/index.html\">https://docs.databricks.com/repos/index.html</a></p></li><li><p><a href=\"https://github.com/git-guides/git-pull\">https://github.com/git-guides/git-pull</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34844752/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>Create pull requests</p>", "<p>Create new remote Git repositories</p>", "<p>Delete branches</p>", "<p>Create CI/CD pipelines</p>", "<p>Pull from a remote Git repository</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following functionalities can be performed in Databricks Repos ?", "related_lectures": []}, {"_class": "assessment", "id": 67214722, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following locations completely hosts the customer data ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>According to the Databricks Lakehouse architecture, the storage account hosting the customer data is provisioned in the data plane in the Databricks customer's cloud account.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/getting-started/overview.html\">https://docs.databricks.com/getting-started/overview.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664668/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li></ul><p><br></p>", "answers": ["<p>Customer's cloud account</p>", "<p>Control plane</p>", "<p>Databricks account</p>", "<p>Databricks-managed cluster</p>", "<p>Repos</p>"]}, "correct_response": ["a"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following locations completely hosts the customer data ?", "related_lectures": []}, {"_class": "assessment", "id": 67214724, "assessment_type": "multiple-choice", "prompt": {"question": "<p>If \u200b\u200bthe default notebook language is Python, which of the following options a data engineer can use to run SQL commands in this Python Notebook ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, cells use the default language of the notebook. You can override the default language in a cell by using the language magic command at the beginning of a cell. The supported magic commands are: <code>%python</code>, <code>%sql</code>, <code>%scala</code>, and <code>%r</code>.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/notebooks/notebooks-code.html\">https://docs.databricks.com/notebooks/notebooks-code.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>They need first to import the SQL library in a cell</p>", "<p>This is not possible! They need to change the default language of the notebook to SQL</p>", "<p>Databricks detects cells language automatically, so they can write SQL syntax in any cell</p>", "<p>They can add <code>%language</code> magic command at the start of a cell to force language detection.</p>", "<p>They can add <code>%sql</code> at the start of a cell.</p>"]}, "correct_response": ["e"], "section": "Databricks Lakehouse Platform", "question_plain": "If \u200b\u200bthe default notebook language is Python, which of the following options a data engineer can use to run SQL commands in this Python Notebook ?", "related_lectures": []}, {"_class": "assessment", "id": 67214728, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A junior data engineer uses the built-in Databricks Notebooks versioning for source control. A senior data engineer recommended using Databricks Repos instead.</p><p><br></p><p>Which of the following could explain why Databricks Repos is recommended instead of Databricks Notebooks versioning?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>One advantage of Databricks Repos over the built-in Databricks Notebooks versioning is that Databricks Repos supports creating and managing branches for development work.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/repos/index.html\">https://docs.databricks.com/repos/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34844752/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Databricks Repos supports creating and managing branches for development work.</p>", "<p>Databricks Repos automatically tracks the changes and keeps the history.</p>", "<p>Databricks Repos allows users to resolve merge conflicts</p>", "<p>Databricks Repos allows users to restore previous versions of a notebook</p>", "<p>All of these advantages explain why Databricks Repos is recommended instead of Notebooks versioning</p>"]}, "correct_response": ["a"], "section": "Databricks Lakehouse Platform", "question_plain": "A junior data engineer uses the built-in Databricks Notebooks versioning for source control. A senior data engineer recommended using Databricks Repos instead.Which of the following could explain why Databricks Repos is recommended instead of Databricks Notebooks versioning?", "related_lectures": []}, {"_class": "assessment", "id": 67214730, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following services provides a data warehousing experience to its users?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks SQL (DB SQL) is a data warehouse on the Databricks Lakehouse Platform that lets you run all your SQL and BI applications at scale.</p><p><br></p><p>Reference: <a href=\"https://www.databricks.com/product/databricks-sql\">https://www.databricks.com/product/databricks-sql</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>Databricks SQL</p>", "<p>Databricks Machine Learning</p>", "<p>Data Science and Engineering Workspace</p>", "<p>Unity Catalog</p>", "<p>Delta Lives Tables (DLT)</p>"]}, "correct_response": ["a"], "section": "Databricks Lakehouse Platform", "question_plain": "Which of the following services provides a data warehousing experience to its users?", "related_lectures": []}, {"_class": "assessment", "id": 67214796, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer noticed that there are unused data files in the directory of a Delta table. They executed the VACUUM command on this table; however, only some of those unused data files have been deleted.</p><p><br></p><p>Which of the following could explain why only some of the unused data files have been deleted after running the VACUUM command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Running the VACUUM command on a Delta table deletes the unused data files older than a specified data retention period. Unused files newer than the default retention threshold are kept untouched.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/delta-vacuum.html\">https://docs.databricks.com/sql/language-manual/delta-vacuum.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672126/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672136/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The deleted data files were larger than the default size threshold. While the remaining files are smaller than the default size threshold and can not be deleted.</p>", "<p>The deleted data files were smaller than the default size threshold. While the remaining files are larger than the default size threshold and can not be deleted.</p>", "<p>The deleted data files were older than the default retention threshold. While the remaining files are newer than the default retention threshold and can not be deleted.</p>", "<p>The deleted data files were newer than the default retention threshold. While the remaining files are older than the default retention threshold and can not be deleted.</p>", "<p>More information is needed to determine the correct answer</p>"]}, "correct_response": ["c"], "section": "Databricks Lakehouse Platform", "question_plain": "A data engineer noticed that there are unused data files in the directory of a Delta table. They executed the VACUUM command on this table; however, only some of those unused data files have been deleted.Which of the following could explain why only some of the unused data files have been deleted after running the VACUUM command ?", "related_lectures": []}, {"_class": "assessment", "id": 67214712, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team has a Delta table called <strong>products</strong> that contains products\u2019 details including the net price.</p><p><br></p><p>Which of the following code blocks will apply a 50% discount on all the products where the price is greater than 1000 and save the new price to the table?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The UPDATE statement is used to modify the existing records in a table that match the WHERE condition. In this case, we are updating the products where the price is strictly greater than 1000.</p><p><br></p><p>Syntax:</p><pre class=\"prettyprint linenums\">UPDATE table_name\nSET column_name = expr\nWHERE condition</pre><p><br></p><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/sql/language-manual/delta-update.html\">https://docs.databricks.com/sql/language-manual/delta-update.html</a></p><p><br></p>", "answers": ["<p>UPDATE products SET price = price * 0.5 WHERE price &gt;= 1000;</p>", "<p>SELECT price * 0.5 AS new_price FROM products WHERE price &gt; 1000;</p>", "<p>MERGE INTO products WHERE price &lt; 1000 WHEN MATCHED UPDATE price = price * 0.5;</p>", "<p>UPDATE products SET price = price * 0.5 WHERE price &gt; 1000;</p>", "<p>MERGE INTO products WHERE price &gt; 1000 WHEN MATCHED UPDATE price = price * 0.5;</p>"]}, "correct_response": ["d"], "section": "Databricks Lakehouse Platform", "question_plain": "The data engineering team has a Delta table called products that contains products\u2019 details including the net price.Which of the following code blocks will apply a 50% discount on all the products where the price is greater than 1000 and save the new price to the table?", "related_lectures": []}, {"_class": "assessment", "id": 67214734, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer wants to create a relational object by pulling data from two tables. The relational object will only be used in the current session. In order to save on storage costs, the date engineer wants to avoid copying and storing physical data.</p><p><br></p><p>Which of the following relational objects should the data engineer create?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In order to avoid copying and storing physical data, the data engineer must create a view object. A view in databricks is a virtual table that has no physical data. It\u2019s just a saved SQL query against actual tables.</p><p>The view type should be Temporary view since it\u2019s tied to a Spark session and dropped when the session ends.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-view.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672154/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672162/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>External table</p>", "<p>Temporary view</p>", "<p>Managed table</p>", "<p>Global Temporary view</p>", "<p>View</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "A data engineer wants to create a relational object by pulling data from two tables. The relational object will only be used in the current session. In order to save on storage costs, the date engineer wants to avoid copying and storing physical data.Which of the following relational objects should the data engineer create?", "related_lectures": []}, {"_class": "assessment", "id": 67214736, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a database named <strong>db_hr</strong>, and they want to know where this database was created in the underlying storage.</p><p><br></p><p>Which of the following commands can the data engineer use to complete this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The <code>DESCRIBE DATABASE</code> or <code>DESCRIBE SCHEMA</code> returns the metadata of an existing database (schema). The metadata information includes the database\u2019s name, comment, and location on the filesystem. If the optional <code>EXTENDED</code> option is specified, database properties are also returned.</p><p><br></p><p>Syntax:</p><p><code>DESCRIBE DATABASE [ EXTENDED ] database_name</code></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-schema.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-schema.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>DESCRIBE db_hr</p>", "<p>DESCRIBE EXTENDED db_hr</p>", "<p>DESCRIBE DATABASE db_hr</p>", "<p>SELECT location FROM db_hr.db</p>", "<p>There is no need for a command since all databases are created under the default hive metastore directory</p>"]}, "correct_response": ["c"], "section": "ELT with Spark SQL and Python", "question_plain": "A data engineer has a database named db_hr, and they want to know where this database was created in the underlying storage.Which of the following commands can the data engineer use to complete this task?", "related_lectures": []}, {"_class": "assessment", "id": 67214738, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following commands a data engineer can use to register the table <strong>orders</strong> from an existing SQLite database ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Using the JDBC library, Spark SQL can extract data from any existing relational database that supports JDBC. Examples include mysql, postgres, SQLite, and more.</p><p><br></p><p>Reference: <a href=\"https://learn.microsoft.com/en-us/azure/databricks/external-data/jdbc\">https://learn.microsoft.com/en-us/azure/databricks/external-data/jdbc</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34673300/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p><p><br></p>", "answers": ["<pre class=\"prettyprint linenums\">CREATE TABLE orders\n&nbsp; USING sqlite\n&nbsp; OPTIONS (\n&nbsp; &nbsp; url \"jdbc:sqlite:/bookstore.db\",\n&nbsp; &nbsp; dbtable \"orders\"\n&nbsp; )</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE orders\n&nbsp; USING org.apache.spark.sql.jdbc\n&nbsp; OPTIONS (\n&nbsp; &nbsp; url \"jdbc:sqlite:/bookstore.db\",\n&nbsp; &nbsp; dbtable \"orders\"\n&nbsp; )</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE orders\n&nbsp; USING cloudfiles\n&nbsp; OPTIONS (\n&nbsp; &nbsp; url \"jdbc:sqlite:/bookstore.db\",\n&nbsp; &nbsp; dbtable \"orders\"\n&nbsp; )</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE orders\n&nbsp; USING EXTERNAL\n&nbsp; OPTIONS (\n&nbsp; &nbsp; url \"jdbc:sqlite:/bookstore.db\",\n&nbsp; &nbsp; dbtable \"orders\"\n&nbsp; )</pre>", "<pre class=\"prettyprint linenums\">CREATE TABLE orders\nUSING DATABASE\nOPTIONS (\n    url \"jdbc:sqlite:/bookstore.db\",\n    dbtable \"orders\"\n)</pre>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following commands a data engineer can use to register the table orders from an existing SQLite database ?", "related_lectures": []}, {"_class": "assessment", "id": 67214740, "assessment_type": "multiple-choice", "prompt": {"question": "<p>When dropping a Delta table, which of the following explains why both the table's metadata and the data files will be deleted ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Managed tables are tables whose metadata and the data are managed by Databricks.</p><p>When you run <code>DROP TABLE</code> on a managed table, both the metadata and the underlying data files are deleted.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/lakehouse/data-objects.html#what-is-a-managed-table\">https://docs.databricks.com/lakehouse/data-objects.html#what-is-a-managed-table</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The table is shallow cloned</p>", "<p>The table is external</p>", "<p>The user running the command has the necessary permissions to delete the data files</p>", "<p>The table is managed</p>", "<p>The data files are older than the default retention period</p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "When dropping a Delta table, which of the following explains why both the table's metadata and the data files will be deleted ?", "related_lectures": []}, {"_class": "assessment", "id": 67214742, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following commands:</p><p><br></p><pre class=\"prettyprint linenums\">CREATE DATABASE db_hr;\n\nUSE db_hr;\nCREATE TABLE employees;</pre><p><br></p><p>In which of the following locations will the employees table be located?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Since we are creating the database here without specifying a <code>LOCATION</code> clause, the database will be created in the default warehouse directory under <strong>dbfs:/user/hive/warehouse</strong>. The database folder have the extension (.db)</p><p>And since we are creating the table also without specifying a <code>LOCATION</code> clause, the table becomes a managed table created under the database directory (in <strong>db_hr.db</strong> folder)</p><p><br></p><p>Reference:<a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html\"> https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>dbfs:/user/hive/warehouse</p>", "<p>dbfs:/user/hive/warehouse/db_hr.db</p>", "<p>dbfs:/user/hive/warehouse/db_hr</p>", "<p>dbfs:/user/hive/databases/db_hr.db</p>", "<p>More information is needed to determine the correct answer</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Given the following commands:CREATE DATABASE db_hr;\n\nUSE db_hr;\nCREATE TABLE employees;In which of the following locations will the employees table be located?", "related_lectures": []}, {"_class": "assessment", "id": 67214744, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following code blocks can a data engineer use to create a Python function to multiply two integers and return the result?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Python, a function is defined using the <code>def</code> keyword. Here, we used the <code>return</code> keyword since the question clearly asks to return the result, and not printing the output.</p><p><br></p><p>Syntax:</p><pre class=\"prettyprint linenums\">def function_name(params):\n    return params</pre><p><br></p><p><br></p><p>Reference: <a href=\"https://www.w3schools.com/python/python_functions.asp\">https://www.w3schools.com/python/python_functions.asp</a></p><p><br></p>", "answers": ["<pre class=\"prettyprint linenums\">def multiply_numbers(num1, num2):\n    print(num1 * num2)</pre>", "<pre class=\"prettyprint linenums\">def fun: multiply_numbers(num1, num2):\n    return num1 * num2</pre>", "<pre class=\"prettyprint linenums\">def multiply_numbers(num1, num2):\n    return num1 * num2</pre>", "<pre class=\"prettyprint linenums\">fun multiply_numbers(num1, num2):\n    return num1 * num2</pre>", "<pre class=\"prettyprint linenums\">fun def multiply_numbers(num1, num2):\n    return num1 * num2</pre>"]}, "correct_response": ["c"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following code blocks can a data engineer use to create a Python function to multiply two integers and return the result?", "related_lectures": []}, {"_class": "assessment", "id": 67214746, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following 2 tables:</p><p><strong><img src=\"https://lh6.googleusercontent.com/DQSIJwFsaeOV4XTrX3RabhX7eZ30SC55_7wI2GaD9-ksX18J0QZBPLpDKxcBD36mNnQqc4Yh9eBY2Rx1RuIy-KHgoN7_EmUG1Vmx77wE1o2ADJKmh9MGn-iY6LFoxR_s01bpU3kA07re4TZl9bmBXJtMMey7Zgw9dnkfkO73Yful6dE655JwK4GbxC_cDw\"></strong></p><p><br></p><p>Fill in the blank to make the following query returns the below result:</p><p><br></p><pre class=\"prettyprint linenums\">SELECT students.name, students.age, enrollments.course_id\nFROM students\n_____________ enrollments\nON students.student_id = enrollments.student_id</pre><p><br></p><p><img src=\"https://lh5.googleusercontent.com/003jfnTYSjRV-uoPswjfnU3mpP6Tsi29dw_bmUALq71SHh0w-G0U98eqH9CDa1yQApn3xw095lQtP3FiXf2A6b44bB-jb3j0c0LIb_YiTIv64qWb2fZPa0DZhih7oSBZHreTzMi2b3OwnIfoA0WKDhvxgci5tFqlGp84hJ3M8QKeO8c-VJM-01Yy7Kescg\"></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>LEFT JOIN</code> returns all values from the left table and the matched values from the right table, or appends NULL if there is no match. In the above example, we see NULL in the course_id of John (U0003) since he is not enrolled in any course.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-join.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-join.html</a></p>", "answers": ["<p>RIGHT JOIN</p>", "<p>LEFT JOIN</p>", "<p>INNER JOIN</p>", "<p>ANTI JOIN</p>", "<p>CROSS JOIN</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Given the following 2 tables:Fill in the blank to make the following query returns the below result:SELECT students.name, students.age, enrollments.course_id\nFROM students\n_____________ enrollments\nON students.student_id = enrollments.student_id", "related_lectures": []}, {"_class": "assessment", "id": 67214748, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following SQL keywords can be used to rotate rows of a table by turning row values into multiple columns ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>PIVOT</code> transforms the rows of a table by rotating unique values of a specified column list into separate columns. In other words, It converts a table from a long format to a wide format.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-pivot.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-pivot.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34673444/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>ROTATE</p>", "<p>TRANSFORM</p>", "<p>PIVOT</p>", "<p>GROUP BY</p>", "<p>ZORDER BY</p>"]}, "correct_response": ["c"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following SQL keywords can be used to rotate rows of a table by turning row values into multiple columns ?", "related_lectures": []}, {"_class": "assessment", "id": 67214750, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Fill in the below blank to get the number of courses incremented by 1 for each student in array column <strong>students</strong>.</p><p><br></p><pre class=\"prettyprint linenums\">SELECT\n  faculty_id,\n  students,\n  ___________ AS new_totals\nFROM faculties</pre>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>transform(input_array, lambd_function)</code> is a higher order function that returns an output array from an input array by transforming each element in the array using a given lambda function.</p><p><br></p><p>Example:</p><p><code>SELECT transform(array(1, 2, 3), x -&gt; x + 1);</code></p><p>output: [2, 3, 4]</p><p><br></p><p><br></p><p>Reference:</p><ul><li><p><a href=\"https://docs.databricks.com/sql/language-manual/functions/transform.html\">https://docs.databricks.com/sql/language-manual/functions/transform.html</a></p></li><li><p><a href=\"https://docs.databricks.com/optimizations/higher-order-lambda-functions.html\">https://docs.databricks.com/optimizations/higher-order-lambda-functions.html</a></p></li></ul><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34823118/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>TRANSFORM (students, total_courses + 1)</p>", "<p>TRANSFORM (students, i -&gt; i.total_courses + 1)</p>", "<p>FILTER (students, total_courses + 1)</p>", "<p>FILTER (students, i -&gt; i.total_courses + 1)</p>", "<p>CASE WHEN students.total_courses IS NOT NULL THEN students.total_courses + 1</p><p>ELSE NULL</p><p>END</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Fill in the below blank to get the number of courses incremented by 1 for each student in array column students.SELECT\n  faculty_id,\n  students,\n  ___________ AS new_totals\nFROM faculties", "related_lectures": []}, {"_class": "assessment", "id": 67214752, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Fill in the below blank to successfully create a table using data from CSV files located at <strong>/path/input</strong></p><p><br></p><pre class=\"prettyprint linenums\">CREATE TABLE my_table\n(col1 STRING, col2 STRING)\n____________\nOPTIONS (header = \"true\",\n        delimiter = \";\")\nLOCATION = \"/path/input\"</pre>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>CREATE TABLE USING</code> allows to specify an external data source type like CSV format, and with any additional options. This creates an external table pointing to files stored in an external location.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34673300/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664704/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>FROM CSV</p>", "<p>USING CSV</p>", "<p>USING DELTA</p>", "<p>AS</p>", "<p>AS CSV</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Fill in the below blank to successfully create a table using data from CSV files located at /path/inputCREATE TABLE my_table\n(col1 STRING, col2 STRING)\n____________\nOPTIONS (header = \"true\",\n        delimiter = \";\")\nLOCATION = \"/path/input\"", "related_lectures": []}, {"_class": "assessment", "id": 67214754, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements best describes the usage of <code>CREATE SCHEMA</code> command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>CREATE SCHEMA</code> is an alias for <code>CREATE DATABASE</code> statement. While usage of SCHEMA and DATABASE is interchangeable, SCHEMA is preferred.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-database.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-database.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672144/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34672150/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>It\u2019s used to create a table schema (columns names and datatype)</p>", "<p>It\u2019s used to create a Hive catalog</p>", "<p>It\u2019s used to infer and store schema in \u201ccloudFiles.schemaLocation\u201d</p>", "<p>It\u2019s used to create a database</p>", "<p>It\u2019s used to merge the schema when writing data into a target table</p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following statements best describes the usage of CREATE SCHEMA command ?", "related_lectures": []}, {"_class": "assessment", "id": 67214756, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statements is <strong>Not</strong> true about CTAS statements ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>CREATE TABLE AS SELECT</code> statements, or CTAS statements create and populate Delta tables using the output of a SELECT query. CTAS statements automatically infer schema information from query results and do not support manual schema declaration.</p><p><br></p><p>Reference: (cf. <code>AS query</code> clause)</p><p><a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34833438/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34664704/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>CTAS statements automatically infer schema information from query results</p>", "<p>CTAS statements support manual schema declaration</p>", "<p>CTAS statements stand for CREATE TABLE _ AS SELECT statement</p>", "<p>With CTAS statements, data will be inserted during the table creation</p>", "<p>All these statements are Not true about CTAS statements</p>"]}, "correct_response": ["b"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following statements is Not true about CTAS statements ?", "related_lectures": []}, {"_class": "assessment", "id": 67214800, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following SQL commands will append this new row to the existing Delta table <strong>users</strong>?</p><p><img src=\"https://lh6.googleusercontent.com/BurbNTZSWDOeacJTh9ePgw0geERvljinDN6BiGsMSD8T3KqbV-MkmCEi1Kcdz-5e71RZdlj_QbP53yQEudV6AidnOsv1Qrq7mkBfeubUSPoPWOUXBqoOBX4C52f9vAfIllGVsjPKWAh_dP0QyGZnv3OR5sLb8264UmmoL52U29mbLytjhGeEpYq3L4w29A\"></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><code>INSERT INTO</code> allows inserting new rows into a Delta table. You specify the inserted rows by value expressions or the result of a query.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/language-manual/sql-ref-syntax-dml-insert-into.html\">https://docs.databricks.com/sql/language-manual/sql-ref-syntax-dml-insert-into.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665592/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>APPEND INTO users VALUES (\u201c0015\u201d, \u201cAdam\u201d, 23)</p>", "<p>INSERT VALUES (\u201c0015\u201d, \u201cAdam\u201d, 23)&nbsp; INTO users</p>", "<p>APPEND VALUES (\u201c0015\u201d, \u201cAdam\u201d, 23) INTO users</p>", "<p>INSERT INTO users VALUES (\u201c0015\u201d, \u201cAdam\u201d, 23)</p>", "<p>UPDATE users VALUES (\u201c0015\u201d, \u201cAdam\u201d, 23) </p>"]}, "correct_response": ["d"], "section": "ELT with Spark SQL and Python", "question_plain": "Which of the following SQL commands will append this new row to the existing Delta table users?", "related_lectures": []}, {"_class": "assessment", "id": 67214758, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n        .___________\n        .table(\"new_orders\") )\n</pre><p><br></p><p>Fill in the blank to make the query executes multiple micro-batches to process all available data, then stops the trigger.</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Spark Structured Streaming, we use <code>trigger(availableNow=True)</code> to run the stream in batch mode where it processes all available data in multiple micro-batches. The trigger will stop on its own once it finishes processing the available data.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing\">https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>trigger(\u201cmicro-batches\u201d)</p>", "<p>trigger(once=True)</p>", "<p>trigger(processingTime=\u201d0 seconds\")</p>", "<p>trigger(micro-batches=True)</p>", "<p>trigger(availableNow=True)</p>"]}, "correct_response": ["e"], "section": "Incremental Data Processing", "question_plain": "Given the following Structured Streaming query:(spark.table(\"orders\")\n        .withColumn(\"total_after_tax\", col(\"total\")+col(\"tax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"append\")\n        .___________\n        .table(\"new_orders\") )\nFill in the blank to make the query executes multiple micro-batches to process all available data, then stops the trigger.", "related_lectures": []}, {"_class": "assessment", "id": 67214760, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following techniques allows Auto Loader to track the ingestion progress and store metadata of the discovered files ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Auto Loader keeps track of discovered files using checkpointing in the checkpoint location. Checkpointing allows Auto loader to provide exactly-once ingestion guarantees.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/ingestion/auto-loader/index.html#how-does-auto-loader-track-ingestion-progress\">https://docs.databricks.com/ingestion/auto-loader/index.html#how-does-auto-loader-track-ingestion-progress</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>mergeSchema</p>", "<p>COPY INTO</p>", "<p>Watermarking</p>", "<p>Checkpointing</p>", "<p>Z-Ordering</p>"]}, "correct_response": ["d"], "section": "Incremental Data Processing", "question_plain": "Which of the following techniques allows Auto Loader to track the ingestion progress and store metadata of the discovered files ?", "related_lectures": []}, {"_class": "assessment", "id": 67214762, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:</p><p><br></p><p><code>CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ </code></p><p><br></p><p>Fill in the above blank so records violating this constraint cause the pipeline to fail. </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>With <code>ON VIOLATION FAIL UPDATE</code>, records that violate the expectation will cause the pipeline to fail. When a pipeline fails because of an expectation violation, you must fix the pipeline code to handle the invalid data correctly before re-running the pipeline.</p><p><br></p><p>Reference:</p><p><a href=\"https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations#--fail-on-invalid-records\">https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-expectations#--fail-on-invalid-records</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>ON VIOLATION FAIL</p>", "<p>ON VIOLATION FAIL UPDATE</p>", "<p>ON VIOLATION DROP ROW</p>", "<p>ON VIOLATION FAIL PIPELINE</p>", "<p>There is no need to add ON VIOLATION clause. By default, records violating the constraint cause the pipeline to fail. </p>"]}, "correct_response": ["b"], "section": "Incremental Data Processing", "question_plain": "A data engineer has defined the following data quality constraint in a Delta Live Tables pipeline:CONSTRAINT valid_id EXPECT (id IS NOT NULL) _____________ Fill in the above blank so records violating this constraint cause the pipeline to fail.", "related_lectures": []}, {"_class": "assessment", "id": 67214764, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In multi-hop architecture, which of the following statements best describes the Silver layer tables? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Silver tables provide a more refined view of the raw data. For example, data can be cleaned and filtered at this level. And we can also join fields from various bronze tables to enrich our silver records</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742310/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>They maintain data that powers analytics, machine learning, and production applications</p>", "<p>They maintain raw data ingested from various sources</p>", "<p>The table structure in this layer resembles that of the source system table structure with any additional metadata columns like the load time, and input file name.</p>", "<p>They provide business-level aggregated version of data</p>", "<p>They provide a more refined view of raw data, where it\u2019s filtered, cleaned, and enriched.</p>"]}, "correct_response": ["e"], "section": "Incremental Data Processing", "question_plain": "In multi-hop architecture, which of the following statements best describes the Silver layer tables?", "related_lectures": []}, {"_class": "assessment", "id": 67214766, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineer team has a DLT pipeline that updates all the tables at defined intervals until manually stopped. The compute resources of the pipeline continue running to allow for quick testing.</p><p><br></p><p>Which of the following best describes the execution modes of this DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run until the pipeline is shut down.</p><p><br></p><p>In Development mode, the Delta Live Tables system ease the development process by</p><ul><li><p>Reusing a cluster to avoid the overhead of restarts. The cluster runs for two hours when development mode is enabled.</p></li><li><p>Disabling pipeline retries so you can immediately detect and fix errors.</p></li></ul><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>The DLT pipeline executes in Continuous Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Continuous Pipeline mode under Development mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Development mode.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["b"], "section": "Incremental Data Processing", "question_plain": "The data engineer team has a DLT pipeline that updates all the tables at defined intervals until manually stopped. The compute resources of the pipeline continue running to allow for quick testing.Which of the following best describes the execution modes of this DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 67214768, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.readStream\n        .table(\"cleanedOrders\")\n        .groupBy(\"productCategory\")\n        .agg(sum(\"totalWithTax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"complete\")\n        .table(\"aggregatedOrders\")\n)</pre><p><br></p><p>Which of the following best describe the purpose of this query in a multi-hop architecture?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The above Structured Streaming query creates business-level aggregates from clean orders data in the silver table cleanedOrders, and loads them in the gold table aggregatedOrders.</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742310/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>The query is performing raw data ingestion into a Bronze table</p>", "<p>The query is performing a hop from a Bronze table to a Silver table</p>", "<p>The query is performing a hop from Silver layer to a Gold table</p>", "<p>The query is performing data transfer from a Gold table into a production application </p>", "<p>This query is performing data quality controls prior to Silver layer</p>"]}, "correct_response": ["c"], "section": "Incremental Data Processing", "question_plain": "Given the following Structured Streaming query:(spark.readStream\n        .table(\"cleanedOrders\")\n        .groupBy(\"productCategory\")\n        .agg(sum(\"totalWithTax\"))\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .outputMode(\"complete\")\n        .table(\"aggregatedOrders\")\n)Which of the following best describe the purpose of this query in a multi-hop architecture?", "related_lectures": []}, {"_class": "assessment", "id": 67214770, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Given the following Structured Streaming query:</p><p><br></p><pre class=\"prettyprint linenums\">(spark.readStream\n        .table(\"orders\")\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"Output_Table\")\n)</pre><p><br></p><p>Which of the following is the trigger Interval for this query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, if you don\u2019t provide any trigger interval, the data will be processed every half second. This is equivalent to <code>trigger(processingTime=\u201d500ms\")</code></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval\">https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34665876/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670586/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>Every half second</p>", "<p>Every half min</p>", "<p>Every half hour</p>", "<p>The query will run in batch mode to process all available data at once, then the trigger stops.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["a"], "section": "Incremental Data Processing", "question_plain": "Given the following Structured Streaming query:(spark.readStream\n        .table(\"orders\")\n    .writeStream\n        .option(\"checkpointLocation\", checkpointPath)\n        .table(\"Output_Table\")\n)Which of the following is the trigger Interval for this query ?", "related_lectures": []}, {"_class": "assessment", "id": 67214772, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has the following query in a Delta Live Tables pipeline</p><p><br></p><pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE sales_silver\nAS\n  SELECT store_id, total + tax AS total_after_tax\n  FROM LIVE.sales_bronze\n</pre><p><br></p><p>The pipeline is failing to start due to an error in this query.</p><p><br></p><p>Which of the following changes should be made to this query to successfully start the DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In DLT pipelines, You can stream data from other tables in the same pipeline by using the <code>STREAM()</code> function. In this case, you must define a streaming live table using <code>CREATE STREAMING LIVE TABLE</code> syntax.</p><p>Remember: to query another live table, prepend always the <code>LIVE.</code> keyword to the table name.</p><p><br></p><pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE table_name\nAS\n    SELECT *\n    FROM STREAM(LIVE.another_table)</pre><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-incremental-data.html#streaming-from-other-datasets-within-a-pipeline&amp;language-sql\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-incremental-data.html#streaming-from-other-datasets-within-a-pipeline&amp;language-sql</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<pre class=\"prettyprint linenums\">CREATE LIVE TABLE sales_silver\nAS\n&nbsp; SELECT store_id, total + tax AS total_after_tax\n&nbsp; FROM STREAMING(LIVE.sales_bronze)</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING TABLE sales_silver\nAS\n&nbsp; SELECT store_id, total + tax AS total_after_tax\n&nbsp; FROM STREAM(LIVE.sales_bronze)</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE sales_silver\nAS\n&nbsp; SELECT store_id, total + tax AS total_after_tax\n&nbsp; FROM STREAMING(sales_bronze)</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE sales_silver\nAS\n&nbsp; SELECT store_id, total + tax AS total_after_tax\n&nbsp; FROM STREAMING(LIVE.sales_bronze)</pre>", "<pre class=\"prettyprint linenums\">CREATE STREAMING LIVE TABLE sales_silver\nAS\n&nbsp; SELECT store_id, total + tax AS total_after_tax\n&nbsp; FROM STREAM(LIVE.sales_bronze)</pre>"]}, "correct_response": ["e"], "section": "Incremental Data Processing", "question_plain": "A data engineer has the following query in a Delta Live Tables pipelineCREATE STREAMING LIVE TABLE sales_silver\nAS\n  SELECT store_id, total + tax AS total_after_tax\n  FROM LIVE.sales_bronze\nThe pipeline is failing to start due to an error in this query.Which of the following changes should be made to this query to successfully start the DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 67214774, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In multi-hop architecture, which of the following statements best describes the Gold layer tables? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Gold layer is the final layer in the multi-hop architecture, where tables provide business level aggregates often used for reporting and dashboarding, or even for Machine learning.</p><p><br></p><p>Reference:</p><p><a href=\"https://www.databricks.com/glossary/medallion-architecture\">https://www.databricks.com/glossary/medallion-architecture</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34742310/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>They provide a more refined view of the data</p>", "<p>They maintain raw data ingested from various sources</p>", "<p>The table structure in this layer resembles that of the source system table structure with any additional metadata columns like the load time, and input file name.</p>", "<p>They provide business-level aggregations that power analytics, machine learning, and production applications</p>", "<p>They represent a filtered, cleaned, and enriched version of data</p>"]}, "correct_response": ["d"], "section": "Incremental Data Processing", "question_plain": "In multi-hop architecture, which of the following statements best describes the Gold layer tables?", "related_lectures": []}, {"_class": "assessment", "id": 67214776, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineer team has a DLT pipeline that updates all the tables once and then stops. The compute resources of the pipeline terminate when the pipeline is stopped.</p><p><br></p><p>Which of the following best describes the execution modes of this DLT pipeline ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Triggered pipelines update each table with whatever data is currently available and then they shut down.</p><p><br></p><p>In Production mode, the Delta Live Tables system:</p><ul><li><p>Terminates the cluster immediately when the pipeline is stopped.</p></li><li><p>Restarts the cluster for recoverable errors (e.g., memory leak or stale credentials). </p></li><li><p>Retries execution in case of specific errors (e.g., a failure to start a cluster)</p></li></ul><p><br></p><p>Reference:</p><p><a href=\"https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html\">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>The DLT pipeline executes in Continuous Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Continuous Pipeline mode under Development mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Production mode.</p>", "<p>The DLT pipeline executes in Triggered Pipeline mode under Development mode.</p>", "<p>More information is needed to determine the correct response</p>"]}, "correct_response": ["c"], "section": "Incremental Data Processing", "question_plain": "The data engineer team has a DLT pipeline that updates all the tables once and then stops. The compute resources of the pipeline terminate when the pipeline is stopped.Which of the following best describes the execution modes of this DLT pipeline ?", "related_lectures": []}, {"_class": "assessment", "id": 67214798, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer needs to determine whether to use Auto Loader or COPY INTO command in order to load input data files incrementally.</p><p><br></p><p>In which of the following scenarios should the data engineer use Auto Loader over COPY INTO command ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Here are a few things to consider when choosing between Auto Loader and COPY INTO command:</p><p><br></p><ul><li><p>If you\u2019re going to ingest files in the order of thousands, you can use COPY INTO. If you are expecting files in the order of millions or more over time, use Auto Loader.</p></li><li><p>If your data schema is going to evolve frequently, Auto Loader provides better primitives around schema inference and evolution.</p></li></ul><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/ingestion/index.html#when-to-use-copy-into-and-when-to-use-auto-loader\">https://docs.databricks.com/ingestion/index.html#when-to-use-copy-into-and-when-to-use-auto-loader</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670312/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34670664/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>If they are going to ingest files in the order of millions or more over time</p>", "<p>If they are going to ingest few number of files in the order of thousands</p>", "<p>If they are going to load a subset of re-uploaded files</p>", "<p>If the data schema is not going to evolve frequently</p>", "<p>There is no difference between using Auto Loader and Copy Into command</p>"]}, "correct_response": ["a"], "section": "Incremental Data Processing", "question_plain": "A data engineer needs to determine whether to use Auto Loader or COPY INTO command in order to load input data files incrementally.In which of the following scenarios should the data engineer use Auto Loader over COPY INTO command ?", "related_lectures": []}, {"_class": "assessment", "id": 67214778, "assessment_type": "multiple-choice", "prompt": {"question": "<p>From which of the following locations can a data engineer set a schedule to automatically refresh a Databricks SQL query ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>In Databricks SQL, you can set a schedule to automatically refresh a query from the query's page.</p><p><br></p><p><img src=\"https://lh6.googleusercontent.com/R-mOsFqlQ0rJXvBiLkmJjxxyIeEJBJLRWlk-dCN30usNy2RcaKQKR-lsbwEz2dgiMjx69QLYJQi_YdoUnJf0yi4vkE_FLrgLr0PB0UDPn-qXXCl2XredLyyyYq-H9cPu18Cm5o-jkfPCHXxdHxU1LZEss9wAzM6azD_-L3sudJqUl3A8QneL-_OznwSuwA\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/sql/user/queries/schedule-query.html\">https://docs.databricks.com/sql/user/queries/schedule-query.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34703008/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>From the jobs Ul</p>", "<p>From the SQL warehouses page in Databricks SQL</p>", "<p>From the Alerts page in Databricks SQL</p>", "<p>From the query's page in Databricks SQL </p>", "<p>There is no way to automatically refresh a query in Databricks SQL. Schedules can be set only for dashboards to refresh their underlying queries.</p>"]}, "correct_response": ["d"], "section": "Production Pipelines", "question_plain": "From which of the following locations can a data engineer set a schedule to automatically refresh a Databricks SQL query ?", "related_lectures": []}, {"_class": "assessment", "id": 67214780, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Databricks provides a declarative ETL framework for building reliable and maintainable data processing pipelines, while maintaining table dependencies and data quality.</p><p><br></p><p>Which of the following technologies is being described above? </p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Delta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/delta-live-tables/index.html\">https://docs.databricks.com/workflows/delta-live-tables/index.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34671142/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>Delta Live Tables</p>", "<p>Delta Lake</p>", "<p>Databricks Jobs</p>", "<p>Unity Catalog Linage</p>", "<p>Databricks SQL</p>"]}, "correct_response": ["a"], "section": "Production Pipelines", "question_plain": "Databricks provides a declarative ETL framework for building reliable and maintainable data processing pipelines, while maintaining table dependencies and data quality.Which of the following technologies is being described above?", "related_lectures": []}, {"_class": "assessment", "id": 67214782, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following services can a data engineer use for orchestration purposes in Databricks platform ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Jobs allow to orchestrate data processing tasks. This means the ability to run and manage multiple tasks as a directed acyclic graph (DAG) in a job.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html\">https://docs.databricks.com/workflows/jobs/jobs.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>Delta Live Tables</p>", "<p>Cluster Pools</p>", "<p>Databricks Jobs</p>", "<p>Data Explorer</p>", "<p>Unity Catalog Linage</p>"]}, "correct_response": ["c"], "section": "Production Pipelines", "question_plain": "Which of the following services can a data engineer use for orchestration purposes in Databricks platform ?", "related_lectures": []}, {"_class": "assessment", "id": 67214784, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer has a Job with multiple tasks that takes more than 2 hours to complete. In the last run, the final task unexpectedly failed.</p><p><br></p><p>Which of the following actions can the data engineer perform to complete this Job Run while minimizing the execution time ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can repair failed multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/repair-job-failures.html\">https://docs.databricks.com/workflows/jobs/repair-job-failures.html</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>They can rerun this Job Run to execute all the tasks</p>", "<p>They can repair this Job Run so only the failed tasks will be re-executed</p>", "<p>They need to delete the failed Run, and start a new Run for the Job</p>", "<p>They can keep the failed Run, and simply start a new Run for the Job</p>", "<p>They can run the Job in Production mode which automatically retries execution in case of errors</p>"]}, "correct_response": ["b"], "section": "Production Pipelines", "question_plain": "A data engineer has a Job with multiple tasks that takes more than 2 hours to complete. In the last run, the final task unexpectedly failed.Which of the following actions can the data engineer perform to complete this Job Run while minimizing the execution time ?", "related_lectures": []}, {"_class": "assessment", "id": 67214786, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineering team has a multi-tasks Job in production. The team members need to be notified in the case of job failure.</p><p><br></p><p>Which of the following approaches can be used to send emails to the team members in the case of job failure ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Databricks Jobs support email notifications to be notified in the case of job start, success, or failure. Simply, click <strong>Edit email notifications</strong> from the details panel in the Job page. From there, you can add one or more email addresses.</p><p><br></p><p><img src=\"https://lh4.googleusercontent.com/j7ilFZj6HDpLqWUn4al3xWQSOo1Ip2-71uZnGpReo8_BUOlRNwV5aVFslJ2r_CLzKWn5M7oIlOb5RtHo-ZO2myvg3JJHnJ7oEVTn6qoxEnuCNJ7WvIPMUJz2VJPWs6lP_9omSgKURMfPOscIsxxjXrXQ1ir_2FiozrZrjAW71kzoaxiOAF2_I6Qj1ugvVA\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#alerts-job\">https://docs.databricks.com/workflows/jobs/jobs.html#alerts-job</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>They can use Job API to programmatically send emails according to each task status</p>", "<p>They can configure email notifications settings in the job page</p>", "<p>There is no way to notify users in the case of job failure</p>", "<p>Only Job owner can be configured to be notified in the case of job failure</p>", "<p>They can configure email notifications settings per notebook in the task page</p>"]}, "correct_response": ["b"], "section": "Production Pipelines", "question_plain": "A data engineering team has a multi-tasks Job in production. The team members need to be notified in the case of job failure.Which of the following approaches can be used to send emails to the team members in the case of job failure ?", "related_lectures": []}, {"_class": "assessment", "id": 67214788, "assessment_type": "multiple-choice", "prompt": {"question": "<p>For production jobs, which of the following cluster types is recommended to use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Job Clusters are dedicated clusters for a job or task run. A job cluster auto terminates once the job is completed, which saves cost compared to all-purpose clusters.</p><p>In addition, Databricks recommends using job clusters in production so that each job runs in a fully isolated environment.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job\">https://docs.databricks.com/workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p>", "answers": ["<p>All-purpose clusters</p>", "<p>Production clusters</p>", "<p>Job clusters</p>", "<p>On-premises clusters</p>", "<p>Serverless clusters</p>"]}, "correct_response": ["c"], "section": "Production Pipelines", "question_plain": "For production jobs, which of the following cluster types is recommended to use?", "related_lectures": []}, {"_class": "assessment", "id": 67214790, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In Databricks Jobs, which of the following approaches can a data engineer use to configure a linear dependency between <strong>Task A</strong> and <strong>Task B</strong> ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can define the order of execution of tasks in a job using the <strong>Depends on</strong> dropdown menu. You can set this field to one or more tasks in the job.</p><p><br></p><p><img src=\"https://lh3.googleusercontent.com/Z-rbWFByd6PQSCO0giUlD3Hy3lxyJ5gn-epJ4yN0BDAVSokRS9SzautfF92o7c-fX5ejxMzxeH-vuEbph3z5Uw6Drh8GnGcq3D80GlMUu7_gi_JOUhxF14ioRFjBkuhDnyahRxX9xGS_O-PJ2P4PnVw2UdPcuzT1rPlAINiiJeyDZYF1mi7FdSsYLtU7DA\"></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/workflows/jobs/jobs.html#task-dependencies\">https://docs.databricks.com/workflows/jobs/jobs.html#task-dependencies</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34680270/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p><p><br></p>", "answers": ["<p>They can select the Task A in the Depends On field of the Task B configuration</p>", "<p>They can assign Task A an Order number of 1, and assign Task B an Order number of 2</p>", "<p>They can visually drag and drop an arrow from Task A to Task B in the Job canvas</p>", "<p>They can configure the dependency at the notebook level using the dbutils.jobs utility </p>", "<p>Databricks Jobs do not support linear dependency between tasks. This can only be achieved in Delta Live Tables pipelines</p>"]}, "correct_response": ["a"], "section": "Production Pipelines", "question_plain": "In Databricks Jobs, which of the following approaches can a data engineer use to configure a linear dependency between Task A and Task B ?", "related_lectures": []}, {"_class": "assessment", "id": 67214732, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which part of the Databricks Platform can a data engineer use to revoke permissions from users on tables ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Data Explorer in Databricks SQL allows you to manage data object permissions. This includes revoking privileges on tables and databases from users or groups of users.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/data-acl.html#data-explorer\">https://docs.databricks.com/security/access-control/data-acl.html#data-explorer</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>Data Explorer</p>", "<p>Cluster event log</p>", "<p>Workspace Admin Console</p>", "<p>DBFS</p>", "<p>There is no way to revoke permissions in Databricks platform. The data engineer needs to clone the table with the updated permissions</p>"]}, "correct_response": ["a"], "section": "Data Governance", "question_plain": "Which part of the Databricks Platform can a data engineer use to revoke permissions from users on tables ?", "related_lectures": []}, {"_class": "assessment", "id": 67214792, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data engineer uses the following SQL query:</p><p><br></p><p><code>GRANT USAGE ON DATABASE sales_db TO finance_team</code></p><p><br></p><p>Which of the following is the benefit of the <strong>USAGE</strong>&nbsp; privilege ?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The <code>USAGE</code> does not give any abilities, but it's an additional requirement to perform any action on a schema (database) object.</p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges\">https://docs.databricks.com/security/access-control/table-acls/object-privileges.html#privileges</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34779100/?referralCode=F0FA48E9A0546C975F14\">Lecture</a></p></li><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul><p><br></p>", "answers": ["<p>Gives read access on the database</p>", "<p>Gives full permissions on the entire database</p>", "<p>Gives the ability to view database objects and their metadata</p>", "<p>No effect! but it's required to perform any action on the database</p>", "<p>USAGE privilege is not part of the Databricks governance model</p>"]}, "correct_response": ["d"], "section": "Data Governance", "question_plain": "A data engineer uses the following SQL query:GRANT USAGE ON DATABASE sales_db TO finance_teamWhich of the following is the benefit of the USAGE&nbsp; privilege ?", "related_lectures": []}, {"_class": "assessment", "id": 67214794, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In which of the following locations can a data engineer change the owner of a table?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>From Data Explorer in Databricks SQL, you can navigate to the table's page to review and change the owner of the table. Simply, click on the Owner field, then <strong>Edit owner</strong> to set the new owner.</p><p><br></p><p><img src=\"https://lh5.googleusercontent.com/ioKuLryVukyrL6sb1RDpCCXXCuZO73SkDKfvH1hIonmwO_QjA2FTDxR3fWamOom9jd31luxJm44xmrGodhZDnegfk4NH4-looufhkg8mb6mxvctENxurcAITTvsIRthUQs-Z61Y6TOuJyoKDkSGg2tUje5vkBPqc0IR8qdB3JoWwkt8MF0yguIsIhd2nZw\"></p><p><br></p><p><br></p><p>Reference: <a href=\"https://docs.databricks.com/security/access-control/data-acl.html#manage-data-object-ownership\">https://docs.databricks.com/security/access-control/data-acl.html#manage-data-object-ownership</a></p><p><br></p><p>Study materials from our exam preparation course on Udemy:</p><ul><li><p><a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/learn/lecture/34807658/?referralCode=F0FA48E9A0546C975F14\">Hands-on</a></p></li></ul>", "answers": ["<p>In DBFS, from the properties tab of the table\u2019s data files</p>", "<p>In Data Explorer, under the Permissions tab of the table's page</p>", "<p>In Data Explorer, from the Owner field in the table's page </p>", "<p>In Data Explorer, under the Permissions tab of the database's page, since owners are set at database-level</p>", "<p>In Data Explorer, from the Owner field in the database's page, since owners are set at database-level</p>"]}, "correct_response": ["c"], "section": "Data Governance", "question_plain": "In which of the following locations can a data engineer change the owner of a table?", "related_lectures": []}]}
