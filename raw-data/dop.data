4724020
~~~
{'count': 45, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 67357112, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.</p>\n\n<p>How should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</strong></p>\n\n<p>For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the <code>ConcurrentExecutions</code> metric shows you the number of concurrent invocations for each function in your account.</p>\n\n<p>When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs. For example, if your function usually peaks at 200 concurrent requests, set your provisioned concurrency at 220 instead (200 concurrent requests + 10% = 220 provisioned concurrency).</p>\n\n<p>You can alleviate cold start issues by configuring the optimum provisioned concurrency for the Lambda function.</p>\n\n<p>Optimizing latency with provisioned concurrency:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Accurately estimating required provisioned concurrency:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</strong> - Reserved concurrency is the maximum number of concurrent instances you want to allocate to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is well suited for use cases where traffic is constant and predictable over the day.</p>\n\n<p><strong>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</strong> - For data-intensive applications such as machine learning inference, and financial computations, customers often need to read and write large amounts of data to ephemeral storage. The Lambda execution environment provides an ephemeral file system for customers’ code to access via /tmp space, which is preserved for the lifetime of the execution environment, and can provide a transient cache for data between invocations. This option acts as a distractor.</p>\n\n<p><strong>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</strong> - A Lambda function layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. This option is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency">https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency</a></p>\n\n<p><a href="https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/">https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/</a></p>\n', 'answers': ['<p>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</p>', '<p>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</p>', '<p>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</p>', '<p>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.\n\nHow should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357100, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it\'s possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n', 'answers': ['<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>', '<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>', '<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>', '<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>']}, 'correct_response': ['d'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.\n\nAs a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357102, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A DevOps Engineer needs to use the AWS CloudFormation stack to deploy an application. But the DevOps Engineer does not have the required permissions to provision the resources specified in the AWS CloudFormation template.</p>\n\n<p>Which solution will allow the DevOps Engineer to deploy the stack while providing the least privileges possible?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong></p>\n\n<p>A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf. You can specify an IAM role that allows AWS CloudFormation to create, update, or delete your stack resources. By default, AWS CloudFormation uses a temporary session that it generates from your user credentials for stack operations. If you specify a service role, AWS CloudFormation uses that role\'s credentials.</p>\n\n<p>Use a service role to explicitly specify the actions that AWS CloudFormation can perform, which might not always be the same actions that you or other users can do. For example, you might have administrative privileges, but you can limit AWS CloudFormation access to only Amazon EC2 actions.</p>\n\n<p>When you specify a service role, AWS CloudFormation always uses that role for all operations that are performed on that stack. It is not possible to remove a service role attached to a stack after the stack is created. Other users that have permission to perform operations on this stack will be able to use this role, but they must have the <code>iam:PassRole</code> permission.</p>\n\n<p>To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant PassRole permission to the user\'s IAM user, role, or group.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q3-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</strong> - Giving full permissions is against the security best practice that says a role should have the least possible privileges to complete the action needed. AWS suggests not using the ResourceTag condition key in a policy with the <code>iam:PassRole</code> action. You cannot use the tag on an IAM role to control access to who can pass that role.</p>\n\n<p><strong>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong> - This statement is incorrect. AWS suggests not using the <code>aws:SourceIp</code> AWS-wide condition. AWS CloudFormation provisions resources by using its own IP address, not the IP address of the originating request. For example, when you create a stack, AWS CloudFormation makes requests from its IP address to launch an Amazon EC2 instance or to create an Amazon S3 bucket, not from the IP address from the CreateStack call or the AWS CloudFormation create-stack command.</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</strong> - To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. To allow a user to pass a role to an AWS service, you must grant the PassRole permission to the user\'s IAM user, role, or group. Granting the developer <code>iam:PassRole</code> permissions to pass the role to the service is a necessary step if the developers have to deploy the CloudFormation stacks.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n', 'answers': ['<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</p>', '<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>', '<p>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>', '<p>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</p>']}, 'correct_response': ['b'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A DevOps Engineer needs to use the AWS CloudFormation stack to deploy an application. But the DevOps Engineer does not have the required permissions to provision the resources specified in the AWS CloudFormation template.\n\nWhich solution will allow the DevOps Engineer to deploy the stack while providing the least privileges possible?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357108, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A production environment has Amazon EC2 instances configured to log all application/system logs via the CloudWatch Logs agent that has been configured on all instances. The company has recently introduced a security policy that mandates terminating any Amazon EC2 instance accessed manually by a user other than the administrators within an hour. All the production instances are configured with Auto Scaling groups.</p>\n\n<p>As a DevOps Engineer, how will you automate this process?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong></p>\n\n<p>You can use CloudWatch Log subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p>\n\n<p>For the given use case, you can invoke a Lambda function via the CloudWatch Logs subscription. The Lambda then adds a decommission tag to the EC2 instance that produced the login event. Lastly, you can set up an Amazon EventBridge rule to invoke another Lambda function on an hourly schedule, to terminate all EC2 instances that are marked with the decommission tag.</p>\n\n<p>An Amazon EventBridge rule matches incoming events and sends them to targets for processing. A rule can run in response to an event, or at certain time intervals. For example, to periodically run an AWS Lambda function, you can create a rule to run on a schedule.</p>\n\n<p>In EventBridge, you can create two types of scheduled rules:</p>\n\n<ol>\n<li>Rules that run at a regular rate</li>\n</ol>\n\n<p>EventBridge runs these rules at regular intervals; for example, every 20 minutes.</p>\n\n<p>To specify the rate for a scheduled rule, you define a rate expression.</p>\n\n<ol>\n<li>Rules that run at specific times</li>\n</ol>\n\n<p>EventBridge runs these rules at specific times and dates; for example, 8:00 a.m. PST on the first Monday of every month.</p>\n\n<p>To specify the time and dates a scheduled rule runs, you define a cron expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</strong> - This option does not provide a solution for complete automation since system administrators are expected to terminate the systems manually.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong> - Amazon CloudWatch alarm does not support having AWS Lambda function as an alarm action. Amazon Simple Notification Service can be used as an intermediatory service if you wish to use AWS Lambda with CloudWatch alarms.</p>\n\n<p><strong>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with the AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</strong> - AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Function is overkill for the simple automation via the AWS Lambda function that the given use case needs.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n', 'answers': ['<p>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</p>', '<p>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>', '<p>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>', '<p>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</p>']}, 'correct_response': ['c'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A production environment has Amazon EC2 instances configured to log all application/system logs via the CloudWatch Logs agent that has been configured on all instances. The company has recently introduced a security policy that mandates terminating any Amazon EC2 instance accessed manually by a user other than the administrators within an hour. All the production instances are configured with Auto Scaling groups.\n\nAs a DevOps Engineer, how will you automate this process?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357106, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A production support team manages a web application running on a fleet of Amazon EC2 instances configured with an Application Load balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A critical bug fix has to be deployed to the production application. The team needs a deployment strategy that can:</p>\n\n<p>a) Create another fleet of instances with the same capacity and configuration as the original one.\nb) Continue access to the original application without a downtime\nc) Transition the traffic to the new fleet when the deployment is fully done. The production test team has requested a two-hour window to complete thorough testing on the new fleet of instances.\nd) Terminate the original fleet automatically once the test window expires.</p>\n\n<p>As a DevOps engineer, which deployment solution will you choose to cater to all the given requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</strong></p>\n\n<p>Traditional deployments with in-place upgrades make it difficult to validate your new application version in a production deployment while also continuing to run the earlier version of the application. Blue/Green deployments provide a level of isolation between your blue and green application environments. This helps ensure spinning up a parallel green environment does not affect the resources underpinning your blue environment. This isolation reduces your deployment risk.</p>\n\n<p>After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime and limiting the blast radius of impact.</p>\n\n<p>This ability to simply roll traffic back to the operational environment is a key benefit of Blue/Green deployments. You can roll back to the blue environment at any time during the deployment process. Impaired operation or downtime is minimized because the impact is limited to the window of time between green environment issue detection and the shift of traffic back to the blue environment. Additionally, the impact is limited to the portion of traffic going to the green environment, not all traffic. If the blast radius of deployment errors is reduced, so is the overall deployment risk.</p>\n\n<p>In AWS CodeDeploy Blue/Green deployment type, for deployment groups that contain more than one instance, the overall deployment succeeds if the application revision is deployed to all of the instances. The exception to this rule is that if deployment to the last instance fails, the overall deployment still succeeds. This is because CodeDeploy allows only one instance at a time to be taken offline with the CodeDeployDefault.OneAtATime configuration (If you don\'t specify a deployment configuration, CodeDeploy uses the CodeDeployDefault.OneAtATime deployment configuration).</p>\n\n<p>If you choose <code>Terminate the original instances in the deployment group</code>: After traffic is rerouted to the replacement environment, the instances that were deregistered from the load balancer are terminated following the wait period you specify.</p>\n\n<p>Blue/Green deployment example:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q5-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirming the DNS changes of the new environment have propagated correctly</strong></p>\n\n<p><strong>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment\'s Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment\'s instances without downtime</strong></p>\n\n<p>These two options use AWS Elastic Beanstalk, which creates a new environment and not a new fleet of EC2 instances, as needed in the use case. Hence, these are incorrect.</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</strong> - Deregistering the original fleet of instances one at a time is not possible. After traffic is successfully routed to the replacement environment, instances in the original environment are deregistered all at once no matter which deployment configuration is selected.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n', 'answers': ['<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</p>', '<p>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirm the DNS changes of the new environment have propagated correctly</p>', "<p>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</p>", "<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to 'Terminate the original instances in the deployment group' and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</p>"]}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A production support team manages a web application running on a fleet of Amazon EC2 instances configured with an Application Load balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A critical bug fix has to be deployed to the production application. The team needs a deployment strategy that can:\n\na) Create another fleet of instances with the same capacity and configuration as the original one.\nb) Continue access to the original application without a downtime\nc) Transition the traffic to the new fleet when the deployment is fully done. The production test team has requested a two-hour window to complete thorough testing on the new fleet of instances.\nd) Terminate the original fleet automatically once the test window expires.\n\nAs a DevOps engineer, which deployment solution will you choose to cater to all the given requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357104, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.</p>\n\n<p>Which of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</strong></p>\n\n<p>Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets. However, each log object reports access log records for a specific source bucket.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage and you can start analyzing your data immediately. Athena uses an approach known as schema-on-read, which allows you to project your schema onto your data at the time you execute a query. This eliminates the need for any data loading or ETL.</p>\n\n<p>Amazon Athena:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg">\n<a href="https://aws.amazon.com/athena/">https://aws.amazon.com/athena/</a></p>\n\n<p><code>CREATE EXTERNAL TABLE [IF NOT EXISTS]</code> syntax is used to create the table in Athena. EXTERNAL keyword specifies that the table is based on an underlying data file that exists in Amazon S3, in the location that you specify.</p>\n\n<p>You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests.</p>\n\n<p>Analysing object access requests using SQL queries in Athena:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</strong> - The request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> will give the overall count of all Get requests and Bytes downloaded through these requests. It does not help in answering the access pattern questions, such as, which files have been viewed/downloaded the most.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Amazon Redshift Spectrum has a dependency on provisioned Amazon Redshift servers, which represents a cost-intensive solution. Therefore, Amazon Redshift Spectrum is a cost overkill for the simple access pattern analysis that the current use case needs.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</strong> -  Amazon OpenSearch is a managed service that makes it easier to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. The service provides support for open-source Elasticsearch APIs, managed Kibana, and integration with other AWS services such as Amazon S3 and Amazon Kinesis for loading streaming data into Amazon ES. OpenSearch is well-suited for real-time analysis of logs or clickstreams. This is not a cost-effective solution as it uses AWS Lambda, Kinesis Data Firehose, and Amazon OpenSearch services for the simple access pattern analysis that the current use case needs.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/athena/">https://aws.amazon.com/athena/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics">https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics</a></p>\n\n<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/">https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/</a></p>\n', 'answers': ['<p>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</p>', '<p>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</p>', '<p>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</p>', '<p>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</p>']}, 'correct_response': ['c'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.\n\nWhich of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357114, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A company is implementing AWS serverless architecture with Amazon API Gateway, AWS Lambda, and Amazon DynamoDB services. The company's existing users are primarily located in Europe and Asia-Pacific regions. The company is now looking for a quick-start solution that offers high reliability and low latency for a global user base across regions as its offerings are getting popular worldwide.</p>\n\n<p>How will you implement this requirement?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong></p>\n\n<p>Route 53: If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you\'ve created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Amazon Route 53 latency-based routing will reduce latency by serving the user from the fastest available route between the two AWS regions.</p>\n\n<p>You can use Amazon Route 53 health checks to control DNS failover from an API Gateway API in a primary AWS Region to one in a secondary Region. This can help mitigate impacts in the event of a Regional issue.</p>\n\n<p>AWS Lambda provides easy scaling and high availability to your serverless architecture without additional effort on your part.</p>\n\n<p>Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance.</p>\n\n<p>A DynamoDB global table is comprised of multiple replica tables. Each replica table exists in a different Region, but all replicas have the same name and primary key. When data is written to any replica table, DynamoDB automatically replicates that data to all other replica tables in the global table. Global tables enable the users of your application to have low-latency access to the data no matter where they are located. In the unlikely event that one AWS Region was to become temporarily unavailable, your customers can still access the replica tables in the other Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - Route 53 geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. While using geolocation routing can help you localize your content and present some or all of your website in the language of your users, it is not suitable to serve content with the lowest latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - While failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy, it is not suitable when the key requirement is to serve content with lowest possible latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - AWS Global Accelerator can be used in conjunction with the Amazon API Gateway to present Internet-facing API via static IP addresses to end users. This design addresses the need for static IP safe listing, however, it is not useful for the given requirements.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html</a></p>\n\n<p><a href="https://aws.amazon.com/dynamodb/global-tables/">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/">https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/</a></p>\n', 'answers': ['<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>', '<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>', '<p>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>', '<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>']}, 'correct_response': ['d'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': "A company is implementing AWS serverless architecture with Amazon API Gateway, AWS Lambda, and Amazon DynamoDB services. The company's existing users are primarily located in Europe and Asia-Pacific regions. The company is now looking for a quick-start solution that offers high reliability and low latency for a global user base across regions as its offerings are getting popular worldwide.\n\nHow will you implement this requirement?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357098, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.</p>\n\n<p>How will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong></p>\n\n<p><strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong></p>\n\n<p>You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.</p>\n\n<p>Execution role and user permissions: If the file system doesn\'t have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function\'s execution role must have the correct <code>elasticfilesystem</code> permissions.</p>\n\n<p>Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.</p>\n\n<p>You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions.</p>\n\n<p>Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.</p>\n\n<p>A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.</p>\n\n<p>An example showcasing the use of EFS with AWS Lambda:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/">https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS.</p>\n\n<p><strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda.</p>\n\n<p><strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html">https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html</a></p>\n', 'answers': ['<p>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</p>', '<p>Update the Lambda execution roles with permission to access the VPC and the EFS file system</p>', '<p>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</p>', '<p>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</p>', '<p>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region, or cross-AZ connectivity between EFS and Lambda</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.\n\nHow will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357096, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A web application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). While using CodeDeploy Blue/Green deployment to deploy a new version, the deployment failed during the <code>AllowTraffic</code> lifecycle event. The DevOps team has found no errors in the deployment logs.</p>\n\n<p>Which of the following would you identify as the root cause behind the failure of the deployment?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</strong></p>\n\n<p>In some cases, a Blue/Green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure.</p>\n\n<p>This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group.</p>\n\n<p>To resolve the issue, review and correct any errors in the health check configuration for the load balancer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</strong> - During a deployment, the CodeDeploy agent runs the scripts specified for ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic in the AppSpec file from the previous successful deployment. (All other scripts are run from the AppSpec file in the current deployment.) If one of these scripts contains an error and does not run successfully, the deployment can fail. This kind of failure generates logs and will not fail at <code>AllowTraffic</code> lifecycle event.</p>\n\n<p><strong>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</strong> - This statement is incorrect. Deployments do not fail for up to an hour when an instance is terminated during a deployment.</p>\n\n<p><strong>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</strong> - During an in-progress deployment, a scale-in event or any other termination event causes the instance to detach from the Amazon EC2 Auto Scaling group and then terminate. Because the deployment cannot be completed, it fails. This error is specific to Auto Scaling Groups (ASG) and hence does not relate to the given issue.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html</a></p>\n', 'answers': ['<p>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</p>', '<p>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</p>', '<p>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</p>', '<p>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</p>']}, 'correct_response': ['d'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A web application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). While using CodeDeploy Blue/Green deployment to deploy a new version, the deployment failed during the AllowTraffic lifecycle event. The DevOps team has found no errors in the deployment logs.\n\nWhich of the following would you identify as the root cause behind the failure of the deployment?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357184, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.</p>\n\n<p>Which of the following represents the most optimal solution to automate the application deployment to different AWS regions?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</strong></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p>Here are the relevant CloudFormation concepts:</p>\n\n<p>Template: A CloudFormation template is a JSON or YAML formatted text file. You can save these files with any extension, such as .json, .yaml, .template, or .txt. CloudFormation uses these templates as blueprints for building your AWS resources.</p>\n\n<p>Stack: When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack\'s CloudFormation template.</p>\n\n<p>Change set: If you need to make changes to the running resources in a stack, you update the stack. Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. Change sets allow you to see how your changes might impact your running resources, especially critical resources, before implementing them.</p>\n\n<p>Stack set: A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set\'s CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires.</p>\n\n<p>After you\'ve defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify. When you create, update, or delete stacks, you can also specify operation preferences, such as the order of Regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. A stack set is a regional resource. If you create a stack set in one AWS Region, you can\'t see it or change it in other Regions.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</strong> - You can create a stack using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. The <code>aws cloudformation create-stack</code> command supports only a <code>region</code> parameter for a single region and not <code>regions</code> parameter for multiple regions.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i3.jpg">\nvia - <a href="https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/">https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/</a></p>\n\n<p><strong>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</strong> - Creating a script is not an elegant solution to provision the same infrastructure in multiple AWS regions since this functionality is directly offered by AWS in the form of stack sets.</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You cannot use change sets to deploy an application to multiple AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n', 'answers': ['<p>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</p>', '<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</p>', '<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</p>', '<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</p>']}, 'correct_response': ['b'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.\n\nWhich of the following represents the most optimal solution to automate the application deployment to different AWS regions?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357110, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A developer has uploaded an object of size 100 MB to an Amazon S3 bucket as a single-part direct upload using the REST API that has checksum enabled. The checksum of the object uploaded via the REST API was the checksum of the entire object. Later that day, the developer used the AWS Management Console to rename the object, copy it and edit its metadata. Later, when the developer checked for the checksum of the object updated via the AWS Management Console, the checksum was not the checksum of the entire object. Confused by the behavior, the developer has reached out to you for a possible solution.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, which of the following options would you identify as the reason for this behavior?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>A new checksum value for the object that is calculated based on the checksum values of the individual parts has been created. This behavior is expected</strong></p>\n\n<p>When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part.</p>\n\n<p>For example, consider an object 100 MB in size that you uploaded as a single-part direct upload using the REST API. The checksum in this case is a checksum of the entire object. If you later use the console to rename that object, copy it, change the storage class, or edit the metadata, Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer\'s initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB (NOT 50 MB) in size.</p>\n\n<p><strong>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</strong> - The checksum algorithm does not change when you change the metadata of the S3 object, so this option is incorrect.</p>\n\n<p><strong>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer\'s initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. On the other hand, when you upload an object as a single-part direct upload using the REST API, the checksum in this case is a checksum of the entire object. So you can say that the developer\'s initial calculation for the REST API based checksum was indeed correct.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n', 'answers': ["<p>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>", '<p>A new checksum value for the object, that is calculated based on the checksum values of the individual parts, has been created. This behavior is expected</p>', "<p>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>", '<p>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</p>']}, 'correct_response': ['b'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A developer has uploaded an object of size 100 MB to an Amazon S3 bucket as a single-part direct upload using the REST API that has checksum enabled. The checksum of the object uploaded via the REST API was the checksum of the entire object. Later that day, the developer used the AWS Management Console to rename the object, copy it and edit its metadata. Later, when the developer checked for the checksum of the object updated via the AWS Management Console, the checksum was not the checksum of the entire object. Confused by the behavior, the developer has reached out to you for a possible solution.\n\nAs an AWS Certified DevOps Engineer - Professional, which of the following options would you identify as the reason for this behavior?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357116, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).</p>\n\n<p>What combination of steps will you take to configure this requirement? (Select three)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A\'s CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</strong></p>\n\n<p><strong>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A\'s CodePipeline service role to allow it to assume the cross-account role in account B</strong></p>\n\n<p><strong>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</strong></p>\n\n<p>Complete list of steps for configuring the requirement:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A\'s CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</strong></p>\n\n<p><strong>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</strong></p>\n\n<p><strong>In account B, add the <code>AssumeRole</code> permission to account A\'s CodePipeline service role to allow it to assume the cross-account role in account A</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n', 'answers': ["<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</p>", "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</p>", "<p>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</p>", '<p>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</p>', "<p>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</p>", '<p>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</p>']}, 'correct_response': ['a', 'c', 'f'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).\n\nWhat combination of steps will you take to configure this requirement? (Select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357118, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>A project has two AWS accounts, a development account and a production account, in the us-east-1 Region. A DevOps engineer has to deploy artifacts from the development account's S3 bucket to the production account's S3 bucket using AWS CodePipeline with Amazon S3 deploy action.</p>\n\n<p>What configurations are mandatory for this cross-account deployment? (Select two)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</strong></p>\n\n<p>You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn\'t configured, then CodePipeline encrypts the objects with default encryption, which can\'t be decrypted by the role in the destination account.</p>\n\n<p>The input bucket must have versioning activated to work with CodePipeline.</p>\n\n<p><strong>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</strong></p>\n\n<p>In the deploy action, the CodePipeline service role assumes the cross-account role in the production account. CodePipeline uses the cross-account role to access the KMS key and artifact bucket in the development account. Then, CodePipeline deploys the extracted files to the production output S3 bucket in the production account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create Secrets Manager key to use with CodePipeline in the development account</strong> - This option is not relevant for the given use case.</p>\n\n<p><strong>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p><strong>You need to use an AWS KMS multi-Region key with multiple replicas</strong> - This option has been added as a distractor. Only when the production account\'s Region is different than your pipeline\'s Region, then you must also use an AWS KMS multi-Region key with multiple replicas.</p>\n\n<p>References:</p>\n\n<p><a href="https://repost.aws/knowledge-center/codepipeline-artifacts-s3">https://repost.aws/knowledge-center/codepipeline-artifacts-s3</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html</a></p>\n', 'answers': ['<p>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</p>', '<p>Create Secrets Manager key to use with CodePipeline in the development account</p>', '<p>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</p>', '<p>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</p>', '<p>You need to use an AWS KMS multi-Region key with multiple replicas</p>']}, 'correct_response': ['a', 'c'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "A project has two AWS accounts, a development account and a production account, in the us-east-1 Region. A DevOps engineer has to deploy artifacts from the development account's S3 bucket to the production account's S3 bucket using AWS CodePipeline with Amazon S3 deploy action.\n\nWhat configurations are mandatory for this cross-account deployment? (Select two)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357120, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company hosts all its web applications on Amazon EC2 instances. The company is looking for a security solution that will proactively detect software vulnerabilities and unintended network exposure of the instances. The solution should also include an audit trail of all login activities on the instances.</p>\n\n<p>Which solution will meet these requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</strong></p>\n\n<p>Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Amazon Inspector uses SSM Agent to collect application inventory, which can be set up as Amazon Virtual Private Cloud (VPC) endpoints to avoid sending information over the internet.</p>\n\n<p>You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.</p>\n\n<p>CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time.</p>\n\n<p>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs.</p>\n\n<p>How Amazon Inspector Works:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q13-i1.jpg">\nvia - <a href="https://aws.amazon.com/inspector/">https://aws.amazon.com/inspector/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Systems Manager\'s Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</strong> - SSM agent does not scan or detect vulnerabilities on Amazon EC2 instances.</p>\n\n<p><strong>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</strong> - Amazon ECR provides basic scanning type which uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. It is for container applications alone. Also, ECR image scanning identifies software vulnerabilities only in operating system packages.</p>\n\n<p><strong>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, Amazon Elastic Compute Cloud (EC2) workloads, container applications, Amazon Aurora databases, and data stored in Amazon Simple Storage Service (S3). It is not a vulnerability management service.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html">https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html</a></p>\n\n<p><a href="https://aws.amazon.com/inspector/faqs/">https://aws.amazon.com/inspector/faqs/</a></p>\n', 'answers': ["<p>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</p>", '<p>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</p>', '<p>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</p>', '<p>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</p>']}, 'correct_response': ['c'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A company hosts all its web applications on Amazon EC2 instances. The company is looking for a security solution that will proactively detect software vulnerabilities and unintended network exposure of the instances. The solution should also include an audit trail of all login activities on the instances.\n\nWhich solution will meet these requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357122, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n', 'answers': ['<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>', '<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>', '<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>', '<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.\n\nRecently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.\n\nHow should a DevOps engineer configure this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357124, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).</p>\n\n<p>Which step did the developer possibly miss for the successful completion of the CloudFormation stack?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</strong></p>\n\n<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p>\n\n<p>The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.</p>\n\n<p>How custom resources work:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</strong> - You can use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. However, this has no bearing on the given use case.</p>\n\n<p><strong>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</strong> - This statement is incorrect. The template developer and custom resource provider can be the same person or entity.</p>\n\n<p><strong>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</strong> - The <code>cfn-response</code> module is available only when you use the ZipFile property to write your source code. This is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p>\n', 'answers': ['<p>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</p>', '<p>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</p>', '<p>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</p>', '<p>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).\n\nWhich step did the developer possibly miss for the successful completion of the CloudFormation stack?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357126, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A DevOps Engineer has been asked to chalk out a disaster recovery (DR) plan for a workload in production. The workload runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are configured with an Auto Scaling group across multiple Availability Zones. Amazon Route 53 is configured to point to the ALB using an alias record. Amazon RDS for PostgreSQL DB instance is the database service. The draft DR plan mandates an RTO of three hours and an RPO of around 15 minutes.</p>\n\n<p>Which Disaster Recovery (DR) strategy should the DevOps Engineer opt for a cost-effective solution?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</strong></p>\n\n<p>With the pilot light approach, you replicate your data from one Region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements, such as application servers, are loaded with application code and configurations, but are "switched off" and are only used during testing or when disaster recovery failover is invoked. In the cloud, you have the flexibility to de-provision resources when you do not need them, and provision them when you do. A best practice for “switched off” is to not deploy the resource, and then create the configuration and capabilities to deploy it (“switch on”) when needed. Unlike the backup and restore approach, your core infrastructure is always available and you always have the option to quickly provision a full-scale production environment by switching on and scaling out your application servers.</p>\n\n<p>A pilot light approach minimizes the ongoing cost of disaster recovery by minimizing the active resources and simplifies recovery at the time of a disaster because the core infrastructure requirements are all in place.</p>\n\n<p>For pilot light, continuous data replication to live databases and data stores in the DR region is the best approach for low RPO. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance.</p>\n\n<p>For an active/passive configuration such as the pilot light, all traffic initially goes to the primary Region and switches to the disaster recovery Region if the primary Region is no longer available. This failover operation can be initiated either automatically or manually.</p>\n\n<p>Using Amazon Route 53, you can associate multiple IP endpoints in one or more AWS Regions with a Route 53 domain name. Then, you can route traffic to the appropriate endpoint under that domain name. On failover you need to switch traffic to the recovery endpoint, and away from the primary endpoint. Amazon Route 53 health checks monitor these endpoints. Using these health checks, you can configure automatically initiated DNS failover to ensure traffic is sent only to healthy endpoints, which is a highly reliable operation done on the data plane.</p>\n\n<p>The pilot light strategy is the one best suited for the given requirements since it keeps the overall costs down when compared to the rest of the options. Also, the core infrastructure is ready to be used as and when required. Since RTO is given in hours, we can spin up these resources well before the given time. The database is update-to-date and only needs a switch from replica to primary. This is the right strategy when RPO is in minutes.</p>\n\n<p>Pilot light DR strategy:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q16-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</strong> - With pilot light DR strategy, resources required to support data replication and backup, such as databases and object storage, are always on. When the expected RPO is 15 minutes, it is not possible to apply incremental backups to the database since this could be huge amounts of data which may take hours. Hence, this option is incorrect.</p>\n\n<p><strong>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</strong> - You can run your workload simultaneously in multiple Regions as part of a multi-site active/active strategy. Multi-site active/active serves traffic from all regions to which it is deployed. With a multi-site active/active approach, users can access your workload in any of the Regions in which it is deployed. This approach is the most complex and costly approach to disaster recovery, but it can reduce your recovery time to near zero for most disasters with the correct technology choices and implementation. Since RTO is given as three hours, opting for this DR strategy will prove to be cost-ineffective.</p>\n\n<p><strong>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</strong> - The warm standby approach involves ensuring that there is a scaled-down, but fully functional, copy of your production environment in another Region. This approach extends the pilot light concept and decreases the time to recovery because your workload is always-on in another Region. Since RTO is given as three hours, the warm standby approach will end up being a costly alternative to an otherwise cost-effective pilot light strategy.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/">https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n', 'answers': ['<p>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</p>', '<p>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</p>', '<p>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</p>', '<p>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</p>']}, 'correct_response': ['b'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A DevOps Engineer has been asked to chalk out a disaster recovery (DR) plan for a workload in production. The workload runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are configured with an Auto Scaling group across multiple Availability Zones. Amazon Route 53 is configured to point to the ALB using an alias record. Amazon RDS for PostgreSQL DB instance is the database service. The draft DR plan mandates an RTO of three hours and an RPO of around 15 minutes.\n\nWhich Disaster Recovery (DR) strategy should the DevOps Engineer opt for a cost-effective solution?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357128, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.</p>\n\n<p>Which step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong></p>\n\n<p>Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability.</p>\n\n<p>By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption.</p>\n\n<p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements.</p>\n\n<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong></p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong></p>\n\n<p>Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect.</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster\'s reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html</a></p>\n', 'answers': ["<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</p>", '<p>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a  Multi-AZ cluster configuration</p>', '<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</p>', '<p>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</p>']}, 'correct_response': ['c'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.\n\nWhich step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357130, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</strong></p>\n\n<p>As per the security policy of the company, encrypt the unencrypted AMI using the KMS key. The encrypted snapshots must be encrypted with a KMS key. You can’t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key.</p>\n\n<p>Amazon EC2 Auto Scaling uses service-linked roles to delegate permissions to other AWS services. Amazon EC2 Auto Scaling service-linked roles are predefined and include permissions that Amazon EC2 Auto Scaling requires to call other AWS services on your behalf. The predefined permissions also include access to your AWS-managed keys. However, they do not include access to your customer-managed keys, allowing you to maintain full control over these keys.</p>\n\n<p>If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached)</p>\n\n<ol>\n<li><p>The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key.</p></li>\n<li><p>Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The <code>Grantee Principal</code> element of the grant is the ARN of the appropriate service-linked role. The <code>key-id</code> is the ARN of the key.</p></li>\n</ol>\n\n<p>Key policy sections that allow cross-account access to the customer-managed key:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n', 'answers': ['<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>', '<p>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</p>', '<p>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>', '<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</p>', '<p>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.\n\nAs a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357132, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn\'t support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n', 'answers': ['<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>', '<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>', '<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>', '<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>']}, 'correct_response': ['c'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.\n\nAs DevOps Engineer, how will you implement this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357134, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>An application runs on a fleet of Amazon EC2 Windows instances configured with an Auto Scaling group (ASG). When scaling-in takes place in the ASG, the instances are terminated without notification. The application team wants to create an AMI and remove the Amazon EC2 Windows instance from its domain before terminating the scaled-in instances.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you choose to implement this requirement? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</strong></p>\n\n<p><strong>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the instance from the domain and create an AMI of the EC2 instance</strong></p>\n\n<p>Oftentimes, you may want to execute some code and actions before terminating an Amazon Elastic Compute Cloud (Amazon EC2) instance that is part of an Amazon EC2 Auto Scaling group.</p>\n\n<p>One way to execute code and actions before terminating an instance is to create a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status. This allows you to perform any desired actions before immediately terminating the instance within the Auto Scaling group. The <code>Terminating:Wait</code> status can be monitored by an Amazon CloudWatch event, which triggers an AWS Systems Manager automation document to perform the action you want.</p>\n\n<p>Broadly, the steps needed for the above configuration:\n1. Add a lifecycle hook.\n2. Create a Systems Manager automation document.\n3. Create AWS Identity and Access Management (IAM) policies and a role to delegate permissions to the Systems Manager automation document.\n4. Create IAM policies and a role to delegate permissions to CloudWatch Events, which invokes the Systems Manager automation document.\n5. Create a CloudWatch Events rule.\n6. Add a Systems Manager automation document as a CloudWatch Event target.</p>\n\n<p>Using Lifecycle hooks to run code before terminating an EC2 Auto Scaling instance:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and set up an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</strong> - <code>Terminating:Pending</code> is not a valid state. The following are the transitions between instance states in the Amazon EC2 Auto Scaling lifecycle.</p>\n\n<p>Amazon EC2 Auto Scaling instance lifecycle:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a></p>\n\n<p><strong>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with both security-related updates and other types of updates. It cannot be used for the processing of custom logic/code.</p>\n\n<p><strong>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Maintenance Windows is a capability of AWS Systems Manager that helps you define a schedule for when to perform potentially disruptive actions on your nodes such as patching an operating system, updating drivers, or installing software or patches. Maintenance Windows is not relevant for the given use case, since we want the custom logic to run immediately and return to the instance termination triggered by Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html</a></p>\n', 'answers': ['<p>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</p>', '<p>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</p>', '<p>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>', '<p>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>', '<p>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>']}, 'correct_response': ['a', 'e'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'An application runs on a fleet of Amazon EC2 Windows instances configured with an Auto Scaling group (ASG). When scaling-in takes place in the ASG, the instances are terminated without notification. The application team wants to create an AMI and remove the Amazon EC2 Windows instance from its domain before terminating the scaled-in instances.\n\nAs a DevOps Engineer, which combination of steps will you choose to implement this requirement? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357136, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don\'t have the specified tag or that aren\'t included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a></p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n', 'answers': ['<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>', '<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>', '<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>', '<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>']}, 'correct_response': ['c'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.\n\nWhat should a DevOps engineer do to meet these requirements with the minimal overhead?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357138, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>An AWS managed <code>cloudformation-stack-drift-detection-check</code> rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:</p>\n\n<p>a) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'</p>\n\n<p>As a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p>The <code>cloudformation-stack-drift-detection-check</code> rule checks if the actual configuration of a Cloud Formation stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. The rule and the stack are COMPLIANT when the stack drift status is IN_SYNC. The rule is NON_COMPLIANT if the stack drift status is DRIFTED.</p>\n\n<p>CloudFormation offers a drift detection feature to detect unmanaged configuration changes to stacks and resources. This will let you take corrective action to put the stack resources back in sync with their definitions in the stack template. To return a resource to compliance, the resource definition changes can be reverted directly.</p>\n\n<p><strong>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</strong></p>\n\n<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code>. You receive a throttling or "Rate Exceeded" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.</p>\n\n<p>Resolve the error resulting from the availability of DetectStackDrift:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/config-cloudformation-drift-detection">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><strong>AWS CloudFormation does not support drift detection of custom resources</strong></p>\n\n<p>AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn\'t have sufficient service permissions</strong> - Any issues with permissions will not result in the shown error. Permissions error looks something like this: "Your stack drift detection operation for the specific stack has failed. Check your existing AWS CloudFormation role permissions and add the missing permissions."</p>\n\n<p><strong>This error is a false positive and can be ignored for this scenario</strong> - This option just acts as a distractor.</p>\n\n<p><strong>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</strong> - It is true that CloudFormation only determines drift for property values that are explicitly set, either through the stack template or by specifying template parameters. However, custom resources are not currently supported for drift.</p>\n\n<p>References:</p>\n\n<p>[[https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource](https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource)]</p>\n\n<p><a href="https://repost.aws/knowledge-center/config-cloudformation-drift-detection">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html</a></p>\n', 'answers': ['<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</p>', "<p>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</p>", '<p>This error is a false positive and can be ignored for this scenario</p>', '<p>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</p>', '<p>AWS CloudFormation does not support drift detection of custom resources</p>']}, 'correct_response': ['a', 'e'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "An AWS managed cloudformation-stack-drift-detection-check rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:\n\na) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'\n\nAs a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357140, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.</p>\n\n<p>Which combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong></p>\n\n<p><strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong></p>\n\n<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:</p>\n\n<ol>\n<li>Choose the remediation action you want to associate from a pre-populated list.</li>\n<li>Create your own custom remediation actions using AWS Systems Manager Automation documents.</li>\n</ol>\n\n<p>If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again.</p>\n\n<p>For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.</p>\n\n<p>Steps to set up Auto Remediation for s3-bucket-logging-enabled:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules.</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case.</p>\n\n<p><strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n', 'answers': ['<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></p>', '<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</p>', '<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</p>', '<p>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</p>', '<p>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</p>']}, 'correct_response': ['a', 'd'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.\n\nWhich combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357142, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.</p>\n\n<p>What steps will you take to rename the CloudFormation stack without deleting the resources created?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</strong></p>\n\n<p>With the DeletionPolicy attribute, you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use <code>Retain</code> for any resource. For resources that support snapshots, such as <code>AWS::EC2::Volume</code>, specify Snapshot to have CloudFormation create a snapshot before deleting the resource.</p>\n\n<p>These steps need to be followed:\n1. Launch a CloudFormation stack that deploys the necessary resources.\n2. Add a <code>Retain</code> attribute to the deletion policy of all the resources deployed by the stack.\n3. Delete the stack and verify that the resources are retained.\n4. Create a new stack and import the resources that were retained from the original stack. This stack is created with a new name.\n5. Remove the <code>Retain</code> attribute from the stack to revert to the original template.</p>\n\n<p>Refer to the example that walks through the process of retaining a single resource—a VPC—when changing the name of a CloudFormation stack:</p>\n\n<p>Process overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of S3 bucket and EC2 instance. Add a \'Snapshot\' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and \'Snapshot\' attribute from the stack to revert to the original template</strong> - \'Snapshot\' attribute will still delete the resource after taking its snapshot. So this option is incorrect.</p>\n\n<p><strong>Use the CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</strong> - A hook is an executable custom logic that automatically inspects resources before they\'re provisioned. Hooks can inspect the resources that CloudFormation is about to provision. If a hook finds any resource that doesn\'t comply with your organizational guidelines, it can prevent CloudFormation from continuing the provisioning process. Any hook based logic is not useful for the given use case.</p>\n\n<p><strong>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both stacks while retaining the resources</strong> - This option just acts as a distractor, as it does not solve the given issue.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html">https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html</a></p>\n', 'answers': ["<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of the S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</p>", '<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</p>', '<p>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, the <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both the stacks while retaining the resources</p>', '<p>Use CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</p>']}, 'correct_response': ['b'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.\n\nWhat steps will you take to rename the CloudFormation stack without deleting the resources created?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357144, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>An application runs on a fleet of Amazon EC2 instances that are configured with an Auto Scaling group (ASG). Both Spot and On-Demand instances are utilized as per the ASG configuration. For the most part, the ASG seems to be working fine as expected. There are a few issues that the DevOps team has flagged:</p>\n\n<p>a) During a scale-in activity, ASG has terminated instance in the Availability Zone (AZ) that already had fewer instances than the other\nb) For some duration, ASG exceeded the specified maximum capacity of the group</p>\n\n<p>What reasons can you identify for this behavior? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</strong></p>\n\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application.</p>\n\n<p>Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the old ones, being at or near the specified maximum capacity could impede or completely stop rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity. The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either because of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the group.</p>\n\n<p><strong>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</strong></p>\n\n<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually. It also identifies which instances (within the identified purchase option) in which Availability Zones to terminate that will result in the Availability Zones being most balanced.</p>\n\n<p>Understanding ASG termination policy for mixed instances groups:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</strong> - Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used. As a result, you might encounter situations in which some newer instances are terminated before older instances. So this option is incorrect.</p>\n\n<p><strong>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</strong> - It is not possible to configure more than one AWS Lambda function in the termination policies for an Auto Scaling group. Also, the AWS Lambda function is used to configure custom termination policy and hence is not related to the given use case.</p>\n\n<p><strong>With the Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</strong> - This statement is incorrect. ASG uses multiple termination criteria before selecting an instance for termination, and not just the next billing hour as the only criterion.</p>\n\n<p>Understanding the default termination policy of ASG:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n', 'answers': ['<p>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</p>', '<p>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</p>', '<p>With Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</p>', '<p>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</p>', '<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</p>']}, 'correct_response': ['d', 'e'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'An application runs on a fleet of Amazon EC2 instances that are configured with an Auto Scaling group (ASG). Both Spot and On-Demand instances are utilized as per the ASG configuration. For the most part, the ASG seems to be working fine as expected. There are a few issues that the DevOps team has flagged:\n\na) During a scale-in activity, ASG has terminated instance in the Availability Zone (AZ) that already had fewer instances than the other\nb) For some duration, ASG exceeded the specified maximum capacity of the group\n\nWhat reasons can you identify for this behavior? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357146, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company implements access control by creating different policies for different job functions. These policies are attached to IAM roles/groups with minimum permissions necessary for the job function. Up until this point, the solution has been functioning effectively. However, with business expansion, the administrator has to frequently update the existing policies to allow access to new resources.</p>\n\n<p>Which of the following solutions can make the access control applicable to all new resources without the need to update the policies?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</strong></p>\n\n<p>The traditional authorization model used in IAM is called role-based access control (RBAC). RBAC defines permissions based on a person\'s job function, known outside of AWS as a role. Within AWS a role usually refers to an IAM role, which is an identity in IAM that you can assume.</p>\n\n<p>Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags. You can attach tags to IAM resources, including IAM entities (users or roles), and to AWS resources. You can create a single ABAC policy or a small set of policies for your IAM principals. These ABAC policies can be designed to allow operations when the principal\'s tag matches the resource tag. ABAC is helpful in environments that are growing rapidly and help with situations where policy management becomes cumbersome.</p>\n\n<p>Comparing ABAC to the traditional RBAC model:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q26-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</strong> - SCP is a policy that specifies the services and actions that users and roles can use in the accounts that the SCP affects. SCPs are similar to IAM permissions policies except that they don\'t grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account.</p>\n\n<p><strong>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</strong> - If you have to create a Resource-based policy for every new resource that is added to the account, then we are pretty much at the same place without offering any solution. Hence, this option is not a valid choice for the given use case.</p>\n\n<p><strong>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account.</p>\n\n<p>Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies.</p>\n\n<p>Neither ACL nor a session policy is the right choice for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session</a></p>\n', 'answers': ['<p>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</p>', '<p>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</p>', '<p>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</p>', '<p>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</p>']}, 'correct_response': ['c'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A company implements access control by creating different policies for different job functions. These policies are attached to IAM roles/groups with minimum permissions necessary for the job function. Up until this point, the solution has been functioning effectively. However, with business expansion, the administrator has to frequently update the existing policies to allow access to new resources.\n\nWhich of the following solutions can make the access control applicable to all new resources without the need to update the policies?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357148, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.</p>\n\n<p>As a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p>AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization’s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks.</p>\n\n<p>Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Policies can include WAF rules drawn from within the organization, and also those created by AWS Partners such as Imperva, F5, Trend Micro, and other AWS Marketplace vendors. This gives your security team the power to duplicate their existing on-premises security posture in the cloud.</p>\n\n<p>Firewall Manager has three prerequisites:</p>\n\n<ol>\n<li><p>AWS Organizations – Your organization must be using AWS Organizations to manage your accounts and all features must be enabled.</p></li>\n<li><p>Firewall Administrator – You must designate one of the AWS accounts in your organization as the administrator for Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.</p></li>\n<li><p>AWS Config – You must enable AWS Config for all of the accounts in the Organization so that Firewall Manager can detect newly created resources.</p></li>\n</ol>\n\n<p>Using AWS Firewall Manager to centrally manage your Web Application Portfolio:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/">https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Amazon GuardDuty can automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings. But, Amazon GuardDuty is a continuous security monitoring and threat detection service and not a security management service like AWS Firewall Manager. Even though GuardDuty can update web ACLs, it\'s a reaction to a threat. It cannot be used to proactively define rules for web ACLs across accounts to centrally manage the security infrastructure of an organization.</p>\n\n<p><strong>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Enabling AWS Config is a prerequisite for using AWS Firewall Manager. AWS Config can track the status of resources, but AWS Firewall Manager is needed for centrally managing the security infrastructure.</p>\n\n<p>AWS Firewall Manager and AWS Config:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg">\nvia - <a href="https://aws.amazon.com/firewall-manager/">https://aws.amazon.com/firewall-manager/</a></p>\n\n<p><strong>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - This option has been added as a distractor and is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n', 'answers': ['<p>Designate one of the AWS accounts in your organization as the administrator for Firewall Manager in AWS Organizations. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>', '<p>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>', '<p>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>', '<p>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.\n\nAs a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357150, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.</p>\n\n<p>As a DevOps Engineer, how will you implement this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</strong></p>\n\n<p>Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following:</p>\n\n<ol>\n<li>Check the incoming event for a specific string.</li>\n<li>Publish a message to Amazon SNS if the string in the event matches the string in the Lambda function.</li>\n</ol>\n\n<p>To use an AWS Lambda function to receive an email from SNS when any of your AWS Glue jobs fail a retry, do the following:\n1. Create an Amazon SNS topic.\n2. Create an AWS Lambda function.\n3. Create an Amazon EventBridge event that uses the Lambda function to initiate email notifications.</p>\n\n<p>AWS Lambda function logic:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg">\nvia - <a href="https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</strong> - Amazon EventBridge cannot be directly used without a Lambda function since the use case needs notifications only for Glue job retry failure. So this logic has to be included in a Lambda function.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</strong> - The use case is not about retrying the Glue job, but about sending an SNS notification when the retry of the job fails.</p>\n\n<p><strong>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</strong> - AWS Personal Health Dashboard provides proactive notifications of scheduled activities, such as any changes to the infrastructure powering your resources, enabling you to better plan for events that may affect you. This option is not relevant to the given requirements.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n', 'answers': ['<p>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</p>', '<p>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</p>', '<p>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</p>', '<p>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</p>']}, 'correct_response': ['d'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.\n\nAs a DevOps Engineer, how will you implement this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357152, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p>As a DevOps Engineer, what solution do you suggest to address the given requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations</strong></p>\n\n<p>With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries.</p>\n\n<p>Set up one or more AWS accounts as monitoring accounts and link them with multiple source accounts. A monitoring account is a central AWS account that can view and interact with observability data generated from source accounts. A source account is an individual AWS account that generates observability data for the resources that reside in it. Source accounts share their observability data with the monitoring account.</p>\n\n<p>The shared observability data can include the following types of telemetry:\n1. Metrics in Amazon CloudWatch\n2. Log groups in Amazon CloudWatch Logs\n3. Traces in AWS X-Ray</p>\n\n<p>There are two options for linking source accounts to your monitoring account. You can use one or both options.\n1. Use AWS Organizations to link accounts in an organization or organizational unit to the monitoring account.\n2. Connect individual AWS accounts to the monitoring account.</p>\n\n<p>AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements.</p>\n\n<p>Demonstration of setting up CloudWatch cross-account observability:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</strong> - While this option is correct, it cannot automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p><strong>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</strong> - Amazon EventBridge does not support Amazon S3 bucket as a target. Hence, this option is incorrect.</p>\n\n<p><strong>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. The use case does not talk about real-time data, so configuring CloudWatch Metric Streams with Firehose is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/">https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/</a></p>\n', 'answers': ['<p>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</p>', '<p>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</p>', '<p>Use Amazon CloudWatch cross-account observability to set up security and operations account as the monitoring account and link it with rest of the member accounts of the organization using AWS Organizations</p>', '<p>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</p>']}, 'correct_response': ['c'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.\n\nAs a DevOps Engineer, what solution do you suggest to address the given requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357154, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A DevOps Engineer is working on multiple applications that need to be configured for Application Auto scaling, however, there are some application constraints to be addressed:</p>\n\n<p>a) A serverless application is built on AWS Lambda with the following traffic pattern - The traffic for the application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nb) Another flagship application runs on Spot Fleet. The CPU utilization of the fleet has to stay at around 50 percent when the load on the application changes.</p>\n\n<p>Which of the following solutions can address these requirements? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</strong></p>\n\n<p>To create a target tracking scaling policy, you specify an Amazon CloudWatch metric and a target value that represents the ideal average utilization or throughput level for your application. Application Auto Scaling can then scale out the scalable target (add capacity) to handle peak traffic, and scale in the scalable target (remove capacity) to reduce costs during periods of low utilization or throughput.</p>\n\n<p>For example, let\'s say that you currently have an application that runs on Spot Fleet, and you want the CPU utilization of the fleet to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive number of idle resources.</p>\n\n<p>You can meet this need by creating a target tracking scaling policy that targets an average CPU utilization of 50 percent. Then, Application Auto Scaling scales the number of instances to keep the actual metric value at or near 50 percent.</p>\n\n<p>Target tracking scaling policies for Application Auto Scaling:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a></p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</strong></p>\n\n<p>Scaling based on a schedule allows you to set your own scaling schedule according to predictable load changes. For example, let\'s say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.</p>\n\n<p>To use scheduled scaling, create scheduled actions, which tell Application Auto Scaling to perform scaling activities at specific times. When you create a scheduled action, you specify the scalable target, when the scaling activity should occur, a minimum capacity, and a maximum capacity. You can create scheduled actions that scale one time only or that scale on a recurring schedule.</p>\n\n<p>Scheduled scaling for Application Auto Scaling:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</strong> - With target tracking scaling policies you do not create, edit, or delete the CloudWatch alarms that are used with a target tracking scaling policy. Application Auto Scaling creates and manages the CloudWatch alarms that are associated with your target tracking scaling policies and deletes them when no longer needed.</p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the minimum and maximum capacity based on the requirements</strong> - Scheduled scaling is not the right option when a parameter has to be tracked and forms the basis of the scale-in and scale-out activity of the ASG. Target tracking is better suited for these use cases.</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html</a></p>\n', 'answers': ['<p>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto-scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</p>', '<p>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</p>', '<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the  minimum and maximum capacity based on the requirements</p>', '<p>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</p>', '<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</p>']}, 'correct_response': ['b', 'e'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A DevOps Engineer is working on multiple applications that need to be configured for Application Auto scaling, however, there are some application constraints to be addressed:\n\na) A serverless application is built on AWS Lambda with the following traffic pattern - The traffic for the application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nb) Another flagship application runs on Spot Fleet. The CPU utilization of the fleet has to stay at around 50 percent when the load on the application changes.\n\nWhich of the following solutions can address these requirements? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357156, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>A company has hired you as an AWS Certified DevOps Engineer - Professional to provide recommendations for a failed security audit of its flagship project. You have been tasked to review the company's buildspec.yaml file for its AWS CodeBuild project. Upon investigation, you have noticed that the file has hard-coded values for the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password. In addition, to perform one-time configuration changes during the build phase, the file has commands to <code>ssh</code> and <code>scp</code> into an EC2 instance using an SSH private key stored on Amazon S3.</p>\n\n<p>What changes would you recommend to comply with AWS security best practices? (Select three)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded AWS credentials in code, configuration, or build files such as the buildspec.yaml file. AWS recommends using an appropriate permissions policy that is attached to the CodeBuild project role to grant the necessary access to the required resources.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a></p>\n\n<p><strong>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded database credentials in code, configuration, or build files such as the buildspec.yaml file. As a security best practice, AWS recommends using the database password as a SecureString value in AWS Systems Manager Parameter Store which can then be referenced in the buildspec file like so:</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>The configuration above is required when you want to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store. This contains a mapping of key/value scalars, where each mapping represents a single custom environment variable stored in the Amazon EC2 Systems Manager Parameter Store. key is the name you use later in your build commands to refer to this custom environment variable, and value is the name of the custom environment variable stored in Amazon EC2 Systems Manager Parameter Store. To allow CodeBuild to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store, you must add the ssm:GetParameters action to your CodeBuild service role.</p>\n\n<p><strong>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong></p>\n\n<p>Using Run Command, a capability of AWS Systems Manager, you can remotely and securely manage the configuration of your managed nodes. A managed node is any Amazon Elastic Compute Cloud (Amazon EC2) instance or non-EC2 machine in your hybrid and multi-cloud environment that has been configured for Systems Manager. Run Command allows you to automate common administrative tasks and perform one-time configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface (AWS CLI), AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p>\n\n<p>It is considered a security bad practice to store the SSH private key on Amazon S3. Using the AWS Systems Manager <code>run</code> command for the given use case meets the security best practices.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong> - In Secrets Manager, a secret consists of secret information, the secret value, plus metadata about the secret. A secret value can be a string or binary. Secrets Manager uses IAM permission policies to make sure that only authorized users can access or modify a secret. There is no such thing as a SecureString value in AWS Secrets Manager. This option has been added as a distractor.</p>\n\n<p><strong>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</strong> - It is considered a security bad practice to store the AWS access credentials on Amazon S3, so this option is incorrect.</p>\n\n<p><strong>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong> - Automation feature of the AWS Systems Manager helps you to build automated solutions to deploy, configure, and manage AWS resources at scale. With Automation, you have granular control over the concurrency of your automation. This means you can specify how many resources to target concurrently, and how many errors can occur before an automation is stopped. Since the given use case involves one-time configuration changes, it is better to use the AWS Systems Manager <code>run</code> command.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works">https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html">https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html</a></p>\n', 'answers': ['<p>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</p>', '<p>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>', '<p>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>', '<p>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>', '<p>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</p>', '<p>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>']}, 'correct_response': ['a', 'b', 'c'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "A company has hired you as an AWS Certified DevOps Engineer - Professional to provide recommendations for a failed security audit of its flagship project. You have been tasked to review the company's buildspec.yaml file for its AWS CodeBuild project. Upon investigation, you have noticed that the file has hard-coded values for the environment variables that reference the AWS Access Key ID, Secret Access Key, and the database password. In addition, to perform one-time configuration changes during the build phase, the file has commands to ssh and scp into an EC2 instance using an SSH private key stored on Amazon S3.\n\nWhat changes would you recommend to comply with AWS security best practices? (Select three)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357158, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>In a multinational company, various AWS accounts are efficiently managed using AWS Control Tower. The company operates both internal and public applications across its infrastructure. To streamline operations, each application team is assigned a dedicated AWS account responsible for hosting their respective applications. These accounts are consolidated under an organization in AWS Organizations. Additionally, a specific AWS Control Tower member account acts as a centralized DevOps hub, offering Continuous Integration/Continuous Deployment (CI/CD) pipelines that application teams utilize to deploy applications to their designated AWS accounts. A specialized IAM role for deployment is available within this central DevOps account.</p>\n\n<p>Currently, a particular application team is facing challenges while attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster situated in their application-specific AWS account. They have an existing IAM role for deployment within the application AWS account. The deployment process relies on an AWS CodeBuild project, configured within the centralized DevOps account, and utilizes an IAM service role for CodeBuild. However, the deployment process is encountering an Unauthorized error when trying to establish connections to the cross-account EKS cluster from the CodeBuild environment.</p>\n\n<p>To resolve this error and facilitate a successful deployment, what solution would you recommend?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Establish a trust relationship in the application account\'s deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account\'s deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>By configuring a trust relationship in the application account\'s deployment IAM role for the centralized DevOps account, using the sts:AssumeRole action, CodeBuild in the centralized DevOps account will be allowed to assume the IAM role in the application account. This will enable CodeBuild to access the EKS cluster in the application account. Additionally, granting the application account\'s deployment IAM role the necessary access to the EKS cluster ensures that it has the required permissions to perform the deployment tasks. Finally, configuring the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions ensures that the IAM role has the required permissions within the EKS cluster.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q32-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Establish a trust relationship in the application account\'s deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account\'s deployment IAM role the required access to CodeBuild and the EKS cluster</strong></p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account\'s deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account\'s deployment IAM role the required access to CodeBuild</strong></p>\n\n<p>AssumeRoleWithSAML is used for federation scenarios involving SAML-based identity providers, which is not relevant to the given use case, so both these options are incorrect.</p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account for the application account\'s deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account\'s deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>You need to configure a trust relationship in the application account\'s deployment IAM role for the centralized DevOps account, so that CodeBuild in the centralized DevOps account can assume the IAM role in the application account, by using the sts:AssumeRole action. However, this option sets up the trust relationship in the reverse direction, so it\'s incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html">https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html</a></p>\n', 'answers': ["<p>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</p>", "<p>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>", "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>", "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</p>"]}, 'correct_response': ['c'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'In a multinational company, various AWS accounts are efficiently managed using AWS Control Tower. The company operates both internal and public applications across its infrastructure. To streamline operations, each application team is assigned a dedicated AWS account responsible for hosting their respective applications. These accounts are consolidated under an organization in AWS Organizations. Additionally, a specific AWS Control Tower member account acts as a centralized DevOps hub, offering Continuous Integration/Continuous Deployment (CI/CD) pipelines that application teams utilize to deploy applications to their designated AWS accounts. A specialized IAM role for deployment is available within this central DevOps account.\n\nCurrently, a particular application team is facing challenges while attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster situated in their application-specific AWS account. They have an existing IAM role for deployment within the application AWS account. The deployment process relies on an AWS CodeBuild project, configured within the centralized DevOps account, and utilizes an IAM service role for CodeBuild. However, the deployment process is encountering an Unauthorized error when trying to establish connections to the cross-account EKS cluster from the CodeBuild environment.\n\nTo resolve this error and facilitate a successful deployment, what solution would you recommend?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357160, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.</p>\n\n<p>What solution would you suggest to meet these requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong></p>\n\n<p>The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.</p>\n\n<p>You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg">\nvia - <a href="https://aws.amazon.com/cloudwatch/faqs/">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect.</p>\n\n<p><strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/cloudwatch/faqs/">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs">https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n', 'answers': ['<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>', '<p>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</p>', '<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>', '<p>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.\n\nWhat solution would you suggest to meet these requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357162, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>During an AWS CloudFormation stack update process, an error occurred in the updated template, causing AWS CloudFormation to initiate an automatic stack rollback. After the rollback attempt, a DevOps engineer noticed that the application remained unavailable, and the stack is now in the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>To ensure the successful completion of the stack rollback, which actions should the DevOps engineer take? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Execute a ContinueUpdateRollback command from the AWS CloudFormation</strong></p>\n\n<p><strong>Manually fix the resources to match the correct state of the stack</strong></p>\n\n<p>When an update to a CloudFormation stack fails, AWS CloudFormation automatically initiates a rollback process to revert the stack to its previous known stable state. In certain cases, such as when there are dependencies on external resources, the rollback process might stall or encounter an error. To help recover from a failed stack update, you can use the ContinueUpdateRollback command. This command instructs CloudFormation to continue the stack rollback and can help resolve the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>A dependent resource can\'t return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). For example, you might have a stack that\'s rolling back to an old database instance that was deleted outside of AWS CloudFormation. Because AWS CloudFormation doesn\'t know the database was deleted, it assumes that the database instance still exists and attempts to roll back to it, causing the update rollback to fail.</p>\n\n<p>Depending on the cause of the failure, you can manually fix the error and continue the rollback. By continuing the rollback, you can return your stack to a working state (the UPDATE_ROLLBACK_COMPLETE state), and then try to update the stack again.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q34-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation drift detection</strong></p>\n\n<p>While AWS CloudFormation drift detection can help identify resources that have drifted from their expected template configurations, it is not directly related to resolving the UPDATE_ROLLBACK_FAILED state. Drift detection is used to detect and compare configuration changes made outside of CloudFormation, and it helps to bring the resources back in line with the template. However, it doesn\'t address the issue of a failed stack rollback.</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation stack sets</strong></p>\n\n<p>AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions. It cannot fix the issue with a failed stack rollback.</p>\n\n<p><img src="https://docs.aws.amazon.com/images/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p><strong>Update the existing AWS CloudFormation stack by using the original template</strong> - Reapplying the original template will not address the issue that caused the stack update to fail initially. The same problem will likely persist, and the stack rollback will continue to fail. To complete the rollback, the cause of the update failure should be addressed, and the rollback should be explicitly continued using the ContinueUpdateRollback command.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a></p>\n\n<p><a href="#">[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html)</a></p>\n', 'answers': ['<p>Update the existing AWS CloudFormation stack by using the original template</p>', '<p>Automatically fix the issue by using AWS CloudFormation drift detection</p>', '<p>Automatically fix the issue by using AWS CloudFormation stack sets</p>', '<p>Execute a ContinueUpdateRollback command from the AWS CloudFormation</p>', '<p>Manually fix the resources to match the correct state of the stack</p>']}, 'correct_response': ['d', 'e'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'During an AWS CloudFormation stack update process, an error occurred in the updated template, causing AWS CloudFormation to initiate an automatic stack rollback. After the rollback attempt, a DevOps engineer noticed that the application remained unavailable, and the stack is now in the UPDATE_ROLLBACK_FAILED state.\n\nTo ensure the successful completion of the stack rollback, which actions should the DevOps engineer take? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357164, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.</p>\n\n<p>To effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg">\nvia - <a href="https://aws.amazon.com/image-builder/">https://aws.amazon.com/image-builder/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect “agent” installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/image-builder/">https://aws.amazon.com/image-builder/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n', 'answers': ['<p>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>', '<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>', '<p>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</p>', '<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>']}, 'correct_response': ['b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.\n\nTo effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357166, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.</p>\n\n<p>What steps should the DevOps Engineer take to fulfill these requirements?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\'s EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment\'s target group</strong></p>\n\n<p>A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.</p>\n\n<p>Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS.</p>\n\n<p>For the given use case, the blue group carries the production load while the green group is staged and deployed with the new code. When it’s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q36-i1.jpg">\nvia - <a href="https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an all-at-once deployment to the blue environment\'s EC2 instances. Perform a Route 53 DNS swap to the green environment\'s endpoint on the ALB</strong></p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\'s EC2 instances. Perform a Route 53 DNS swap to the green environment\'s endpoint on the ALB</strong></p>\n\n<p>Both these options have been added as distractors. Since there is only a single ALB per the given use case, so there is no alternate endpoint available for a Route 53 DNS update.</p>\n\n<p><strong>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment\'s target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\'s EC2 instances</strong> - The order of execution for this option is incorrect as it points the ALB to the green environment\'s target group before deploying the new software on the green environment\'s EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href="https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n', 'answers': ["<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>", "<p>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</p>", "<p>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>", "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</p>"]}, 'correct_response': ['d'], 'section': 'Domain 1: SDLC Automation', 'question_plain': "The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.\n\nWhat steps should the DevOps Engineer take to fulfill these requirements?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357168, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.</p>\n\n<p>During a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.</p>\n\n<p>Considering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3\'s call return status to check for an error</strong></p>\n\n<p>Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata.</p>\n\n<p>One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match.</p>\n\n<p><strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong></p>\n\n<p>The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3\'s call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect.</p>\n\n<p><strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3\'s call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object.</p>\n\n<p><strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html</a></p>\n', 'answers': ["<p>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</p>", "<p>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</p>", "<p>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</p>", '<p>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</p>', '<p>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</p>']}, 'correct_response': ['c', 'd'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.\n\nDuring a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.\n\nConsidering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357170, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p>You cannot use any S3 API to invalidate the CloudFront cache, so both of these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong> - It is wasteful to invoke an EventBridge rule every 5 minutes to invalidate the CloudFront cache. You should only invalidate the cache when the API has actually been updated.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n', 'answers': ['<p>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>', '<p>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>', '<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>', '<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>']}, 'correct_response': ['c'], 'section': 'Domain 1: SDLC Automation', 'question_plain': "A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you suggest?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357172, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.</p>\n\n<p>What measures can help resolve this issue?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</strong></p>\n\n<p><code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the "BeforeAllowTraffic" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q39-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</strong></p>\n\n<p><strong>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</strong></p>\n\n<p><strong>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing test traffic</strong></p>\n\n<p>None of these three lifecycle event hooks are available for an AWS Lambda deployment, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n', 'answers': ['<p>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</p>', '<p>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</p>', '<p>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing a test traffic</p>', '<p>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</p>']}, 'correct_response': ['d'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.\n\nWhat measures can help resolve this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357174, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.</p>\n\n<p>Which of the following options represents the BEST solution for the given scenario?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong></p>\n\n<p>You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules.</p>\n\n<p>The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.</p>\n\n<p>For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p>You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n', 'answers': ['<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>', '<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>', '<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>', '<p>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>']}, 'correct_response': ['b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.\n\nWhich of the following options represents the BEST solution for the given scenario?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357176, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong></p>\n\n<p>CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs.</p>\n\n<p>You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs</p>\n\n<p>For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n', 'answers': ['<p>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</p>', '<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</p>', '<p>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</p>', '<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357178, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.</p>\n\n<p>Which of the following solutions would you suggest for the given use case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.</p>\n\n<p>To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot.</p>\n\n<p>You can use CodeDeploy to create a deployment process that publishes the new Lambda version but does not send any traffic to it. Then it executes a PreTraffic test to ensure that your new function works as expected. After the test succeeds, CodeDeploy automatically shifts traffic gradually to the new version of the Lambda function. This workflow addresses one of the key requirements of reducing the time to detect errors. You can roll back to the previous version in case the new version errors out.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</strong> - You can use CloudFormation change sets to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p>\n\n<p>This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</strong> - This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack and point to the new endpoint.</p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</strong> - This option has been added as a distractor since CloudFormation change sets do not have pre-traffic and post-traffic test functions. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p>\n', 'answers': ['<p>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</p>', '<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</p>', '<p>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</p>', '<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.\n\nWhich of the following solutions would you suggest for the given use case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357180, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The DevOps team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources.</p>\n\n<p>Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule\'s scope changes in configuration</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>How AWS Config Works:\n<img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png">\nvia - <a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p>For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant.</p>\n\n<p>There are two types of evaluation trigger types for Config rules:</p>\n\n<p>Configuration changes – AWS Config triggers the evaluation when any resource that matches the rule\'s scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p>\n\n<p>Periodic – AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p>\n\n<p><strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong></p>\n\n<p>CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.</p>\n\n<p>CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren\'t viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events.</p>\n\n<p>CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q43-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using the CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution.</p>\n\n<p><strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p><strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n', 'answers': ["<p>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</p>", '<p>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</p>', '<p>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</p>', '<p>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</p>', '<p>Leverage EventBridge events near-real-time capabilities to monitor system events patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The DevOps team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources.\n\nWhich of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357182, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.</p>\n\n<p>Which of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</strong></p>\n\n<p><strong>Apply patch baselines using the AWS-RunPatchBaseline SSM document</strong></p>\n\n<p>Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. AWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources.</p>\n\n<p><img src="https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png">\nvia - <a href="https://aws.amazon.com/systems-manager/">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications). Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p>\n\n<p>For the given use case, you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p>\n\n<p>Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the "default" for an operating system type.</p>\n\n<p>The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn\'t support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</strong> - AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictable, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>There is no such thing as CloudFormation automatic patching support, as you need to use Systems Manager for patch management. So, this option acts as a distractor.</p>\n\n<p><strong>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</strong> - You could use an OS-native patching service to manage the update frequency and release approval for all instances but it would take considerable effort to set up and configure this solution. This violates the minimal effort requirement of the given use case.</p>\n\n<p><strong>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</strong> - As mentioned in the explanation above, the AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn\'t support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/systems-manager/">https://aws.amazon.com/systems-manager/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n', 'answers': ['<p>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</p>', '<p>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</p>', '<p>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</p>', '<p>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</p>', '<p>Apply patch baselines using the AWS-RunPatchBaseline SSM document</p>']}, 'correct_response': ['d', 'e'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.\n\nWhich of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)", 'related_lectures': []}]}
4716374
~~~
{'count': 75, 'next': None, 'previous': None, 'results': [{'_class': 'assessment', 'id': 67357308, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A healthcare technology company provides a Software as a Service (SaaS) solution to hospitals throughout the United States to use the company’s proprietary system to integrate their clinical documentation and coding workflows. The DevOps team at the company would like to enable a CICD pipeline that enables safe deployments to production and the ability to work on new features of the product roadmap.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend for the given use-case?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src="https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png">\nvia - <a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>It\'s a best practice to work with branches in your git repository to create features, as it\'s the intended usage of branches. Don\'t create separate repositories for features. To protect the master branch you need to set a Deny policy on the IAM group that the developer group should be assigned to.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q1-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</strong> - As mentioned in the explanation above, you should not create a separate repository for each new feature. So this option is incorrect.</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</strong> - This option has been added as a distractor as there is no such thing as a repository access policy.</p>\n\n<p><strong>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong> - Although you can create a separate CICD pipeline for each branch, you cannot merge multiple pipelines into one to make it a "master" pipeline or merge multiple branches into a master branch as the last step of a CICD pipeline. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p><a href="https://aws.amazon.com/codecommit/faqs/">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>', '<p>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</p>', '<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</p>', '<p>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A healthcare technology company provides a Software as a Service (SaaS) solution to hospitals throughout the United States to use the company’s proprietary system to integrate their clinical documentation and coding workflows. The DevOps team at the company would like to enable a CICD pipeline that enables safe deployments to production and the ability to work on new features of the product roadmap.\n\nAs an AWS Certified DevOps Engineer, which solution would you recommend for the given use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357312, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.</p>\n\n<p>As a DevOps Engineer, which solution would you implement for the given requirement?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src="https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png">\nvia - <a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.</p>\n\n<p>Code Pipeline Overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q2-i1.jpg">\nvia - <a href="https://aws.amazon.com/codepipeline/faqs/">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</strong> - As mentioned in the explanation above, a key requirement is that two versions of the code need to be deployed to different environments. If you use a manual approval step after the deployment to staging then the same version of the code from the master branch would also be deployed to the production environment. Instead, you need to maintain a production branch of the code that can be deployed to the production environment.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</strong> - It\'s a best practice to work with branches in your git repository to create features, as it\'s the intended usage of branches. Don\'t create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to merging through a pull request has been added as a distractor.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</strong> - It\'s a best practice to work with branches in your git repository to create features, as it\'s the intended usage of branches. Don\'t create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to the manual approval step has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/codecommit/faqs/">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p><a href="https://aws.amazon.com/codepipeline/faqs/">https://aws.amazon.com/codepipeline/faqs/</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</p>', '<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</p>', '<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</p>', '<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.\n\nAs a DevOps Engineer, which solution would you implement for the given requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357270, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.</p>\n\n<p>As an AWS Certified DevOps Engineer, how would you implement this most efficiently?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, you can set up a CloudWatch Event rule for every push to the CodeCommit repository that would trigger the Lambda function configured as a target. The Lambda function would in turn request the code from CodeCommit, zip it and send it to the 3rd party API.</p>\n\n<p>CloudWatch Events Configuration:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i2.jpg"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</strong> - CloudWatch Event Rules cannot have S3 buckets as a target. Although you can set an S3 trigger as a target, eventually you would still need to invoke the Lambda function via an S3 trigger to process the code via the API. Therefore it\'s efficient to directly invoke the Lambda function from the CloudWatch Event rule.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i3.jpg"></p>\n\n<p><strong>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</strong> - CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So this option is ruled out.</p>\n\n<p><strong>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</strong> - The EC2 CodeCommit hook is a distractor and does not exist.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</p>', '<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</p>', '<p>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</p>', '<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.\n\nAs an AWS Certified DevOps Engineer, how would you implement this most efficiently?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357282, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>As part of the CICD pipeline, the DevOps team at a retail company wants to deploy the latest application code to a staging environment and the team also wants to ensure it can execute an automated functional test suite before deploying to production. The code is managed via CodeCommit. Usually, the functional test suite runs for over two hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you create the CICD pipeline to run your test suite in the most efficient way?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn\'t fail, the last stage will deploy the application to production</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p>Sample AWS CodePipeline pipeline architecture:\n<img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2020/03/24/AWSCodePipelineAndPostman_drawio.png"></p>\n\n<p>Highly recommend reading this excellent reference AWS DevOps blog on using CodePipeline with CodeBuild to automate testing - <a href="https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. CodeBuild automatically scales up and down and processes multiple builds concurrently, so your builds don’t have to wait in a queue. CodeBuild has recently announced the launch of a new feature in CodeBuild called Reports. This feature allows you to view the reports generated by functional or integration tests. The reports can be in the JUnit XML or Cucumber JSON format. You can view metrics such as Pass Rate %, Test Run Duration, and the number of Passed versus Failed/Error test cases in one location.</p>\n\n<p>AWS CodeBuild Test Reports:\n<img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/11/21/Picture1-1.png">\nvia - <a href="https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p>\n\n<p>For the given use-case, you need to use a CodeBuild build to run the test suite, but you must first deploy to staging before running CodeBuild! It\'s common in the exam for multiple same steps to be shown in a different order, so be careful.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production</strong> - As mentioned in the explanation above, you cannot have the CodeBuild Test as a stage prior to deploying in the staging environment, so this option is incorrect.</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage doesn\'t fail, the last stage will deploy the application to production</strong> - Lambda should be ruled out for running the test suite as the maximum timeout for a Lambda function is 15 minutes, so it will not support the given use-case since the functional test suite runs for over two hours.</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will invoke a Step Function execution. The Step Function will run the test suite. Create a CloudWatch Event Rule on the execution termination of your Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn\'t fail, the last stage will deploy the application to production</strong> - AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. While the solution involving Step Functions would work, it\'s extremely convoluted and not the most efficient solution.</p>\n\n<p>How Step Functions Work:\n<img src="https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png">\nvia - <a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a></p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p>\n\n<p><a href="https://aws.amazon.com/codebuild/faqs/">https://aws.amazon.com/codebuild/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p>\n\n<p><a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a></p>\n', 'relatedLectureIds': '', 'answers': ["<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</p>", '<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production</p>', "<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</p>", "<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will invoke a Step Function execution. The Step Function will run the test suite. Create a CloudWatch Event Rule on the execution termination of your Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn't fail, the last stage will deploy the application to production</p>"]}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'As part of the CICD pipeline, the DevOps team at a retail company wants to deploy the latest application code to a staging environment and the team also wants to ensure it can execute an automated functional test suite before deploying to production. The code is managed via CodeCommit. Usually, the functional test suite runs for over two hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you create the CICD pipeline to run your test suite in the most efficient way?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357246, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>An online coding platform wants to fully customize the build tasks and automatically run builds concurrently to take the pain out of managing the build environments. The DevOps team at the company wants to use CodeBuild for all build-tasks and would like the artifacts created by CodeBuild to be named based on the branch being tested. The team wants this solution to be scalable to newer branches with a minimal amount of rework.</p>\n\n<p>As a DevOps Engineer, how would you go about implementing the simplest possible solution to address the given use-case?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers.</p>\n\n<p>A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>For the given use-case, we need to use environment variables. The variable <code>CODEBUILD_SOURCE_VERSION</code> is exposed at runtime directly within CodeBuild and represents the branch name of the code being tested for CodeCommit. This is the best solution.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong> - Providing the branch name as <code>BRANCH_NAME</code> and creating separate CodeBuild would be highly tedious to maintain and error-prone. This is certainly not the simplest solution possible.</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</strong> - Maintaining a different <code>buildspec.yml</code> for each branch is not efficient and it\'s error-prone. So this option is incorrect.</p>\n\n<p><strong>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</strong> - The answer involving a Lambda function would work but is highly convoluted. This is something that can be directly accomplished using the <code>CODEBUILD_SOURCE_VERSION</code> environment variable.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>', '<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>', '<p>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</p>', '<p>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'An online coding platform wants to fully customize the build tasks and automatically run builds concurrently to take the pain out of managing the build environments. The DevOps team at the company wants to use CodeBuild for all build-tasks and would like the artifacts created by CodeBuild to be named based on the branch being tested. The team wants this solution to be scalable to newer branches with a minimal amount of rework.\n\nAs a DevOps Engineer, how would you go about implementing the simplest possible solution to address the given use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357240, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A mobility company connects people with taxi drivers and the DevOps team at the company uses CodeCommit as a backup and disaster recovery service for several of its DevOps processes. The team is creating a CICD pipeline so that your code in the CodeCommit master branch automatically gets packaged as a Docker container and published to ECR. The team would then like that image to be automatically deployed to an ECS cluster using a Blue/Green strategy.</p>\n\n<p>As an AWS Certified DevOps Engineer, which of the following options would you recommend as the most efficient solution to meet the given requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p>You can use CodeBuild to acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. You should note that acquiring ECR credentials must be done using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your user access and secret key.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</strong> - You should note that both CodeDeploy and CloudFormation can support blue/green deployment for ECS. However, you cannot create a new task definition using CodeCommit, so this option is incorrect.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</strong> - CloudWatch Event Rule does not support CodeDeploy as a target, therefore CodeDeploy must be invoked from your CodePipeline.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i3.jpg"></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</strong> - As mentioned in the explanation above, ECR credentials must be acquired using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your AWS access key ID and secret access key.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><a href="https://aws.amazon.com/codepipeline/faqs/">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/">https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/</a></p>\n', 'answers': ['<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</p>', '<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</p>', '<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</p>', '<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A mobility company connects people with taxi drivers and the DevOps team at the company uses CodeCommit as a backup and disaster recovery service for several of its DevOps processes. The team is creating a CICD pipeline so that your code in the CodeCommit master branch automatically gets packaged as a Docker container and published to ECR. The team would then like that image to be automatically deployed to an ECS cluster using a Blue/Green strategy.\n\nAs an AWS Certified DevOps Engineer, which of the following options would you recommend as the most efficient solution to meet the given requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357222, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.</p>\n\n<p>How can you implement the validation of Pull Requests by CodeBuild efficiently?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i2.jpg"></p>\n\n<p>The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i3.jpg"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong> - Using a scheduled rate of 5 minutes would work but would be inefficient. It is much better to configure a CloudWatch Event Rule that would trigger on CodeCommit Pull Request and carry out the rest of the solution workflow as outlined earlier.</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p>For both these options, invoking a Lambda function to start CodeBuild would work, but having the function wait on CodeBuild has two issues:\n1) The Lambda function may timeout and has a maximum timeout of 15 minutes. What if the test suite takes longer to run?\n2) You will be billed for the Lambda function wait time.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>', '<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>', '<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>', '<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.\n\nHow can you implement the validation of Pull Requests by CodeBuild efficiently?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357320, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A data intelligence and analytics company enables publishers to measure, analyze, and improve the impact of the advertising across their range of online deliverables. The DevOps team at the company wants to use CodePipeline to deploy code from CodeCommit with CodeDeploy. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you configure the EC2 instances to facilitate the deployment?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong></p>\n\n<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy Concepts:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i1.jpg"></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i2.jpg">\nvia - <a href="https://aws.amazon.com/codedeploy/faqs/">https://aws.amazon.com/codedeploy/faqs/</a></p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. A configuration file is placed on the instance when the agent is installed. This file is used to specify how the agent works. This configuration file specifies directory paths and other settings for AWS CodeDeploy to use as it interacts with the instance.</p>\n\n<p>For the given use-case, you can have the CodePipeline chain CodeCommit and CodeDeploy and have the source code available as a zip file in an S3 bucket to be used as a CodePipeline artifact. The EC2 instance must have an IAM role, and not an IAM user, to pull that file from S3. Finally, the EC2 instance must be properly tagged to be part of the correct deployment group and have the CodeDeploy agent installed on it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It\'s a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</strong> - CodeDeploy cannot automatically install the agent on the EC2 instance. You must ensure that the EC2 instance has the CodeDeploy agent installed. You must also tag the instance to have it part of a deployment group.</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It\'s a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>', '<p>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>', '<p>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</p>', '<p>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A data intelligence and analytics company enables publishers to measure, analyze, and improve the impact of the advertising across their range of online deliverables. The DevOps team at the company wants to use CodePipeline to deploy code from CodeCommit with CodeDeploy. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you configure the EC2 instances to facilitate the deployment?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357220, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy components overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i1.jpg">\nvia - <a href="https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p>For the given use-case, you should create four deployment groups for development, staging, canary testing, production and then chain these together using CodePipeline. For EC2, to do a canary deployment, you must create a small deployment group made of few instances from production and deploy to these. Add a manual step after the deployment to the canary testing stage.</p>\n\n<p>Sample workflow for CI/CD with AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>The exam may try to trap you on some of the following details on deployment-related processes. Be aware of what\'s possible.</p>\n\n<ul>\n<li><p>A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p></li>\n<li><p>Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type. During an in-place deployment, CodeDeploy performs a rolling update across Amazon EC2 instances. During a blue/green deployment, the latest application revision is installed on replacement instances.</p></li>\n<li><p>If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.</p></li>\n<li><p>You CANNOT use canary, linear, or all-at-once configuration for EC2/On-Premises compute platform.</p></li>\n<li><p>You can manage the way in which traffic is shifted to the updated Lambda function versions during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>You can deploy an Amazon ECS containerized application as a task set. You can manage the way in which traffic is shifted to the updated task set during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>Amazon ECS blue/green deployments are supported using both CodeDeploy and AWS CloudFormation. For blue/green deployments through AWS CloudFormation, you don\'t create a CodeDeploy application or deployment group.</p></li>\n<li><p>Your deployable content and the AppSpec file are combined into an archive file (also known as application revision) and then upload it to an Amazon S3 bucket or a GitHub repository. Remember these two locations. AWS Lambda revisions can be stored in Amazon S3 buckets. EC2/On-Premises revisions are stored in Amazon S3 buckets or GitHub repositories.</p></li>\n<li><p>AWS Lambda and Amazon ECS deployments CANNOT use an in-place deployment type.</p></li>\n</ul>\n\n<p>Incorrect options:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</strong> - Creating separate CodePipelines is not a good idea and won\'t allow you to create manual approval steps before deploying to production.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it\'s only meant for AWS Lambda and ECS platforms and there\'s no option to pause it manually through an approval step.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it\'s only meant for AWS Lambda and ECS platforms. In addition, creating separate CodePipelines is not a good idea and won\'t allow you to create manual approval steps before deploying to production.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n', 'answers': ['<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</p>', '<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</p>', '<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>', '<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357300, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': "<p>The DevOps team at a business travel solutions company wants to use CodeDeploy to ensure zero downtime during deployments through rolling updates. The team wants to deploy the company's flagship web application on a set of 5 EC2 instances running behind an Application Load Balancer. The team would like the deployment to be gradual and to automatically rollback in case of a failed deployment, which is determined by the application not being able to pass health checks.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend for the given use-case?</p>\n", 'explanation': '<p>Correct option:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>Sample appspec file:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p>List of Lifecycle Event hooks for EC2 deployment:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Lifecycle Event hooks availability for EC2 deployment and rollback scenarios:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>For the given use-case, you can use <code>ValidateService</code> hook to verify that the deployment was completed successfully. This is the last deployment lifecycle event. You can configure CodeDeploy to rollback if this hook fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</strong> - Integrating CodeDeploy with the Application Load Balancer will ensure traffic isn\'t forwarded to the instances that CodeDeploy is currently deploying to, but the health check feature is not integrated with CodeDeploy and therefore you cannot rollback when the Application Load Balancers fails the health check.</p>\n\n<p><strong>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - The <code>AfterInstall</code> hook in <code>appspec.yml</code> is before <code>StartApplication</code> and therefore won\'t be able to test the application\'s health checks. You can use the <code>AfterInstall</code> hook for tasks such as configuring your application or changing file permissions.</p>\n\n<p><strong>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</strong> - The CloudWatch Event rule won\'t work as it is not granular at each instance\'s level, and CodeDeploy has a native feature for doing rollbacks, instead of doing API calls via <code>StopDeployment</code> and <code>CreateDeployment</code>.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>', '<p>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</p>', '<p>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>', '<p>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': "The DevOps team at a business travel solutions company wants to use CodeDeploy to ensure zero downtime during deployments through rolling updates. The team wants to deploy the company's flagship web application on a set of 5 EC2 instances running behind an Application Load Balancer. The team would like the deployment to be gradual and to automatically rollback in case of a failed deployment, which is determined by the application not being able to pass health checks.\n\nAs a DevOps Engineer, which of the following options would you recommend for the given use-case?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357260, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A Silicon Valley based startup runs a news discovery web application and it uses CodeDeploy to deploy the web application on a set of 20 EC2 instances behind an Application Load Balancer. The ALB is integrated with CodeDeploy. The DevOps teams at the startup would like the deployment to be gradual and to automatically rollback in case of unusually high maximum CPU utilization for the EC2 instances while traffic is being served.</p>\n\n<p>How can you implement this?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated. CodeDeploy will redeploy the last known working version of the application when it rolls back. Previously, you needed to manually initiate a deployment if you wanted to roll back a deployment. For the given use-case, you should use the underlying metric as the maximum CPU for your EC2 instances.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - If you are using the <code>ValidateService</code> hook because your CodeDeploy is integrated with the ALB, traffic will not be served and you won\'t observe high CPU utilization.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics. So this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - This option has been added as a distractor as you would want to watch out for the maximum CPU utilization of the EC2 instances and not the Application Load Balancer. In addition, CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</p>', '<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>', '<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>', '<p>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A Silicon Valley based startup runs a news discovery web application and it uses CodeDeploy to deploy the web application on a set of 20 EC2 instances behind an Application Load Balancer. The ALB is integrated with CodeDeploy. The DevOps teams at the startup would like the deployment to be gradual and to automatically rollback in case of unusually high maximum CPU utilization for the EC2 instances while traffic is being served.\n\nHow can you implement this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357288, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': "<p>The DevOps team at your company is using CodeDeploy to deploy new versions of a Lambda function after it has passed a CodeBuild check via your CodePipeline. Before deploying, the CodePipeline has a step in which it optionally kickstarts a restructuring of files on an S3 bucket that is forward compatible. That restructuring is done using a Step Function execution which invokes a Fargate task. The new Lambda function cannot work until the restructuring task has fully completed.</p>\n\n<p>As a DevOps Engineer, how can you ensure traffic isn't served to your new Lambda function until the task is completed?</p>\n", 'explanation': '<p>Correct option:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong></p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>For AWS Lambda compute platform applications, the AppSpec file is used by CodeDeploy to determine:</p>\n\n<p>Which Lambda function version to deploy.</p>\n\n<p>Which Lambda functions to use as validation tests.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n\n<p>The <code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. So for the given use-case, you can use this hook to check that the restructuring task has fully completed and then shift traffic to the newly deployed Lambda function version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong> - If you use an <code>AfterAllowTraffic</code> hook the new Lambda function will already serve traffic, so this option is incorrect.</p>\n\n<p><strong>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</strong> - Canary Deployments will send some traffic to the new Lambda function while the restructuring in S3 is still happening so that won\'t work.</p>\n\n<p><strong>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</strong> - There\'s no API to tell CodeDeploy to switch traffic to the new version of the Lambda function, therefore adding a step in your Step Function won\'t help.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>', '<p>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>', '<p>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</p>', '<p>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': "The DevOps team at your company is using CodeDeploy to deploy new versions of a Lambda function after it has passed a CodeBuild check via your CodePipeline. Before deploying, the CodePipeline has a step in which it optionally kickstarts a restructuring of files on an S3 bucket that is forward compatible. That restructuring is done using a Step Function execution which invokes a Fargate task. The new Lambda function cannot work until the restructuring task has fully completed.\n\nAs a DevOps Engineer, how can you ensure traffic isn't served to your new Lambda function until the task is completed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357314, 'assessment_type': 'multi-select', 'prompt': {'feedbacks': ['', '', '', '', ''], 'question': "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n", 'explanation': '<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Pipeline Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg"></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a \'Slack send\'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Action Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Stage Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/codepipeline/faqs/">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Pipeline Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>', '<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>', "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>", '<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Action Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>', '<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  "source": [\n    "aws.codepipeline"\n  ],\n  "detail-type": [\n    "CodePipeline Stage Execution State Change"\n  ],\n  "detail": {\n    "state": [\n      "FAILED"\n    ]\n  }\n}\n</code></pre>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 1: SDLC Automation', 'question_plain': "The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.\n\nWhich of the following options would you suggest? (Select two)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357262, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>As part of your CodePipeline, you are running multiple test suites. Two are bundled as Docker containers and run directly on CodeBuild, while another one runs as a Lambda function executing Python code. All these test suites are based on HTTP requests and upon analyzing, these are found to be network bound, not CPU bound. Right now, the CodePipeline takes a long time to execute as these actions happen one after the other. They prevent the company from adding further tests. The whole pipeline is managed by CloudFormation.</p>\n\n<p>As a DevOps Engineer, which of the following would you recommend improving the completion time of your pipeline?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Change the <code>runOrder</code> of your actions so that they have the same value</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p><img src="https://docs.aws.amazon.com/codepipeline/latest/userguide/images/PipelineFlow.png">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p>The pipeline structure format is used to build actions and stages in a pipeline. An action type consists of an action category and provider type. Valid action providers for each action category:</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>You can use the <code>runOrder</code> to specify parallel actions and use the same integer for each action you want to run in parallel. The default runOrder value for an action is 1. The value must be a positive integer (natural number). You cannot use fractions, decimals, negative numbers, or zero. Here, you need to specify a common <code>runOrder</code> value in your CloudFormation template so that all the stage actions happen in parallel.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i2.jpg"></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i3.jpg"></p>\n\n<p>via - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</strong> - As the test suites are HTTP and network-bound, increasing the RAM for Lambda and vCPU capacity of CodeBuild won\'t affect the performance (the bottleneck remains the network latency between each HTTP calls).</p>\n\n<p><strong>Enable CloudFormation StackSets to run the actions in parallel</strong> - CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.</p>\n\n<p><img src="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>CloudFormation StackSets is a distractor here, as they do not enable parallel actions.</p>\n\n<p><strong>Migrate all the test suites to Jenkins and use the ECS plugin</strong> - Migrating to Jenkins also would not solve the problem, as the test suites would still happen sequentially.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Change the <code>runOrder</code> of your actions so that they have the same value</p>', '<p>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</p>', '<p>Enable CloudFormation StackSets to run the actions in parallel</p>', '<p>Migrate all the test suites to Jenkins and use the ECS plugin</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'As part of your CodePipeline, you are running multiple test suites. Two are bundled as Docker containers and run directly on CodeBuild, while another one runs as a Lambda function executing Python code. All these test suites are based on HTTP requests and upon analyzing, these are found to be network bound, not CPU bound. Right now, the CodePipeline takes a long time to execute as these actions happen one after the other. They prevent the company from adding further tests. The whole pipeline is managed by CloudFormation.\n\nAs a DevOps Engineer, which of the following would you recommend improving the completion time of your pipeline?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357264, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>The DevOps team at a geological hazard monitoring agency maintains an application that provides near real-time notifications to Android and iOS devices during tremors, volcanic eruptions and tsunamis. The team has created a CodePipeline pipeline, which consists of CodeCommit and CodeBuild, and the application is deployed on Elastic Beanstalk. The team would like to enable Blue/Green deployments for Beanstalk through CodePipeline.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution for this requirement?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a Custom Job using AWS Lambda, which will perform the API call to swap the CNAME of the environments</strong></p>\n\n<p>AWS Elastic Beanstalk makes it easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.</p>\n\n<p>When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments — blue and green — increases availability and reduces risk. The blue environment is the production environment that normally handles live traffic. The CI/CD pipeline architecture creates a clone (green) of the live Elastic Beanstalk environment (blue). The pipeline then swaps the URLs between the two environments. While CodePipeline deploys application code to the original environment — and testing and maintenance take place — the temporary clone environment handles the live traffic. Once deployment to the blue environment is successful, and code review and code testing are done, the pipeline again swaps the URLs between the green and blue environments. The blue environment starts serving the live traffic again, and the pipeline terminates the green environment.</p>\n\n<p>Blue-Green Deployments to AWS Elastic Beanstalk using Code Pipeline:\n<img src="https://d1.awsstatic.com/partner-network/QuickStart/datasheets/blue-green-deployment-on-aws-architecture1.68038404e92ea779a2f8f011139eabf9d8678bd2.png"></p>\n\n<p>via - <a href="https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf">https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf</a></p>\n\n<p>To perform Blue/Green in Elastic Beanstalk, you need to deploy to a new environment and do a CNAME swap. The CNAME swap feature is not supported by CloudFormation itself, therefore you need to create a custom Lambda function that will perform that API call for you and invoke it as part of a Custom Job in CodePipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make CodePipeline deploy to the current Beanstalk environment using a rolling with additional batch strategy. Add a CodeDeploy stage action afterward to enable Blue / Green</strong></p>\n\n<p><strong>Make CodePipeline deploy to the current Beanstalk environment using an immutable strategy. Add a CodeStar stage action afterward to enable Blue / Green configured through the <code>template.yml</code> file</strong></p>\n\n<p>As explained above, to perform Blue/Green in Elastic Beanstalk, you need to deploy to a new environment and NOT to the current environment. So both these options are incorrect. You should also note that CodeStar is not a stage actor, it\'s a service that wraps up all CICD services from AWS into one simple UI to use as a developer.</p>\n\n<p><strong>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a CloudFormation template that will perform a CNAME swap</strong> - As mentioned in the explanation above, The CNAME swap feature is not supported by CloudFormation itself, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf">https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a Custom Job using AWS Lambda, which will perform the API call to swap the CNAME of the environments</p>', '<p>Make CodePipeline deploy to the current Beanstalk environment using a rolling with additional batch strategy. Add a CodeDeploy stage action afterward to enable Blue / Green</p>', '<p>Make CodePipeline deploy to the current Beanstalk environment using an immutable strategy. Add a CodeStar stage action afterward to enable Blue / Green configured through the <code>template.yml</code> file</p>', '<p>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a CloudFormation template that will perform a CNAME swap</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'The DevOps team at a geological hazard monitoring agency maintains an application that provides near real-time notifications to Android and iOS devices during tremors, volcanic eruptions and tsunamis. The team has created a CodePipeline pipeline, which consists of CodeCommit and CodeBuild, and the application is deployed on Elastic Beanstalk. The team would like to enable Blue/Green deployments for Beanstalk through CodePipeline.\n\nAs a DevOps Engineer, how would you implement a solution for this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357232, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p>In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. It enables you to achieve greater levels of fault tolerance in your applications and seamlessly provides the required amount of load balancing capacity needed to distribute application traffic. If your business requirements demand a fault-tolerant Jenkins environment, your preferred setup might be a scenario in which multiple masters with their own workers are placed in separate Availability Zones.</p>\n\n<p>You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes.</p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/10/20/Diagram2.png">\nvia - <a href="https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p>For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p>As mentioned in the explanation above, if configured with EC2 instances in an Auto Scaling Group, the setup will be elastic in some ways, but probably expensive if the EC2 instances are not fully utilized at capacity. So these three options are not the best fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html">https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html</a></p>\n\n<p><a href="https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf">https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>', '<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>', '<p>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</p>', '<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</p>']}, 'correct_response': ['a'], 'section': 'Domain 1: SDLC Automation', 'question_plain': 'A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.\n\nWhich of the following solutions would you recommend?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357198, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>Your application is deployed on Elastic Beanstalk and you manage the configuration of the stack using a CloudFormation template. A new golden AMI is created every week and contains a hardened AMI that has all the necessary recent security patches. You have deployed over 100 applications using CloudFormation &amp; Beanstalk this way and you would like to ensure the newer AMI used for EC2 instances is updated every week. There are no standardization or naming conventions made across all the CloudFormation templates.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution for this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values.</p>\n\n<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. When you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation using the <code>UpdateStack</code> API.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q17-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n\n<p>This question is a hard one as many solutions are feasible with a degree of complexity. It\'s about identifying the simplest solution.</p>\n\n<p>For the given use-case, by having the CloudFormation parameters directly pointing at SSM Parameter Store, on any refresh made to the CloudFormation template by the Lambda function which is in turn triggered by CloudWatch events, the template itself will fetch the latest value from the SSM Parameter Store and will apply it accordingly. So this solution is the best fit for the given requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong> - Storing the AMI id in S3 is possible, but CloudFormation cannot source parameters from S3 and therefore there\'s no integration possible.</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Having a Lambda function fetch the parameter and pass it as a parameter to CloudFormation seems like a good idea, but if we remember the constraint that the parameters are not standardized and that there are no naming conventions, it is difficult for us to imagine a solution that would scale.</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Creating a Lambda function that would update the mapping section of each template would introduce changes to each template content at every update and would be highly complicated to implement. Additionally, the Lambda function would be hard to write and would have a lot of complexity in updating the mapping as there\'s no standardization.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n', 'answers': ['<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>', '<p>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>', '<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>', '<p>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'Your application is deployed on Elastic Beanstalk and you manage the configuration of the stack using a CloudFormation template. A new golden AMI is created every week and contains a hardened AMI that has all the necessary recent security patches. You have deployed over 100 applications using CloudFormation &amp; Beanstalk this way and you would like to ensure the newer AMI used for EC2 instances is updated every week. There are no standardization or naming conventions made across all the CloudFormation templates.\n\nAs a DevOps Engineer, how would you implement a solution for this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357212, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>The DevOps team at a leading travel-booking services company is using a CloudFormation template to deploy a Lambda function. The Lambda function code is uploaded into S3 into a file named <code>s3://my-bucket/my-lambda-code.zip</code> by CodePipeline after having passed all the required build checks. CodePipeline then invokes the CloudFormation template to deploy the new code. The team has found that although the CloudFormation template successfully runs, the Lambda function is not updated.</p>\n\n<p>As a DevOps Engineer, what can you do to quickly fix this issue? (Select three)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Upload the code every time to a new S3 bucket</strong></p>\n\n<p><strong>Upload the code every time with a new filename in the same bucket</strong></p>\n\n<p><strong>Enable S3 versioning and provide an <code>S3ObjectVersion</code> key</strong></p>\n\n<p>You can use CloudFormation to deploy and update compute, database, and many other resources in a simple, declarative style that abstracts away the complexity of specific resource APIs. CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p><img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png">\nvia - <a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Here, the issue is that CloudFormation does not detect a new file has been uploaded to S3 unless one of these parameters change:\n- S3Bucket\n- S3Key\n- S3ObjectVersion</p>\n\n<p>Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, you need to change the object key or version in the template.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q18-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Clear the Lambda cache with a Custom Job in CodePipeline before the CloudFormation step</strong> - This option has been added as a distractor as there\'s no such thing as a Lambda cache.</p>\n\n<p><strong>Add a pause of 3 seconds before starting the CloudFormation job. This is an eventual consistency issue due to overwriting PUT</strong> - This option has been added as a distractor as there\'s no such thing as an eventual consistency for CloudFormation.</p>\n\n<p><strong>Enable the SAM Framework option</strong> - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. Enabling SAM would require a re-write of the template, which won\'t be quick.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n', 'answers': ['<p>Upload the code every time to a new S3 bucket</p>', '<p>Upload the code every time with a new filename in the same bucket</p>', '<p>Enable S3 versioning and provide an <code>S3ObjectVersion</code> key</p>', '<p>Clear the Lambda cache with a Custom Job in CodePipeline before the CloudFormation step</p>', '<p>Add a pause of 3 seconds before starting the CloudFormation job. This is an eventual consistency issue due to overwriting PUT</p>', '<p>Enable the SAM Framework option</p>']}, 'correct_response': ['a', 'b', 'c'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'The DevOps team at a leading travel-booking services company is using a CloudFormation template to deploy a Lambda function. The Lambda function code is uploaded into S3 into a file named s3://my-bucket/my-lambda-code.zip by CodePipeline after having passed all the required build checks. CodePipeline then invokes the CloudFormation template to deploy the new code. The team has found that although the CloudFormation template successfully runs, the Lambda function is not updated.\n\nAs a DevOps Engineer, what can you do to quickly fix this issue? (Select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357318, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an <code>InsufficientCapabilitiesException</code>.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</strong></p>\n\n<p>With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack. Use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack, within a pipeline.</p>\n\n<p>You can use IAM with AWS CloudFormation to control what users can do with AWS CloudFormation, such as whether they can view stack templates, create stacks, or delete stacks.</p>\n\n<p>For the given use-case, <code>InsufficientCapabilitiesException</code> means that the CloudFormation stack is trying to create an IAM role but it doesn\'t have those specified capabilities. As such it must be configured in CodePipeline configuration for the Deploy CloudFormation stage action.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q19-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</strong> - The given exception is not related to the permissions of the user or the CodePipeline IAM Role running the CloudFormation template, so this option is incorrect.</p>\n\n<p><strong>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</strong> - A circular dependency, as the name implies, means that two resources are dependent on each other or that a resource is dependent on itself.</p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/circular-dependency-sg-group.png">\nvia - <a href="https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p>This option is incorrect as a circular dependency would trigger another error such as this:</p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/CircularDependencyerror.png">\nvia - <a href="https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p><strong>Increase the service limits for your S3 bucket limits as you\'ve reached it</strong> - This option has been added as a distractor as the exception has nothing to do with service limits for the S3 bucket.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n', 'answers': ['<p>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</p>', '<p>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</p>', '<p>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</p>', "<p>Increase the service limits for your S3 bucket limits as you've reached it</p>"]}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an InsufficientCapabilitiesException.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357224, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.</p>\n\n<p>What's the reason and how could this issue be fixed?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</strong></p>\n\n<p>In a CloudFormation template, you can use the AWS::CloudFormation::CustomResource or Custom::String resource type to specify custom resources. Custom resources provide a way for you to write custom provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation, such as when you create, update or delete a stack.</p>\n\n<p>Some resources must be empty before they can be deleted. For example, you must delete all objects in an Amazon S3 bucket or remove all instances in an Amazon EC2 security group before you can delete the bucket or security group.</p>\n\n<p>For this use-case, the issue is that the S3 bucket is not empty before being deleted, therefore you must implement a Custom Resource backed by Lambda which will clean the bucket for you.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q20-i1.jpg">\nvia - <a href="https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</strong> - CloudFormation can delete resources while they\'re being used, and a <code>WaitCondition</code> can be attached to EC2 instances and Auto Scaling Groups and NOT to Lambda function. AWS further recommends that for Amazon EC2 and Auto Scaling resources, you use a CreationPolicy attribute instead of wait conditions. Add a CreationPolicy attribute to those resources, and use the cfn-signal helper script to signal when an instance creation process has completed successfully.</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</strong> - This option has been added as a distractor. To clean it, you cannot use a <code>Delete: Force</code> as this is not a feature of CloudFormation.</p>\n\n<p><strong>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</strong> -  A stack policy is a JSON document that defines the update actions that can be performed on designated resources. Stack Policies are only used during CloudFormation stack updates.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p><a href="https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket">https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket</a></p>\n', 'answers': ['<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</p>', '<p>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</p>', '<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</p>', '<p>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.\n\nWhat's the reason and how could this issue be fixed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357256, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A 3D modeling company would like to deploy applications on Elastic Beanstalk with support for various programming languages with predictable and standardized deployment strategies. Some of these languages are supported (such as Node.js, Java, Golang) but others such as Rust are not supported. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following options would you recommend as the MOST efficient solution for this use-case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</strong></p>\n\n<p>Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can also choose your own platform, programming language, and any application dependencies (such as package managers or tools), which typically aren\'t supported by other platforms. Elastic Beanstalk can deploy a Docker image and source code to EC2 instances running the Elastic Beanstalk Docker platform. The platform offers multi-container (and single-container) support.</p>\n\n<p>A Dockerrun.aws.json file is an Elastic Beanstalk–specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. You can use a Dockerrun.aws.json file for a multi-container Docker environment. Dockerrun.aws.json describes the containers to deploy to each container instance (Amazon EC2 instance that hosts Docker containers) in the environment as well as the data volumes to create on the host instance for the containers to mount.</p>\n\n<p>Here, the most simple solution is to create a Docker container for each application. By using a Multi-Docker container configuration, we will be able to have a standardized deployment system across all the languages that we want to support.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q21-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</strong> - Creating custom platforms and packaging applications in S3 will be cumbersome across a wide array of platforms. Using a multi-Docker container configuration is more efficient.</p>\n\n<p><strong>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</strong> - Packaging each application as an AMI might work but it\'s not going to help you standardize the way applications are deployed.</p>\n\n<p><strong>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</strong> - AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks is a distractor in the question and doesn\'t have integration with Elastic Beanstalk.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html</a></p>\n', 'answers': ['<p>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</p>', '<p>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</p>', '<p>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</p>', '<p>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A 3D modeling company would like to deploy applications on Elastic Beanstalk with support for various programming languages with predictable and standardized deployment strategies. Some of these languages are supported (such as Node.js, Java, Golang) but others such as Rust are not supported. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following options would you recommend as the MOST efficient solution for this use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357254, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.</p>\n\n<p>As a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application\'s source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Note: Recommended values are applied when you create or update an environment on the Elastic Beanstalk API by a client. For example, the client could be the AWS Management Console, Elastic Beanstalk Command Line Interface (EB CLI), AWS Command Line Interface (AWS CLI), or SDKs. Recommended values are directly set at the API level and have the highest precedence. The configuration setting applied at the API level can\'t be changed using option_settings, as the API has the highest precedence.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n\n<p>Configuration changes made to your Elastic Beanstalk environment won\'t persist if you use the following configuration methods:</p>\n\n<p>Configuring an Elastic Beanstalk resource directly from the console of a specific AWS service.</p>\n\n<p>Installing a package, creating a file, or running a command directly from your Amazon EC2 instance.</p>\n\n<p>For the given use-case, using a <code>.ebextensions</code> file and configuring the rules in the <code>option_settings</code> block is the right option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</strong> - This option has been added as a distractor as you cannot configure CodePipeline to deploy using the EB CLI.</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</strong> - Using the EB CLI on your computer would normally work, but here the question specifies that we don\'t have the necessary permissions to make direct changes against the Beanstalk environment. We, therefore, have to use CodePipeline.</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</strong> - Using a <code>container_command</code> may work, but it wouldn\'t be best practice as the EC2 would issue a command to the ALB and therefore the configuration of it would be different from the one specified by Beanstalk itself, and the EC2 instance may not have enough permissions through IAM role to issue that command in the first place. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n', 'answers': ['<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</p>', '<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</p>', '<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</p>', '<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.\n\nAs a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357316, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a <code>.ebextensions</code> file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.</p>\n\n<p>Which of the following options would you suggest to address the given requirements?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use a rolling update with 20% at a time</strong></p>\n\n<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it\'s a scalable environment (you didn\'t specify the --single option), it uses rolling deployments.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Comparison of deployment method properties:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment\'s Amazon EC2 instances into batches (for this requirement, we shall use a batch with 20% of the instances) and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version. Therefore, for the given use-case, we should use a rolling update, which will keep our ASG, our instances, and ensure our application can still serve traffic.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a blue/green deployment and swap CNAMEs</strong> - In a blue/green deployment, you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. A blue/green deployment would create a new load balancer and ASG, but the CNAME swap would allow us to keep the same DNS name. So it does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use immutable</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. So this option does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use in-place</strong> - In-place would not work even though it doesn\'t create any new resources because your application will be unavailable as all your instances will be updated at the same time.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n', 'answers': ['<p>Use a rolling update with 20% at a time</p>', '<p>Use a blue/green deployment and swap CNAMEs</p>', '<p>Use immutable</p>', '<p>Use in-place</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a .ebextensions file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.\n\nWhich of the following options would you suggest to address the given requirements?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357304, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>What do you recommend for the application to ensure a good performance and address scalability requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a separate Beanstalk environment that\'s a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>AWS Elastic Beanstalk enables you to manage all of the resources that run your application as environments. An environment is a collection of AWS resources running an application version. When you launch an Elastic Beanstalk environment, you need to choose an environment tier. An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>Elastic Beanstalk Concepts:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p>When you create a web server environment, Beanstalk provisions the resources required to run your application. AWS resources created for this type of environment include one elastic load balancer, an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p>AWS resources created for a worker environment tier include an ASG, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Beanstalk also creates and provisions an SQS queue if you don’t already have one. When you launch a worker environment, Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the ASG. The daemon reads messages from an SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing.</p>\n\n<p>For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the <code>cron.yml</code> file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.</p>\n\n<p>Elastic Beanstalk Worker environment:\n<img src="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-architecture_worker.png">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that\'s a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong> - As mentioned in the explanation above, the worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk environment that\'s a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The <code>cron.yml</code> file must be defined on the worker tier, it is not supported by the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that\'s a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n', 'answers': ["<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>", "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>", "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>", "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>"]}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhat do you recommend for the application to ensure a good performance and address scalability requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357294, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.</p>\n\n<p>How can you fix this issue?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application\'s source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p>You may want to customize and configure the software that your application depends on. You can use the commands key to execute commands on the EC2 instance. The commands run before the application and web server are set up and the application version file is extracted.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>You can use the container_commands key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>If you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. Therefore you must use <code>container_commands</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong> - As mentioned earlier, if you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. So this option is incorrect.</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p>The <code>lock_mode: true</code> attribute has been added as a distractor and it does not exist. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352">https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n', 'answers': ['<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>', '<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>', '<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>', '<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.\n\nHow can you fix this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357336, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>The DevOps team at a social media company has created a CodePipeline pipeline and the final step is to use CodeDeploy to update an AWS Lambda function. As a DevOps Engineering Lead at the company, you have decided that for every deployment, the new Lambda function must sustain a small amount of traffic for 10 minutes and then shift all the traffic to the new function. It has also been decided that safety must be put in place to automatically roll-back if the Lambda function experiences too many crashes.</p>\n\n<p>Which of the following recommendations would you provide to address the given use-case? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.</p>\n\n<p>For the given use-case, the CodeDeploy deployment must be associated with a CloudWatch Alarm for automated rollbacks.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></strong></p>\n\n<p>A deployment configuration is a set of rules and success and failure conditions used by CodeDeploy during a deployment. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p>For canary deployments, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p>\n\n<p>A canary deployment of <code>LambdaCanary10Percent10Minutes</code> means the traffic is 10% on the new function for 10 minutes, and then all the traffic is shifted to the new version after the time has elapsed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaAllAtOnce</code></strong> - An all at once deployment means all the traffic is shifted to the new function right away and this option does not meet the given requirements.</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></strong> - For linear deployments, traffic is shifted in equal increments with an equal number of minutes between each increment. For example, a linear deployment of <code>LambdaLinear10PercentEvery10Minutes</code> would shift 10 percent of traffic every minute until all traffic is shifted.</p>\n\n<p><strong>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</strong> - The CodeDeploy deployment must be associated with a CloudWatch Alarm and not CloudWatch Event for automated rollbacks to work.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n', 'answers': ['<p>Choose a deployment configuration of <code>LambdaAllAtOnce</code></p>', '<p>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</p>', '<p>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></p>', '<p>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></p>', '<p>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</p>']}, 'correct_response': ['b', 'c'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'The DevOps team at a social media company has created a CodePipeline pipeline and the final step is to use CodeDeploy to update an AWS Lambda function. As a DevOps Engineering Lead at the company, you have decided that for every deployment, the new Lambda function must sustain a small amount of traffic for 10 minutes and then shift all the traffic to the new function. It has also been decided that safety must be put in place to automatically roll-back if the Lambda function experiences too many crashes.\n\nWhich of the following recommendations would you provide to address the given use-case? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357326, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</strong></p>\n\n<p>While using CloudFormation, you work with templates and stacks. You create templates to describe your AWS resources and their properties. When you use AWS CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack\'s AWS CloudFormation template.</p>\n\n<p>In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file.</p>\n\n<p>CloudFormation best practices:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q27-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</strong></p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</strong></p>\n\n<p>The issue with both these options is that different teams are working on different pieces of the infrastructure with their own timelines, so it\'s difficult to combine all elements of the infrastructure into a single master template. It\'s much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks. Nested Stacks can be helpful if a component configuration (such as a Load Balancer) can be reused across many stacks.</p>\n\n<p><strong>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</strong> - Using outputs and exports for individual templates is much better than collaborating via pull requests at code repository level. Using individual templates gives ownership to the contributing team to make sure that the CloudFormation templates are always functional and ready to be referenced in other stacks.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n', 'answers': ['<p>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</p>', '<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</p>', '<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</p>', '<p>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357194, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>An ed-tech company has created a paid-per-use API using API Gateway. This API is available at <code>http://edtech.com/api/v1</code>. The website's static files have been uploaded in S3 and now support a new API route <code>http://edtech.com/api/v1/new-feature</code> if available. Your team has decided it is safer to send a small amount of traffic to that route first and test if the metrics look okay. Your API gateway routes are backed by AWS Lambda.</p>\n\n<p>As a DevOps Engineer, what steps should you take to enable this testing?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using CloudWatch</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a "front door" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications.</p>\n\n<p>How API Gateway Works:\n<img src="https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png">\nvia - <a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>Simply creating and developing an API Gateway API doesn\'t automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API. You can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing. In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. The updated API features are only visible to the canary release. The canary release receives a small percentage of API traffic and the production release takes up the rest.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q28-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n\n<p>For the given use-case, you must deploy API to a new stage called v1, enable canary deployment on this v1 stage and assign a small amount of traffic to this canary stage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using Amazon ES</strong> - API Gateway &amp; AWS Lambda have a direct integration with CloudWatch and NOT with Amazon ES. So this option is incorrect.</p>\n\n<p><strong>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using CloudWatch</strong> - When a new API route is implemented, you must create a new API Gateway stage and NOT a Lambda alias. Lambda aliases are only used to update the behavior of an existing route. Remember that one route in API Gateway is mapped to one AWS Lambda function (or another service).</p>\n\n<p><strong>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using Amazon ES</strong> - When a new API route is implemented, you must create a new API Gateway stage and NOT a Lambda alias. In addition, API Gateway &amp; AWS Lambda have a direct integration with CloudWatch and NOT with Amazon ES.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n', 'answers': ['<p>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using CloudWatch</p>', '<p>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using Amazon ES</p>', '<p>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using CloudWatch</p>', '<p>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using Amazon ES</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "An ed-tech company has created a paid-per-use API using API Gateway. This API is available at http://edtech.com/api/v1. The website's static files have been uploaded in S3 and now support a new API route http://edtech.com/api/v1/new-feature if available. Your team has decided it is safer to send a small amount of traffic to that route first and test if the metrics look okay. Your API gateway routes are backed by AWS Lambda.\n\nAs a DevOps Engineer, what steps should you take to enable this testing?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357276, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It\'s a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src="https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png">\nvia - <a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src="https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png">\nvia - <a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider\'s response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href="https://aws.amazon.com/step-functions/faqs/">https://aws.amazon.com/step-functions/faqs/</a></p>\n', 'answers': ['<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>', '<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>', '<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>', "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"]}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.\n\nAs a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357192, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>"color"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>"color": "none"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>"color": "none"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a "front door" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn\'t automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src="https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png">\nvia - <a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>"color": "none"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>"color"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>"color": "none"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>"color": "none"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint\'s responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a></p>\n', 'answers': ['<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>"color": "none"</code> on requests</p>', '<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>"color"</code> field</p>', '<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>"color": "none"</code> to the JSON request being passed on your API Gateway stage</p>', '<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>"color": "none"</code> default value</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'A graphics design company is experimenting with a new feature for an API and the objective is to pass the field "color" in the JSON payload to enable this feature. The new Lambda function should treat "color": "none" as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the v1 stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357338, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local <code>Dockerfile</code>, and then pushes to ECR at <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code>. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.</p>\n\n<p>How should you implement a solution to address this issue?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</strong></p>\n\n<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the <code>IMAGEID</code> property, which is the SHA digest for the Docker image used to start the container.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q31-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n\n<p>The issue here is that the ECS instances do not detect that a newer image version is available, because the name <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is re-used. Therefore, by specifying the sha256 e.g.: <code>aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE</code>, we are certain that newer versions of the Docker image will have a different hash value and therefore the ECS cluster will always pull the newest image at the end of our CICD Pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</strong> - SSM Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. SSM Run Command may work but it\'s not an elegant solution.</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</strong> - Lambda Functions can\'t SSH into EC2 instances, so this option is incorrect.</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</strong> - Adding the <code>latest</code> tag won\'t help because <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is same as <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app:latest</code>. The <code>latest</code> tag cannot provide visibility and identification to track where container images are deployed.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/">https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n', 'answers': ['<p>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</p>', '<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</p>', '<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</p>', '<p>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': 'As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local Dockerfile, and then pushes to ECR at 123456789.dkr.ecr.region.amazonaws.com/my-web-app. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.\n\nHow should you implement a solution to address this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357332, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer\'s instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH\'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance\'s first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href="https://aws.amazon.com/opsworks/">https://aws.amazon.com/opsworks/</a></p>\n', 'answers': ['<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>', '<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>', '<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>', '<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>']}, 'correct_response': ['a'], 'section': 'Domain 2: Configuration Management and IaC', 'question_plain': "A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.\n\nWhich of the following solutions would you recommend for the given requirement?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357252, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An Internet-of-Things (IoT) solutions company has decided to release every single application as a Docker container and to use ECS classic (on EC2) as the container orchestration system and ECR as the Docker registry. Part of implementing a monitoring pipeline is to ensure all application logs can be stored in CloudWatch logs.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to provide the simplest possible instructions to accomplish this objective. What are these instructions?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</strong></p>\n\n<p>Here many solutions may work but we\'re looking for the simplest possible solution. The important thing to remember is that the ECS task definitions can include the <code>awslogs</code> driver and write to CloudWatch Logs natively. But the EC2 instance will be the one writing to CloudWatch, and therefore it must have an EC2 Instance Role with the appropriate permissions to write to CloudWatch. Your Amazon ECS container instances also require logs:CreateLogStream and logs:PutLogEvents permission on the IAM role with which you launch your container instances</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q33-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - As mentioned in the explanation above, you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</strong> - This is a roundabout way of getting the container logs to the CloudWatch Logs, so not the best fit for the given use-case.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application\'s <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - Sidecar containers are a common software pattern that has been embraced by engineering organizations. It’s a way to keep server-side architecture easier to understand by building with smaller, modular containers that each serve a simple purpose. Just like an application can be powered by multiple microservices, each microservice can also be powered by multiple containers that work together. A sidecar container is simply a way to move part of the core responsibility of a service out into a containerized module that is deployed alongside a core application container. This again seems to be a roundabout way of getting the container logs to the CloudWatch Logs, but it\'s not correct for the given use-case. You should note that you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html</a></p>\n', 'answers': ['<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>', '<p>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</p>', '<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</p>', "<p>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application's <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>"]}, 'correct_response': ['c'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'An Internet-of-Things (IoT) solutions company has decided to release every single application as a Docker container and to use ECS classic (on EC2) as the container orchestration system and ECR as the Docker registry. Part of implementing a monitoring pipeline is to ensure all application logs can be stored in CloudWatch logs.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to provide the simplest possible instructions to accomplish this objective. What are these instructions?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357214, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following would you suggest as the most effective solution?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p>How CloudTrail Works:\n<img src="https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png">\nvia - <a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p>For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail <code>validate-logs</code> command.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</strong> -  S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn\'t provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won\'t give us the information on who made the API calls.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won\'t give us the information of who made the API calls. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn\'t provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p><a href="https://aws.amazon.com/cloudtrail/faqs/">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n', 'answers': ['<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</p>', '<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</p>', '<p>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</p>', '<p>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following would you suggest as the most effective solution?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357296, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution to meet these requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</strong></p>\n\n<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p>\n\n<p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore we have to subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q35-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon ES which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</strong> - If the log destination is a Lambda function, this could work, but it will be a problem as this Lambda function sends the data to Amazon ES, which is not a serverless service and requires provisioning.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n', 'answers': ['<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</p>', '<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</p>', '<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</p>', '<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.\n\nAs a DevOps Engineer, how would you implement a solution to meet these requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357278, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An analytics company is capturing metrics for its AWS services and applications using CloudWatch metrics. It needs to be able to go back up to 7 years in time for visualizing these metrics due to regulatory requirements. As a DevOps Engineer at the company, you have been tasked with designing a solution that will help the company comply with the regulations.</p>\n\n<p>Which of the following options requires the minimum development effort to address the given requirements?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</strong></p>\n\n<p>You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations.</p>\n\n<p>You can create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3.  You can then use tools such as Amazon Athena to get insight into cost optimization, resource performance, and resource utilization for given data ranges. In additon, you can use a QuickSight dashboard to visualize the metrics.</p>\n\n<p>The Kinesis Data Firehose delivery stream must trust CloudWatch through an IAM role that has write permissions to Kinesis Data Firehose. These permissions can be limited to the single Kinesis Data Firehose delivery stream that the CloudWatch metric stream uses. The IAM role must trust the streams.metrics.cloudwatch.amazonaws.com service principal.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</strong></p>\n\n<p>A CloudWatch metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. For example, the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2. The data points themselves can come from any application or business activity from which you collect data.</p>\n\n<p>Metrics cannot be deleted, but they automatically expire after 15 months if no new data is published to them. Data points older than 15 months expire on a rolling basis; as new data points come in, data older than 15 months is dropped.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q36-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p>As the CloudWatch metrics can only be retained for 15 months, we need to use a CloudWatch Event rule and trigger a Lambda function to extract metrics and send them for long term retention to facilitate visual analysis. Here, the only solution that works end-to-end is to send the data to Amazon ES, and use Kibana to create graphs.</p>\n\n<p>Amazon Elasticsearch (ES) Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.</p>\n\n<p>How Amazon ElasticSearch Works:\n<img src="https://d1.awsstatic.com/re19/ESS_HIW.db99588614cbf44e2d62ef9c9c173ebfe41e2834.png">\nvia - <a href="https://aws.amazon.com/elasticsearch-service/">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p>ES is commonly deployed as part of the ELK stack which is an acronym used to describe a stack that comprises three popular open-source projects: Elasticsearch, Logstash, and Kibana. The ELK stack gives you the ability to aggregate logs from all your systems and applications, analyze these logs, and create visualizations for application and infrastructure monitoring, faster troubleshooting, security analytics, and more.</p>\n\n<p><img src="https://d1.awsstatic.com/product-marketing/Elasticsearch/Amazon%20ES%20ELK%20diagram.9d830908067fb7bedb52c6738126f2dfe18b611a.png">\nvia - <a href="https://aws.amazon.com/elasticsearch-service/the-elk-stack/">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n\n<p>As this option requires you to create a Lambda function that will execute a custom API to export the metrics into an ES cluster, so it\'s not the best fit for the given requirement as it involves significant development effort.</p>\n\n<p><strong>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable \'Extended Retention\' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</strong> - This option has been added as a distractor as CloudWatch metrics do not have an \'Extended Retention\' feature.</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</strong> - S3 based data can be integrated easily with QuickSight, however, CloudWatch dashboards can only consume CloudWatch metrics and NOT data/metrics from S3.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p><a href="https://aws.amazon.com/elasticsearch-service/">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p><a href="https://aws.amazon.com/elasticsearch-service/the-elk-stack/">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n', 'answers': ['<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</p>', "<p>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable 'Extended Retention' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</p>", '<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</p>', '<p>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</p>']}, 'correct_response': ['d'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'An analytics company is capturing metrics for its AWS services and applications using CloudWatch metrics. It needs to be able to go back up to 7 years in time for visualizing these metrics due to regulatory requirements. As a DevOps Engineer at the company, you have been tasked with designing a solution that will help the company comply with the regulations.\n\nWhich of the following options requires the minimum development effort to address the given requirements?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357202, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A global financial services company manages over 100 accounts using AWS Organizations and it has recently come to light that several accounts and regions did not have AWS CloudTrail enabled. It also wants to be able to track the compliance of the CloudTrail enablement as a dashboard, and automatically be alerted in case of issues. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you go about implementing a solution for this use-case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong></p>\n\n<p>CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.</p>\n\n<p><img src="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src="https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>For the given use-case, we need to enable CloudTrail and AWS Config in all accounts and all regions. For this, we\'ll need separate StackSets to create CloudTrail and enable Config in all accounts and all regions. Note that we\'ll also need an AWS Config aggregator in a centralized account. Finally, compliance breaches would generate CloudWatch events that can be subscribed by a Lambda function to further send out notifications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - This option has been added as a distractor. There is no such thing as an SQS topic.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n', 'answers': ['<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>', '<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>', '<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>', '<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A global financial services company manages over 100 accounts using AWS Organizations and it has recently come to light that several accounts and regions did not have AWS CloudTrail enabled. It also wants to be able to track the compliance of the CloudTrail enablement as a dashboard, and automatically be alerted in case of issues. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you go about implementing a solution for this use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357266, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.</p>\n\n<p>As a DevOps Engineer, how can you implement a solution for this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src="https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png">\nvia - <a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png">\nvia - <a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i1.jpg">\nvia - <a href="https://aws.amazon.com/config/faq/">https://aws.amazon.com/config/faq/</a></p>\n\n<p>You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the <code>cloudtrail-enabled</code> Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use <code>cloudtrail-security-trail-enabled</code> Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named \'everyone\'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the \'everyone\' group automatically</strong></p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named \'everyone\'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong></p>\n\n<p>IAM users in a group with a deny policy sounds like a great idea at first, but then you have to remember you can create IAM roles, and they won\'t have that restriction, and as such you will be able to assume these roles and then issue API calls on CloudTrail to de-activate it. This solution won\'t work and therefore both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong> - You need to have an AWS Config rule to maintain auditability and track compliance over time, as using the Lambda function to trigger an API call would tell you about the CloudTrail status only at that point in time.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/cloudtrail/faqs/">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n\n<p><a href="https://aws.amazon.com/config/faq/">https://aws.amazon.com/config/faq/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n', 'answers': ['<p>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</p>', "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</p>", '<p>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>', "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>"]}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.\n\nAs a DevOps Engineer, how can you implement a solution for this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357190, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the "100 errors" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n', 'answers': ['<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>', '<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>', '<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>', '<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.\n\nWhich of the following options represents the most efficient solution in your opinion?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357334, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src="https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png">\nvia - <a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the \'AWS API Call via CloudTrail\' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we\'re streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n', 'answers': ['<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>', '<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>', '<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>', '<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A gaming company would like to be able to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.\n\nAs a DevOps Engineer at the company, how would you implement this at a minimal cost?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357280, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png">\nvia - <a href="https://aws.amazon.com/config/">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/tagging.html">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/tagging.html">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n', 'answers': ['<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>', '<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>', '<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>', '<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>']}, 'correct_response': ['a'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.\n\nWhich of the following options would you suggest to address the use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357310, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A social media company has multiple EC2 instances that are behind an Auto Scaling group (ASG) and you would like to retrieve all the log files within the instances before they are terminated. You would like to also build a metadata index of all the log files so you can efficiently find them by instance id and date range.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend to address the given requirements? (Select three)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an ASG launches or terminates them. For example, when a scale-in event occurs, the terminating instance is first deregistered from the load balancer (if the Auto Scaling group is being used with Elastic Load Balancing). Then, a lifecycle hook pauses the instance before it is terminated. While the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>You can use a CloudWatch Events rule to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to CloudWatch Events. The event contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action. Finally, the Lambda function can invoke an SSM Run Command to send the log files from the EC2 instance to S3. SSM Run Command lets you remotely and securely manage the configuration of your managed instances.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i2.jpg"></p>\n\n<p><strong>Create a Lambda function that is triggered by S3 events for <code>PUT</code>. Write to the DynamoDB table</strong></p>\n\n<p>You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted. Amazon S3 invokes your function asynchronously with an event that contains details about the object. The Lambda would further write the event information into the DynamoDB table.</p>\n\n<p><strong>Create a DynamoDB table with a primary key of instance-id and a sort key of datetime</strong></p>\n\n<p>When you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. For the given use-case, you need to set the primary key as a combination of partition key of instance-id and a sort key of datetime as we are looking for a specific instance id and a date range.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to CloudWatch Logs. Create a log subscription to send it to Firehose and then S3</strong> - We must send the log files to S3 directly from the EC2 instance instead of through CloudWatch, as we\'re doing a one time dump of them. CloudWatch Logs are a good solution for streaming logs as they are created.</p>\n\n<p><strong>Create a Lambda function that is triggered by CloudWatch Events for <code>PUT</code>. Write to the DynamoDB table</strong> - We need to have the Lambda function triggered by S3 events instead of CloudWatch Events, as for CloudWatch Events we would need to also have a CloudTrail trail recording action on the specific S3 bucket.</p>\n\n<p><strong>Create a DynamoDB table with a primary key of datetime and a sort key of instance-id</strong> - As mentioned in the explanation above, since the use-case requires looking up for a specific instance id and a date range, you should use instance-id as the Partition Key and datetime as the Sort Key. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey</a></p>\n', 'answers': ['<p>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to S3</p>', '<p>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to CloudWatch Logs. Create a log subscription to send it to Firehose and then S3</p>', '<p>Create a Lambda function that is triggered by S3 events for <code>PUT</code>. Write to the DynamoDB table</p>', '<p>Create a Lambda function that is triggered by CloudWatch Events for <code>PUT</code>. Write to the DynamoDB table</p>', '<p>Create a DynamoDB table with a primary key of instance-id and a sort key of datetime</p>', '<p>Create a DynamoDB table with a primary key of datetime and a sort key of instance-id</p>']}, 'correct_response': ['a', 'c', 'e'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'A social media company has multiple EC2 instances that are behind an Auto Scaling group (ASG) and you would like to retrieve all the log files within the instances before they are terminated. You would like to also build a metadata index of all the log files so you can efficiently find them by instance id and date range.\n\nAs a DevOps Engineer, which of the following options would you recommend to address the given requirements? (Select three)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357290, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.</p>\n\n<p>Which of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</strong></p>\n\n<p>SSM Agent is an Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then run them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).</p>\n\n<p>SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated.</p>\n\n<p>Sample Inventory Cards:\n<img src="https://docs.aws.amazon.com/systems-manager/latest/userguide/images/inventory-cards.png">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><strong>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</strong></p>\n\n<p>Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</strong> - SSM does not have any native capability to find out which instances are not currently tracked by the SSM service.</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</strong> - You can use SSM Automation to build Automation workflows to configure and manage instances and AWS resources. You can also create custom workflows or use pre-defined workflows maintained by AWS. For the given requirement, SSM Automation could be used to get the list of packages but it would require a lot of manual work, so it is not the best fit for the given use-case.</p>\n\n<p><strong>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</strong> - Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n', 'answers': ['<p>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</p>', '<p>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</p>', '<p>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</p>', '<p>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</p>', '<p>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 4: Monitoring and Logging', 'question_plain': 'The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.\n\nWhich of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357218, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix \'mi-\' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src="https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png">\nvia - <a href="https://aws.amazon.com/systems-manager/">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don\'t need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix "mi-". Amazon EC2 instance IDs use the prefix "i-".</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix \'i-\' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix "mi-" whereas the Amazon EC2 instance IDs use the prefix "i-".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix \'mi-\' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix \'i-\' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href="#">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n', 'answers': ["<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>", "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>", "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>", "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"]}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.\n\nHow would you set up the on-premise server to achieve this objective?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357302, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.</p>\n\n<p>Which of the following options represents the BEST solution to meet this requirement?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong></p>\n\n<p>SSM Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p>Patch Manager provides predefined patch baselines for each of the operating systems supported by Patch Manager. You can use these baselines as they are currently configured (you can\'t customize them) or you can create your own custom patch baselines. Custom patch baselines allow you greater control over which patches are approved or rejected for your environment.</p>\n\n<p>When you use the default repositories configured on an instance for patching operations, Patch Manager scans for or installs security-related patches. This is the default behavior for Patch Manager.</p>\n\n<p>On Linux systems, however, you can also use Patch Manager to install patches that are not related to security, or that are in a different source repository than the default one configured on the instance. You can specify alternative patch source repositories when you create a custom patch baseline. In each custom patch baseline, you can specify patch source configurations for up to 20 versions of a supported Linux operating system. You can then set up a weekly maintenance window and include the Run Command RunPatchBaseline.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using SSM Parameter Store, configure the custom repositories in the OS\' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - SSM Parameter Store is used to store parameter values but cannot write configuration files on EC2 instances (the EC2 instances would have to fetch the value from the Parameter Store instead).</p>\n\n<p><strong>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the \'configure\' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</strong> - Using chef cookbooks via OpsWorks may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p><strong>Using SSM, do a RunCommand to install the custom repositories in the OS\' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - Using SSM RunCommand may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html</a></p>\n', 'answers': ['<p>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>', "<p>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>", "<p>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</p>", "<p>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>"]}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.\n\nWhich of the following options represents the BEST solution to meet this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357286, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n', 'answers': ['<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>', '<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>', '<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>', '<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>', '<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.\n\nWhich of the following options would you recommend for this use-case? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357306, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.</p>\n\n<p>How should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your EBS volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules.</p>\n\n<p>AWS Config uses Amazon SNS to deliver notifications to subscription endpoints. These notifications provide the delivery status for configuration snapshots and configuration histories, and they provide each configuration item that AWS Config creates when the configurations of recorded AWS resources change. AWS Config also sends notifications that show whether your resources are compliant with your rules. SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and NOT selectively for a given rule.</p>\n\n<p>AWS Config has a managed rule to check for EBS volume encryption. For the given use-case, you need to isolate alerts for this managed rule, so you have to use CloudWatch Events which can then have a specific SNS topic as a target for alerting.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i2.jpg"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p><strong>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p>As mentioned in the explanation above, SNS topics in Config can only be used to stream all the notifications and configuration changes. To isolate alerts for a single rule, you have to use CloudWatch Events. Therefore both these options are incorrect.</p>\n\n<p><strong>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</strong> - Using AWS Lambda may work, but it will not provide you the auditing capability that AWS Config provides (a timeline dashboard with compliance over time).</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/">https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/</a></p>\n', 'answers': ['<p>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</p>', '<p>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</p>', '<p>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</p>', '<p>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</p>']}, 'correct_response': ['b'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.\n\nHow should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357228, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.</p>\n\n<p>As a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</strong></p>\n\n<p>AWS Service Catalog allows IT administrators to create, manage, and distribute catalogs of approved products to end-users, who can then access the products they need in a personalized portal. Administrators can control which users have access to each product to enforce compliance with organizational business policies.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p>A product is a service or application for end-users. A portfolio is a collection of products, with configuration information that determines who can use those products and how they can use them. A catalog is a collection of products that the administrator creates, adds to portfolios, and provides updates for using AWS Service Catalog. To create a Service Catalog product, you first need to create an AWS CloudFormation template by using an existing AWS CloudFormation template or creating a custom template. Then you can use the AWS Service Catalog console to upload the template and create the product.</p>\n\n<p>Therefore, for the given use-case, we need to use Service Catalog as it was precisely designed for that purpose and give users only access to the stack they should be able to create in Service Catalog.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i2.jpg">\nvia - <a href="https://aws.amazon.com/servicecatalog/faqs/">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i3.jpg">\nvia - <a href="https://aws.amazon.com/servicecatalog/faqs/">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</strong> - If you let IAM users use the CloudFormation service directly, they will have the power to create any resource through their permissions. You cannot restrict templates using IAM policies in CloudFormation.</p>\n\n<p><strong>Create AWS Config custom rules that will check for the compliance of your company\'s resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</strong> - AWS Config Rules would be a way to "monitor" the situation but not prevent resources from being created the wrong way.</p>\n\n<p><strong>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</strong> - An IAM policy cannot have a "conditional approval", so this option is a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/servicecatalog/faqs/">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/">https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/</a></p>\n', 'answers': ['<p>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</p>', '<p>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</p>', "<p>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</p>", '<p>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': "A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.\n\nAs a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357292, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A financial services company is using security-hardened AMI due to strong regulatory compliance requirements. The company must be able to check every day for AMI vulnerabilities based on the newly disclosed ones through the common vulnerabilities and exposures (CVEs) program. Currently, all the instances are launched through an Auto Scaling group (ASG) leveraging the latest security-hardened AMI.</p>\n\n<p>As a DevOps Engineer, how can you implement this while minimizing cost and application disruption?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule, the target being a Step Function. The Step Function should launch an EC2 instance from the AMI and tag it with CheckVulnerabilities: True. The Step Function then starts an AMI assessment template using AWS Inspector and the above tag. Terminate the instance afterward</strong></p>\n\n<p>AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>A golden AMI is an AMI that contains the latest security patches, software, configuration, and software agents that you need to install for logging, security maintenance, and performance monitoring. A security best practice is to perform routine vulnerability assessments of your golden AMIs to identify if newly found vulnerabilities apply to them. If you identify a vulnerability, you can update your golden AMIs with the appropriate security patches, test the AMIs, and deploy the patched AMIs in your environment.</p>\n\n<p>You can create an EC2 instance from the golden AMI and then run an Amazon Inspector security assessment on the created instance. Amazon Inspector performs security assessments of Amazon EC2 instances by using AWS managed rules packages such as the Common Vulnerabilities and Exposures (CVEs) package.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q49-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p>\n\n<p>So to summarize, the most cost-effective and the least disruptive way to do an assessment is to create an EC2 instance from an AMI for that very purpose, run the assessment and then finally terminate the instance. Step Functions are perfect to orchestrate that workflow by targeting the instances tagged with CheckVulnerabilities: True.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule. Invoke a Lambda Function that will start an AWS Inspector Run directly from the AMI reference in the API call. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</strong> - AWS Inspector cannot run an assessment directly on an AMI, it will not launch an EC2 instance for you. Therefore, you need to make sure an EC2 instance is created in advance from that AMI, with the proper tag on the EC2 instance to match the assessment target.</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule. Make the target of the rule being AWS Inspector and pass some extra data in the rule using the AMI ID to inspect. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</strong> - AWS Inspector cannot run an assessment directly on an AMI, it will not launch an EC2 instance for you. Therefore, you need to make sure an EC2 instance is created in advance from that AMI, with the proper tag on the EC2 instance to match the assessment target.</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule, the target being a Lambda Function. Tag all the instances in your ASG with <code>CheckVulnerabilities: True</code>. The Lambda function should start an assessment in AWS Inspector targeting all instances having the tag</strong> - If you launch an assessment on all the instances in an ASG, it will be problematic from a cost perspective as you will be testing the same AMI for as many instances that are part of the ASG. This will also incur extra AWS Inspector charges.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p>\n\n<p><a href="https://aws.amazon.com/step-functions/faqs/">https://aws.amazon.com/step-functions/faqs/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cves.html">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cves.html</a></p>\n', 'answers': ['<p>Create a CloudWatch Event with a daily schedule, the target being a Step Function. The Step Function should launch an EC2 instance from the AMI and tag it with CheckVulnerabilities: True. The Step Function then starts an AMI assessment template using AWS Inspector and the above tag. Terminate the instance afterward</p>', '<p>Create a CloudWatch Event with a daily schedule. Invoke a Lambda Function that will start an AWS Inspector Run directly from the AMI reference in the API call. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</p>', '<p>Create a CloudWatch Event with a daily schedule. Make the target of the rule being AWS Inspector and pass some extra data in the rule using the AMI ID to inspect. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</p>', '<p>Create a CloudWatch Event with a daily schedule, the target being a Lambda Function. Tag all the instances in your ASG with <code>CheckVulnerabilities: True</code>. The Lambda function should start an assessment in AWS Inspector targeting all instances having the tag</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A financial services company is using security-hardened AMI due to strong regulatory compliance requirements. The company must be able to check every day for AMI vulnerabilities based on the newly disclosed ones through the common vulnerabilities and exposures (CVEs) program. Currently, all the instances are launched through an Auto Scaling group (ASG) leveraging the latest security-hardened AMI.\n\nAs a DevOps Engineer, how can you implement this while minimizing cost and application disruption?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357284, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The technology team at a leading bank is using software that has a license type that gets billed based on the number of CPU sockets that are being used. The team would like to ensure that they are using the most appropriate EC2 launch mode and create a compliance dashboard that highlights any violation of that decision. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend as the best fit?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong></p>\n\n<p>An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated for your use. When you bring your own licenses to Amazon EC2 Dedicated Hosts, you can let AWS take care of all these administrative tasks on your behalf. AWS gives administrators the option to perform a one-time onboarding set up in AWS License Manager.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i1.jpg">\nvia - <a href="https://aws.amazon.com/ec2/dedicated-hosts/faqs/">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p>To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts. Reserved Instances are here to save cost on a yearly utilization of EC2. Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone.</p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time. An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule that will check the application tag and ensure the instance is launched as a Dedicated Host.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> -  Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong> - Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> - Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config. Besides, Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/ec2/dedicated-hosts/faqs/">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n', 'answers': ['<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>', '<p>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>', '<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>', '<p>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'The technology team at a leading bank is using software that has a license type that gets billed based on the number of CPU sockets that are being used. The team would like to ensure that they are using the most appropriate EC2 launch mode and create a compliance dashboard that highlights any violation of that decision. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following solutions would you recommend as the best fit?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357210, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A data analytics company would like to create an automated solution to be alerted in case of EC2 instances being under-utilized for over 24 hours in order to save some costs. The solution should require a manual intervention of an operator validating the assessment before proceeding for instance termination.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution with the LEAST development effort?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Create a CloudWatch Event that tracks the events created by Trusted Advisor and use a Lambda Function as a target for that event. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong></p>\n\n<p>Trusted Advisor inspects your AWS infrastructure across all AWS Regions, and then presents a summary of check results. It recommends stopping or terminating EC2 instances with low utilization. You can also choose to scale your instances using Amazon EC2 Auto Scaling.</p>\n\n<p>Trusted Advisor cost optimization check allows you to check EC2 instances that were running at any time during the last 14 days and alerts you if the daily CPU utilization was 10% or less and network I/O was 5 MB or less on 4 or more days. Running instances generate hourly usage charges. Estimated monthly savings are calculated by using the current usage rate for On-Demand Instances and the estimated number of days the instance might be underutilized.</p>\n\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions. Finally, SSM Automation can have a manual approval step and terminate instances.</p>\n\n<p>Monitoring Trusted Advisor check results with Amazon CloudWatch Events:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q51-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p>\n\n<p>Sample CloudWatch Event for Trusted Advisor check for Low Utilization Amazon EC2 Instances:</p>\n\n<pre><code>{\n  "version": "0",\n  "id": "8dee56b0-b19f-441a-a05c-aa26e583c6c4",\n  "detail-type": "Trusted Advisor Check Item Refresh Notification",\n  "source": "aws.trustedadvisor",\n  "account": "123456789012",\n  "time": "2016-11-13T13:31:34Z",\n  "region": "us-east-1",\n  "resources": [],\n  "detail": {\n    "check-name": "Low Utilization Amazon EC2 Instances",\n    "check-item-detail": {\n      "Day 1": "0.0%  0.00MB",\n      "Day 2": "0.0%  0.00MB",\n      "Day 3": "0.0%  0.00MB",\n      "Region/AZ": "eu-central-1a",\n      "Estimated Monthly Savings": "$10.80",\n      "14-Day Average CPU Utilization": "0.0%",\n      "Day 14": "0.0%  0.00MB",\n      "Day 13": "0.0%  0.00MB",\n      "Day 12": "0.0%  0.00MB",\n      "Day 11": "0.0%  0.00MB",\n      "Day 10": "0.0%  0.00MB",\n      "14-Day Average Network I/O": "0.00MB",\n      "Number of Days Low Utilization": "14 days",\n      "Instance Type": "t2.micro",\n      "Instance ID": "i-917b1a5f",\n      "Day 8": "0.0%  0.00MB",\n      "Instance Name": null,\n      "Day 9": "0.0%  0.00MB",\n      "Day 4": "0.0%  0.00MB",\n      "Day 5": "0.0%  0.00MB",\n      "Day 6": "0.0%  0.00MB",\n      "Day 7": "0.0%  0.00MB"\n    },\n    "status": "WARN",\n    "resource_id": "arn:aws:ec2:eu-central-1:123456789012:instance/i-917b1a5f",\n    "uuid": "6ba6d96a-d3dd-4fca-8020-350bbee4719c"\n  }\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Connect Trusted Advisor to an SNS topic for that check and use a Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - As mentioned in the explanation above, you need to use CloudWatch Events to track the events for a particular rule and NOT SNS.</p>\n\n<p><strong>Create a CloudWatch Event rule that triggers every 5 minutes and use a Lambda function as a target. The Lambda function should issue API calls to AWS CloudWatch Metrics and store the information in DynamoDB. Use a DynamoDB Stream to detect a stream of the low-utilized event for a period of 24 hours and trigger a Lambda function. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - The workflow using Lambda as described in this option will involve significant development effort. Also, this option uses resources such as DynamoDB streams which are really not required to build a solution.</p>\n\n<p><strong>Create a CloudWatch Alarm tracking the minimal CPU utilization across all your EC2 instances. Connect the CloudWatch Alarm to an SNS topic and use the Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - CloudWatch Alarm won\'t work as it won\'t allow you to track the CPU utilization of each individual instance if you create one aggregated one tracking the minimal CPU utilization. Side note, it\'ll be very expensive to create an Alarm for each EC2 instance as well.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p>\n\n<p><a href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n', 'answers': ['<p>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Create a CloudWatch Event that tracks the events created by Trusted Advisor and use a Lambda Function as a target for that event. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>', '<p>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Connect Trusted Advisor to an SNS topic for that check and use a Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>', '<p>Create a CloudWatch Event rule that triggers every 5 minutes and use a Lambda function as a target. The Lambda function should issue API calls to AWS CloudWatch Metrics and store the information in DynamoDB. Use a DynamoDB Stream to detect a stream of the low-utilized event for a period of 24 hours and trigger a Lambda function. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>', '<p>Create a CloudWatch Alarm tracking the minimal CPU utilization across all your EC2 instances. Connect the CloudWatch Alarm to an SNS topic and use the Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'A data analytics company would like to create an automated solution to be alerted in case of EC2 instances being under-utilized for over 24 hours in order to save some costs. The solution should require a manual intervention of an operator validating the assessment before proceeding for instance termination.\n\nAs a DevOps Engineer, how would you implement a solution with the LEAST development effort?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357208, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they\'re best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It\'s not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/secrets-manager/faqs/">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n', 'answers': ['<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>', '<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>', '<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>', '<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>']}, 'correct_response': ['a'], 'section': 'Domain 6: Security and Compliance', 'question_plain': 'The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.\n\nHow can you do this most securely?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357244, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>An e-commerce company is managing its entire application stack and infrastructure using AWS OpsWorks Stacks. The DevOps team at the company has noticed that a lot of instances have been automatically replaced in the stack and the team would henceforth like to be notified via Slack notifications when these events happen.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you implement to meet this requirement?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer\'s instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>OpsWorks Stacks provides an integrated management experience that spans the entire application lifecycle including resource provisioning, EBS volume setup, configuration management, application deployment, monitoring, and access control. You can send state changes in OpsWorks Stacks, such as instance stopped or deployment failed, to CloudWatch Events. The initiated_by field is only populated when the instance is in the requested, terminating, or stopping states. The initiated_by field can contain one of the following values.</p>\n\n<p>user - A user requested the instance state change by using either the API or AWS Management Console.</p>\n\n<p>auto-scaling - The AWS OpsWorks Stacks automatic scaling feature initiated the instance state change.</p>\n\n<p>auto-healing - The AWS OpsWorks Stacks automatic healing feature initiated the instance state change.</p>\n\n<p>For the given use-case, you need to use CloudWatch Events, and the value of <code>initiated_by</code> must be <code>auto-healing</code>. CloudWatch Events does not have a Slack integration, so you need to configure a Lambda function as the target for the CloudWatch rule which would in turn send the Slack notification.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q53-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</strong> -  This option is incorrect as <code>auto-scaling</code> is just a supported value but it\'s not meant for the healing events, instead, it is used for the scaling events.</p>\n\n<p><strong>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</strong> - Opsworks does not send notifications to SNS directly for auto-healing, so this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</strong> - <code>auto-scaling</code> is just a supported value but it\'s not meant for the healing events, instead, it is used for the scaling events. Besides, CloudWatch Events does not have a direct integration with Slack , so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html">https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</p>', '<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</p>', '<p>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</p>', '<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'An e-commerce company is managing its entire application stack and infrastructure using AWS OpsWorks Stacks. The DevOps team at the company has noticed that a lot of instances have been automatically replaced in the stack and the team would henceforth like to be notified via Slack notifications when these events happen.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you implement to meet this requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357226, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.</p>\n\n<p>As a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></strong></p>\n\n<p>In a typical Kinesis Data Streams architecture, you have producers that continually push data to Kinesis Data Streams, and the consumers process the data in real-time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p>\n\n<p><img src="https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png">\nvia - <a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Key concepts for Kinesis Data Streams:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data.</p>\n\n<p>For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric <code>MillisBehindLatest</code>, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n\n<p><strong>Increase the stream data retention period</strong></p>\n\n<p>The retention period is the length of time that data records are accessible after they are added to the stream. A stream’s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it\'s good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - Migrating the application to AWS Lambda will not help with the processing time, as eventually, the same processing code would run under EC2 or Lambda.</p>\n\n<p><strong>Increase the number of shards in Kinesis to increase throughput</strong> - Increasing the number of shards in Kinesis can increase the total throughput of the stream, but this does not impact the processing performance of your processes (which is bound by what you do with the messages). Increasing the number of shards though would help you increase the number of processing processes in KCL if that was already an upper bound (but currently we only have one KCL process running so it\'s not running at capacity).</p>\n\n<p><strong>Decrease the numbers of shards in Kinesis to decrease the load</strong> - Decrease the number of shards would decrease the throughput but again would have no effect on processing applications regarding their performance.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n', 'answers': ['<p>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></p>', '<p>Increase the stream data retention period</p>', '<p>Migrate the application to AWS Lambda</p>', '<p>Increase the number of shards in Kinesis to increase throughput</p>', '<p>Decrease the numbers of shards in Kinesis to decrease the load</p>']}, 'correct_response': ['a', 'b'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.\n\nAs a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357234, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': "<p>A retail company is storing the users' information along with their purchase history in a DynamoDB table and it has also enabled the DynamoDB Streams. Three use cases are implemented for this table: a Lambda function reads the stream to send emails for new users subscriptions, another Lambda function which sends an email after a user has done their first purchase and finally the last Lambda function which awards discounts to users every 10 purchase. When there is a high volume of data on your DynamoDB table, the Lambda functions are experiencing a throttling issue. As you plan on adding future Lambda functions to read from that stream, you need to update the existing solution.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend?</p>\n", 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function that will read from the stream and pass on the payload to SNS. Have the other three and upcoming Lambda functions directly read from the SNS topic</strong></p>\n\n<p>DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time. A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q55-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q55-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p>DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p>\n\n<p>If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table\'s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p>\n\n<p>No more than two processes at most should be reading from the same streams shard at the same time. Having more than two readers per shard can result in throttling. Therefore, you need to use a fan-out pattern for this, SNS being perfect for that.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the RCUs on your DynamoDB table to avoid throttling issues</strong> - DynamoDB Streams operates asynchronously, so there is no performance impact on a table if you enable a stream. So, RCUs have no bearing on throttling issues and this option just acts as a distractor.</p>\n\n<p><strong>Create a DynamoDB DAX cluster to cache the reads</strong> - DAX won\'t help here, it\'s meant to improve reads on your DynamoDB table through a cache, and NOT for DynamoDB Streams.</p>\n\n<p><strong>Increase the memory on the Lambda function so that they have an increased vCPU allocation and process the data faster while making fewer requests to DynamoDB</strong> - The Lambda function memory won\'t help, the issue is that too many processes are reading from the same shard.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a new Lambda function that will read from the stream and pass on the payload to SNS. Have the other three and upcoming Lambda functions directly read from the SNS topic</p>', '<p>Increase the RCUs on your DynamoDB table to avoid throttling issues</p>', '<p>Create a DynamoDB DAX cluster to cache the reads</p>', '<p>Increase the memory on the Lambda function so that they have an increased vCPU allocation and process the data faster while making fewer requests to DynamoDB</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': "A retail company is storing the users' information along with their purchase history in a DynamoDB table and it has also enabled the DynamoDB Streams. Three use cases are implemented for this table: a Lambda function reads the stream to send emails for new users subscriptions, another Lambda function which sends an email after a user has done their first purchase and finally the last Lambda function which awards discounts to users every 10 purchase. When there is a high volume of data on your DynamoDB table, the Lambda functions are experiencing a throttling issue. As you plan on adding future Lambda functions to read from that stream, you need to update the existing solution.\n\nAs a DevOps Engineer, which of the following options would you recommend?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357230, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How can you improve the instance utilization while reducing cost and maintaining application availability?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</strong></p>\n\n<p>With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>Target tracking scaling policies for Amazon EC2 Auto Scaling:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. For the given use-case, you can create two separate scheduled actions that take care of the required minimum capacity during both peak and off-peak times.</p>\n\n<p>Here, we need a scaling policy that tracks a good CPU usage of 75% and adjusts the minimum desired capacity through scheduled actions so it doesn\'t disrupt the number of EC2 instances negatively at any time.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</strong></p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</strong></p>\n\n<p>If a Lambda function terminates 9 instances because they\'re in an ASG, the desired capacity won\'t have changed and the ASG will re-create instances automatically. Therefore both these options are incorrect.</p>\n\n<p><strong>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</strong> - UpdatePolicy for CloudFormation cannot help define Scheduled Actions. There\'s a special ScheduledActions property for that.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</p>', '<p>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</p>', '<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</p>', '<p>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow can you improve the instance utilization while reducing cost and maintaining application availability?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357238, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': "<p>As a DevOps Engineer at an e-commerce company, you have deployed a web application in an Auto Scaling group (ASG)  that is being distributed by an Application Load Balancer (ALB). The web application is using RDS Multi-AZ as a back-end and has been experiencing some issues to connect to the database. The health check implemented in the application currently returns an un-healthy status if the application cannot connect to the database. The ALB / ASG health check integration has been enabled, and therefore the ASG keeps on terminating instances right after they're done booting up.</p>\n\n<p>You need to be able to isolate one instance for troubleshooting for an undetermined amount of time, how should you proceed?</p>\n", 'explanation': '<p>Correct option:</p>\n\n<p><strong>Set an instance in Standby right after it has launched</strong></p>\n\n<p>The Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered.</p>\n\n<p>The default health checks for an Auto Scaling group are EC2 status checks only. If you configure the Auto Scaling group to use ELB health checks, it considers the instance unhealthy if it fails either the EC2 status checks or the ELB health checks.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q57-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p><img src="https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/auto_scaling_lifecycle.png">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p>\n\n<p>You can put an instance that is in the InService state into the Standby state, update or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q57-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Suspend the Launch process</strong> - Suspending the Launch process would prevent instances from being created, which wouldn\'t work here. Please note that suspending the terminate or health check processes may help the situation (but they\'re not options in this question)</p>\n\n<p><strong>Create an autoscaling hook for instance termination. Troubleshoot the instance while it is in the Terminating:Wait state</strong> - Auto Scaling Hooks may work but they come with a one-hour default timeout and therefore we may not get enough time to perform all the troubleshooting we need.</p>\n\n<p><strong>Enable termination protection for EC2</strong> - Termination protection prevents users from terminating an instance but doesn\'t prevent the ASG from terminating instances. For the instances in an Auto Scaling group, use Amazon EC2 Auto Scaling features to protect an instance when a scale-in event occurs. If you want to protect your instance from being accidentally terminated, use Amazon EC2 termination protection.</p>\n\n<p><img src="https://media.amazonwebservices.com/blog/2015/ec2_console_menu_set_scale_in_protection_1.png">\nvia - <a href="https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/">https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/</a></p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/">https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Set an instance in Standby right after it has launched</p>', '<p>Suspend the Launch process</p>', '<p>Create an autoscaling hook for instance termination. Troubleshoot the instance while it is in the Terminating:Wait state</p>', '<p>Enable termination protection for EC2</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': "As a DevOps Engineer at an e-commerce company, you have deployed a web application in an Auto Scaling group (ASG)  that is being distributed by an Application Load Balancer (ALB). The web application is using RDS Multi-AZ as a back-end and has been experiencing some issues to connect to the database. The health check implemented in the application currently returns an un-healthy status if the application cannot connect to the database. The ALB / ASG health check integration has been enabled, and therefore the ASG keeps on terminating instances right after they're done booting up.\n\nYou need to be able to isolate one instance for troubleshooting for an undetermined amount of time, how should you proceed?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357206, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>The DevOps team at an analytics company is deploying an Apache Kafka cluster that contains 6 instances and is distributed across 3 Availability Zones (AZs). Apache Kafka is a stateful service and needs to store its data in an EBS volume. Therefore each instance must have the auto-healing capability and always attach the correct EBS volumes.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following solutions would you suggest for the given requirement?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template with an ASG of min/max capacity of 1, and an EBS volume. Tag the ASG and EBS volume. Create a User Data script that will acquire the EBS volume at boot time. Use a master CloudFormation template and reference the nested template 6 times</strong></p>\n\n<p>You can use CloudFormation to create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>Auto Scaling group enables you to automatically scale Amazon EC2 instances, either with scaling policies or with scheduled scaling. Auto Scaling groups are collections of Amazon EC2 instances that enable automatic scaling and fleet management features, such as health checks and integration with Elastic Load Balancing.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q58-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html</a></p>\n\n<p>For the given use-case, you need to leverage CloudFormation to set up 6 ASGs of 1 instance each and EBS volumes with the appropriate tags and then use an EC2 user data script to attach the corresponding EBS volumes correctly.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group in CloudFormation with a min/max desired capacity of 6 instances spread across 3 AZs, and 6 EBS volumes also across the 3 AZs. Create a user data script so that instances launching from the ASG automatically acquire an available EBS volume in the corresponding AZ</strong> - If you use an ASG of 6 instances, this may seem like a good idea but then you may get into a situation where an AZ is down and 3 instances are created in the other 2 AZ. EBS volumes cannot transfer cross regions and you\'ll be stuck.</p>\n\n<p><strong>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, it will be automatically re-created by CloudFormation with the correct EBS attachment</strong> - If you define 6 instances and attachments in CloudFormation, in case an instance is terminated it won\'t come back automatically.</p>\n\n<p><strong>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, launch a drift detection in CloudFormation and then use CloudFormation remediation</strong> - If you define 6 instances and attachments in CloudFormation, in case an instance is terminated it won\'t come back automatically. Drift detection will allow you to see what has changed, but it will not allow you to fix it through CloudFormation.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudFormation template with an ASG of min/max capacity of 1, and an EBS volume. Tag the ASG and EBS volume. Create a User Data script that will acquire the EBS volume at boot time. Use a master CloudFormation template and reference the nested template 6 times</p>', '<p>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, it will be automatically re-created by CloudFormation with the correct EBS attachment</p>', '<p>Create an Auto Scaling Group in CloudFormation with a min/max desired capacity of 6 instances spread across 3 AZs, and 6 EBS volumes also across the 3 AZs. Create a user data script so that instances launching from the ASG automatically acquire an available EBS volume in the corresponding AZ</p>', '<p>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, launch a drift detection in CloudFormation and then use CloudFormation remediation</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'The DevOps team at an analytics company is deploying an Apache Kafka cluster that contains 6 instances and is distributed across 3 Availability Zones (AZs). Apache Kafka is a stateful service and needs to store its data in an EBS volume. Therefore each instance must have the auto-healing capability and always attach the correct EBS volumes.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following solutions would you suggest for the given requirement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357204, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn’t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn’t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg">\nvia - <a href="https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>', '<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>', '<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>', '<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with AWS::RDS::DBInstance and setup using Multi-AZ.\n\nYou have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357298, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>The development team at a social media company is using AWS CodeCommit to store code. As a Lead DevOps Engineer at the company, you have defined a company-wide rule so that the team should not be able to push to the master branch. You have added all the developers in an IAM group <code>developers</code> and attached the AWS managed IAM policy <code>arn:aws:iam::aws:policy/AWSCodeCommitPowerUser</code> to the group. This policy provides full access to AWS CodeCommit repositories but does not allow repository deletion, however, your developers can still push to the master branch.</p>\n\n<p>How should you prevent the developers from pushing to the master branch?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong></p>\n\n<p>Any CodeCommit repository user who has sufficient permissions to push code to the repository can contribute to any branch in that repository. You can configure a branch so that only some repository users can push or merge code to that branch. For example, you might want to configure a branch used for production code so that only a subset of senior developers can push or merge changes to that branch. Other developers can still pull from the branch, make their own branches, and create pull requests, but they cannot push or merge changes to that branch. You can configure this access by creating a conditional policy that uses a context key for one or more branches in IAM.</p>\n\n<p>For the given use-case, you need to add an extra policy with an explicit Deny. Please note an Explicit Deny always has priority over anything else.</p>\n\n<p>Limit pushes and merges to branches in AWS CodeCommit:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q60-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></strong> - This option has been added as a distractor since CodeCommit repository policies do not exist.</p>\n\n<p><strong>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong> - You cannot modify an AWS managed IAM policy, so this option is incorrect.</p>\n\n<p><strong>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</strong> - Although it would be cool, CodeCommit still does not have a pre-hook feature to integrate with Lambda.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>', '<p>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></p>', '<p>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>', '<p>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'The development team at a social media company is using AWS CodeCommit to store code. As a Lead DevOps Engineer at the company, you have defined a company-wide rule so that the team should not be able to push to the master branch. You have added all the developers in an IAM group developers and attached the AWS managed IAM policy arn:aws:iam::aws:policy/AWSCodeCommitPowerUser to the group. This policy provides full access to AWS CodeCommit repositories but does not allow repository deletion, however, your developers can still push to the master branch.\n\nHow should you prevent the developers from pushing to the master branch?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357322, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A cyber-security company has had a dubious distinction of their own AWS account credentials being put in public GitHub repositories. The company wants to implement a workflow to be alerted in case credentials are leaked, generate a report of API calls made recently using the credentials, and de-activate the credentials. All executions of the workflow must be auditable.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a robust solution for this requirement. Which of the following solutions would you implement?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src="https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png">\nvia - <a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a></p>\n\n<p>AWS monitors popular code repository sites for IAM access keys that have been publicly exposed. AWS Health generates an AWS_RISK_CREDENTIALS_EXPOSED event when an IAM access key has been publicly exposed on GitHub. A CloudWatch Events rule further detects this event and invokes a Step Function that orchestrates the automated workflow to delete the exposed IAM access key, and summarize the recent API activity for the exposed key. The workflow will also issue API calls to IAM, CloudTrail, and SNS. The AWS_RISK_CREDENTIALS_EXPOSED is exposed by the Personal Health Dashboard service.</p>\n\n<p>Mitigating security events using AWS Health and CloudTrail:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q61-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>As the way to react to that event is complex and may have retries, and you want to have a full audit trail of each workflow, you should use a Step Function instead of an AWS Lambda function. So both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong> - AWS_RISK_CREDENTIALS_EXPOSED event is generated by AWS Health service and NOT CloudTrail, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html">https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>', '<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>', '<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>', '<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A cyber-security company has had a dubious distinction of their own AWS account credentials being put in public GitHub repositories. The company wants to implement a workflow to be alerted in case credentials are leaked, generate a report of API calls made recently using the credentials, and de-activate the credentials. All executions of the workflow must be auditable.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a robust solution for this requirement. Which of the following solutions would you implement?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357328, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': "<p>As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.</p>\n\n<p>How can you implement this efficiently and in a fail-safe way?</p>\n", 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src="https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png">\nvia - <a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to combine Step Functions, Lambda and CloudWatch Events into a single coherent solution. You can use the Step Functions to coordinate the business logic to automate the snapshot management flow with error handling, retry logic, and workflow logic all baked into the Step Functions definition. CloudWatch Events integrates with Step Functions and Lambda to let you execute your custom code when relevant events occur.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q62-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>For a deep-dive on this solution, highly recommend the following reference material:\n<a href="https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</strong> - Creating an EC2 instance may work, but if it gets terminated we have to re-create a new one. Failure scenarios may be tough to analyze and having the audit trail in DynamoDB probably won\'t be easy to use.</p>\n\n<p><strong>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</strong> - Creating a CW event rule + Lambda function may work, but the Lambda function may have a timeout issue if the backup is taking longer than 15 minutes, and AWS Config cannot store the history of the execution. AWS Config only provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><strong>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</strong> - An SSM automation cannot contain complex logic to handle failures, although it would provide an execution history.</p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</p>', '<p>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</p>', '<p>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</p>', '<p>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': "As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.\n\nHow can you implement this efficiently and in a fail-safe way?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357250, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>A financial planning company runs a tax optimization application that allows people to enter their personal financial information and get recommendations. The company is committed to the maximum security for the Personally identifiable information (PII) data in S3 buckets, and as part of compliance requirements, it needs to implement a solution to be alerted in case of new PII and its access in S3.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend such that it needs MINIMUM development effort?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</strong></p>\n\n<p>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3.</p>\n\n<p>How Macie Works:\n<img src="https://d1.awsstatic.com/product-marketing/macie/Product-Page-Diagram_AWS-Macie@2x.369dcc5a001e7a44b121d65637ff82b60b809148.png">\nvia - <a href="https://aws.amazon.com/macie/">https://aws.amazon.com/macie/</a></p>\n\n<p>For the given use-case, you can enable Macie on specific S3 buckets and then configure SNS notifications via CloudWatch events for Macie alerts.\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q63-i1.jpg"></p>\n\n<p>For a deep-dive on how to query PII data using Macie, please refer to this excellent blog:\n<a href="https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p>\n\n<p>How GuardDuty Works:\n<img src="https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png">\nvia - <a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p>\n\n<p><strong>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</strong> - Amazon Lambda + Sagemager might work but it requires significant development effort and probably won\'t yield excellent results.</p>\n\n<p><strong>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</strong> - S3 bucket policies cannot be used to analyze the data payload in a request. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p><a href="https://aws.amazon.com/macie/">https://aws.amazon.com/macie/</a></p>\n\n<p><a href="https://aws.amazon.com/guardduty/">https://aws.amazon.com/guardduty/</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</p>', '<p>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</p>', '<p>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</p>', '<p>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'A financial planning company runs a tax optimization application that allows people to enter their personal financial information and get recommendations. The company is committed to the maximum security for the Personally identifiable information (PII) data in S3 buckets, and as part of compliance requirements, it needs to implement a solution to be alerted in case of new PII and its access in S3.\n\nAs an AWS Certified DevOps Engineer, which solution would you recommend such that it needs MINIMUM development effort?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357236, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>As the Lead DevOps Engineer at an analytics company, you are deploying a global application using a CICD pipeline comprising of AWS CodeCommit, CodeBuild, CodeDeploy and orchestrated by AWS CodePipeline. Your pipeline is currently setup in eu-west-1 and you would like to extend the pipeline to deploy your application in us-east-2. This will require a multi-step CodePipeline to be created there and invoked.</p>\n\n<p>How would you implement a solution to address this use-case?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include an S3 step to copy the artifacts being used by CodeDeploy to an S3 bucket in us-east-2. Make the CodePipeline in us-east-2 source files from S3</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.</p>\n\n<p>CodePipeline Overview:\n<img src="https://docs.aws.amazon.com/codepipeline/latest/userguide/images/PipelineFlow.png">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p>CodePipeline Key Concepts:</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i1.jpg"></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts.html</a></p>\n\n<p>For the given use-case, you can use an S3 deploy step to copy artifacts into another bucket. Then CodePipeline in the other region will respond to an event and source the files from the other bucket and kickstart the deployment pipeline there.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodeDeploy step to deploy the application to the CodePipeline in us-east-2</strong> - CodeDeploy cannot deploy to AWS CodePipeline. CodeDeploy can only deploy to EC2, on-premise, Lambda, and ECS.</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodeCommit step to push the changes to the code into the master branch of another CodeCommit repository in us-east-2. Make the CodePipeline in us-east-2 source files from CodeCommit</strong> - CodePipeline can only source from CodeCommit, it cannot push commits to it.</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodePipeline step to invoke the CodePipeline in us-east-2. Ensure the CodePipeline in us-east-2 has the necessary IAM permission to read the artifacts in S3 in eu-west-1</strong> - CodePipeline cannot invoke another CodePipeline directly. This is something you might be able to achieve using a Custom Action and a Lambda function, but you would need to make sure artifacts are copied locally as well.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>At the end of the pipeline in eu-west-1, include an S3 step to copy the artifacts being used by CodeDeploy to an S3 bucket in us-east-2. Make the CodePipeline in us-east-2 source files from S3</p>', '<p>At the end of the pipeline in eu-west-1, include a CodeDeploy step to deploy the application to the CodePipeline in us-east-2</p>', '<p>At the end of the pipeline in eu-west-1, include a CodeCommit step to push the changes to the code into the master branch of another CodeCommit repository in us-east-2. Make the CodePipeline in us-east-2 source files from CodeCommit</p>', '<p>At the end of the pipeline in eu-west-1, include a CodePipeline step to invoke the CodePipeline in us-east-2. Ensure the CodePipeline in us-east-2 has the necessary IAM permission to read the artifacts in S3 in eu-west-1</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'As the Lead DevOps Engineer at an analytics company, you are deploying a global application using a CICD pipeline comprising of AWS CodeCommit, CodeBuild, CodeDeploy and orchestrated by AWS CodePipeline. Your pipeline is currently setup in eu-west-1 and you would like to extend the pipeline to deploy your application in us-east-2. This will require a multi-step CodePipeline to be created there and invoked.\n\nHow would you implement a solution to address this use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357274, 'assessment_type': 'multiple-choice', 'prompt': {'feedbacks': ['', '', '', ''], 'question': '<p>The engineering team at a multi-national retail company is deploying its flagship web application onto an Auto Scaling Group using CodeDeploy. The team has chosen a strategy of a rolling update so that instances are updated in small batches in the ASG. The ASG has five instances running. At the end of the deployment, it seems that three instances are running the new version of the application, while the other two are running the old version. CodeDeploy is reporting a successful deployment.</p>\n\n<p>As a DevOps Engineer, what is the most likely reason that you would attribute for this issue?</p>\n', 'explanation': '<p>Correct option:</p>\n\n<p><strong>Two new instances were created during the deployment</strong></p>\n\n<p>If an Amazon EC2 Auto Scaling scale-up event occurs while a deployment is underway, the new instances will be updated with the application revision that was most recently deployed, not the application revision that is currently being deployed. If the deployment succeeds, the old instances and the newly scaled-up instances will be hosting different application revisions.</p>\n\n<p>To resolve this problem after it occurs, you can redeploy the newer application revision to the affected deployment groups.</p>\n\n<p>To avoid this problem, AWS recommends suspending the Amazon EC2 Auto Scaling scale-up processes while deployments are taking place. You can do this through a setting in the common_functions.sh script that is used for load balancing with CodeDeploy. If HANDLE_PROCS=true, the following Amazon EC2 Auto Scaling events are suspended automatically during the deployment process:</p>\n\n<p>AZRebalance</p>\n\n<p>AlarmNotification</p>\n\n<p>ScheduledActions</p>\n\n<p>ReplaceUnhealthy</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Two instances are having an IAM permissions issue and cannot download the new code revision from S3</strong> - IAM permissions issue would result in the overall deployment status being returned as a failure, but CodeDeploy does report the status as a success. This option is just a distractor.</p>\n\n<p><strong>The auto-scaling group launch configuration has not been updated</strong> - Launch configuration would affect all instances in the same way and not just 2 instances. So this option is incorrect.</p>\n\n<p><strong>A CloudWatch alarm has been triggered during the deployment</strong> - This is another distractor added to the mix of options. CloudWatch alarm would have no bearing on the version of the CodeDeploy application deployed to the instances.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html#integrations-aws-auto-scaling-behaviors">https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html#integrations-aws-auto-scaling-behaviors</a></p>\n', 'relatedLectureIds': '', 'answers': ['<p>Two new instances were created during the deployment</p>', '<p>Two instances are having an IAM permissions issue and cannot download the new code revision from S3</p>', '<p>The auto-scaling group launch configuration has not been updated</p>', '<p>A CloudWatch alarm has been triggered during the deployment</p>']}, 'correct_response': ['a'], 'section': 'Domain 5: Incident and Event Response', 'question_plain': 'The engineering team at a multi-national retail company is deploying its flagship web application onto an Auto Scaling Group using CodeDeploy. The team has chosen a strategy of a rolling update so that instances are updated in small batches in the ASG. The ASG has five instances running. At the end of the deployment, it seems that three instances are running the new version of the application, while the other two are running the old version. CodeDeploy is reporting a successful deployment.\n\nAs a DevOps Engineer, what is the most likely reason that you would attribute for this issue?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357258, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it\'s primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n', 'answers': ['<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>', '<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>', '<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>', '<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': "As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.\n\nWhat can be done to improve the performance of the setup?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357268, 'assessment_type': 'multi-select', 'prompt': {'question': '<p>The DevOps team at a multi-national financial services company manages hundreds of accounts through AWS Organizations. As part of the security compliance requirements, the team must enforce the use of a security-hardened AMI in each AWS account. When a new AMI is created, the team wants to make sure new EC2 instances cannot be instantiated from the old AMI. Additionally, the team also wants to track and audit compliance of AMI usage across all the accounts.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What do you recommend? (Select two)</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</strong></p>\n\n<p>The DevOps team needs to provide approved AMIs that include the latest operating system updates, hardening requirements, and required\nthird-party software agents thereby enabling a repeatable, scalable, and approved application stack factory that increases innovation velocity and reduces effort. This solution uses Amazon EC2 Systems Manager Automation to drive the workflow.</p>\n\n<p>AMI hardening process:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i1.jpg"></p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i2.jpg"></p>\n\n<p>via - <a href="https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p>After you have an approved AMI, you can distribute the AMI across AWS Regions, and then share it with any other AWS accounts. To do this, you use an Amazon EC2 Systems Manager Automation document that uses an AWS Lambda function to copy the AMIs across a specified list of regions, and then another Lambda function to share this copied AMI with the other accounts. The resulting AMI IDs can be stored in the SSM Parameter Store or Amazon DynamoDB for later consumption.</p>\n\n<p>Copying and sharing across AWS Regions and accounts:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i3.jpg">\nvia - <a href="https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><strong>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule\'s result using an AWS Config aggregation</strong></p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i4.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p>An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule to check that only the new AMI is being used and then report the rule\'s result using an AWS Config aggregation.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i5.jpg">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src="https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png">\nvia - <a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>So to summarize, the key is to enforce AMI usage. As such, you don\'t want the AMI to be created or copied locally onto the other accounts, you want it to be available only in a central account and "shared" with other accounts. This way, if you have a new AMI, you unshare the previous one and share the new one. Finally, to monitor the EC2 instances and their AMI ID over time, an AWS Config custom rule is perfect for that.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</strong> - You don\'t want the AMI to be created in a master account and then copied locally onto the other accounts, you want it to be available only in a central account and "shared" with other accounts.</p>\n\n<p><strong>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</strong> - You can\'t create the AMI in a master account using AWS Automation document and then deploy it to all the accounts using AWS CloudFormation StackSets, rather you want it to be available only in a central account and then "share" it with other accounts.</p>\n\n<p><strong>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</strong> - You could use the Lambda function in all accounts to check the AMI id of all the EC2 instances in the account, but it would not allow you to track as well as audit the compliance of AMI usage across all the accounts.</p>\n\n<p>References:</p>\n\n<p><a href="https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n', 'answers': ['<p>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</p>', '<p>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</p>', '<p>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</p>', "<p>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation</p>", '<p>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</p>']}, 'correct_response': ['a', 'd'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'The DevOps team at a multi-national financial services company manages hundreds of accounts through AWS Organizations. As part of the security compliance requirements, the team must enforce the use of a security-hardened AMI in each AWS account. When a new AMI is created, the team wants to make sure new EC2 instances cannot be instantiated from the old AMI. Additionally, the team also wants to track and audit compliance of AMI usage across all the accounts.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What do you recommend? (Select two)', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357272, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.</p>\n\n<p>How should a DevOps engineer implement a solution for this use-case?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. You can use these metrics to verify that your system is performing as expected.</p>\n\n<p>Using custom metrics for your Auto Scaling groups and instances:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>RPO and RTO explained:\n<img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p>For the given use-case, we need to create a custom metric via the application that captures the lag in file reads and then uses it for scaling the ASG managing the EC2 instances to replicate the source EFS cluster into S3. Use another ASG to copy data from S3 into EFS in the target AWS Region. Here we want minimum RPO so we want continuous replication, and minimum RTO so we want a hot EFS system ready to go. Please note that because the RPO and RTO are low, the cost of the solution will be very high.</p>\n\n<p>Side note (for your knowledge) the AWS DataSync service (not covered in the exam) can achieve EFS to EFS replication in a much more native way.</p>\n\n<p>Note: With this solution, as the files are copied to S3, the file Linux permissions would not be replicated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</strong> - As the VPCs are not peered, it\'s not possible to mount the EFS of two different regions onto the same EC2 cluster. We need to go through S3 for the replication.</p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</strong></p>\n\n<p>As the target, EFS needs to have a hot copy of the data, so both these options are ruled out since there is a delay of an hour.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n', 'answers': ['<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>', '<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</p>', '<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</p>', '<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.\n\nHow should a DevOps engineer implement a solution for this use-case?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357248, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won\'t help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won\'t help for routing to specific instances behind an ALB (that\'s why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href="https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n', 'answers': ['<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>', '<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>', '<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>', '<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.\n\nHow can you improve the user experience with the least effort?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357242, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>An e-commerce company has deployed its flagship application in two Auto Scaling groups (ASGs) and two Application Load Balancers (ALBs). You have a Route 53 record that points to the ALB+ASG group where the application has been the most recently deployed. Deployments are alternating between the two groups, and every time a deployment happens it is done on the non-active ALB+ASG group. Finally, the Route53 record is updated. It turns out that some of your clients are not behaving correctly towards the DNS record and thus making requests to the inactive ALB+ASG group.</p>\n\n<p>The company would like to improve this behavior at a minimal cost and also reduce the complexity of the solution. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What of the following would you suggest?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Remove one ALB and keep the two ASG. When new deployments happen, deploy to the older ASG, and then swap the target group in the ALB rule. Keep the Route53 record pointing to the ALB</strong></p>\n\n<p>An ALB distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes the requests to its registered targets. Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups.</p>\n\n<p><img src="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>The issue is because of using the second load balancer for the second application stack and then changing the DNS route to direct the traffic to the other stack when required. The correct solution is to replace only the infrastructure behind the load balancer. To summarize, we can migrate to one ALB only and then just use one target group at a time behind each ASG for correct routing. This will have the added benefit that we won\'t need to pre-warm each ALB at each deployment.</p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/10/06/2019-10-06_16-01-07-1.png"></p>\n\n<p><img src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/10/06/2019-10-06_17-50-17.png"></p>\n\n<p>via - <a href="https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/">https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy a set of NGINX proxy onto each application instance so that if requests are made through the inactive ALB, they are proxied onto the correct ALB</strong> - Deploying an NGINX proxy will work but will be tedious to manage and will complicate the deployments.</p>\n\n<p><strong>Change the TTL of the Route53 to 1 minute before doing a deployment. Do the deployment and then increase the TTL back to the old value</strong> - Changing the TTL won\'t help as the clients are misbehaving already regarding the way they handle DNS records.</p>\n\n<p><strong>Deploy the application to Elastic Beanstalk under two environments. To do a deployment, deploy to the older environment, then perform a CNAME swap</strong> - Migrating to Elastic Beanstalk will not help either as CNAME swap is a DNS record change and clients do not seem to respect the DNS responses.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/">https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n', 'answers': ['<p>Remove one ALB and keep the two ASG. When new deployments happen, deploy to the older ASG, and then swap the target group in the ALB rule. Keep the Route53 record pointing to the ALB</p>', '<p>Deploy a set of NGINX proxy onto each application instance so that if requests are made through the inactive ALB, they are proxied onto the correct ALB</p>', '<p>Change the TTL of the Route53 to 1 minute before doing a deployment. Do the deployment and then increase the TTL back to the old value</p>', '<p>Deploy the application to Elastic Beanstalk under two environments. To do a deployment, deploy to the older environment, then perform a CNAME swap</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'An e-commerce company has deployed its flagship application in two Auto Scaling groups (ASGs) and two Application Load Balancers (ALBs). You have a Route 53 record that points to the ALB+ASG group where the application has been the most recently deployed. Deployments are alternating between the two groups, and every time a deployment happens it is done on the non-active ALB+ASG group. Finally, the Route53 record is updated. It turns out that some of your clients are not behaving correctly towards the DNS record and thus making requests to the inactive ALB+ASG group.\n\nThe company would like to improve this behavior at a minimal cost and also reduce the complexity of the solution. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What of the following would you suggest?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357324, 'assessment_type': 'multi-select', 'prompt': {'question': "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'explanation': '<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client\'s IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don’t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don\'t have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href="https://aws.amazon.com/athena/">https://aws.amazon.com/athena/</a></p>\n', 'answers': ['<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>', '<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>', '<p>Enable Access Logs at the Application Load Balancer level</p>', '<p>Enable Access Logs at the Target Group level</p>', '<p>Analyze the logs using AWS Athena</p>', '<p>Analyze the logs using an EMR cluster</p>']}, 'correct_response': ['a', 'c', 'e'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': "As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.\n\nWhich of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357216, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src="https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n', 'answers': ['<p>AutoScalingRollingUpdate</p>', '<p>AutoScalingReplacingUpdate</p>', '<p>AutoScalingLaunchTemplateUpdate</p>', '<p>AutoScalingLaunchConfigurationUpdate</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.\n\nWhich configuration should you use in the CloudFormation template?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357200, 'assessment_type': 'multiple-choice', 'prompt': {'question': "<p>The technology team at a health-care solutions company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon doing some analytics, it's found that 85% of the read requests are shared across all users.</p>\n\n<p>As a DevOps Engineer, how can you improve the application performance while decreasing the cost?</p>\n", 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don\'t have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src="https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png">\nvia - <a href="https://aws.amazon.com/elasticache/redis/">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it\'s much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/dynamodb/dax/">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href="https://aws.amazon.com/elasticache/redis/">https://aws.amazon.com/elasticache/redis/</a></p>\n', 'answers': ['<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>', '<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>', '<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>', '<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': "The technology team at a health-care solutions company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon doing some analytics, it's found that 85% of the read requests are shared across all users.\n\nAs a DevOps Engineer, how can you improve the application performance while decreasing the cost?", 'related_lectures': []}, {'_class': 'assessment', 'id': 67357330, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A media streaming solutions company has deployed an application that allows its customers to view movies in real-time. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the <code>movies</code> table to be accessible globally but needs the <code>users</code> and <code>movies_watched</code> table to be regional only.</p>\n\n<p>As a DevOps Engineer, how would you implement this with minimal application refactoring?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (movies table) and the other one for the local tables (users and movies_watched tables).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Here, we want minimal application refactoring. DynamoDB and Aurora have a completely different API, due to Aurora being SQL and DynamoDB being NoSQL. So all three options are incorrect, as they have DynamoDB as one of the components.</p>\n\n<p>Reference:</p>\n\n<p><a href="https://aws.amazon.com/rds/aurora/faqs/">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n', 'answers': ['<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>', '<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>', '<p>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>', '<p>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A media streaming solutions company has deployed an application that allows its customers to view movies in real-time. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the movies table to be accessible globally but needs the users and movies_watched table to be regional only.\n\nAs a DevOps Engineer, how would you implement this with minimal application refactoring?', 'related_lectures': []}, {'_class': 'assessment', 'id': 67357196, 'assessment_type': 'multiple-choice', 'prompt': {'question': '<p>A multi-national retail company is planning for disaster recovery and needs the data to be stored in Amazon S3 in two different regions that are in different continents. The data is written at a high rate of 10000 objects per second. For regulatory reasons, the data also needs to be encrypted in transit and at rest. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n', 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'explanation': '<p>Correct option:</p>\n\n<p><em>*Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "false"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication *</em></p>\n\n<p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the requirements, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p>\n\n<p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key "aws:SecureTransport". When this key is true, this means that the request is sent through HTTPS. Create a bucket policy that explicitly denies access when the request meets the condition "aws:SecureTransport": "false". This policy explicitly denies access to HTTP requests.</p>\n\n<p>Finally, if we encrypt using KMS, we may get throttled at 10000 objects per second. SSE-S3 is a better choice in this case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "true"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</strong> - As mentioned in the explanation above, you need to set the condition "aws:SecureTransport": "false" for the solution to work.</p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "false"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</strong></p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "true"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</strong></p>\n\n<p>If we encrypt using KMS, we may get throttled at 10000 objects per second. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href="https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p>\n\n<p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/resource-limits.html">https://docs.aws.amazon.com/kms/latest/developerguide/resource-limits.html</a></p>\n', 'answers': ['<p>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "false"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</p>', '<p>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "true"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</p>', '<p>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "false"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</p>', '<p>Create a bucket policy to create a condition for Denying any request that is <code>"aws:SecureTransport": "true"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</p>']}, 'correct_response': ['a'], 'section': 'Domain 3: Resilient Cloud Solutions', 'question_plain': 'A multi-national retail company is planning for disaster recovery and needs the data to be stored in Amazon S3 in two different regions that are in different continents. The data is written at a high rate of 10000 objects per second. For regulatory reasons, the data also needs to be encrypted in transit and at rest. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following solutions would you recommend?', 'related_lectures': []}]}
