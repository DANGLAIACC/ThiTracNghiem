4724020
~~~
{"count": 45, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 67357112, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.</p>\n\n<p>How should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</strong></p>\n\n<p>For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account.</p>\n\n<p>Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the <code>ConcurrentExecutions</code> metric shows you the number of concurrent invocations for each function in your account.</p>\n\n<p>When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs. For example, if your function usually peaks at 200 concurrent requests, set your provisioned concurrency at 220 instead (200 concurrent requests + 10% = 220 provisioned concurrency).</p>\n\n<p>You can alleviate cold start issues by configuring the optimum provisioned concurrency for the Lambda function.</p>\n\n<p>Optimizing latency with provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Accurately estimating required provisioned concurrency:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q1-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</strong> - Reserved concurrency is the maximum number of concurrent instances you want to allocate to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is well suited for use cases where traffic is constant and predictable over the day.</p>\n\n<p><strong>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</strong> - For data-intensive applications such as machine learning inference, and financial computations, customers often need to read and write large amounts of data to ephemeral storage. The Lambda execution environment provides an ephemeral file system for customers\u2019 code to access via /tmp space, which is preserved for the lifetime of the execution environment, and can provide a transient cache for data between invocations. This option acts as a distractor.</p>\n\n<p><strong>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</strong> - A Lambda function layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. This option is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html#calculating-concurrency</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/\">https://aws.amazon.com/about-aws/whats-new/2022/03/aws-lambda-configure-ephemeral-storage/</a></p>\n", "answers": ["<p>Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively</p>", "<p>Use the Lambda function layers for parallel processing of Lambda runtime requests. Deploy the Lambda function as a .zip file archive to utilize Lambda layer functionality</p>", "<p>Configure the Lambda function execution environment to use up to 10,240 MB of ephemeral storage. This transient cache for data between invocations can be accessed from the Lambda function code via /tmp space</p>", "<p>Configure the AWS Lambda function to use Reserved concurrency. Configure the application Auto Scaling on the Lambda function with reserved concurrency as half of the Lambda function instances invoked during peak traffic hours</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A company uses serverless application architecture to process thousands of requests using AWS Lambda with Amazon DynamoDB as the database. The Amazon API Gateway REST API is used to invoke an AWS Lambda function that loads a large amount of data from the Amazon DynamoDB database. This results in cold start latencies of 7-10 seconds. The DynamoDB tables have already been configured with DynamoDB Accelerator (DAX) to reduce latency. Yet, customers report of application latency, especially during peak access hours. The application receives maximum traffic between 2 PM -5 PM every day and gradually reduces thereafter, reporting a minimum traffic post 8 PM.\n\nHow should a DevOps engineer configure the AWS Lambda function to reduce its latency at all times?", "related_lectures": []}, {"_class": "assessment", "id": 67357100, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.</p>\n\n<p>As a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Volume</code> that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong></p>\n\n<p>Set up AWS Config in the AWS account that needs the security best practice implemented. You can use the managed rule <code>required-tags</code> to check if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instances have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time. This managed rule is applicable for the <code>EC2::Volume</code> Resource Type.</p>\n\n<p>AWS Config managed rule that checks required-tags:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p>This managed rule checks for the existence of the custom tag on the EBS volume. When an EBS volume is found to be non-compliant, you can specify a remediation action through a custom AWS Systems Manager Automation document to apply the custom tag with a predefined backup frequency to all non-compliant EBS volumes.</p>\n\n<p>AWS Config rules remediation with automation runbooks architecture:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q2-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</strong> - In this option, the Resource type is mentioned as <code>AWS::EC2::Instance</code>. This is incorrect since the compliance check has to be done only for the EBS volume and not for the EC2 instance.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - This option is incorrect, as it's possible to remove the custom tag post the creation of the EBS volume and this action would still stay undetected. The right solution is to leverage AWS Config to ensure ongoing backup compliance for EBS volumes for the given use case.</p>\n\n<p><strong>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</strong> - AWS Trusted Advisor scans your AWS infrastructure, compares with AWS best practices, and provides recommended actions. Remedial actions are not possible with AWS Trusted Advisor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html\">https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/\">https://aws.amazon.com/blogs/mt/remediate-noncompliant-aws-config-rules-with-aws-systems-manager-automation-runbooks/</a></p>\n", "answers": ["<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>EC2::Instance</code> that returns a compliance failure if the custom tag is not applied on the EBS volume attached to the instance. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>", "<p>Create an Amazon EventBridge rule that responds to an EBS CreateVolume event from AWS CloudTrail logs. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>", "<p>Create an Amazon EventBridge rule that responds to an EBS VolumeBackup check event from AWS Trusted Advisor. Configure a custom AWS Systems Manager Automation runbook to apply the custom tag with the default weekly value. Specify this runbook as the target of the EventBridge rule</p>", "<p>Set up AWS Config in the AWS account. Use a managed rule for Resource Type <code>AWS::EC2::Volume</code> that returns a compliance failure if the custom tag is not applied on the EBS volume. Configure a remediation action that uses a custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes</p>"]}, "correct_response": ["d"], "section": "Domain 4: Monitoring and Logging", "question_plain": "As a security best practice, a company has decided to back up all of its Amazon Elastic Block Store (Amazon EBS) volumes every week. To implement this change, developers are mandated to tag all Amazon EBS volumes with a custom tag. The company runs an automated solution that reads the custom tag having the value of the desired backup frequency as weekly for each EBS volume and then the solution schedules the backup. However, a recent audit report has highlighted the fact that a few EBS volumes were not backed up as expected because of the missing custom tag.\n\nAs a DevOps engineer which solution will you choose to enforce backup for all EBS volumes used by an AWS account?", "related_lectures": []}, {"_class": "assessment", "id": 67357102, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps Engineer needs to use the AWS CloudFormation stack to deploy an application. But the DevOps Engineer does not have the required permissions to provision the resources specified in the AWS CloudFormation template.</p>\n\n<p>Which solution will allow the DevOps Engineer to deploy the stack while providing the least privileges possible?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong></p>\n\n<p>A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf. You can specify an IAM role that allows AWS CloudFormation to create, update, or delete your stack resources. By default, AWS CloudFormation uses a temporary session that it generates from your user credentials for stack operations. If you specify a service role, AWS CloudFormation uses that role's credentials.</p>\n\n<p>Use a service role to explicitly specify the actions that AWS CloudFormation can perform, which might not always be the same actions that you or other users can do. For example, you might have administrative privileges, but you can limit AWS CloudFormation access to only Amazon EC2 actions.</p>\n\n<p>When you specify a service role, AWS CloudFormation always uses that role for all operations that are performed on that stack. It is not possible to remove a service role attached to a stack after the stack is created. Other users that have permission to perform operations on this stack will be able to use this role, but they must have the <code>iam:PassRole</code> permission.</p>\n\n<p>To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant PassRole permission to the user's IAM user, role, or group.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</strong> - Giving full permissions is against the security best practice that says a role should have the least possible privileges to complete the action needed. AWS suggests not using the ResourceTag condition key in a policy with the <code>iam:PassRole</code> action. You cannot use the tag on an IAM role to control access to who can pass that role.</p>\n\n<p><strong>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</strong> - This statement is incorrect. AWS suggests not using the <code>aws:SourceIp</code> AWS-wide condition. AWS CloudFormation provisions resources by using its own IP address, not the IP address of the originating request. For example, when you create a stack, AWS CloudFormation makes requests from its IP address to launch an Amazon EC2 instance or to create an Amazon S3 bucket, not from the IP address from the CreateStack call or the AWS CloudFormation create-stack command.</p>\n\n<p><strong>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</strong> - To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. To allow a user to pass a role to an AWS service, you must grant the PassRole permission to the user's IAM user, role, or group. Granting the developer <code>iam:PassRole</code> permissions to pass the role to the service is a necessary step if the developers have to deploy the CloudFormation stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n", "answers": ["<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Use this newly created service role during stack deployments</p>", "<p>Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>", "<p>Create an AWS CloudFormation service role with necessary permissions and use <code>aws:SourceIp</code> AWS-wide condition to specify the IP addresses of the developers. Associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions to pass the role to the service. Use this newly created service role during stack deployments</p>", "<p>Create an AWS CloudFormation service role with full permissions and associate this service role to the stack. Grant the developer <code>iam:PassRole</code> permissions. Limit the permissions to pass a role based on tags attached to the role using the <code>ResourceTag/key-name</code> condition key</p>"]}, "correct_response": ["b"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A DevOps Engineer needs to use the AWS CloudFormation stack to deploy an application. But the DevOps Engineer does not have the required permissions to provision the resources specified in the AWS CloudFormation template.\n\nWhich solution will allow the DevOps Engineer to deploy the stack while providing the least privileges possible?", "related_lectures": []}, {"_class": "assessment", "id": 67357108, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A production environment has Amazon EC2 instances configured to log all application/system logs via the CloudWatch Logs agent that has been configured on all instances. The company has recently introduced a security policy that mandates terminating any Amazon EC2 instance accessed manually by a user other than the administrators within an hour. All the production instances are configured with Auto Scaling groups.</p>\n\n<p>As a DevOps Engineer, how will you automate this process?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong></p>\n\n<p>You can use CloudWatch Log subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p>\n\n<p>For the given use case, you can invoke a Lambda function via the CloudWatch Logs subscription. The Lambda then adds a decommission tag to the EC2 instance that produced the login event. Lastly, you can set up an Amazon EventBridge rule to invoke another Lambda function on an hourly schedule, to terminate all EC2 instances that are marked with the decommission tag.</p>\n\n<p>An Amazon EventBridge rule matches incoming events and sends them to targets for processing. A rule can run in response to an event, or at certain time intervals. For example, to periodically run an AWS Lambda function, you can create a rule to run on a schedule.</p>\n\n<p>In EventBridge, you can create two types of scheduled rules:</p>\n\n<ol>\n<li>Rules that run at a regular rate</li>\n</ol>\n\n<p>EventBridge runs these rules at regular intervals; for example, every 20 minutes.</p>\n\n<p>To specify the rate for a scheduled rule, you define a rate expression.</p>\n\n<ol>\n<li>Rules that run at specific times</li>\n</ol>\n\n<p>EventBridge runs these rules at specific times and dates; for example, 8:00 a.m. PST on the first Monday of every month.</p>\n\n<p>To specify the time and dates a scheduled rule runs, you define a cron expression.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</strong> - This option does not provide a solution for complete automation since system administrators are expected to terminate the systems manually.</p>\n\n<p><strong>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</strong> - Amazon CloudWatch alarm does not support having AWS Lambda function as an alarm action. Amazon Simple Notification Service can be used as an intermediatory service if you wish to use AWS Lambda with CloudWatch alarms.</p>\n\n<p><strong>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with the AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</strong> - AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Step Function is overkill for the simple automation via the AWS Lambda function that the given use case needs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p>\n", "answers": ["<p>Create an Amazon CloudWatch alarm that is triggered by the login event data. Create alarm action that notifies system administrators through a message using the Amazon Simple Notification Service topic. The system administrators will then manually terminate the instance</p>", "<p>Set up an Amazon CloudWatch alarm that is triggered by login event data to call an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke the Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>", "<p>Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag</p>", "<p>Create a CloudWatch Logs subscription to an AWS Step Function that coordinates with AWS Lambda function and Amazon EventBridge to create a serverless automated solution. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function every hour, which will terminate all instances with this tag</p>"]}, "correct_response": ["c"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A production environment has Amazon EC2 instances configured to log all application/system logs via the CloudWatch Logs agent that has been configured on all instances. The company has recently introduced a security policy that mandates terminating any Amazon EC2 instance accessed manually by a user other than the administrators within an hour. All the production instances are configured with Auto Scaling groups.\n\nAs a DevOps Engineer, how will you automate this process?", "related_lectures": []}, {"_class": "assessment", "id": 67357106, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A production support team manages a web application running on a fleet of Amazon EC2 instances configured with an Application Load balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A critical bug fix has to be deployed to the production application. The team needs a deployment strategy that can:</p>\n\n<p>a) Create another fleet of instances with the same capacity and configuration as the original one.\nb) Continue access to the original application without a downtime\nc) Transition the traffic to the new fleet when the deployment is fully done. The production test team has requested a two-hour window to complete thorough testing on the new fleet of instances.\nd) Terminate the original fleet automatically once the test window expires.</p>\n\n<p>As a DevOps engineer, which deployment solution will you choose to cater to all the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</strong></p>\n\n<p>Traditional deployments with in-place upgrades make it difficult to validate your new application version in a production deployment while also continuing to run the earlier version of the application. Blue/Green deployments provide a level of isolation between your blue and green application environments. This helps ensure spinning up a parallel green environment does not affect the resources underpinning your blue environment. This isolation reduces your deployment risk.</p>\n\n<p>After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime and limiting the blast radius of impact.</p>\n\n<p>This ability to simply roll traffic back to the operational environment is a key benefit of Blue/Green deployments. You can roll back to the blue environment at any time during the deployment process. Impaired operation or downtime is minimized because the impact is limited to the window of time between green environment issue detection and the shift of traffic back to the blue environment. Additionally, the impact is limited to the portion of traffic going to the green environment, not all traffic. If the blast radius of deployment errors is reduced, so is the overall deployment risk.</p>\n\n<p>In AWS CodeDeploy Blue/Green deployment type, for deployment groups that contain more than one instance, the overall deployment succeeds if the application revision is deployed to all of the instances. The exception to this rule is that if deployment to the last instance fails, the overall deployment still succeeds. This is because CodeDeploy allows only one instance at a time to be taken offline with the CodeDeployDefault.OneAtATime configuration (If you don't specify a deployment configuration, CodeDeploy uses the CodeDeployDefault.OneAtATime deployment configuration).</p>\n\n<p>If you choose <code>Terminate the original instances in the deployment group</code>: After traffic is rerouted to the replacement environment, the instances that were deregistered from the load balancer are terminated following the wait period you specify.</p>\n\n<p>Blue/Green deployment example:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirming the DNS changes of the new environment have propagated correctly</strong></p>\n\n<p><strong>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</strong></p>\n\n<p>These two options use AWS Elastic Beanstalk, which creates a new environment and not a new fleet of EC2 instances, as needed in the use case. Hence, these are incorrect.</p>\n\n<p><strong>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</strong> - Deregistering the original fleet of instances one at a time is not possible. After traffic is successfully routed to the replacement environment, instances in the original environment are deregistered all at once no matter which deployment configuration is selected.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n", "answers": ["<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to <code>Terminate the original instances in the deployment group</code> and choose a waiting period of two hours</p>", "<p>Configure AWS Elastic Beanstalk to perform a Blue/Green deployment. This will create a new environment different from the original environment to continue serving the production traffic. Terminate the original environment after two hours and confirm the DNS changes of the new environment have propagated correctly</p>", "<p>Configure AWS Elastic Beanstalk to use rolling deployment policy. Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Production traffic is served unaffected. Use rolling restarts to restart the proxy and application servers running on your environment's instances without downtime</p>", "<p>Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set <code>Original instances</code> value to 'Terminate the original instances in the deployment group' and choose a waiting period of two hours. Choose <code>OneAtATime</code> Deployment configuration setting to deregister the original fleet of instances one at a time to provide increased test time for the production team</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A production support team manages a web application running on a fleet of Amazon EC2 instances configured with an Application Load balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A critical bug fix has to be deployed to the production application. The team needs a deployment strategy that can:\n\na) Create another fleet of instances with the same capacity and configuration as the original one.\nb) Continue access to the original application without a downtime\nc) Transition the traffic to the new fleet when the deployment is fully done. The production test team has requested a two-hour window to complete thorough testing on the new fleet of instances.\nd) Terminate the original fleet automatically once the test window expires.\n\nAs a DevOps engineer, which deployment solution will you choose to cater to all the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357104, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.</p>\n\n<p>Which of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</strong></p>\n\n<p>Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.</p>\n\n<p>Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets. However, each log object reports access log records for a specific source bucket.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage and you can start analyzing your data immediately. Athena uses an approach known as schema-on-read, which allows you to project your schema onto your data at the time you execute a query. This eliminates the need for any data loading or ETL.</p>\n\n<p>Amazon Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i1.jpg\">\n<a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><code>CREATE EXTERNAL TABLE [IF NOT EXISTS]</code> syntax is used to create the table in Athena. EXTERNAL keyword specifies that the table is based on an underlying data file that exists in Amazon S3, in the location that you specify.</p>\n\n<p>You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests.</p>\n\n<p>Analysing object access requests using SQL queries in Athena:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q6-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-s3-access-logs-to-identify-requests.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</strong> - The request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> will give the overall count of all Get requests and Bytes downloaded through these requests. It does not help in answering the access pattern questions, such as, which files have been viewed/downloaded the most.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</strong> - Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to run very fast against large datasets. Amazon Redshift Spectrum has a dependency on provisioned Amazon Redshift servers, which represents a cost-intensive solution. Therefore, Amazon Redshift Spectrum is a cost overkill for the simple access pattern analysis that the current use case needs.</p>\n\n<p><strong>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</strong> -  Amazon OpenSearch is a managed service that makes it easier to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. The service provides support for open-source Elasticsearch APIs, managed Kibana, and integration with other AWS services such as Amazon S3 and Amazon Kinesis for loading streaming data into Amazon ES. OpenSearch is well-suited for real-time analysis of logs or clickstreams. This is not a cost-effective solution as it uses AWS Lambda, Kinesis Data Firehose, and Amazon OpenSearch services for the simple access pattern analysis that the current use case needs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html#s3-cloudwatch-metrics</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/\">https://aws.amazon.com/blogs/big-data/analyzing-amazon-s3-server-access-logs-using-amazon-opensearch-service/</a></p>\n", "answers": ["<p>Leverage Amazon S3 request metrics from CloudWatch for analysis via an AWS Lambda function. The <code>AWS/S3</code> namespace includes the request metrics <code>GetRequests</code> and <code>BytesDownloaded</code> that can be passed to the Lambda function for analyzing the access pattern of the S3 bucket. The output of the Lambda function can be stored in an S3 bucket and visually analyzed using Amazon QuickSight</p>", "<p>Enable server access logging on the S3 bucket. Use Amazon Redshift Spectrum to efficiently query large datasets leveraging the massive parallelism offered by dedicated Amazon Redshift servers</p>", "<p>Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena</p>", "<p>Enable server access logging on the S3 bucket. Configure file create event notification on Access log S3 bucket to trigger an AWS Lambda function. Configure Lambda to write the data to Kinesis Firehose which then writes the log data to Amazon OpenSearch Service. Use Elasticsearch search and analytics engine for analyzing usage patterns of the S3 access logs</p>"]}, "correct_response": ["c"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A video-sharing application stores its files on an Amazon S3 bucket. During the last year, the user traffic has multiplied by thousands and the company is planning on introducing subscription services for its video sharing application. The company needs the access pattern of the video files to identify the most viewed and downloaded videos.\n\nWhich of the following would you identify as the MOST cost-effective solution that can be implemented at the earliest?", "related_lectures": []}, {"_class": "assessment", "id": 67357114, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is implementing AWS serverless architecture with Amazon API Gateway, AWS Lambda, and Amazon DynamoDB services. The company's existing users are primarily located in Europe and Asia-Pacific regions. The company is now looking for a quick-start solution that offers high reliability and low latency for a global user base across regions as its offerings are getting popular worldwide.</p>\n\n<p>How will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong></p>\n\n<p>Route 53: If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency. To use latency-based routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server. Amazon Route 53 latency-based routing will reduce latency by serving the user from the fastest available route between the two AWS regions.</p>\n\n<p>You can use Amazon Route 53 health checks to control DNS failover from an API Gateway API in a primary AWS Region to one in a secondary Region. This can help mitigate impacts in the event of a Regional issue.</p>\n\n<p>AWS Lambda provides easy scaling and high availability to your serverless architecture without additional effort on your part.</p>\n\n<p>Amazon DynamoDB global tables is a fully managed, serverless, multi-Region, and multi-active database. Global tables provide you 99.999% availability, increased application resiliency, and improved business continuity. As global tables replicate your Amazon DynamoDB tables automatically across your choice of AWS Regions, you can achieve fast, local read and write performance.</p>\n\n<p>A DynamoDB global table is comprised of multiple replica tables. Each replica table exists in a different Region, but all replicas have the same name and primary key. When data is written to any replica table, DynamoDB automatically replicates that data to all other replica tables in the global table. Global tables enable the users of your application to have low-latency access to the data no matter where they are located. In the unlikely event that one AWS Region was to become temporarily unavailable, your customers can still access the replica tables in the other Regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - Route 53 geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. While using geolocation routing can help you localize your content and present some or all of your website in the language of your users, it is not suitable to serve content with the lowest latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - While failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy, it is not suitable when the key requirement is to serve content with lowest possible latency.</p>\n\n<p><strong>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</strong> - AWS Global Accelerator can be used in conjunction with the Amazon API Gateway to present Internet-facing API via static IP addresses to end users. This design addresses the need for static IP safe listing, however, it is not useful for the given requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-geo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/\">https://aws.amazon.com/blogs/networking-and-content-delivery/accessing-an-aws-api-gateway-via-static-ip-addresses-provided-by-aws-global-accelerator/</a></p>\n", "answers": ["<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use geolocation routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>", "<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use failover routing and Application Recovery Controller health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>", "<p>Configure Amazon Route 53 to point to AWS Global Accelerator. Configure Global Accelerator to point to API Gateway API endpoint(s) of both regions via an Application Load Balancer(ALB). Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>", "<p>Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table</p>"]}, "correct_response": ["d"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A company is implementing AWS serverless architecture with Amazon API Gateway, AWS Lambda, and Amazon DynamoDB services. The company's existing users are primarily located in Europe and Asia-Pacific regions. The company is now looking for a quick-start solution that offers high reliability and low latency for a global user base across regions as its offerings are getting popular worldwide.\n\nHow will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357098, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.</p>\n\n<p>How will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</strong></p>\n\n<p><strong>Update the Lambda execution roles with permission to access the VPC and the EFS file system</strong></p>\n\n<p>You can configure a function to mount an Amazon Elastic File System (Amazon EFS) file system to a local directory. With Amazon EFS, your function code can access and modify shared resources safely and at high concurrency.</p>\n\n<p>Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct <code>elasticfilesystem</code> permissions.</p>\n\n<p>Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.</p>\n\n<p>You can access the same EFS file system from multiple functions, using the same or different access points. For example, using different EFS access points, each Lambda function can access different paths in a file system, or use different file system permissions.</p>\n\n<p>Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.</p>\n\n<p>A Lambda function in one account can mount a file system in a different account. For this scenario, you configure VPC peering between the function VPC and the file system VPC.</p>\n\n<p>An example showcasing the use of EFS with AWS Lambda:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/\">https://aws.amazon.com/blogs/aws/new-a-shared-file-system-for-your-lambda-functions/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</strong> - A VPC peering connection is needed since EFS and AWS Lambda are in different AWS accounts. SCPs alone are not sufficient to grant permissions to the accounts in your organization. You should note that no permissions are granted by an SCP. The Lambda execution role will need explicit permissions for access to EFS.</p>\n\n<p><strong>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region or cross-AZ connectivity between EFS and Lambda</strong> - This statement is incorrect. AWS does not support cross-region, or cross AZ connectivity between EFS and Lambda.</p>\n\n<p><strong>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</strong> - It is mentioned in the use case that EFS and AWS Lambda will remain in their respective accounts. Hence, this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html</a></p>\n", "answers": ["<p>Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A</p>", "<p>Update the Lambda execution roles with permission to access the VPC and the EFS file system</p>", "<p>Create a VPC peering connection to connect Account A to Account B. Create Service control policies (SCPs) to set permission guardrails for access to Amazon EFS from AWS Lambda function execution role</p>", "<p>Lambda charges for data transfer between VPCs. To manage costs, create a new EFS file system in Account B. Configure AWS DataSync to transfer data from EFS in Account B to EFS in Account B</p>", "<p>Configure the Lambda functions in Account B to assume an existing IAM role in Account A for the cross-region, or cross-AZ connectivity between EFS and Lambda</p>"]}, "correct_response": ["a", "b"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A company has configured AWS Organizations to manage its multiple AWS accounts. The company uses Amazon Elastic File System (Amazon EFS) as a shared storage service, configured in AWS Account A of the company. To implement a serverless architecture, the company has decided to move its applications to AWS Lambda. The Lambda functions will be managed through another AWS account (Account B). All the Lambda functions will be deployed in a VPC. A DevOps team needs help to continue using Amazon EFS in Account A with the Lambda function in Account B.\n\nHow will you reconfigure the existing EFS file system for use with AWS Lambda function? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357096, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A web application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). While using CodeDeploy Blue/Green deployment to deploy a new version, the deployment failed during the <code>AllowTraffic</code> lifecycle event. The DevOps team has found no errors in the deployment logs.</p>\n\n<p>Which of the following would you identify as the root cause behind the failure of the deployment?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</strong></p>\n\n<p>In some cases, a Blue/Green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure.</p>\n\n<p>This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group.</p>\n\n<p>To resolve the issue, review and correct any errors in the health check configuration for the load balancer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</strong> - During a deployment, the CodeDeploy agent runs the scripts specified for ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic in the AppSpec file from the previous successful deployment. (All other scripts are run from the AppSpec file in the current deployment.) If one of these scripts contains an error and does not run successfully, the deployment can fail. This kind of failure generates logs and will not fail at <code>AllowTraffic</code> lifecycle event.</p>\n\n<p><strong>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</strong> - This statement is incorrect. Deployments do not fail for up to an hour when an instance is terminated during a deployment.</p>\n\n<p><strong>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</strong> - During an in-progress deployment, a scale-in event or any other termination event causes the instance to detach from the Amazon EC2 Auto Scaling group and then terminate. Because the deployment cannot be completed, it fails. This error is specific to Auto Scaling Groups (ASG) and hence does not relate to the given issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html</a></p>\n", "answers": ["<p>The cause of the failure could be a script from the last successful deployment that never runs successfully. Create a new deployment and specify that the ApplicationStop, BeforeBlockTraffic, and AfterBlockTraffic failures should be ignored</p>", "<p>If an instance is terminated between lifecycle events or before the first lifecycle event step starts, then <code>AllowTraffic</code> lifecycle event fails without generating logs</p>", "<p>A scale-in event or any other termination event, during an in-progress deployment, causes the instance to detach from the Amazon EC2 Auto Scaling group and the instance fails the <code>AllowTraffic</code> lifecycle event</p>", "<p>Incorrectly configured health checks on Application Load Balancer (ALB) are responsible for this issue</p>"]}, "correct_response": ["d"], "section": "Domain 1: SDLC Automation", "question_plain": "A web application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). While using CodeDeploy Blue/Green deployment to deploy a new version, the deployment failed during the AllowTraffic lifecycle event. The DevOps team has found no errors in the deployment logs.\n\nWhich of the following would you identify as the root cause behind the failure of the deployment?", "related_lectures": []}, {"_class": "assessment", "id": 67357184, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.</p>\n\n<p>Which of the following represents the most optimal solution to automate the application deployment to different AWS regions?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</strong></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p>Here are the relevant CloudFormation concepts:</p>\n\n<p>Template: A CloudFormation template is a JSON or YAML formatted text file. You can save these files with any extension, such as .json, .yaml, .template, or .txt. CloudFormation uses these templates as blueprints for building your AWS resources.</p>\n\n<p>Stack: When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template.</p>\n\n<p>Change set: If you need to make changes to the running resources in a stack, you update the stack. Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. Change sets allow you to see how your changes might impact your running resources, especially critical resources, before implementing them.</p>\n\n<p>Stack set: A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set's CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires.</p>\n\n<p>After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify. When you create, update, or delete stacks, you can also specify operation preferences, such as the order of Regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. A stack set is a regional resource. If you create a stack set in one AWS Region, you can't see it or change it in other Regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p>For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</strong> - You can create a stack using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. The <code>aws cloudformation create-stack</code> command supports only a <code>region</code> parameter for a single region and not <code>regions</code> parameter for multiple regions.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q45-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/multiple-account-multiple-region-aws-cloudformation/</a></p>\n\n<p><strong>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</strong> - Creating a script is not an elegant solution to provision the same infrastructure in multiple AWS regions since this functionality is directly offered by AWS in the form of stack sets.</p>\n\n<p><strong>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</strong> - When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You cannot use change sets to deploy an application to multiple AWS regions.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n", "answers": ["<p>Create a shell script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions</p>", "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions</p>", "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application</p>", "<p>Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation change set from an administrator account to launch stack instances that deploy the application to various other regions</p>"]}, "correct_response": ["b"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A social media company has its web application hosted on Amazon EC2 instances that are deployed in a single AWS Region. The company has now expanded its operations into new geographies and the company wants to offer low-latency access for the application to its customers. To comply with different financial regulations of each geography, the application needs to operate in silos and the underlying instances in one region should not interact with instances running in other regions.\n\nWhich of the following represents the most optimal solution to automate the application deployment to different AWS regions?", "related_lectures": []}, {"_class": "assessment", "id": 67357110, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A developer has uploaded an object of size 100 MB to an Amazon S3 bucket as a single-part direct upload using the REST API that has checksum enabled. The checksum of the object uploaded via the REST API was the checksum of the entire object. Later that day, the developer used the AWS Management Console to rename the object, copy it and edit its metadata. Later, when the developer checked for the checksum of the object updated via the AWS Management Console, the checksum was not the checksum of the entire object. Confused by the behavior, the developer has reached out to you for a possible solution.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, which of the following options would you identify as the reason for this behavior?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>A new checksum value for the object that is calculated based on the checksum values of the individual parts has been created. This behavior is expected</strong></p>\n\n<p>When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part.</p>\n\n<p>For example, consider an object 100 MB in size that you uploaded as a single-part direct upload using the REST API. The checksum in this case is a checksum of the entire object. If you later use the console to rename that object, copy it, change the storage class, or edit the metadata, Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB (NOT 50 MB) in size.</p>\n\n<p><strong>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</strong> - The checksum algorithm does not change when you change the metadata of the S3 object, so this option is incorrect.</p>\n\n<p><strong>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</strong> - This option is incorrect. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. On the other hand, when you upload an object as a single-part direct upload using the REST API, the checksum in this case is a checksum of the entire object. So you can say that the developer's initial calculation for the REST API based checksum was indeed correct.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n", "answers": ["<p>If an object is greater than 16 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>", "<p>A new checksum value for the object, that is calculated based on the checksum values of the individual parts, has been created. This behavior is expected</p>", "<p>If an object is greater than 50 MB in size, checksum will be a calculation based on the checksum values of each individual parts. The developer's initial calculation for the REST API based checksum was incorrect. This resulted in the mismatch of the two checksum values</p>", "<p>When you change metadata of an object in S3, the checksum algorithm of the objects changes by default. This is an expected behavior</p>"]}, "correct_response": ["b"], "section": "Domain 5: Incident and Event Response", "question_plain": "A developer has uploaded an object of size 100 MB to an Amazon S3 bucket as a single-part direct upload using the REST API that has checksum enabled. The checksum of the object uploaded via the REST API was the checksum of the entire object. Later that day, the developer used the AWS Management Console to rename the object, copy it and edit its metadata. Later, when the developer checked for the checksum of the object updated via the AWS Management Console, the checksum was not the checksum of the entire object. Confused by the behavior, the developer has reached out to you for a possible solution.\n\nAs an AWS Certified DevOps Engineer - Professional, which of the following options would you identify as the reason for this behavior?", "related_lectures": []}, {"_class": "assessment", "id": 67357116, "assessment_type": "multi-select", "prompt": {"question": "<p>For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).</p>\n\n<p>What combination of steps will you take to configure this requirement? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</strong></p>\n\n<p><strong>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</strong></p>\n\n<p><strong>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</strong></p>\n\n<p>Complete list of steps for configuring the requirement:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q11-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</strong></p>\n\n<p><strong>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</strong></p>\n\n<p><strong>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/\">https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-deploy-cloudformation/</a></p>\n", "answers": ["<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket</p>", "<p>In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. In account B, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account A access to the bucket</p>", "<p>In account B, create a cross-account IAM role. In account A, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B</p>", "<p>In account A, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account B, update the CodePipeline configuration to include the resources associated with account A</p>", "<p>In account B, add the <code>AssumeRole</code> permission to account A's CodePipeline service role to allow it to assume the cross-account role in account A</p>", "<p>In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B</p>"]}, "correct_response": ["a", "c", "f"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "For deployments across AWS accounts, an e-commerce company has decided to use AWS CodePipeline to deploy an AWS CloudFormation stack in an AWS account (account A) to a different AWS account (account B).\n\nWhat combination of steps will you take to configure this requirement? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 67357118, "assessment_type": "multi-select", "prompt": {"question": "<p>A project has two AWS accounts, a development account and a production account, in the us-east-1 Region. A DevOps engineer has to deploy artifacts from the development account's S3 bucket to the production account's S3 bucket using AWS CodePipeline with Amazon S3 deploy action.</p>\n\n<p>What configurations are mandatory for this cross-account deployment? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</strong></p>\n\n<p>You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn't configured, then CodePipeline encrypts the objects with default encryption, which can't be decrypted by the role in the destination account.</p>\n\n<p>The input bucket must have versioning activated to work with CodePipeline.</p>\n\n<p><strong>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</strong></p>\n\n<p>In the deploy action, the CodePipeline service role assumes the cross-account role in the production account. CodePipeline uses the cross-account role to access the KMS key and artifact bucket in the development account. Then, CodePipeline deploys the extracted files to the production output S3 bucket in the production account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create Secrets Manager key to use with CodePipeline in the development account</strong> - This option is not relevant for the given use case.</p>\n\n<p><strong>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</strong> - This statement is incorrect and given only as a distractor.</p>\n\n<p><strong>You need to use an AWS KMS multi-Region key with multiple replicas</strong> - This option has been added as a distractor. Only when the production account's Region is different than your pipeline's Region, then you must also use an AWS KMS multi-Region key with multiple replicas.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/codepipeline-artifacts-s3\">https://repost.aws/knowledge-center/codepipeline-artifacts-s3</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-S3Deploy.html</a></p>\n", "answers": ["<p>Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline</p>", "<p>Create Secrets Manager key to use with CodePipeline in the development account</p>", "<p>Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created</p>", "<p>Configure a cross-account role in the development account. Attach a policy to the S3 bucket in the production account that allows access to the cross-account role that you created</p>", "<p>You need to use an AWS KMS multi-Region key with multiple replicas</p>"]}, "correct_response": ["a", "c"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A project has two AWS accounts, a development account and a production account, in the us-east-1 Region. A DevOps engineer has to deploy artifacts from the development account's S3 bucket to the production account's S3 bucket using AWS CodePipeline with Amazon S3 deploy action.\n\nWhat configurations are mandatory for this cross-account deployment? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357120, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company hosts all its web applications on Amazon EC2 instances. The company is looking for a security solution that will proactively detect software vulnerabilities and unintended network exposure of the instances. The solution should also include an audit trail of all login activities on the instances.</p>\n\n<p>Which solution will meet these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</strong></p>\n\n<p>Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Amazon Inspector uses SSM Agent to collect application inventory, which can be set up as Amazon Virtual Private Cloud (VPC) endpoints to avoid sending information over the internet.</p>\n\n<p>You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.</p>\n\n<p>CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time.</p>\n\n<p>Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs.</p>\n\n<p>How Amazon Inspector Works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q13-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</strong> - SSM agent does not scan or detect vulnerabilities on Amazon EC2 instances.</p>\n\n<p><strong>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</strong> - Amazon ECR provides basic scanning type which uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. It is for container applications alone. Also, ECR image scanning identifies software vulnerabilities only in operating system packages.</p>\n\n<p><strong>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, Amazon Elastic Compute Cloud (EC2) workloads, container applications, Amazon Aurora databases, and data stored in Amazon Simple Storage Service (S3). It is not a vulnerability management service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html\">https://docs.aws.amazon.com/inspector/v1/userguide/inspector_introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/faqs/\">https://aws.amazon.com/inspector/faqs/</a></p>\n", "answers": ["<p>Configure AWS Systems Manager's Systems Manager (SSM) agent to collect software vulnerabilities of the Amazon EC2 instances. Configure a Systems Manager Automation runbook to automatically patch the vulnerabilities identified by the SSM agent</p>", "<p>Configure Amazon ECR image scanning to scan for vulnerabilities on the EC2 instances. Amazon ECR sends an event to Amazon EventBridge when an image scan is completed. Configure CloudTrail to send its trail data to Amazon EventBridge for further processing/notification</p>", "<p>Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs</p>", "<p>Configure Amazon GuardDuty to detect vulnerabilities and threats on the EC2 instances. Integrate with a workflow system to review the findings and trigger an AWS Lambda function to automate the remediation process</p>"]}, "correct_response": ["c"], "section": "Domain 6: Security and Compliance", "question_plain": "A company hosts all its web applications on Amazon EC2 instances. The company is looking for a security solution that will proactively detect software vulnerabilities and unintended network exposure of the instances. The solution should also include an audit trail of all login activities on the instances.\n\nWhich solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357122, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.</p>\n\n<p>Recently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.</p>\n\n<p>How should a DevOps engineer configure this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong></p>\n\n<p>You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on.</p>\n\n<p>A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in Systems Manager.</p>\n\n<p>Amazon CloudWatch metrics and alarms:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send a notification to an Amazon Simple Notification Service (Amazon SNS) topic if a Critical event is detected. Subscribe the email address of the security team to the SNS topic</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. Kinesis Data Firehose is not supported as a destination for the metric streams.</p>\n\n<p><strong>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. This does not meet our objective of analyzing firewall log data.</p>\n\n<p><strong>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</strong> - CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. This option acts as a distractor for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/tgw/tgw-flow-logs.html</a></p>\n", "answers": ["<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>", "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Configure a metric stream using Kinesis Data Firehose delivery stream and AWS Lambda as the destination. Process the stream data with Lambda and send notification to an Amazon Simple Notification Service (Amazon SNS) topic if Critical event is detected. Subscribe the email address of the security team to the SNS topic</p>", "<p>Create a Transit Gateway Flow Log to capture all the information sent by the firewall appliance. Publish the flow log data to Amazon CloudWatch logs. Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>", "<p>Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding. Use CloudWatch Lambda Insights to filter out the Critical event and send a notification using an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A company uses multiple AWS accounts to help isolate and manage business applications. This multi-account environment consists of an AWS Transit Gateway to route all outbound traffic through a common network account. A firewall appliance inspects all traffic before it is forwarded to an internet gateway. The firewall appliance is configured to send logs to Amazon CloudWatch Logs for all events generated.\n\nRecently, the security team has advised about probable illegal access of resources. As DevOps Engineer, you have been advised to configure an alert to the security team if the firewall appliance generates an event of Critical severity.\n\nHow should a DevOps engineer configure this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357124, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).</p>\n\n<p>Which step did the developer possibly miss for the successful completion of the CloudFormation stack?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</strong></p>\n\n<p>Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic.</p>\n\n<p>The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.</p>\n\n<p>How custom resources work:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q15-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</strong> - You can use the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::MyCustomResourceTypeName</code> resource type to define custom resources in your templates. However, this has no bearing on the given use case.</p>\n\n<p><strong>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</strong> - This statement is incorrect. The template developer and custom resource provider can be the same person or entity.</p>\n\n<p><strong>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</strong> - The <code>cfn-response</code> module is available only when you use the ZipFile property to write your source code. This is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-lambda-function-code-cfnresponsemodule.html</a></p>\n", "answers": ["<p>Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL</p>", "<p>The AWS CloudFormation resource <code>AWS::CloudFormation::CustomResource</code> should be used to specify a custom resource in the template</p>", "<p>If the template developer and custom resource provider are configured to the same person or entity then CloudFormation stack completion fails</p>", "<p>After executing the <code>send</code> method in the <code>cfn-response</code> module, the Lambda function terminates, so anything written after this method is ignored</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A developer configured an AWS CloudFormation template to create custom resource necessary for the project. The AWS Lambda function for the custom resource executed successfully as seen by the successful creation of the custom resource. But, the CloudFormation stack is not transitioning from in-progress status (CREATE_IN_PROGRESS) to completion status (CREATE_COMPLETE).\n\nWhich step did the developer possibly miss for the successful completion of the CloudFormation stack?", "related_lectures": []}, {"_class": "assessment", "id": 67357126, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps Engineer has been asked to chalk out a disaster recovery (DR) plan for a workload in production. The workload runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are configured with an Auto Scaling group across multiple Availability Zones. Amazon Route 53 is configured to point to the ALB using an alias record. Amazon RDS for PostgreSQL DB instance is the database service. The draft DR plan mandates an RTO of three hours and an RPO of around 15 minutes.</p>\n\n<p>Which Disaster Recovery (DR) strategy should the DevOps Engineer opt for a cost-effective solution?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</strong></p>\n\n<p>With the pilot light approach, you replicate your data from one Region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements, such as application servers, are loaded with application code and configurations, but are \"switched off\" and are only used during testing or when disaster recovery failover is invoked. In the cloud, you have the flexibility to de-provision resources when you do not need them, and provision them when you do. A best practice for \u201cswitched off\u201d is to not deploy the resource, and then create the configuration and capabilities to deploy it (\u201cswitch on\u201d) when needed. Unlike the backup and restore approach, your core infrastructure is always available and you always have the option to quickly provision a full-scale production environment by switching on and scaling out your application servers.</p>\n\n<p>A pilot light approach minimizes the ongoing cost of disaster recovery by minimizing the active resources and simplifies recovery at the time of a disaster because the core infrastructure requirements are all in place.</p>\n\n<p>For pilot light, continuous data replication to live databases and data stores in the DR region is the best approach for low RPO. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance.</p>\n\n<p>For an active/passive configuration such as the pilot light, all traffic initially goes to the primary Region and switches to the disaster recovery Region if the primary Region is no longer available. This failover operation can be initiated either automatically or manually.</p>\n\n<p>Using Amazon Route 53, you can associate multiple IP endpoints in one or more AWS Regions with a Route 53 domain name. Then, you can route traffic to the appropriate endpoint under that domain name. On failover you need to switch traffic to the recovery endpoint, and away from the primary endpoint. Amazon Route 53 health checks monitor these endpoints. Using these health checks, you can configure automatically initiated DNS failover to ensure traffic is sent only to healthy endpoints, which is a highly reliable operation done on the data plane.</p>\n\n<p>The pilot light strategy is the one best suited for the given requirements since it keeps the overall costs down when compared to the rest of the options. Also, the core infrastructure is ready to be used as and when required. Since RTO is given in hours, we can spin up these resources well before the given time. The database is update-to-date and only needs a switch from replica to primary. This is the right strategy when RPO is in minutes.</p>\n\n<p>Pilot light DR strategy:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q16-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html\">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</strong> - With pilot light DR strategy, resources required to support data replication and backup, such as databases and object storage, are always on. When the expected RPO is 15 minutes, it is not possible to apply incremental backups to the database since this could be huge amounts of data which may take hours. Hence, this option is incorrect.</p>\n\n<p><strong>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</strong> - You can run your workload simultaneously in multiple Regions as part of a multi-site active/active strategy. Multi-site active/active serves traffic from all regions to which it is deployed. With a multi-site active/active approach, users can access your workload in any of the Regions in which it is deployed. This approach is the most complex and costly approach to disaster recovery, but it can reduce your recovery time to near zero for most disasters with the correct technology choices and implementation. Since RTO is given as three hours, opting for this DR strategy will prove to be cost-ineffective.</p>\n\n<p><strong>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</strong> - The warm standby approach involves ensuring that there is a scaled-down, but fully functional, copy of your production environment in another Region. This approach extends the pilot light concept and decreases the time to recovery because your workload is always-on in another Region. Since RTO is given as three hours, the warm standby approach will end up being a costly alternative to an otherwise cost-effective pilot light strategy.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/\">https://aws.amazon.com/blogs/mt/establishing-rpo-and-rto-targets-for-cloud-applications/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p>\n", "answers": ["<p>Opt for a pilot light DR strategy. Provision a copy of your entire workload infrastructure to a different AWS Region. Copy the first backup that consists of a full instance backup to the new RDS instance. In case of disaster, apply the incremental backup to the RDS instance in the new AWS Region. Configure Amazon Route 53 health checks to automatically initiate DNS failover to new Region</p>", "<p>Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster</p>", "<p>Configure your workload to simultaneously run in multiple AWS Regions as part of a multi-site active/active DR strategy. Replicate your entire workload to another AWS Region. With this strategy, asynchronous data replication between the regions enables near-zero RPO. Configure Amazon Route 53 with latency-based routing to choose between the active regional endpoint for directing user traffic</p>", "<p>Opt for a Warm standby approach by ensuring that there is a scaled-down, but fully functional, copy of your production environment in another AWS Region. Then, deploy enough resources to handle initial traffic, ensuring low RTO, and then rely on Auto Scaling to ramp up for subsequent traffic</p>"]}, "correct_response": ["b"], "section": "Domain 5: Incident and Event Response", "question_plain": "A DevOps Engineer has been asked to chalk out a disaster recovery (DR) plan for a workload in production. The workload runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are configured with an Auto Scaling group across multiple Availability Zones. Amazon Route 53 is configured to point to the ALB using an alias record. Amazon RDS for PostgreSQL DB instance is the database service. The draft DR plan mandates an RTO of three hours and an RPO of around 15 minutes.\n\nWhich Disaster Recovery (DR) strategy should the DevOps Engineer opt for a cost-effective solution?", "related_lectures": []}, {"_class": "assessment", "id": 67357128, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.</p>\n\n<p>Which step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</strong></p>\n\n<p>Aurora Replicas also referred to as reader instances have two main purposes. You can issue queries to them to scale the read operations for your application. You typically do so by connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for read-only connections across as many Aurora Replicas as you have in the cluster. Aurora Replicas also help to increase availability.</p>\n\n<p>By adding a reader instance to the Aurora cluster, the read-only traffic requests can be served from the reader instance, greatly reducing the traffic to the primary DB and avoiding interruption.</p>\n\n<p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements.</p>\n\n<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</strong></p>\n\n<p><strong>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a Multi-AZ cluster configuration</strong></p>\n\n<p>Once you have created an Aurora cluster, you cannot change its configuration to Multi-AZ, so both of these options are incorrect.</p>\n\n<p><strong>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</strong> - The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. Leveraging a custom endpoint for this use case is overkill, so this option is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html</a></p>\n", "answers": ["<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster custom endpoints by creating two groups of DB instances, one for read and the other for write requests. Update the Aurora cluster's reader endpoint to point to the read DB group of instances</p>", "<p>Turn on the Multi-AZ option on the Aurora cluster write operations. Update the application configuration to use the Aurora cluster endpoint for write operations. Read operations will automatically be served since its a  Multi-AZ cluster configuration</p>", "<p>Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations</p>", "<p>Turn on the Multi-AZ option on the Aurora cluster. Update the application configuration to use the Aurora cluster Multi-AZ instance endpoint for read/write operations</p>"]}, "correct_response": ["c"], "section": "Domain 5: Incident and Event Response", "question_plain": "An Aurora cluster is configured with a single DB instance for a web application. The application uses the instance endpoint to read/write data to the database. The operations team has scheduled an update on the cluster during the upcoming maintenance window. The application support team has requested help to ensure uninterrupted access to the application during the maintenance window.\n\nWhich step should a DevOps Engineer take so that the users experience the least possible interruption during the maintenance window?", "related_lectures": []}, {"_class": "assessment", "id": 67357130, "assessment_type": "multi-select", "prompt": {"question": "<p>The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</strong></p>\n\n<p>As per the security policy of the company, encrypt the unencrypted AMI using the KMS key. The encrypted snapshots must be encrypted with a KMS key. You can\u2019t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key.</p>\n\n<p>Amazon EC2 Auto Scaling uses service-linked roles to delegate permissions to other AWS services. Amazon EC2 Auto Scaling service-linked roles are predefined and include permissions that Amazon EC2 Auto Scaling requires to call other AWS services on your behalf. The predefined permissions also include access to your AWS-managed keys. However, they do not include access to your customer-managed keys, allowing you to maintain full control over these keys.</p>\n\n<p>If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached)</p>\n\n<ol>\n<li><p>The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key.</p></li>\n<li><p>Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The <code>Grantee Principal</code> element of the grant is the ARN of the appropriate service-linked role. The <code>key-id</code> is the ARN of the key.</p></li>\n</ol>\n\n<p>Key policy sections that allow cross-account access to the customer-managed key:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</strong></p>\n\n<p><strong>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/key-policy-requirements-EBS-encryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n", "answers": ["<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>", "<p>In Account B, create a KMS grant that delegates permissions to the service-linked role attached to the Auto Scaling group</p>", "<p>In Account A, create an encrypted AMI from the unencrypted version with AWS managed key. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B</p>", "<p>In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify permissions on the AMI to be accessible from Account B</p>", "<p>In Account A, create an encrypted AMI from the unencrypted version and encrypt the associated EBS snapshots with it. Specify the KMS key in the copy action. Share the encrypted AMI with Account B</p>"]}, "correct_response": ["a", "b"], "section": "Domain 6: Security and Compliance", "question_plain": "The security policy of a company mandates encrypting all AMIs that the company shares across its AWS accounts. An AWS account (Account A) has a custom AMI that is not encrypted. This AMI needs to be shared with another AWS Account B. Account B has Amazon EC2 instances configured with an Auto Scaling group that will use the AMI. Account A already has an AWS Key Management Service (AWS KMS) key.\n\nAs a DevOps Engineer, which combination of steps will you take to share the AMI with Account B while adhering to the company's security policy? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357132, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.</p>\n\n<p>As DevOps Engineer, how will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</strong></p>\n\n<p>You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.</p>\n\n<p>To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.</p>\n\n<p>The maximum file size for a log file is 75 MB. If the log file reaches the file size limit within the 5-minute period, the log stops adding records to it, publishes it to the Amazon S3 bucket, and then creates a new log file.</p>\n\n<p>Your bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For example, <code>aws-waf-logs-DOC-EXAMPLE-BUCKET-SUFFIX</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</strong> - AWS WAF supports encryption with Amazon S3 buckets for key type Amazon S3 key (SSE-S3) and AWS Key Management Service (SSE-KMS) AWS KMS keys. AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.</p>\n\n<p><strong>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as a WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data Firehose name should start with the prefix <code>aws-waf-logs-</code></strong> - AWS recommends that the Kinesis stream should not be used as a source when configuring Data Firehose as a WAF logging destination.</p>\n\n<p><strong>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</strong> - This is a made-up option, given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-s3.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html\">https://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html</a></p>\n", "answers": ["<p>To send logs to Amazon CloudWatch Logs, create a CloudWatch Logs log group. When you enable logging in AWS WAF, provide the log group ARN as the WAF logging destination. Log group names must start with <code>aws-waf-logs-</code> and end with any suffix. Define CloudWatch Metrics for the logs collected and trigger CloudWatch alarm(s) based on the logs generated</p>", "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want. For added security, you can add log encryption configuration by providing AWS Key Management Service keys that are managed by AWS</p>", "<p>Create an Amazon S3 bucket. Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with <code>aws-waf-logs-</code> and can end with any suffix you want</p>", "<p>Enable logging on AWS WAF and configure Amazon Kinesis Data Firehose with a configured storage destination as WAF logging destination. Configure Firehose to use a Kinesis stream as its source. Data firehose name should start with the prefix <code>aws-waf-logs-</code></p>"]}, "correct_response": ["c"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A media application runs on a host of Amazon EC2 instances fronted with an Application Load Balancer (ALB) and Amazon S3 buckets as storage service. For enhanced security, an AWS Web Application Firewall (AWS WAF) has been set up to monitor the requests coming to the ALB. The DevOps team needs to submit a quarterly report on the web requests received by AWS WAF, having detailed information about each web request as well as the details about rules that the request matched. The team has reached out to you for implementing the changes needed for collecting the security data for the coming months.\n\nAs DevOps Engineer, how will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357134, "assessment_type": "multi-select", "prompt": {"question": "<p>An application runs on a fleet of Amazon EC2 Windows instances configured with an Auto Scaling group (ASG). When scaling-in takes place in the ASG, the instances are terminated without notification. The application team wants to create an AMI and remove the Amazon EC2 Windows instance from its domain before terminating the scaled-in instances.</p>\n\n<p>As a DevOps Engineer, which combination of steps will you choose to implement this requirement? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</strong></p>\n\n<p><strong>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the instance from the domain and create an AMI of the EC2 instance</strong></p>\n\n<p>Oftentimes, you may want to execute some code and actions before terminating an Amazon Elastic Compute Cloud (Amazon EC2) instance that is part of an Amazon EC2 Auto Scaling group.</p>\n\n<p>One way to execute code and actions before terminating an instance is to create a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status. This allows you to perform any desired actions before immediately terminating the instance within the Auto Scaling group. The <code>Terminating:Wait</code> status can be monitored by an Amazon CloudWatch event, which triggers an AWS Systems Manager automation document to perform the action you want.</p>\n\n<p>Broadly, the steps needed for the above configuration:\n1. Add a lifecycle hook.\n2. Create a Systems Manager automation document.\n3. Create AWS Identity and Access Management (IAM) policies and a role to delegate permissions to the Systems Manager automation document.\n4. Create IAM policies and a role to delegate permissions to CloudWatch Events, which invokes the Systems Manager automation document.\n5. Create a CloudWatch Events rule.\n6. Add a Systems Manager automation document as a CloudWatch Event target.</p>\n\n<p>Using Lifecycle hooks to run code before terminating an EC2 Auto Scaling instance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/\">https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and set up an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</strong> - <code>Terminating:Pending</code> is not a valid state. The following are the transitions between instance states in the Amazon EC2 Auto Scaling lifecycle.</p>\n\n<p>Amazon EC2 Auto Scaling instance lifecycle:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q20-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a></p>\n\n<p><strong>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Patch Manager is a capability of AWS Systems Manager that automates the process of patching managed nodes with both security-related updates and other types of updates. It cannot be used for the processing of custom logic/code.</p>\n\n<p><strong>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</strong> - Maintenance Windows is a capability of AWS Systems Manager that helps you define a schedule for when to perform potentially disruptive actions on your nodes such as patching an operating system, updating drivers, or installing software or patches. Maintenance Windows is not relevant for the given use case, since we want the custom logic to run immediately and return to the instance termination triggered by Auto Scaling Group.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-lifecycle.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html#lifecycle-hooks-availability</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html</a></p>\n", "answers": ["<p>Add a lifecycle hook that puts the instance in <code>Terminating:Wait</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Wait</code> status</p>", "<p>Add a lifecycle hook that puts the instance in <code>Terminating:Pending</code> status and setup an Amazon CloudWatch event to monitor the <code>Terminating:Pending</code> status</p>", "<p>Add an AWS Systems Manager Patch Manager as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>", "<p>Configure an AWS Systems Manager Maintenance Window to schedule an action to run a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>", "<p>Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the computer from the domain and creates an AMI of the EC2 instance</p>"]}, "correct_response": ["a", "e"], "section": "Domain 4: Monitoring and Logging", "question_plain": "An application runs on a fleet of Amazon EC2 Windows instances configured with an Auto Scaling group (ASG). When scaling-in takes place in the ASG, the instances are terminated without notification. The application team wants to create an AMI and remove the Amazon EC2 Windows instance from its domain before terminating the scaled-in instances.\n\nAs a DevOps Engineer, which combination of steps will you choose to implement this requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357136, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.</p>\n\n<p>What should a DevOps engineer do to meet these requirements with the minimal overhead?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</strong></p>\n\n<p>You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (Amazon EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.</p>\n\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing <code>Service managed permissions</code> allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n\n<p>In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.</p>\n\n<p>Lastly, you choose whether to deploy a stack to your entire organization or just to one or more Organization Units (OU). You also choose a couple of deployment options: how many accounts will be prepared in parallel, and how many failures you tolerate before stopping the entire deployment.</p>\n\n<p>Use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by an AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</strong> - This option involves using too many services which unnecessarily adds to the complexity and cost of the overall solution. So, this option is incorrect.</p>\n\n<p><strong>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</strong> - While you can use AWS Systems Manager to automate tasks across multiple accounts in AWS Organization, the other details in this option are irrelevant to the given use case.</p>\n\n<p>When you run automation across multiple Regions and accounts, you target resources by using tags or the name of an AWS resource group. The resource group must exist in each target account and Region. The resource group name must be the same in each target account and Region. The automation fails to run on those resources that don't have the specified tag or that aren't included in the specified resource group.</p>\n\n<p><strong>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</strong> - It is possible to enable CloudTrail logging from the management account of AWS Organizations, and is referred to as the organization trail. The use case is to be able to log all Trail events to a commonplace. Also, creating an IAM role in the management account and sharing it across all member accounts is not straightforward and requires manual work. Hence, this option is incorrect for the given use case.</p>\n\n<p>IAM policies usage in AWS Organizations:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q21-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html</a></p>\n", "answers": ["<p>From the management account of AWS Organizations, create an Amazon EventBridge rule that is triggered by AWS account creation API call. Configure an AWS Lambda function to enable CloudTrail logging and to attach the necessary IAM roles to the account</p>", "<p>Run automation across multiple accounts using AWS System Manager Automation. Create an AWS resource group from the management account (or any centralized account) and name it exactly the same for all accounts and OUs and add the account ID or OU as a prefix as per standard naming convention. Include the CloudTrail configuration and the IAM role to be created</p>", "<p>From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations</p>", "<p>From the management account of AWS Organizations, enable AWS CloudTrail logs for all member accounts. Similarly, create an IAM role and share it across accounts and OUs of the AWS Organization</p>"]}, "correct_response": ["c"], "section": "Domain 1: SDLC Automation", "question_plain": "A multi-national company with hundreds of AWS accounts has slowly adopted AWS Organizations with all features enabled. The company has also configured a few Organization Units (OUs) to serve its business objectives. The company has some AWS Identity and Access Management (IAM) roles that need to be configured for every new AWS account created for the company. Also, the security policy mandates enabling AWS CloudTrail for all AWS accounts. The company is looking for an automated solution that can add the mandatory IAM Roles and CloudTrail configurations to all newly created accounts and also delete the resources/configurations when an account leaves the organization without manual intervention.\n\nWhat should a DevOps engineer do to meet these requirements with the minimal overhead?", "related_lectures": []}, {"_class": "assessment", "id": 67357138, "assessment_type": "multi-select", "prompt": {"question": "<p>An AWS managed <code>cloudformation-stack-drift-detection-check</code> rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:</p>\n\n<p>a) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'</p>\n\n<p>As a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p>The <code>cloudformation-stack-drift-detection-check</code> rule checks if the actual configuration of a Cloud Formation stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. The rule and the stack are COMPLIANT when the stack drift status is IN_SYNC. The rule is NON_COMPLIANT if the stack drift status is DRIFTED.</p>\n\n<p>CloudFormation offers a drift detection feature to detect unmanaged configuration changes to stacks and resources. This will let you take corrective action to put the stack resources back in sync with their definitions in the stack template. To return a resource to compliance, the resource definition changes can be reverted directly.</p>\n\n<p><strong>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</strong></p>\n\n<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code>. You receive a throttling or \"Rate Exceeded\" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.</p>\n\n<p>Resolve the error resulting from the availability of DetectStackDrift:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q22-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><strong>AWS CloudFormation does not support drift detection of custom resources</strong></p>\n\n<p>AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</strong> - Any issues with permissions will not result in the shown error. Permissions error looks something like this: \"Your stack drift detection operation for the specific stack has failed. Check your existing AWS CloudFormation role permissions and add the missing permissions.\"</p>\n\n<p><strong>This error is a false positive and can be ignored for this scenario</strong> - This option just acts as a distractor.</p>\n\n<p><strong>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</strong> - It is true that CloudFormation only determines drift for property values that are explicitly set, either through the stack template or by specifying template parameters. However, custom resources are not currently supported for drift.</p>\n\n<p>References:</p>\n\n<p>[[https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource](https://repost.aws/questions/QUolH-EWnHRNGDbnUiu3chXw/how-to-detect-drifts-of-cloudformation-custom-resource)]</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/config-cloudformation-drift-detection\">https://repost.aws/knowledge-center/config-cloudformation-drift-detection</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html</a></p>\n", "answers": ["<p>AWS Config rule depends on the availability of <code>DetectStackDrift</code> action of CloudFormation API. AWS Config defaults the rule to NON_COMPLIANT when throttling occurs</p>", "<p>You receive the error when the AWS Identity and Access Management (IAM) role for the required <code>cloudformationRoleArn</code> parameter doesn't have sufficient service permissions</p>", "<p>This error is a false positive and can be ignored for this scenario</p>", "<p>AWS CloudFormation only determines drift for property values that are explicitly set. Explicitly set the property values for your custom resource to be included in drift</p>", "<p>AWS CloudFormation does not support drift detection of custom resources</p>"]}, "correct_response": ["a", "e"], "section": "Domain 6: Security and Compliance", "question_plain": "An AWS managed cloudformation-stack-drift-detection-check rule is defined in AWS Config for drift detection in AWS CloudFormation resources. The DevOps team is facing two issues:\n\na) How to detect drifts of Cloudformation custom resources\nb) Drift status of the stack shows as IN_SYNC in the CloudFormation console, the following is the drift detection error  - 'While AWS CloudFormation failed to detect drift, defaulting to NON_COMPLIANT. Re-evaluate the rule and try again. If the problem persists contact AWS CloudFormation support'\n\nAs a DevOps Engineer, which steps will you combine to fix the aforementioned issues? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357140, "assessment_type": "multi-select", "prompt": {"question": "<p>A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.</p>\n\n<p>Which combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></strong></p>\n\n<p><strong>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</strong></p>\n\n<p>The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You can:</p>\n\n<ol>\n<li>Choose the remediation action you want to associate from a pre-populated list.</li>\n<li>Create your own custom remediation actions using AWS Systems Manager Automation documents.</li>\n</ol>\n\n<p>If a resource is still non-compliant after auto-remediation, you can set the rule to try auto-remediation again.</p>\n\n<p>For the above use case: You must have AWS Config enabled in your AWS account. The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.</p>\n\n<p>Steps to set up Auto Remediation for s3-bucket-logging-enabled:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q23-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</strong> - While you can create a custom remediation action using SSM, for this particular use case, it is not required since auto-remediation for S3 server logging is already present in the remediation action list of AWS Config rules.</p>\n\n<p><strong>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</strong> - As discussed above, the AWS config remediation action pre-populated list already has an action defined for enabling S3 logging. Hence, custom code is unnecessary for this use case.</p>\n\n<p><strong>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</strong>- While setting up remediation action if you want to pass the resource ID of non-compliant resources to the remediation action, choose the Resource ID parameter. If selected at runtime, the parameter is substituted with the ID of the resource to be remediated. This is not mandatory though.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n", "answers": ["<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose <code>AWS-ConfigureS3BucketLogging</code></p>", "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-logging-enabled</code>. Create your own custom remediation action using AWS Systems Manager Automation documents to enable logging on the S3 bucket</p>", "<p>Configure AWS Config Auto Remediation for the AWS Config rule <code>s3-bucket-logging-enabled</code>. From the remediation action list choose AWS Lambda to implement a custom function that will enable S3 logging for the S3 bucket ID passed</p>", "<p>The <code>AutomationAssumeRole</code> in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config</p>", "<p>While setting up remediation action, pass the resource ID of non-compliant resources to the remediation action. This configuration is mandatory for auto-remediation to work</p>"]}, "correct_response": ["a", "d"], "section": "Domain 6: Security and Compliance", "question_plain": "A media company extensively uses Amazon S3 buckets for storing images files, documents, and other business-specific data. The company has mandated enabling logging for all Amazon S3 buckets. The audit team publishes the reports of all AWS resources failing company security standards. Until recently, the security team would pick the list of noncompliant Amazon S3 buckets from the audit list and execute remediation actions manually for each resource. This process is not only time-consuming but also leaves noncompliant resources vulnerable for a long duration.\n\nWhich combination of steps should a DevOps Engineer take to meet these requirements using an automated solution? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357142, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.</p>\n\n<p>What steps will you take to rename the CloudFormation stack without deleting the resources created?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</strong></p>\n\n<p>With the DeletionPolicy attribute, you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use <code>Retain</code> for any resource. For resources that support snapshots, such as <code>AWS::EC2::Volume</code>, specify Snapshot to have CloudFormation create a snapshot before deleting the resource.</p>\n\n<p>These steps need to be followed:\n1. Launch a CloudFormation stack that deploys the necessary resources.\n2. Add a <code>Retain</code> attribute to the deletion policy of all the resources deployed by the stack.\n3. Delete the stack and verify that the resources are retained.\n4. Create a new stack and import the resources that were retained from the original stack. This stack is created with a new name.\n5. Remove the <code>Retain</code> attribute from the stack to revert to the original template.</p>\n\n<p>Refer to the example that walks through the process of retaining a single resource\u2014a VPC\u2014when changing the name of a CloudFormation stack:</p>\n\n<p>Process overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q24-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</strong> - 'Snapshot' attribute will still delete the resource after taking its snapshot. So this option is incorrect.</p>\n\n<p><strong>Use the CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</strong> - A hook is an executable custom logic that automatically inspects resources before they're provisioned. Hooks can inspect the resources that CloudFormation is about to provision. If a hook finds any resource that doesn't comply with your organizational guidelines, it can prevent CloudFormation from continuing the provisioning process. Any hook based logic is not useful for the given use case.</p>\n\n<p><strong>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both stacks while retaining the resources</strong> - This option just acts as a distractor, as it does not solve the given issue.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/\">https://aws.amazon.com/blogs/infrastructure-and-automation/keep-your-aws-resources-when-you-rename-an-aws-cloudformation-stack/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html\">https://docs.aws.amazon.com/cloudformation-cli/latest/userguide/hooks.html</a></p>\n", "answers": ["<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of the S3 bucket and EC2 instance. Add a 'Snapshot' attribute to the deletion policy of Amazon EBS Volume. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> and 'Snapshot' attribute from the stack to revert to the original template</p>", "<p>Launch a CloudFormation stack that deploys all the resources of the stack. Add a <code>Retain</code> attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the <code>Retain</code> attribute from the stack to revert to the original template</p>", "<p>Launch a new CloudFormation stack that deploys all the resources of the stack. Launch another CloudFormation stack that deploys all the resources of the stack with a new name. In the second CloudFormation stack use, the <code>DependsOn</code> attribute to create the resource dependency on the first stack. Delete both the stacks while retaining the resources</p>", "<p>Use CloudFormation registry to create custom hooks for all the resources of the CloudFormation stack. Then, delete the original CloudFormation stack and recreate a new one with the updated name. Using the hooks created earlier, import the resources back into the newly created stack</p>"]}, "correct_response": ["b"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A CloudFormation stack consists of the following AWS resources - Amazon Simple Storage Service (Amazon S3) bucket, Amazon Amazon Elastic Compute Cloud (Amazon EC2) instance, and an Amazon EBS Volume. Due to a high-impact security issue, the DevOps team has been asked to rename the AWS CloudFormation stack. However, the resources created by the stack cannot be deleted for business purposes.\n\nWhat steps will you take to rename the CloudFormation stack without deleting the resources created?", "related_lectures": []}, {"_class": "assessment", "id": 67357144, "assessment_type": "multi-select", "prompt": {"question": "<p>An application runs on a fleet of Amazon EC2 instances that are configured with an Auto Scaling group (ASG). Both Spot and On-Demand instances are utilized as per the ASG configuration. For the most part, the ASG seems to be working fine as expected. There are a few issues that the DevOps team has flagged:</p>\n\n<p>a) During a scale-in activity, ASG has terminated instance in the Availability Zone (AZ) that already had fewer instances than the other\nb) For some duration, ASG exceeded the specified maximum capacity of the group</p>\n\n<p>What reasons can you identify for this behavior? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</strong></p>\n\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application.</p>\n\n<p>Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the old ones, being at or near the specified maximum capacity could impede or completely stop rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity. The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either because of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the group.</p>\n\n<p><strong>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</strong></p>\n\n<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually. It also identifies which instances (within the identified purchase option) in which Availability Zones to terminate that will result in the Availability Zones being most balanced.</p>\n\n<p>Understanding ASG termination policy for mixed instances groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</strong> - Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used. As a result, you might encounter situations in which some newer instances are terminated before older instances. So this option is incorrect.</p>\n\n<p><strong>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</strong> - It is not possible to configure more than one AWS Lambda function in the termination policies for an Auto Scaling group. Also, the AWS Lambda function is used to configure custom termination policy and hence is not related to the given use case.</p>\n\n<p><strong>With the Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</strong> - This statement is incorrect. ASG uses multiple termination criteria before selecting an instance for termination, and not just the next billing hour as the only criterion.</p>\n\n<p>Understanding the default termination policy of ASG:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html#default-termination-policy-mixed-instances-groups</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n", "answers": ["<p>The instance that was launched from the oldest launch template or launch configuration is terminated first. This can temporarily lead to an unbalanced group</p>", "<p>You can only specify one Lambda function in the termination policies for an Auto Scaling group. If more than one Lambda functions are configured, the ASG behavior is ambiguous</p>", "<p>With Default termination policy, instances that are closest to the next billing hour are terminated first. This can temporarily cause instance distribution imbalance among the AZs</p>", "<p>Amazon EC2 Auto Scaling can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity</p>", "<p>When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs</p>"]}, "correct_response": ["d", "e"], "section": "Domain 5: Incident and Event Response", "question_plain": "An application runs on a fleet of Amazon EC2 instances that are configured with an Auto Scaling group (ASG). Both Spot and On-Demand instances are utilized as per the ASG configuration. For the most part, the ASG seems to be working fine as expected. There are a few issues that the DevOps team has flagged:\n\na) During a scale-in activity, ASG has terminated instance in the Availability Zone (AZ) that already had fewer instances than the other\nb) For some duration, ASG exceeded the specified maximum capacity of the group\n\nWhat reasons can you identify for this behavior? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357146, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company implements access control by creating different policies for different job functions. These policies are attached to IAM roles/groups with minimum permissions necessary for the job function. Up until this point, the solution has been functioning effectively. However, with business expansion, the administrator has to frequently update the existing policies to allow access to new resources.</p>\n\n<p>Which of the following solutions can make the access control applicable to all new resources without the need to update the policies?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</strong></p>\n\n<p>The traditional authorization model used in IAM is called role-based access control (RBAC). RBAC defines permissions based on a person's job function, known outside of AWS as a role. Within AWS a role usually refers to an IAM role, which is an identity in IAM that you can assume.</p>\n\n<p>Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags. You can attach tags to IAM resources, including IAM entities (users or roles), and to AWS resources. You can create a single ABAC policy or a small set of policies for your IAM principals. These ABAC policies can be designed to allow operations when the principal's tag matches the resource tag. ABAC is helpful in environments that are growing rapidly and help with situations where policy management becomes cumbersome.</p>\n\n<p>Comparing ABAC to the traditional RBAC model:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</strong> - SCP is a policy that specifies the services and actions that users and roles can use in the accounts that the SCP affects. SCPs are similar to IAM permissions policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account.</p>\n\n<p><strong>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</strong> - If you have to create a Resource-based policy for every new resource that is added to the account, then we are pretty much at the same place without offering any solution. Hence, this option is not a valid choice for the given use case.</p>\n\n<p><strong>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account.</p>\n\n<p>Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies.</p>\n\n<p>Neither ACL nor a session policy is the right choice for the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session</a></p>\n", "answers": ["<p>Create an AWS Organization and enable all the features. Attach identity-based IAM roles for newly created resources and share them using Service control policies (SCPs) with all member accounts of the organization</p>", "<p>Create a Resource-based policy on the newly created resources to add to the existing permissions of the IAM roles accessing the created resources</p>", "<p>Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created</p>", "<p>Configure Access control lists (ACLs) to allow access to certain principals in the account. Add the ACLs to a session policy to dynamically add permissions to the IAM role that the user already has</p>"]}, "correct_response": ["c"], "section": "Domain 6: Security and Compliance", "question_plain": "A company implements access control by creating different policies for different job functions. These policies are attached to IAM roles/groups with minimum permissions necessary for the job function. Up until this point, the solution has been functioning effectively. However, with business expansion, the administrator has to frequently update the existing policies to allow access to new resources.\n\nWhich of the following solutions can make the access control applicable to all new resources without the need to update the policies?", "related_lectures": []}, {"_class": "assessment", "id": 67357148, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.</p>\n\n<p>As a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p>AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization\u2019s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks.</p>\n\n<p>Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Policies can include WAF rules drawn from within the organization, and also those created by AWS Partners such as Imperva, F5, Trend Micro, and other AWS Marketplace vendors. This gives your security team the power to duplicate their existing on-premises security posture in the cloud.</p>\n\n<p>Firewall Manager has three prerequisites:</p>\n\n<ol>\n<li><p>AWS Organizations \u2013 Your organization must be using AWS Organizations to manage your accounts and all features must be enabled.</p></li>\n<li><p>Firewall Administrator \u2013 You must designate one of the AWS accounts in your organization as the administrator for Firewall Manager. This gives the account permission to deploy AWS WAF rules across the organization.</p></li>\n<li><p>AWS Config \u2013 You must enable AWS Config for all of the accounts in the Organization so that Firewall Manager can detect newly created resources.</p></li>\n</ol>\n\n<p>Using AWS Firewall Manager to centrally manage your Web Application Portfolio:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/\">https://aws.amazon.com/blogs/aws/aws-firewall-manager-central-management-for-your-web-application-portfolio/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Amazon GuardDuty can automatically update the AWS Web Application Firewall Web Access Control Lists (WebACLs) and VPC Network Access Control Lists (NACLs) in response to GuardDuty findings. But, Amazon GuardDuty is a continuous security monitoring and threat detection service and not a security management service like AWS Firewall Manager. Even though GuardDuty can update web ACLs, it's a reaction to a threat. It cannot be used to proactively define rules for web ACLs across accounts to centrally manage the security infrastructure of an organization.</p>\n\n<p><strong>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - Enabling AWS Config is a prerequisite for using AWS Firewall Manager. AWS Config can track the status of resources, but AWS Firewall Manager is needed for centrally managing the security infrastructure.</p>\n\n<p>AWS Firewall Manager and AWS Config:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q27-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/firewall-manager/\">https://aws.amazon.com/firewall-manager/</a></p>\n\n<p><strong>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</strong> - This option has been added as a distractor and is irrelevant to the given use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html\">https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html</a></p>\n", "answers": ["<p>Designate one of the AWS accounts in your organization as the administrator for Firewall Manager in AWS Organizations. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>", "<p>Create an Amazon Systems Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>", "<p>Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>", "<p>Configure a managed rule in AWS Config to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "A company having hundreds of AWS accounts manages its operations and security through a single organization created in AWS Organizations. As per the company's policy, AWS Config and AWS CloudTrail are enabled for all accounts. The security policy mandates configuring AWS Web Application Firewall (AWS WAF) web ACLs for all internet-facing Application Load Balancers (ALBs) and Amazon API Gateway APIs. However, monthly audit reports consistently report unsecured ALBs and API Gateway APIs.\n\nAs a DevOps engineer, the security team has requested you to automate these configurations for all accounts to avoid oversight. What steps will you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 67357150, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.</p>\n\n<p>As a DevOps Engineer, how will you implement this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</strong></p>\n\n<p>Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following:</p>\n\n<ol>\n<li>Check the incoming event for a specific string.</li>\n<li>Publish a message to Amazon SNS if the string in the event matches the string in the Lambda function.</li>\n</ol>\n\n<p>To use an AWS Lambda function to receive an email from SNS when any of your AWS Glue jobs fail a retry, do the following:\n1. Create an Amazon SNS topic.\n2. Create an AWS Lambda function.\n3. Create an Amazon EventBridge event that uses the Lambda function to initiate email notifications.</p>\n\n<p>AWS Lambda function logic:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q28-i1.jpg\">\nvia - <a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</strong> - Amazon EventBridge cannot be directly used without a Lambda function since the use case needs notifications only for Glue job retry failure. So this logic has to be included in a Lambda function.</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</strong> - The use case is not about retrying the Glue job, but about sending an SNS notification when the retry of the job fails.</p>\n\n<p><strong>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</strong> - AWS Personal Health Dashboard provides proactive notifications of scheduled activities, such as any changes to the infrastructure powering your resources, enabling you to better plan for events that may affect you. This option is not relevant to the given requirements.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts\">https://repost.aws/knowledge-center/glue-job-fail-retry-lambda-sns-alerts</a></p>\n", "answers": ["<p>Configure Amazon EventBridge events for AWS Glue. Configure an Amazon Simple Notification Service (Amazon SNS) notification when the Glue job fails a retry</p>", "<p>Amazon Simple Notification Service (Amazon SNS) cannot retry failures, leverage Amazon Simple Queue Service (Amazon SQS) dead-letter queues to retry the failed Glue jobs</p>", "<p>Check the AWS Personal Health Dashboard for failed AWS Glue jobs. Schedule an AWS Lambda function to pick the failed event from the service health dashboard and trigger an Amazon Simple Notification Service (Amazon SNS) notification when a retry fails</p>", "<p>Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found</p>"]}, "correct_response": ["d"], "section": "Domain 5: Incident and Event Response", "question_plain": "A support team wants to be notified via an Amazon Simple Notification Service (Amazon SNS) notification when an AWS Glue job fails a retry.\n\nAs a DevOps Engineer, how will you implement this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357152, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p>As a DevOps Engineer, what solution do you suggest to address the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations</strong></p>\n\n<p>With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries.</p>\n\n<p>Set up one or more AWS accounts as monitoring accounts and link them with multiple source accounts. A monitoring account is a central AWS account that can view and interact with observability data generated from source accounts. A source account is an individual AWS account that generates observability data for the resources that reside in it. Source accounts share their observability data with the monitoring account.</p>\n\n<p>The shared observability data can include the following types of telemetry:\n1. Metrics in Amazon CloudWatch\n2. Log groups in Amazon CloudWatch Logs\n3. Traces in AWS X-Ray</p>\n\n<p>There are two options for linking source accounts to your monitoring account. You can use one or both options.\n1. Use AWS Organizations to link accounts in an organization or organizational unit to the monitoring account.\n2. Connect individual AWS accounts to the monitoring account.</p>\n\n<p>AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements.</p>\n\n<p>Demonstration of setting up CloudWatch cross-account observability:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q29-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</strong> - While this option is correct, it cannot automatically onboard any new AWS accounts created later in the organization.</p>\n\n<p><strong>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</strong> - Amazon EventBridge does not support Amazon S3 bucket as a target. Hence, this option is incorrect.</p>\n\n<p><strong>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</strong> - You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. The use case does not talk about real-time data, so configuring CloudWatch Metric Streams with Firehose is not the best fit.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Unified-Cross-Account.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/\">https://aws.amazon.com/blogs/aws/cloudwatch-metric-streams-send-aws-metrics-to-partners-and-to-your-apps-in-real-time/</a></p>\n", "answers": ["<p>Create a CloudWatch alarm for the CloudWatch metrics and trigger an event on Amazon EventBridge. Write the metrics data to the Amazon S3 bucket. Use Amazon Athena to create visualizations and dashboards from CloudWatch metrics data, logs data, and traces</p>", "<p>Configure CloudWatch Metric Streams to stream real-time metrics data to Kinesis Data Firehose. Firehose will push the metrics data to Amazon Simple Storage Service (Amazon S3) bucket. configure Amazon Athena to create a dashboard with the metrics data</p>", "<p>Use Amazon CloudWatch cross-account observability to set up security and operations account as the monitoring account and link it with rest of the member accounts of the organization using AWS Organizations</p>", "<p>Use the Amazon CloudWatch cross-account observability feature from the CloudWatch console to create the monitoring account and connect the individual AWS accounts to the monitoring account</p>"]}, "correct_response": ["c"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A company has hundreds of AWS accounts and has also created an organization in AWS Organizations to manage the accounts. The company wants a dashboard to seamlessly search, visualize, and analyze CloudWatch metrics data, logs data, and traces (from AWS X-Ray) from all the linked accounts into a single security and operations account. The solution should automatically onboard any new AWS accounts created later in the organization.\n\nAs a DevOps Engineer, what solution do you suggest to address the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357154, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps Engineer is working on multiple applications that need to be configured for Application Auto scaling, however, there are some application constraints to be addressed:</p>\n\n<p>a) A serverless application is built on AWS Lambda with the following traffic pattern - The traffic for the application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nb) Another flagship application runs on Spot Fleet. The CPU utilization of the fleet has to stay at around 50 percent when the load on the application changes.</p>\n\n<p>Which of the following solutions can address these requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</strong></p>\n\n<p>To create a target tracking scaling policy, you specify an Amazon CloudWatch metric and a target value that represents the ideal average utilization or throughput level for your application. Application Auto Scaling can then scale out the scalable target (add capacity) to handle peak traffic, and scale in the scalable target (remove capacity) to reduce costs during periods of low utilization or throughput.</p>\n\n<p>For example, let's say that you currently have an application that runs on Spot Fleet, and you want the CPU utilization of the fleet to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive number of idle resources.</p>\n\n<p>You can meet this need by creating a target tracking scaling policy that targets an average CPU utilization of 50 percent. Then, Application Auto Scaling scales the number of instances to keep the actual metric value at or near 50 percent.</p>\n\n<p>Target tracking scaling policies for Application Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a></p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</strong></p>\n\n<p>Scaling based on a schedule allows you to set your own scaling schedule according to predictable load changes. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday.</p>\n\n<p>To use scheduled scaling, create scheduled actions, which tell Application Auto Scaling to perform scaling activities at specific times. When you create a scheduled action, you specify the scalable target, when the scaling activity should occur, a minimum capacity, and a maximum capacity. You can create scheduled actions that scale one time only or that scale on a recurring schedule.</p>\n\n<p>Scheduled scaling for Application Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q30-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</strong> - With target tracking scaling policies you do not create, edit, or delete the CloudWatch alarms that are used with a target tracking scaling policy. Application Auto Scaling creates and manages the CloudWatch alarms that are associated with your target tracking scaling policies and deletes them when no longer needed.</p>\n\n<p><strong>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the minimum and maximum capacity based on the requirements</strong> - Scheduled scaling is not the right option when a parameter has to be tracked and forms the basis of the scale-in and scale-out activity of the ASG. Target tracking is better suited for these use cases.</p>\n\n<p><strong>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</strong> - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html\">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html</a></p>\n", "answers": ["<p>Create an Amazon CloudWatch metric to track average CPU utilization of 50 percent for the Spot Fleet. Create a target tracking auto-scaling policy CloudWatch alarm against the CloudWatch metric created from the AWS console</p>", "<p>Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation</p>", "<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with Spot instances as a scalable target. Specify the  minimum and maximum capacity based on the requirements</p>", "<p>Create an Amazon CloudWatch metric to track the utilization of the AWS Lambda function. Setup step scaling policy by configuring the upper bound, lower bound, and breach threshold values for the Lambda function</p>", "<p>Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling</p>"]}, "correct_response": ["b", "e"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A DevOps Engineer is working on multiple applications that need to be configured for Application Auto scaling, however, there are some application constraints to be addressed:\n\na) A serverless application is built on AWS Lambda with the following traffic pattern - The traffic for the application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nb) Another flagship application runs on Spot Fleet. The CPU utilization of the fleet has to stay at around 50 percent when the load on the application changes.\n\nWhich of the following solutions can address these requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357156, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has hired you as an AWS Certified DevOps Engineer - Professional to provide recommendations for a failed security audit of its flagship project. You have been tasked to review the company's buildspec.yaml file for its AWS CodeBuild project. Upon investigation, you have noticed that the file has hard-coded values for the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password. In addition, to perform one-time configuration changes during the build phase, the file has commands to <code>ssh</code> and <code>scp</code> into an EC2 instance using an SSH private key stored on Amazon S3.</p>\n\n<p>What changes would you recommend to comply with AWS security best practices? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded AWS credentials in code, configuration, or build files such as the buildspec.yaml file. AWS recommends using an appropriate permissions policy that is attached to the CodeBuild project role to grant the necessary access to the required resources.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a></p>\n\n<p><strong>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong></p>\n\n<p>It is considered a bad practice to store hard-coded database credentials in code, configuration, or build files such as the buildspec.yaml file. As a security best practice, AWS recommends using the database password as a SecureString value in AWS Systems Manager Parameter Store which can then be referenced in the buildspec file like so:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q31-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>The configuration above is required when you want to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store. This contains a mapping of key/value scalars, where each mapping represents a single custom environment variable stored in the Amazon EC2 Systems Manager Parameter Store. key is the name you use later in your build commands to refer to this custom environment variable, and value is the name of the custom environment variable stored in Amazon EC2 Systems Manager Parameter Store. To allow CodeBuild to retrieve custom environment variables stored in Amazon EC2 Systems Manager Parameter Store, you must add the ssm:GetParameters action to your CodeBuild service role.</p>\n\n<p><strong>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong></p>\n\n<p>Using Run Command, a capability of AWS Systems Manager, you can remotely and securely manage the configuration of your managed nodes. A managed node is any Amazon Elastic Compute Cloud (Amazon EC2) instance or non-EC2 machine in your hybrid and multi-cloud environment that has been configured for Systems Manager. Run Command allows you to automate common administrative tasks and perform one-time configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface (AWS CLI), AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p>\n\n<p>It is considered a security bad practice to store the SSH private key on Amazon S3. Using the AWS Systems Manager <code>run</code> command for the given use case meets the security best practices.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</strong> - In Secrets Manager, a secret consists of secret information, the secret value, plus metadata about the secret. A secret value can be a string or binary. Secrets Manager uses IAM permission policies to make sure that only authorized users can access or modify a secret. There is no such thing as a SecureString value in AWS Secrets Manager. This option has been added as a distractor.</p>\n\n<p><strong>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</strong> - It is considered a security bad practice to store the AWS access credentials on Amazon S3, so this option is incorrect.</p>\n\n<p><strong>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</strong> - Automation feature of the AWS Systems Manager helps you to build automated solutions to deploy, configure, and manage AWS resources at scale. With Automation, you have granular control over the concurrency of your automation. This means you can specify how many resources to target concurrently, and how many errors can occur before an automation is stopped. Since the given use case involves one-time configuration changes, it is better to use the AWS Systems Manager <code>run</code> command.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works\">https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html#concepts-how-it-works</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/auth-and-access-control-iam-access-control-identity-based.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.env.parameter-store</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html</a></p>\n", "answers": ["<p>Configure the CodeBuild project role with the necessary permissions policy and then delete the environment variables referencing the AWS credentials from the buildspec.yaml file</p>", "<p>Store the database password as a SecureString value in AWS Systems Manager Parameter Store which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>", "<p>Leverage the AWS Systems Manager <code>run</code> command to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>", "<p>Store the database password as a SecureString value in AWS Secrets Manager which is then referenced in the buildspec environment. Also, delete the environment variable referencing the hard-coded value of the database password from the buildspec.yaml file</p>", "<p>Store the environment variables that reference the AWS <code>Access Key ID</code>, <code>Secret Access Key</code>, and the database password in an encrypted format on Amazon S3. Leverage a Python script to dynamically import the values from S3 during the pre-build phase</p>", "<p>Leverage the AWS Systems Manager automation document to manage the EC2 instance, rather than using commands to <code>ssh</code> and <code>scp</code> into the EC2 instance via the SSH private key stored on Amazon S3</p>"]}, "correct_response": ["a", "b", "c"], "section": "Domain 6: Security and Compliance", "question_plain": "A company has hired you as an AWS Certified DevOps Engineer - Professional to provide recommendations for a failed security audit of its flagship project. You have been tasked to review the company's buildspec.yaml file for its AWS CodeBuild project. Upon investigation, you have noticed that the file has hard-coded values for the environment variables that reference the AWS Access Key ID, Secret Access Key, and the database password. In addition, to perform one-time configuration changes during the build phase, the file has commands to ssh and scp into an EC2 instance using an SSH private key stored on Amazon S3.\n\nWhat changes would you recommend to comply with AWS security best practices? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 67357158, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In a multinational company, various AWS accounts are efficiently managed using AWS Control Tower. The company operates both internal and public applications across its infrastructure. To streamline operations, each application team is assigned a dedicated AWS account responsible for hosting their respective applications. These accounts are consolidated under an organization in AWS Organizations. Additionally, a specific AWS Control Tower member account acts as a centralized DevOps hub, offering Continuous Integration/Continuous Deployment (CI/CD) pipelines that application teams utilize to deploy applications to their designated AWS accounts. A specialized IAM role for deployment is available within this central DevOps account.</p>\n\n<p>Currently, a particular application team is facing challenges while attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster situated in their application-specific AWS account. They have an existing IAM role for deployment within the application AWS account. The deployment process relies on an AWS CodeBuild project, configured within the centralized DevOps account, and utilizes an IAM service role for CodeBuild. However, the deployment process is encountering an Unauthorized error when trying to establish connections to the cross-account EKS cluster from the CodeBuild environment.</p>\n\n<p>To resolve this error and facilitate a successful deployment, what solution would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>By configuring a trust relationship in the application account's deployment IAM role for the centralized DevOps account, using the sts:AssumeRole action, CodeBuild in the centralized DevOps account will be allowed to assume the IAM role in the application account. This will enable CodeBuild to access the EKS cluster in the application account. Additionally, granting the application account's deployment IAM role the necessary access to the EKS cluster ensures that it has the required permissions to perform the deployment tasks. Finally, configuring the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions ensures that the IAM role has the required permissions within the EKS cluster.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</strong></p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</strong></p>\n\n<p>AssumeRoleWithSAML is used for federation scenarios involving SAML-based identity providers, which is not relevant to the given use case, so both these options are incorrect.</p>\n\n<p><strong>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</strong></p>\n\n<p>You need to configure a trust relationship in the application account's deployment IAM role for the centralized DevOps account, so that CodeBuild in the centralized DevOps account can assume the IAM role in the application account, by using the sts:AssumeRole action. However, this option sets up the trust relationship in the reverse direction, so it's incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html\">https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html</a></p>\n", "answers": ["<p>Establish a trust relationship in the centralized DevOps account's deployment IAM role for the application account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild</p>", "<p>Establish a trust relationship in the centralized DevOps account for the application account's deployment IAM role, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>", "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions</p>", "<p>Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRoleWithSAML action. Also, grant the centralized DevOps account's deployment IAM role the required access to CodeBuild and the EKS cluster</p>"]}, "correct_response": ["c"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "In a multinational company, various AWS accounts are efficiently managed using AWS Control Tower. The company operates both internal and public applications across its infrastructure. To streamline operations, each application team is assigned a dedicated AWS account responsible for hosting their respective applications. These accounts are consolidated under an organization in AWS Organizations. Additionally, a specific AWS Control Tower member account acts as a centralized DevOps hub, offering Continuous Integration/Continuous Deployment (CI/CD) pipelines that application teams utilize to deploy applications to their designated AWS accounts. A specialized IAM role for deployment is available within this central DevOps account.\n\nCurrently, a particular application team is facing challenges while attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster situated in their application-specific AWS account. They have an existing IAM role for deployment within the application AWS account. The deployment process relies on an AWS CodeBuild project, configured within the centralized DevOps account, and utilizes an IAM service role for CodeBuild. However, the deployment process is encountering an Unauthorized error when trying to establish connections to the cross-account EKS cluster from the CodeBuild environment.\n\nTo resolve this error and facilitate a successful deployment, what solution would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 67357160, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.</p>\n\n<p>What solution would you suggest to meet these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong></p>\n\n<p>The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.</p>\n\n<p>You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify. The CloudWatch Logs Agent will send log data every five seconds by default and is configurable by the user. You can monitor log events as they are sent to CloudWatch Logs by creating Metric Filters. Metric Filters turn log data into Amazon CloudWatch Metrics for graphing or alarming.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q33-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. You need to use metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming. Therefore, this option is incorrect.</p>\n\n<p><strong>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</strong> - AWS CloudTrail enables the logging of AWS API calls, including login events, which are then sent to CloudWatch Logs. By subscribing CloudWatch Logs to Amazon Kinesis, the logs can be processed in real-time. An AWS Lambda function attached to Kinesis can parse the logs and identify user logins. If a login is detected, Amazon SNS is used to immediately notify the security team. While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.</p>\n\n<p><strong>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</strong> - Amazon Inspector is a vulnerability management service that continuously scans your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions for known software vulnerabilities and unintended network exposure. You cannot use Amazon Inspector to detect user login events in real time, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs\">https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n", "answers": ["<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>", "<p>Configure AWS CloudTrail to track AWS API calls and log them to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Set up an AWS Lambda function as a consumer for the Kinesis stream to process the logs and detect any user logins. Use Amazon SNS to send notifications to the security team upon detecting a login event</p>", "<p>Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch subscription filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>", "<p>Set up the Amazon Inspector Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A company wants to enforce regulations to prevent frequent logins by DevOps engineers to the Amazon EC2 instances, with the added condition that immediate notification must be sent to the security team if any login occurs.\n\nWhat solution would you suggest to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357162, "assessment_type": "multi-select", "prompt": {"question": "<p>During an AWS CloudFormation stack update process, an error occurred in the updated template, causing AWS CloudFormation to initiate an automatic stack rollback. After the rollback attempt, a DevOps engineer noticed that the application remained unavailable, and the stack is now in the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>To ensure the successful completion of the stack rollback, which actions should the DevOps engineer take? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Execute a ContinueUpdateRollback command from the AWS CloudFormation</strong></p>\n\n<p><strong>Manually fix the resources to match the correct state of the stack</strong></p>\n\n<p>When an update to a CloudFormation stack fails, AWS CloudFormation automatically initiates a rollback process to revert the stack to its previous known stable state. In certain cases, such as when there are dependencies on external resources, the rollback process might stall or encounter an error. To help recover from a failed stack update, you can use the ContinueUpdateRollback command. This command instructs CloudFormation to continue the stack rollback and can help resolve the UPDATE_ROLLBACK_FAILED state.</p>\n\n<p>A dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). For example, you might have a stack that's rolling back to an old database instance that was deleted outside of AWS CloudFormation. Because AWS CloudFormation doesn't know the database was deleted, it assumes that the database instance still exists and attempts to roll back to it, causing the update rollback to fail.</p>\n\n<p>Depending on the cause of the failure, you can manually fix the error and continue the rollback. By continuing the rollback, you can return your stack to a working state (the UPDATE_ROLLBACK_COMPLETE state), and then try to update the stack again.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation drift detection</strong></p>\n\n<p>While AWS CloudFormation drift detection can help identify resources that have drifted from their expected template configurations, it is not directly related to resolving the UPDATE_ROLLBACK_FAILED state. Drift detection is used to detect and compare configuration changes made outside of CloudFormation, and it helps to bring the resources back in line with the template. However, it doesn't address the issue of a failed stack rollback.</p>\n\n<p><strong>Automatically fix the issue by using AWS CloudFormation stack sets</strong></p>\n\n<p>AWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions. It cannot fix the issue with a failed stack rollback.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/images/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p><strong>Update the existing AWS CloudFormation stack by using the original template</strong> - Reapplying the original template will not address the issue that caused the stack update to fail initially. The same problem will likely persist, and the stack rollback will continue to fail. To complete the rollback, the cause of the update failure should be addressed, and the rollback should be explicitly continued using the ContinueUpdateRollback command.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errors-update-rollback-failed</a></p>\n\n<p><a href=\"#\">[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html)</a></p>\n", "answers": ["<p>Update the existing AWS CloudFormation stack by using the original template</p>", "<p>Automatically fix the issue by using AWS CloudFormation drift detection</p>", "<p>Automatically fix the issue by using AWS CloudFormation stack sets</p>", "<p>Execute a ContinueUpdateRollback command from the AWS CloudFormation</p>", "<p>Manually fix the resources to match the correct state of the stack</p>"]}, "correct_response": ["d", "e"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "During an AWS CloudFormation stack update process, an error occurred in the updated template, causing AWS CloudFormation to initiate an automatic stack rollback. After the rollback attempt, a DevOps engineer noticed that the application remained unavailable, and the stack is now in the UPDATE_ROLLBACK_FAILED state.\n\nTo ensure the successful completion of the stack rollback, which actions should the DevOps engineer take? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357164, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.</p>\n\n<p>To effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong></p>\n\n<p>This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the AmazonSSMManagedInstanceCore role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</strong> - This option includes unnecessary internet access for sensitive data via the combination of NAT Gateway and bastion host, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - EC2 Instance Connect is a way to distribute short-lived SSH keys and control access by IAM policies. The actual connection is created with SSH client and all normal requirements (network connectivity, user on host etc) apply. The host must also have ec2-instance-connect \u201cagent\u201d installed. EC2 Instance Connect still requires public IP and network connectivity, so it is not the best fit for the given use case.</p>\n\n<p><strong>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</strong> - SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for the affected accounts. So, for the given use case, you cannot use SCP for granting EC2 instance access to centralized and automated login for the Systems Manager.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/image-builder/\">https://aws.amazon.com/image-builder/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n", "answers": ["<p>Utilize AWS Systems Manager Automation to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version.  Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager and grant access to centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>", "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>", "<p>Set up a NAT gateway and a bastion host with internet access. Configure a security group that permits incoming traffic from the bastion host to all EC2 instances. Set up AWS Systems Manager Agent on the EC2 instances for monitoring and troubleshooting. Leverage Auto Scaling group lifecycle hooks for monitoring access. Utilize Systems Manager Session Manager for centralized login. Direct EC2 instance logs to an Amazon CloudWatch Logs log group. For auditing purposes, export data to Amazon S3 and notify the security team through S3 event notifications</p>", "<p>Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage EC2 Instance Connect for centralized access and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic</p>"]}, "correct_response": ["b"], "section": "Domain 6: Security and Compliance", "question_plain": "Consider a multi-account setup within AWS Organizations where a company is running a data ingestion application on Amazon EC2 instances through several Auto Scaling groups. These instances lack internet access due to sensitive data handling, and VPC endpoints have been deployed accordingly. The application operates on a custom AMI designed specifically for its needs.\n\nTo effectively manage and troubleshoot the application, system administrators require automated and centralized login access to the EC2 instances. Additionally, the company's security team needs to be notified whenever such instances are accessed.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you suggest to satisfy these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357166, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.</p>\n\n<p>What steps should the DevOps Engineer take to fulfill these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</strong></p>\n\n<p>A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.</p>\n\n<p>Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS.</p>\n\n<p>For the given use case, the blue group carries the production load while the green group is staged and deployed with the new code. When it\u2019s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q36-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p><strong>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS swap to the green environment's endpoint on the ALB</strong></p>\n\n<p>Both these options have been added as distractors. Since there is only a single ALB per the given use case, so there is no alternate endpoint available for a Route 53 DNS update.</p>\n\n<p><strong>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</strong> - The order of execution for this option is incorrect as it points the ALB to the green environment's target group before deploying the new software on the green environment's EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n", "answers": ["<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>", "<p>Leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group. Then initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances</p>", "<p>Set up an all-at-once deployment to the blue environment's EC2 instances. Perform a Route 53 DNS update to point to the green environment's endpoint on the ALB</p>", "<p>Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group</p>"]}, "correct_response": ["d"], "section": "Domain 1: SDLC Automation", "question_plain": "The flagship application at a company is deployed on Amazon EC2 instances running behind an Application Load Balancer (ALB) within an Auto Scaling group. A DevOps Engineer wants to configure a Blue/Green deployment for this application and has already created launch templates and Auto Scaling groups for both blue and green environments, each deploying to their respective target groups. The ALB can direct traffic to either environment's target group, and an Amazon Route 53 record points to the ALB. The goal is to enable an all-at-once transition of traffic from the software running on the blue environment's EC2 instances to the newly deployed software on the green environment's EC2 instances.\n\nWhat steps should the DevOps Engineer take to fulfill these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357168, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.</p>\n\n<p>During a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.</p>\n\n<p>Considering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</strong></p>\n\n<p>Amazon S3 uses checksum values to verify the integrity of data that you upload to or download from Amazon S3. In addition, you can request that another checksum value be calculated for any object that you store in Amazon S3. You can select from one of several checksum algorithms to use when uploading or copying your data. Amazon S3 uses this algorithm to compute an additional checksum value and store it as part of the object metadata.</p>\n\n<p>One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. The request succeeds only if the two digests match.</p>\n\n<p><strong>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</strong></p>\n\n<p>The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</strong> - When uploading objects to Amazon S3, you can either provide a precalculated checksum for the object or use an AWS SDK to automatically create trailing checksums on your behalf. You cannot provide a trailing checksum as it is calculated by AWS, so this option is incorrect.</p>\n\n<p><strong>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</strong> - You need to provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command, and NOT as a custom name-value pair in the metadata of the object.</p>\n\n<p><strong>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</strong> - Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. When you enable versioning in a bucket, all new objects are versioned and given a unique version ID. You cannot use version ID to compare with the Content-MD5 digest.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html</a></p>\n", "answers": ["<p>Provide the MD5 digest as a trailing checksum of the object. Examine the Amazon S3's call return status to check for an error</p>", "<p>Provide the MD5 digest as a custom name-value pair in the metadata of the object. Examine the Amazon S3's call return status to check for an error</p>", "<p>Provide the MD5 digest within the Content-MD5 parameter of the <code>PUT</code> command. Examine the Amazon S3's call return status to check for an error</p>", "<p>Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest</p>", "<p>Examine the returned response for the version ID. Compare the version ID value of the object with a calculated or previously stored Content-MD5 digest</p>"]}, "correct_response": ["c", "d"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A DevOps engineer is currently involved in a data archival project where the task is to migrate on-premises data to an Amazon S3 bucket. The engineer has created a script that handles the incremental archiving of on-premises data, specifically transferring data older than 6 months to Amazon S3. As part of the process, the data is removed from the on-premises location after being successfully transferred using the S3 PutObject operation.\n\nDuring a thorough code review, the DevOps engineer identified a crucial issue in the script. The script does not include any validation to confirm whether the data is copied to Amazon S3 without any corruption. To ensure data integrity throughout the transmission, the DevOps engineer needs to update the script accordingly. The new solution must use MD5 checksums to verify the data integrity before allowing the deletion of the on-premises data.\n\nConsidering these requirements, what modifications or solutions should the DevOps engineer implement in the script to ensure successful data transfer and integrity validation? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357170, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you suggest?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong></p>\n\n<p>AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p><strong>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</strong></p>\n\n<p>You cannot use any S3 API to invalidate the CloudFront cache, so both of these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</strong> - It is wasteful to invoke an EventBridge rule every 5 minutes to invalidate the CloudFront cache. You should only invalidate the cache when the API has actually been updated.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n", "answers": ["<p>Set up an Amazon EventBridge rule on a schedule that is invoked every 5 minutes. Configure this rule to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>", "<p>Set up an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure this rule to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>", "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path</p>", "<p>Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to leverage the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Trigger another action that calls the S3 API to invalidate the cache for the SDK path</p>"]}, "correct_response": ["c"], "section": "Domain 1: SDLC Automation", "question_plain": "A company uses an AWS CodePipeline pipeline to deploy updates to the API several times a month. As part of this process, the DevOps team exports the JavaScript SDK for the API from the API Gateway console and uploads it to an Amazon S3 bucket, which is being used as an origin for an Amazon CloudFront distribution. Web clients access the SDK through the CloudFront distribution's endpoint. The goal is to have an automated solution that ensures the latest SDK is always available to clients whenever there's a new API deployment.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you suggest?", "related_lectures": []}, {"_class": "assessment", "id": 67357172, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.</p>\n\n<p>What measures can help resolve this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</strong></p>\n\n<p><code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the \"BeforeAllowTraffic\" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</strong></p>\n\n<p><strong>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</strong></p>\n\n<p><strong>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing test traffic</strong></p>\n\n<p>None of these three lifecycle event hooks are available for an AWS Lambda deployment, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n", "answers": ["<p>Add a <code>BeforeInstall</code> hook to the AppSpec file that tests and waits for any necessary database changes before deploying a new version of the Lambda function</p>", "<p>Add a <code>ValidateService</code> hook to the AppSpec file that validates that any necessary database changes are propagated</p>", "<p>Add an <code>AfterAllowTestTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes after allowing a test traffic</p>", "<p>Add a <code>BeforeAllowTraffic</code> hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function</p>"]}, "correct_response": ["d"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A company uses AWS CodeDeploy to deploy an AWS Lambda function as the final step of a CI/CD pipeline. The company has developed the Lambda function to handle incoming orders through an order-processing API. The DevOps team has noticed intermittent failures in the API occurring for a brief period after deploying the Lambda function. Upon investigation, the team suspects that the failures are a result of incomplete propagation of database changes before the Lambda function gets invoked.\n\nWhat measures can help resolve this issue?", "related_lectures": []}, {"_class": "assessment", "id": 67357174, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.</p>\n\n<p>Which of the following options represents the BEST solution for the given scenario?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong></p>\n\n<p>You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules.</p>\n\n<p>The restricted-ssh rule checks if the incoming SSH traffic for the security groups is accessible. The rule is COMPLIANT when IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0). This rule applies only to IPv4.</p>\n\n<p>For the given use case, you need to monitor for the NON_COMPLIANT evaluation result of the rule, which implies that the rule has failed the conditions of the compliance check. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</strong></p>\n\n<p>You should note that you can only set up a filter policy on an SNS subscription and NOT on the SNS topic itself. In addition, it is wasteful to set up Amazon EventBridge rule on all AWS Config managed rules, rather than only the restricted-ssh rule. Therefore, both these options are incorrect.</p>\n\n<p><strong>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</strong> - You get an ERROR evaluation result when one of the required/optional parameters is not valid, or not of the correct type, or is formatted incorrectly. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\">https://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html</a></p>\n", "answers": ["<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of ERROR for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>", "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic</p>", "<p>Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for all AWS Config managed rules. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>", "<p>Set up an Amazon EventBridge rule that matches all AWS Config evaluation results for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic. Set up a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers</p>"]}, "correct_response": ["b"], "section": "Domain 6: Security and Compliance", "question_plain": "A company wants to create an automated monitoring solution to generate real-time customized notifications regarding unrestricted security groups in the company's production AWS account. The notification must contain the name and ID of the noncompliant security group. The DevOps team at the company has already activated the restricted-ssh AWS Config managed rule. The team has also set up an Amazon Simple Notification Service (Amazon SNS) topic and subscribed relevant personnel to it.\n\nWhich of the following options represents the BEST solution for the given scenario?", "related_lectures": []}, {"_class": "assessment", "id": 67357176, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.</p>\n\n<p>As an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</strong></p>\n\n<p>CloudTrail is enabled by default for your AWS account. You can use Event history in the CloudTrail console to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure. This includes activity made through the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs. For an ongoing record of events in your AWS account, you can create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all AWS Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs.</p>\n\n<p>You can also configure CloudTrail with CloudWatch Logs to monitor your trail API logs and be notified when specific activity occurs. When you configure your trail to send events to CloudWatch Logs, CloudTrail sends only the events that match your trail settings. For example, if you configure your trail to log data events only, your trail sends data events only to your CloudWatch Logs log group. CloudTrail supports sending data, Insights, and management events to CloudWatch Logs.</p>\n\n<p>You can collect metrics and logs from Amazon EC2 instances and on-premises servers with the CloudWatch agent. You can store and view the metrics that you collect with the CloudWatch agent in CloudWatch just as you can with any other CloudWatch metrics. The default namespace for metrics collected by the CloudWatch agent is CWAgent, although you can specify a different namespace when you configure the agent. The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs</p>\n\n<p>For the given use case, you can have both AWS CloudTrail as well as the Amazon CloudWatch Agent deliver the respective logs to CloudWatch Logs. You can then use the CloudWatch Logs Insights to query both sets of logs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. So, this option is incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</strong> - You cannot deliver the API logs from AWS CloudTrail to Kinesis Data Streams. Similarly, you cannot have the Amazon CloudWatch Agent deliver logs from the EC2 instances to Kinesis Data Streams. So, both these options are incorrect.</p>\n\n<p><strong>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</strong> - You cannot use Amazon Athena to query logs published to Amazon CloudWatch Logs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-getting-started.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html</a></p>\n", "answers": ["<p>Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs</p>", "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon S3. Utilize Amazon Athena to query both sets of logs</p>", "<p>Set up AWS CloudTrail to deliver the API logs to Kinesis Data Streams. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Kinesis Data Streams. Direct both the Kinesis Data Streams to direct the stream output to Kinesis Data Analytics for running near-real-time queries on both sets of logs</p>", "<p>Set up AWS CloudTrail to deliver the API logs to Amazon S3. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize Amazon Athena to query both sets of logs</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "An e-commerce company is deploying its flagship application on Amazon EC2 instances. The DevOps team at the company needs a solution to query both the application logs as well as the AWS account API activity.\n\nAs an AWS Certified DevOps Engineer - Professional, what solution will you recommend to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357178, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.</p>\n\n<p>Which of the following solutions would you suggest for the given use case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</strong></p>\n\n<p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities.</p>\n\n<p>To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot.</p>\n\n<p>You can use CodeDeploy to create a deployment process that publishes the new Lambda version but does not send any traffic to it. Then it executes a PreTraffic test to ensure that your new function works as expected. After the test succeeds, CodeDeploy automatically shifts traffic gradually to the new version of the Lambda function. This workflow addresses one of the key requirements of reducing the time to detect errors. You can roll back to the previous version in case the new version errors out.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</strong> - You can use CloudFormation change sets to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p>\n\n<p>This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q42-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</strong> - This option does not help in reducing the time to detect any potential deployment errors as you would not know about any potential failures until you actually deploy the stack and point to the new endpoint.</p>\n\n<p>Instead, you should use SAM to create your serverless application as it comes built-in with CodeDeploy to provide gradual Lambda deployments. Also, you can define pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. You can roll back the deployment if CloudWatch alarms are triggered.</p>\n\n<p><strong>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</strong> - This option has been added as a distractor since CloudFormation change sets do not have pre-traffic and post-traffic test functions. Therefore this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/\">https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p>\n", "answers": ["<p>Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered</p>", "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. In case the Lambda function errors out, rollback the CloudFormation change set to the previous version</p>", "<p>Set up and deploy a CloudFormation stack containing a new API Gateway endpoint that points to the new Lambda version. Test the updated CloudFront origin that points to this new API Gateway endpoint and in case errors are detected then revert the CloudFront origin to the previous working API Gateway endpoint</p>", "<p>Set up and deploy nested CloudFormation stacks with the CloudFront distribution as well as the API Gateway in the parent stack. Create and deploy a child stack containing the Lambda functions. To address any changes in a Lambda function, create a CloudFormation change set and deploy. Use pre-traffic and post-traffic test functions of the change set to verify the deployment. Rollback in case CloudWatch alarms are triggered</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "An e-commerce company has a serverless application stack that consists of CloudFront, API Gateway and Lambda functions. The company has hired you to improve the current deployment process which creates a new version of the Lambda function and then runs an AWS CLI script for deployment. In case the new version errors out, then another CLI script is invoked to deploy the previous working version of the Lambda function. The company has mandated you to decrease the time to deploy new versions of the Lambda functions and also reduce the time to detect and roll back when errors are identified.\n\nWhich of the following solutions would you suggest for the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 67357180, "assessment_type": "multi-select", "prompt": {"question": "<p>A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The DevOps team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources.</p>\n\n<p>Which of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</strong></p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \u201cWhat did my AWS resource look like at xyz point in time?\u201d.</p>\n\n<p>How AWS Config Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>For the given use case, you can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. You can also create your own custom rules. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes violate any of the conditions in your rules. If a resource violates a rule, AWS Config flags the resource and marks the rule as non-compliant.</p>\n\n<p>There are two types of evaluation trigger types for Config rules:</p>\n\n<p>Configuration changes \u2013 AWS Config triggers the evaluation when any resource that matches the rule's scope changes in configuration. The evaluation runs after AWS Config sends a configuration item change notification.</p>\n\n<p>Periodic \u2013 AWS Config runs evaluations for the rule at a frequency that you choose (for example, every 24 hours).</p>\n\n<p><strong>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</strong></p>\n\n<p>CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.</p>\n\n<p>CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren't viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events.</p>\n\n<p>CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q43-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage the CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on using the CloudWatch Logs agent to collect all the AWS SDK logs. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. So this option is not the best-fit solution.</p>\n\n<p><strong>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</strong> - One of the key constraints for the given scenario is that the AWS Management Console is the preferred method for the in-house teams wanting to provision resources. Although this option is technically feasible, it focuses on capturing unauthorized API activities. The given use case has no specific requirements for AWS SDKs or AWS APIs because AWS Management Console is the preferred method to provision resources. In addition, the use case just talks about assessing, auditing, and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p><strong>Leverage EventBridge events near-real-time capabilities to monitor system event patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</strong> - The use-case just talks about assessing, auditing and monitoring the configurations of AWS resources. Reverting non-authorized changes in AWS resources is not part of the mandate. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_manage-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-management-events</a></p>\n", "answers": ["<p>Leverage AWS Config rules to audit changes to AWS resources and monitor the compliance of the configuration by running the evaluations for the rule at a frequency that you choose. Develop AWS Config custom rules to establish a test-driven development approach by triggering the evaluation when any resource that matches the rule's scope changes in configuration</p>", "<p>Enable trails and set up CloudTrail events to review and monitor management activities of all AWS accounts by logging these activities into CloudWatch Logs using a KMS key. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services</p>", "<p>Leverage CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that match mutating API calls. Use CloudWatch alarms to send notifications via SNS when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and analyze via Athena</p>", "<p>Leverage CloudTrail integration with SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled for all accounts as well as all available AWS services. Use Lambda functions to automatically revert non-authorized changes in AWS resources</p>", "<p>Leverage EventBridge events near-real-time capabilities to monitor system events patterns to trigger Lambda functions to automatically revert non-authorized changes in AWS resources. Send notifications via SNS topics to improve the incidence response time</p>"]}, "correct_response": ["a", "b"], "section": "Domain 6: Security and Compliance", "question_plain": "A data analytics company wants to move all its clients belonging to the regulated and security-sensitive industries such as financial services and healthcare to the AWS Cloud as it wants to leverage the out-of-box security-specific capabilities offered by AWS. The DevOps team at the company is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for the in-house teams wanting to provision resources.\n\nWhich of the following strategies would you adopt to address these business requirements for continuously assessing, auditing and monitoring the configurations of AWS resources? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357182, "assessment_type": "multi-select", "prompt": {"question": "<p>The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.</p>\n\n<p>Which of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</strong></p>\n\n<p><strong>Apply patch baselines using the AWS-RunPatchBaseline SSM document</strong></p>\n\n<p>Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. Systems Manager helps you maintain security and compliance by scanning your managed instances and reporting on (or taking corrective action on) any policy violations it detects. AWS Systems Manager Agent (SSM Agent) is Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for the Systems Manager to update, manage, and configure these resources.</p>\n\n<p><img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications). Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches individually or to large groups of instances by using Amazon EC2 tags.</p>\n\n<p>For the given use case, you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p>\n\n<p>Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the \"default\" for an operating system type.</p>\n\n<p>The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt2-q44-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</strong> - AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictable, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \u201cWhat did my AWS resource look like at xyz point in time?\u201d.</p>\n\n<p>There is no such thing as CloudFormation automatic patching support, as you need to use Systems Manager for patch management. So, this option acts as a distractor.</p>\n\n<p><strong>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</strong> - You could use an OS-native patching service to manage the update frequency and release approval for all instances but it would take considerable effort to set up and configure this solution. This violates the minimal effort requirement of the given use case.</p>\n\n<p><strong>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</strong> - As mentioned in the explanation above, the AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html</a></p>\n", "answers": ["<p>Configure CloudFormation automatic patching support for all applications which will keep the OS up-to-date following the initial installation. Set up AWS Config to provide audit and compliance reporting</p>", "<p>Set up an OS-native patching service to manage the update frequency and release approval for all instances. Set up AWS Config to provide audit and compliance reporting</p>", "<p>Apply patch baselines using the AWS-ApplyPatchBaseline SSM document</p>", "<p>Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval</p>", "<p>Apply patch baselines using the AWS-RunPatchBaseline SSM document</p>"]}, "correct_response": ["d", "e"], "section": "Domain 6: Security and Compliance", "question_plain": "The DevOps team for an e-commerce company wants to implement a patching plan on AWS Cloud for a large mixed fleet of Windows and Linux servers. The patching plan has to be auditable and must be implemented securely to ensure compliance with the company's business requirements.\n\nWhich of the following options would you recommend to address these requirements with MINIMAL effort? (Select two)", "related_lectures": []}]}
4716374
~~~
{"count": 75, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 67357308, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A healthcare technology company provides a Software as a Service (SaaS) solution to hospitals throughout the United States to use the company\u2019s proprietary system to integrate their clinical documentation and coding workflows. The DevOps team at the company would like to enable a CICD pipeline that enables safe deployments to production and the ability to work on new features of the product roadmap.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend for the given use-case?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. To protect the master branch you need to set a Deny policy on the IAM group that the developer group should be assigned to.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q1-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</strong> - As mentioned in the explanation above, you should not create a separate repository for each new feature. So this option is incorrect.</p>\n\n<p><strong>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</strong> - This option has been added as a distractor as there is no such thing as a repository access policy.</p>\n\n<p><strong>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</strong> - Although you can create a separate CICD pipeline for each branch, you cannot merge multiple pipelines into one to make it a \"master\" pipeline or merge multiple branches into a master branch as the last step of a CICD pipeline. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>", "<p>Create the main CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new CodeCommit repository and create pull requests to merge into the main repository. Set an IAM policy on your developer group to prevent direct pushes to the main repository</p>", "<p>Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set a repository access policy on your repository to prevent direct pushes to master</p>", "<p>Create a CodeCommit repository and create a branch for each feature. Create a CICD pipeline for each branch, and the last step of the CICD pipeline should be to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A healthcare technology company provides a Software as a Service (SaaS) solution to hospitals throughout the United States to use the company\u2019s proprietary system to integrate their clinical documentation and coding workflows. The DevOps team at the company would like to enable a CICD pipeline that enables safe deployments to production and the ability to work on new features of the product roadmap.\n\nAs an AWS Certified DevOps Engineer, which solution would you recommend for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357312, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.</p>\n\n<p>As a DevOps Engineer, which solution would you implement for the given requirement?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Projects/CICD%20Pipeline/setup-cicd-pipeline2.5cefde1406fa6787d9d3c38ae6ba3a53e8df3be8.png\">\nvia - <a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p>Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.</p>\n\n<p>Code Pipeline Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</strong> - As mentioned in the explanation above, a key requirement is that two versions of the code need to be deployed to different environments. If you use a manual approval step after the deployment to staging then the same version of the code from the master branch would also be deployed to the production environment. Instead, you need to maintain a production branch of the code that can be deployed to the production environment.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to merging through a pull request has been added as a distractor.</p>\n\n<p><strong>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</strong> - It's a best practice to work with branches in your git repository to create features, as it's the intended usage of branches. Don't create separate repositories for features. You should not maintain separate repositories to manage two versions of the code that need to be deployed to different environments. The reference to the manual approval step has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codecommit/faqs/\">https://aws.amazon.com/codecommit/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/\">https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request</p>", "<p>Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a manual approval step after the deployment to staging to ensure the application is reviewed before being deployed to production in the last pipeline stage</p>", "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and  CodePipeline pipeline that will deploy changes from the production branch to the production environment after the code is merged through a pull request</p>", "<p>Create a CodeCommit repository for the development code and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodeCommit repository and CodePipeline pipeline that will deploy changes from the production branch to the production environment after a manual approval step has happened in the first CodePipeline</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "Your company has adopted a git repository technology to store and have version control on the application code. Your company would like to make sure the production branch of the code is deployed to the production environment, but also would like to enable other versions of the code to be deployed to the development and staging environments for performing various kinds of user acceptance testing.\n\nAs a DevOps Engineer, which solution would you implement for the given requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357270, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.</p>\n\n<p>As an AWS Certified DevOps Engineer, how would you implement this most efficiently?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, you can set up a CloudWatch Event rule for every push to the CodeCommit repository that would trigger the Lambda function configured as a target. The Lambda function would in turn request the code from CodeCommit, zip it and send it to the 3rd party API.</p>\n\n<p>CloudWatch Events Configuration:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</strong> - CloudWatch Event Rules cannot have S3 buckets as a target. Although you can set an S3 trigger as a target, eventually you would still need to invoke the Lambda function via an S3 trigger to process the code via the API. Therefore it's efficient to directly invoke the Lambda function from the CloudWatch Event rule.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q3-i3.jpg\"></p>\n\n<p><strong>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</strong> - CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So this option is ruled out.</p>\n\n<p><strong>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</strong> - The EC2 CodeCommit hook is a distractor and does not exist.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API</p>", "<p>Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an S3 bucket so that the code will be automatically zipped into S3. Create an S3 Event rule to trigger a Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API</p>", "<p>Create a CloudWatch Event rule on a schedule of 5 minutes that triggers a Lambda function that will check for new commits done on your CodeCommit repository. If new commits are detected, download and zip the code and then send it to the 3rd party API</p>", "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "The DevOps team at an e-commerce company is working with the in-house security team to improve the security workflow of the code release process. The DevOps team would like to initiate a 3rd party code vulnerability analysis tool for every push done to code in your CodeCommit repository. The code has to be sent via an external API.\n\nAs an AWS Certified DevOps Engineer, how would you implement this most efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 67357282, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As part of the CICD pipeline, the DevOps team at a retail company wants to deploy the latest application code to a staging environment and the team also wants to ensure it can execute an automated functional test suite before deploying to production. The code is managed via CodeCommit. Usually, the functional test suite runs for over two hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you create the CICD pipeline to run your test suite in the most efficient way?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</strong></p>\n\n<p>CodeCommit is a secure, highly scalable, managed source control service that makes it easier for teams to collaborate on code. A CICD pipeline helps you automate steps in your software delivery process, such as initiating automatic builds and then deploying to Amazon EC2 instances. You may use AWS CodePipeline, a service that builds, tests, and deploys your code every time there is a code change, based on the release process models you define to orchestrate each step in your release process.</p>\n\n<p>Sample AWS CodePipeline pipeline architecture:\n<img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2020/03/24/AWSCodePipelineAndPostman_drawio.png\"></p>\n\n<p>Highly recommend reading this excellent reference AWS DevOps blog on using CodePipeline with CodeBuild to automate testing - <a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. CodeBuild automatically scales up and down and processes multiple builds concurrently, so your builds don\u2019t have to wait in a queue. CodeBuild has recently announced the launch of a new feature in CodeBuild called Reports. This feature allows you to view the reports generated by functional or integration tests. The reports can be in the JUnit XML or Cucumber JSON format. You can view metrics such as Pass Rate %, Test Run Duration, and the number of Passed versus Failed/Error test cases in one location.</p>\n\n<p>AWS CodeBuild Test Reports:\n<img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/11/21/Picture1-1.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/\">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p>\n\n<p>For the given use-case, you need to use a CodeBuild build to run the test suite, but you must first deploy to staging before running CodeBuild! It's common in the exam for multiple same steps to be shown in a different order, so be careful.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production</strong> - As mentioned in the explanation above, you cannot have the CodeBuild Test as a stage prior to deploying in the staging environment, so this option is incorrect.</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</strong> - Lambda should be ruled out for running the test suite as the maximum timeout for a Lambda function is 15 minutes, so it will not support the given use-case since the functional test suite runs for over two hours.</p>\n\n<p><strong>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will invoke a Step Function execution. The Step Function will run the test suite. Create a CloudWatch Event Rule on the execution termination of your Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn't fail, the last stage will deploy the application to production</strong> - AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. While the solution involving Step Functions would work, it's extremely convoluted and not the most efficient solution.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codebuild/faqs/\">https://aws.amazon.com/codebuild/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/\">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</p>", "<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production</p>", "<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production</p>", "<p>Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will invoke a Step Function execution. The Step Function will run the test suite. Create a CloudWatch Event Rule on the execution termination of your Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn't fail, the last stage will deploy the application to production</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "As part of the CICD pipeline, the DevOps team at a retail company wants to deploy the latest application code to a staging environment and the team also wants to ensure it can execute an automated functional test suite before deploying to production. The code is managed via CodeCommit. Usually, the functional test suite runs for over two hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you create the CICD pipeline to run your test suite in the most efficient way?", "related_lectures": []}, {"_class": "assessment", "id": 67357246, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>An online coding platform wants to fully customize the build tasks and automatically run builds concurrently to take the pain out of managing the build environments. The DevOps team at the company wants to use CodeBuild for all build-tasks and would like the artifacts created by CodeBuild to be named based on the branch being tested. The team wants this solution to be scalable to newer branches with a minimal amount of rework.</p>\n\n<p>As a DevOps Engineer, how would you go about implementing the simplest possible solution to address the given use-case?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong></p>\n\n<p>AWS CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers.</p>\n\n<p>A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p>For the given use-case, we need to use environment variables. The variable <code>CODEBUILD_SOURCE_VERSION</code> is exposed at runtime directly within CodeBuild and represents the branch name of the code being tested for CodeCommit. This is the best solution.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q5-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</strong> - Providing the branch name as <code>BRANCH_NAME</code> and creating separate CodeBuild would be highly tedious to maintain and error-prone. This is certainly not the simplest solution possible.</p>\n\n<p><strong>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</strong> - Maintaining a different <code>buildspec.yml</code> for each branch is not efficient and it's error-prone. So this option is incorrect.</p>\n\n<p><strong>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</strong> - The answer involving a Lambda function would work but is highly convoluted. This is something that can be directly accomplished using the <code>CODEBUILD_SOURCE_VERSION</code> environment variable.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>CODEBUILD_SOURCE_VERSION</code> at runtime. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>", "<p>Create a <code>buildspec.yml</code> file that will look for the environment variable <code>BRANCH_NAME</code> at runtime. For each existing branch and new branch, create a separate CodeBuild and set the <code>BRANCH_NAME</code> variable accordingly. Use the variable in the <code>artifacts</code> section of your <code>buildspec.yml</code> file</p>", "<p>Create a <code>buildspec.yml</code> file that will be different for every single branch. Create a new CodeBuild for each branch. Upon adding a new branch, ensure to edit the <code>buildspec.yml</code> file</p>", "<p>Create a unique <code>buildspec.yml</code> file that will be the same for each branch and will name the artifacts the same way. When the artifact is uploaded into S3, create an S3 Event that will trigger a Lambda function that will issue an API call against CodeBuild, extract the branch name from it and rename the file on S3</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "An online coding platform wants to fully customize the build tasks and automatically run builds concurrently to take the pain out of managing the build environments. The DevOps team at the company wants to use CodeBuild for all build-tasks and would like the artifacts created by CodeBuild to be named based on the branch being tested. The team wants this solution to be scalable to newer branches with a minimal amount of rework.\n\nAs a DevOps Engineer, how would you go about implementing the simplest possible solution to address the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357240, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A mobility company connects people with taxi drivers and the DevOps team at the company uses CodeCommit as a backup and disaster recovery service for several of its DevOps processes. The team is creating a CICD pipeline so that your code in the CodeCommit master branch automatically gets packaged as a Docker container and published to ECR. The team would then like that image to be automatically deployed to an ECS cluster using a Blue/Green strategy.</p>\n\n<p>As an AWS Certified DevOps Engineer, which of the following options would you recommend as the most efficient solution to meet the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>CodeBuild is a fully managed continuous integration service in the cloud. CodeBuild compiles source code, runs tests, and produces packages that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. You can include a buildspec as part of the source code or you can define a buildspec when you create a build project.</p>\n\n<p>You can use CodeBuild to acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. You should note that acquiring ECR credentials must be done using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your user access and secret key.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</strong> - You should note that both CodeDeploy and CloudFormation can support blue/green deployment for ECS. However, you cannot create a new task definition using CodeCommit, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</strong> - CloudWatch Event Rule does not support CodeDeploy as a target, therefore CodeDeploy must be invoked from your CodePipeline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q6-i3.jpg\"></p>\n\n<p><strong>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</strong> - As mentioned in the explanation above, ECR credentials must be acquired using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your AWS access key ID and secret access key.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-bluegreen.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/\">https://aws.amazon.com/about-aws/whats-new/2020/05/aws-cloudformation-now-supports-blue-green-deployments-for-amazon-ecs/</a></p>\n", "answers": ["<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service</p>", "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, create a new task definition automatically using CodeCommit and apply that task definition to the ECS service using a CloudFormation action</p>", "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Create a CloudWatch Event Rule that will react to pushes to ECR and invoke CodeDeploy, the target of which should be the ECS cluster</p>", "<p>Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables passed in through CodeBuild configuration, the values being those from your user. Upon the success of that CodeBuild stage, create a new task definition automatically using CodePipeline and apply that task definition to the ECS service using a CloudFormation action</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A mobility company connects people with taxi drivers and the DevOps team at the company uses CodeCommit as a backup and disaster recovery service for several of its DevOps processes. The team is creating a CICD pipeline so that your code in the CodeCommit master branch automatically gets packaged as a Docker container and published to ECR. The team would then like that image to be automatically deployed to an ECS cluster using a Blue/Green strategy.\n\nAs an AWS Certified DevOps Engineer, which of the following options would you recommend as the most efficient solution to meet the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357222, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.</p>\n\n<p>How can you implement the validation of Pull Requests by CodeBuild efficiently?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong></p>\n\n<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p>\n\n<p>CloudWatch Events Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n\n<p>For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i2.jpg\"></p>\n\n<p>The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q7-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</strong> - Using a scheduled rate of 5 minutes would work but would be inefficient. It is much better to configure a CloudWatch Event Rule that would trigger on CodeCommit Pull Request and carry out the rest of the solution workflow as outlined earlier.</p>\n\n<p><strong>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p><strong>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</strong></p>\n\n<p>For both these options, invoking a Lambda function to start CodeBuild would work, but having the function wait on CodeBuild has two issues:\n1) The Lambda function may timeout and has a maximum timeout of 15 minutes. What if the test suite takes longer to run?\n2) You will be billed for the Lambda function wait time.</p>\n\n<p>Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>", "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. Create a CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome</p>", "<p>Create a CloudWatch Event Rule with a scheduled rate of 5 minutes that invokes a Lambda function. This function checks for the creation and updates done to Pull Requests in the source repository, and invokes CodeBuild when needed. The function waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>", "<p>Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be AWS Lambda. This function invokes CodeBuild and waits for CodeBuild to be done and then updates the Pull Request with a message with the build outcome</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "Your company has adopted CodeCommit and forces developers to create new branches and create pull requests before merging the code to master. The development team lead reviewing the pull request needs high confidence in the quality of the code and therefore would like the CICD system to automatically build a Pull Request to provide a testing badge with a pass/fail status.\n\nHow can you implement the validation of Pull Requests by CodeBuild efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 67357320, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A data intelligence and analytics company enables publishers to measure, analyze, and improve the impact of the advertising across their range of online deliverables. The DevOps team at the company wants to use CodePipeline to deploy code from CodeCommit with CodeDeploy. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you configure the EC2 instances to facilitate the deployment?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong></p>\n\n<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q8-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/codedeploy/faqs/\">https://aws.amazon.com/codedeploy/faqs/</a></p>\n\n<p>The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. A configuration file is placed on the instance when the agent is installed. This file is used to specify how the agent works. This configuration file specifies directory paths and other settings for AWS CodeDeploy to use as it interacts with the instance.</p>\n\n<p>For the given use-case, you can have the CodePipeline chain CodeCommit and CodeDeploy and have the source code available as a zip file in an S3 bucket to be used as a CodePipeline artifact. The EC2 instance must have an IAM role, and not an IAM user, to pull that file from S3. Finally, the EC2 instance must be properly tagged to be part of the correct deployment group and have the CodeDeploy agent installed on it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It's a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p><strong>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</strong> - CodeDeploy cannot automatically install the agent on the EC2 instance. You must ensure that the EC2 instance has the CodeDeploy agent installed. You must also tag the instance to have it part of a deployment group.</p>\n\n<p><strong>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</strong> - It's a best practice to avoid using the IAM user access credentials to give the EC2 instance access to the S3 bucket where CodeDeploy is deploying from. You must leverage an IAM role to facilitate this access for the EC2 instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/primary-components.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-configure.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>", "<p>Create an EC2 instance with an IAM user access credentials giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>", "<p>Create an EC2 instance with an IAM role giving access to the CodeCommit repository where CodeDeploy is deploying from. CodeDeploy will install the agent on the EC2 instance</p>", "<p>Create an EC2 instance with an IAM user access credentials giving access to the CodeCommit repository where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A data intelligence and analytics company enables publishers to measure, analyze, and improve the impact of the advertising across their range of online deliverables. The DevOps team at the company wants to use CodePipeline to deploy code from CodeCommit with CodeDeploy. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you configure the EC2 instances to facilitate the deployment?", "related_lectures": []}, {"_class": "assessment", "id": 67357220, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>CodeDeploy components overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p>For the given use-case, you should create four deployment groups for development, staging, canary testing, production and then chain these together using CodePipeline. For EC2, to do a canary deployment, you must create a small deployment group made of few instances from production and deploy to these. Add a manual step after the deployment to the canary testing stage.</p>\n\n<p>Sample workflow for CI/CD with AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q9-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n\n<p>Exam Alert:</p>\n\n<p>The exam may try to trap you on some of the following details on deployment-related processes. Be aware of what's possible.</p>\n\n<ul>\n<li><p>A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.</p></li>\n<li><p>Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type. During an in-place deployment, CodeDeploy performs a rolling update across Amazon EC2 instances. During a blue/green deployment, the latest application revision is installed on replacement instances.</p></li>\n<li><p>If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.</p></li>\n<li><p>You CANNOT use canary, linear, or all-at-once configuration for EC2/On-Premises compute platform.</p></li>\n<li><p>You can manage the way in which traffic is shifted to the updated Lambda function versions during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>You can deploy an Amazon ECS containerized application as a task set. You can manage the way in which traffic is shifted to the updated task set during deployment by choosing a canary, linear, or all-at-once configuration.</p></li>\n<li><p>Amazon ECS blue/green deployments are supported using both CodeDeploy and AWS CloudFormation. For blue/green deployments through AWS CloudFormation, you don't create a CodeDeploy application or deployment group.</p></li>\n<li><p>Your deployable content and the AppSpec file are combined into an archive file (also known as application revision) and then upload it to an Amazon S3 bucket or a GitHub repository. Remember these two locations. AWS Lambda revisions can be stored in Amazon S3 buckets. EC2/On-Premises revisions are stored in Amazon S3 buckets or GitHub repositories.</p></li>\n<li><p>AWS Lambda and Amazon ECS deployments CANNOT use an in-place deployment type.</p></li>\n</ul>\n\n<p>Incorrect options:</p>\n\n<p><strong>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</strong> - Creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms and there's no option to pause it manually through an approval step.</p>\n\n<p><strong>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</strong> - While CodeDeploy does have a Canary Deployment setting, it's only meant for AWS Lambda and ECS platforms. In addition, creating separate CodePipelines is not a good idea and won't allow you to create manual approval steps before deploying to production.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html\">https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/welcome.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/\">https://aws.amazon.com/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/</a></p>\n", "answers": ["<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances</p>", "<p>In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create separate CodePipeline for each deployment group all having the same source being your code repository. Introducing a manual approval step in the pipeline that deploys to production</p>", "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create one CodePipeline and chain up these together. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>", "<p>In CodeDeploy, create three deployment groups - one for development, one for staging, and one for the entire production instances. Create three separate CodePipeline for each deployment group having all the same sources being your code repository. For the deployment to production, enable the Canary deployment setting for CodeDeploy, and introduce a manual step after the canary deployment that will pause the rest of the deployment. Upon approval, the rest of the instances in production will have a deployment made to them</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A retail company is implementing a CodePipeline pipeline in which every push to the CodeCommit master branch gets deployed to development, staging, and production environment consisting of EC2 instances. When deploying to production, traffic should be deployed on a few instances so that metrics can be gathered before a manual approval step is done to deploy to all the instances.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?", "related_lectures": []}, {"_class": "assessment", "id": 67357300, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The DevOps team at a business travel solutions company wants to use CodeDeploy to ensure zero downtime during deployments through rolling updates. The team wants to deploy the company's flagship web application on a set of 5 EC2 instances running behind an Application Load Balancer. The team would like the deployment to be gradual and to automatically rollback in case of a failed deployment, which is determined by the application not being able to pass health checks.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend for the given use-case?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong></p>\n\n<p>CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.</p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>Sample appspec file:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p>List of Lifecycle Event hooks for EC2 deployment:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>Lifecycle Event hooks availability for EC2 deployment and rollback scenarios:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q10-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n\n<p>For the given use-case, you can use <code>ValidateService</code> hook to verify that the deployment was completed successfully. This is the last deployment lifecycle event. You can configure CodeDeploy to rollback if this hook fails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</strong> - Integrating CodeDeploy with the Application Load Balancer will ensure traffic isn't forwarded to the instances that CodeDeploy is currently deploying to, but the health check feature is not integrated with CodeDeploy and therefore you cannot rollback when the Application Load Balancers fails the health check.</p>\n\n<p><strong>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - The <code>AfterInstall</code> hook in <code>appspec.yml</code> is before <code>StartApplication</code> and therefore won't be able to test the application's health checks. You can use the <code>AfterInstall</code> hook for tasks such as configuring your application or changing file permissions.</p>\n\n<p><strong>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</strong> - The CloudWatch Event rule won't work as it is not granular at each instance's level, and CodeDeploy has a native feature for doing rollbacks, instead of doing API calls via <code>StopDeployment</code> and <code>CreateDeployment</code>.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>", "<p>Integrate CodeDeploy with the Application Load Balancer. In case the Application Load Balancers fails the health checks on the instances where the new version has been deployed, it will notify CodeDeploy. Configure CodeDeploy to rollback on deployment failures</p>", "<p>In the <code>AfterInstall</code> hook in <code>appspec.yml</code>, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>", "<p>Create a CloudWatch Event rule on CodeDeploy to invoke a Lambda function upon deployment on every instance. The Lambda function tests the health check, and if it fails, stops the CodeDeploy deployment using the <code>StopDeployment</code> API, and then start a new deployment of the old version using the <code>CreateDeployment</code> API</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "The DevOps team at a business travel solutions company wants to use CodeDeploy to ensure zero downtime during deployments through rolling updates. The team wants to deploy the company's flagship web application on a set of 5 EC2 instances running behind an Application Load Balancer. The team would like the deployment to be gradual and to automatically rollback in case of a failed deployment, which is determined by the application not being able to pass health checks.\n\nAs a DevOps Engineer, which of the following options would you recommend for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357260, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A Silicon Valley based startup runs a news discovery web application and it uses CodeDeploy to deploy the web application on a set of 20 EC2 instances behind an Application Load Balancer. The ALB is integrated with CodeDeploy. The DevOps teams at the startup would like the deployment to be gradual and to automatically rollback in case of unusually high maximum CPU utilization for the EC2 instances while traffic is being served.</p>\n\n<p>How can you implement this?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated. CodeDeploy will redeploy the last known working version of the application when it rolls back. Previously, you needed to manually initiate a deployment if you wanted to roll back a deployment. For the given use-case, you should use the underlying metric as the maximum CPU for your EC2 instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q11-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</strong> - If you are using the <code>ValidateService</code> hook because your CodeDeploy is integrated with the ALB, traffic will not be served and you won't observe high CPU utilization.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics. So this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</strong> - This option has been added as a distractor as you would want to watch out for the maximum CPU utilization of the EC2 instances and not the Application Load Balancer. In addition, CodeDeploy rollbacks only work with CloudWatch alarms, not CloudWatch metrics.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm</p>", "<p>In the <code>ValidateService</code> hook in <code>appspec.yml</code>, measure the CPU utilization for 5 minutes. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback</p>", "<p>Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>", "<p>Create a CloudWatch metric for the maximum CPU utilization of your Application Load Balancer. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch metric</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A Silicon Valley based startup runs a news discovery web application and it uses CodeDeploy to deploy the web application on a set of 20 EC2 instances behind an Application Load Balancer. The ALB is integrated with CodeDeploy. The DevOps teams at the startup would like the deployment to be gradual and to automatically rollback in case of unusually high maximum CPU utilization for the EC2 instances while traffic is being served.\n\nHow can you implement this?", "related_lectures": []}, {"_class": "assessment", "id": 67357288, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The DevOps team at your company is using CodeDeploy to deploy new versions of a Lambda function after it has passed a CodeBuild check via your CodePipeline. Before deploying, the CodePipeline has a step in which it optionally kickstarts a restructuring of files on an S3 bucket that is forward compatible. That restructuring is done using a Step Function execution which invokes a Fargate task. The new Lambda function cannot work until the restructuring task has fully completed.</p>\n\n<p>As a DevOps Engineer, how can you ensure traffic isn't served to your new Lambda function until the task is completed?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong></p>\n\n<p>The AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file. During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file.</p>\n\n<p>For AWS Lambda compute platform applications, the AppSpec file is used by CodeDeploy to determine:</p>\n\n<p>Which Lambda function version to deploy.</p>\n\n<p>Which Lambda functions to use as validation tests.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q12-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n\n<p>The <code>BeforeAllowTraffic</code> hook is used to run tasks before traffic is shifted to the deployed Lambda function version. So for the given use-case, you can use this hook to check that the restructuring task has fully completed and then shift traffic to the newly deployed Lambda function version.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</strong> - If you use an <code>AfterAllowTraffic</code> hook the new Lambda function will already serve traffic, so this option is incorrect.</p>\n\n<p><strong>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</strong> - Canary Deployments will send some traffic to the new Lambda function while the restructuring in S3 is still happening so that won't work.</p>\n\n<p><strong>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</strong> - There's no API to tell CodeDeploy to switch traffic to the new version of the Lambda function, therefore adding a step in your Step Function won't help.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-lambda</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p>\n", "relatedLectureIds": "", "answers": ["<p>In your <code>appspec.yml</code> file, include a <code>BeforeAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>", "<p>In your <code>appspec.yml</code> file, include an <code>AfterAllowTraffic</code> hook that checks on the completion of the Step Function execution</p>", "<p>Enable Canary Deployment in CodeDeploy so that only a fraction of the service is served by the new Lambda function while the restructuring is happening</p>", "<p>Include an extra step in the Step Function to signal to CodeDeploy the completion of the restructuring and serve new traffic to the new Lambda function</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "The DevOps team at your company is using CodeDeploy to deploy new versions of a Lambda function after it has passed a CodeBuild check via your CodePipeline. Before deploying, the CodePipeline has a step in which it optionally kickstarts a restructuring of files on an S3 bucket that is forward compatible. That restructuring is done using a Step Function execution which invokes a Fargate task. The new Lambda function cannot work until the restructuring task has fully completed.\n\nAs a DevOps Engineer, how can you ensure traffic isn't served to your new Lambda function until the task is completed?", "related_lectures": []}, {"_class": "assessment", "id": 67357314, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", ""], "question": "<p>The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.</p>\n\n<p>Which of the following options would you suggest? (Select two)</p>\n", "explanation": "<p>Correct options:</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p><strong>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p>Understand how a pipeline execution state change rule works:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n\n<p>Here we are only interested in pipeline failures, so we need to choose <code>CodePipeline Pipeline Execution State Change</code>.\nFinally, CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q13-i3.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</strong> - CloudWatch Event rules do not support Slack as a target, so this option is incorrect.</p>\n\n<p>**\nCreate a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>**\nCreate a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>\n\n<p>**</p>\n\n<p>Here we are only interested in pipeline failures, so we just need to choose <code>CodePipeline Pipeline Execution State Change</code>. Therefore both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/codepipeline/faqs/\">https://aws.amazon.com/codepipeline/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Pipeline Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>", "<p>The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook</p>", "<p>The target of the rule should be a 'Slack send'. Provide the channel name and webhook URL</p>", "<p>Create a CloudWatch Event Rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Action Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>", "<p>Create a CloudWatch Event rule with the source corresponding to</p>\n\n<pre><code>{\n  \"source\": [\n    \"aws.codepipeline\"\n  ],\n  \"detail-type\": [\n    \"CodePipeline Stage Execution State Change\"\n  ],\n  \"detail\": {\n    \"state\": [\n      \"FAILED\"\n    ]\n  }\n}\n</code></pre>"]}, "correct_response": ["a", "b"], "section": "Domain 1: SDLC Automation", "question_plain": "The DevOps team at a yoga-inspired apparel company wants to stand up development environments for testing new features. The team would like to receive all CodePipeline pipeline failures to be sent to the company's #devops Slack channel. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case.\n\nWhich of the following options would you suggest? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357262, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As part of your CodePipeline, you are running multiple test suites. Two are bundled as Docker containers and run directly on CodeBuild, while another one runs as a Lambda function executing Python code. All these test suites are based on HTTP requests and upon analyzing, these are found to be network bound, not CPU bound. Right now, the CodePipeline takes a long time to execute as these actions happen one after the other. They prevent the company from adding further tests. The whole pipeline is managed by CloudFormation.</p>\n\n<p>As a DevOps Engineer, which of the following would you recommend improving the completion time of your pipeline?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Change the <code>runOrder</code> of your actions so that they have the same value</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/images/PipelineFlow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p>The pipeline structure format is used to build actions and stages in a pipeline. An action type consists of an action category and provider type. Valid action providers for each action category:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>You can use the <code>runOrder</code> to specify parallel actions and use the same integer for each action you want to run in parallel. The default runOrder value for an action is 1. The value must be a positive integer (natural number). You cannot use fractions, decimals, negative numbers, or zero. Here, you need to specify a common <code>runOrder</code> value in your CloudFormation template so that all the stage actions happen in parallel.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i2.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q14-i3.jpg\"></p>\n\n<p>via - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</strong> - As the test suites are HTTP and network-bound, increasing the RAM for Lambda and vCPU capacity of CodeBuild won't affect the performance (the bottleneck remains the network latency between each HTTP calls).</p>\n\n<p><strong>Enable CloudFormation StackSets to run the actions in parallel</strong> - CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>CloudFormation StackSets is a distractor here, as they do not enable parallel actions.</p>\n\n<p><strong>Migrate all the test suites to Jenkins and use the ECS plugin</strong> - Migrating to Jenkins also would not solve the problem, as the test suites would still happen sequentially.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Change the <code>runOrder</code> of your actions so that they have the same value</p>", "<p>Increase the number of vCPU assigned to the CodeBuild builds and the RAM assigned to your Lambda function</p>", "<p>Enable CloudFormation StackSets to run the actions in parallel</p>", "<p>Migrate all the test suites to Jenkins and use the ECS plugin</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "As part of your CodePipeline, you are running multiple test suites. Two are bundled as Docker containers and run directly on CodeBuild, while another one runs as a Lambda function executing Python code. All these test suites are based on HTTP requests and upon analyzing, these are found to be network bound, not CPU bound. Right now, the CodePipeline takes a long time to execute as these actions happen one after the other. They prevent the company from adding further tests. The whole pipeline is managed by CloudFormation.\n\nAs a DevOps Engineer, which of the following would you recommend improving the completion time of your pipeline?", "related_lectures": []}, {"_class": "assessment", "id": 67357264, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The DevOps team at a geological hazard monitoring agency maintains an application that provides near real-time notifications to Android and iOS devices during tremors, volcanic eruptions and tsunamis. The team has created a CodePipeline pipeline, which consists of CodeCommit and CodeBuild, and the application is deployed on Elastic Beanstalk. The team would like to enable Blue/Green deployments for Beanstalk through CodePipeline.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution for this requirement?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a Custom Job using AWS Lambda, which will perform the API call to swap the CNAME of the environments</strong></p>\n\n<p>AWS Elastic Beanstalk makes it easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.</p>\n\n<p>When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments \u2014 blue and green \u2014 increases availability and reduces risk. The blue environment is the production environment that normally handles live traffic. The CI/CD pipeline architecture creates a clone (green) of the live Elastic Beanstalk environment (blue). The pipeline then swaps the URLs between the two environments. While CodePipeline deploys application code to the original environment \u2014 and testing and maintenance take place \u2014 the temporary clone environment handles the live traffic. Once deployment to the blue environment is successful, and code review and code testing are done, the pipeline again swaps the URLs between the green and blue environments. The blue environment starts serving the live traffic again, and the pipeline terminates the green environment.</p>\n\n<p>Blue-Green Deployments to AWS Elastic Beanstalk using Code Pipeline:\n<img src=\"https://d1.awsstatic.com/partner-network/QuickStart/datasheets/blue-green-deployment-on-aws-architecture1.68038404e92ea779a2f8f011139eabf9d8678bd2.png\"></p>\n\n<p>via - <a href=\"https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf\">https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf</a></p>\n\n<p>To perform Blue/Green in Elastic Beanstalk, you need to deploy to a new environment and do a CNAME swap. The CNAME swap feature is not supported by CloudFormation itself, therefore you need to create a custom Lambda function that will perform that API call for you and invoke it as part of a Custom Job in CodePipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Make CodePipeline deploy to the current Beanstalk environment using a rolling with additional batch strategy. Add a CodeDeploy stage action afterward to enable Blue / Green</strong></p>\n\n<p><strong>Make CodePipeline deploy to the current Beanstalk environment using an immutable strategy. Add a CodeStar stage action afterward to enable Blue / Green configured through the <code>template.yml</code> file</strong></p>\n\n<p>As explained above, to perform Blue/Green in Elastic Beanstalk, you need to deploy to a new environment and NOT to the current environment. So both these options are incorrect. You should also note that CodeStar is not a stage actor, it's a service that wraps up all CICD services from AWS into one simple UI to use as a developer.</p>\n\n<p><strong>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a CloudFormation template that will perform a CNAME swap</strong> - As mentioned in the explanation above, The CNAME swap feature is not supported by CloudFormation itself, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf\">https://aws-quickstart.s3.amazonaws.com/quickstart-codepipeline-bluegreen-deployment/doc/blue-green-deployments-to-aws-elastic-beanstalk-on-the-aws-cloud.pdf</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a Custom Job using AWS Lambda, which will perform the API call to swap the CNAME of the environments</p>", "<p>Make CodePipeline deploy to the current Beanstalk environment using a rolling with additional batch strategy. Add a CodeDeploy stage action afterward to enable Blue / Green</p>", "<p>Make CodePipeline deploy to the current Beanstalk environment using an immutable strategy. Add a CodeStar stage action afterward to enable Blue / Green configured through the <code>template.yml</code> file</p>", "<p>Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a CloudFormation template that will perform a CNAME swap</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "The DevOps team at a geological hazard monitoring agency maintains an application that provides near real-time notifications to Android and iOS devices during tremors, volcanic eruptions and tsunamis. The team has created a CodePipeline pipeline, which consists of CodeCommit and CodeBuild, and the application is deployed on Elastic Beanstalk. The team would like to enable Blue/Green deployments for Beanstalk through CodePipeline.\n\nAs a DevOps Engineer, how would you implement a solution for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357232, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p>In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances in the cloud. It enables you to achieve greater levels of fault tolerance in your applications and seamlessly provides the required amount of load balancing capacity needed to distribute application traffic. If your business requirements demand a fault-tolerant Jenkins environment, your preferred setup might be a scenario in which multiple masters with their own workers are placed in separate Availability Zones.</p>\n\n<p>You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2019/10/20/Diagram2.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p>For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p><strong>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</strong></p>\n\n<p>As mentioned in the explanation above, if configured with EC2 instances in an Auto Scaling Group, the setup will be elastic in some ways, but probably expensive if the EC2 instances are not fully utilized at capacity. So these three options are not the best fit for the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/\">https://aws.amazon.com/blogs/devops/setting-up-a-ci-cd-pipeline-by-integrating-jenkins-with-aws-codebuild-and-aws-codedeploy/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/jenkins-plugin.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf\">https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>", "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds</p>", "<p>Deploy Jenkins as a multi-master setup across multiple AZ. Create an Auto Scaling Group made of EC2 instances that are Jenkins slave. Configure Jenkins to launch build on these slaves</p>", "<p>Deploy Jenkins as a multi-master setup across one AZ, managed by an Auto Scaling Group. Configure Jenkins to launch build on these slaves</p>"]}, "correct_response": ["a"], "section": "Domain 1: SDLC Automation", "question_plain": "A retail company uses the open-source tool Jenkins on its on-premise infrastructure to perform CICD. It has decided to move to AWS and take advantage of the elasticity properties of the cloud provider to have more efficient workloads. It needs to ensure the Jenkins setup is highly available, fault-tolerant and also elastic to perform builds. The company has hired you as an AWS Certified DevOps Engineer Professional to build the most cost-effective solution for this requirement.\n\nWhich of the following solutions would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 67357198, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Your application is deployed on Elastic Beanstalk and you manage the configuration of the stack using a CloudFormation template. A new golden AMI is created every week and contains a hardened AMI that has all the necessary recent security patches. You have deployed over 100 applications using CloudFormation &amp; Beanstalk this way and you would like to ensure the newer AMI used for EC2 instances is updated every week. There are no standardization or naming conventions made across all the CloudFormation templates.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution for this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong></p>\n\n<p>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values.</p>\n\n<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. When you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation using the <code>UpdateStack</code> API.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q17-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n\n<p>This question is a hard one as many solutions are feasible with a degree of complexity. It's about identifying the simplest solution.</p>\n\n<p>For the given use-case, by having the CloudFormation parameters directly pointing at SSM Parameter Store, on any refresh made to the CloudFormation template by the Lambda function which is in turn triggered by CloudWatch events, the template itself will fetch the latest value from the SSM Parameter Store and will apply it accordingly. So this solution is the best fit for the given requirement.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</strong> - Storing the AMI id in S3 is possible, but CloudFormation cannot source parameters from S3 and therefore there's no integration possible.</p>\n\n<p><strong>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Having a Lambda function fetch the parameter and pass it as a parameter to CloudFormation seems like a good idea, but if we remember the constraint that the parameters are not standardized and that there are no naming conventions, it is difficult for us to imagine a solution that would scale.</p>\n\n<p><strong>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</strong> - Creating a Lambda function that would update the mapping section of each template would introduce changes to each template content at every update and would be highly complicated to implement. Additionally, the Lambda function would be hard to write and would have a lot of complexity in updating the mapping as there's no standardization.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ssm-parameter.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p>\n", "answers": ["<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>", "<p>Store the Golden AMI id in an S3 object. Create a CloudFormation parameter that points to the S3 object, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API</p>", "<p>Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter of type String, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should fetch the parameter from the SSM Parameter Store and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>", "<p>Store the Golden AMI id in an S3 object. Create a CloudFormation mapping to contain the last value of the Golden AMI id. That mapping is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should update the mapping section of every CloudFormation template using a YAML parser, upload the new templates to S3 and trigger a refresh of all the CloudFormation templates using the <code>UpdateStack</code> API while passing the new parameter</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "Your application is deployed on Elastic Beanstalk and you manage the configuration of the stack using a CloudFormation template. A new golden AMI is created every week and contains a hardened AMI that has all the necessary recent security patches. You have deployed over 100 applications using CloudFormation &amp; Beanstalk this way and you would like to ensure the newer AMI used for EC2 instances is updated every week. There are no standardization or naming conventions made across all the CloudFormation templates.\n\nAs a DevOps Engineer, how would you implement a solution for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357212, "assessment_type": "multi-select", "prompt": {"question": "<p>The DevOps team at a leading travel-booking services company is using a CloudFormation template to deploy a Lambda function. The Lambda function code is uploaded into S3 into a file named <code>s3://my-bucket/my-lambda-code.zip</code> by CodePipeline after having passed all the required build checks. CodePipeline then invokes the CloudFormation template to deploy the new code. The team has found that although the CloudFormation template successfully runs, the Lambda function is not updated.</p>\n\n<p>As a DevOps Engineer, what can you do to quickly fix this issue? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Upload the code every time to a new S3 bucket</strong></p>\n\n<p><strong>Upload the code every time with a new filename in the same bucket</strong></p>\n\n<p><strong>Enable S3 versioning and provide an <code>S3ObjectVersion</code> key</strong></p>\n\n<p>You can use CloudFormation to deploy and update compute, database, and many other resources in a simple, declarative style that abstracts away the complexity of specific resource APIs. CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\">\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p>Here, the issue is that CloudFormation does not detect a new file has been uploaded to S3 unless one of these parameters change:\n- S3Bucket\n- S3Key\n- S3ObjectVersion</p>\n\n<p>Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, you need to change the object key or version in the template.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Clear the Lambda cache with a Custom Job in CodePipeline before the CloudFormation step</strong> - This option has been added as a distractor as there's no such thing as a Lambda cache.</p>\n\n<p><strong>Add a pause of 3 seconds before starting the CloudFormation job. This is an eventual consistency issue due to overwriting PUT</strong> - This option has been added as a distractor as there's no such thing as an eventual consistency for CloudFormation.</p>\n\n<p><strong>Enable the SAM Framework option</strong> - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. Enabling SAM would require a re-write of the template, which won't be quick.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p>\n", "answers": ["<p>Upload the code every time to a new S3 bucket</p>", "<p>Upload the code every time with a new filename in the same bucket</p>", "<p>Enable S3 versioning and provide an <code>S3ObjectVersion</code> key</p>", "<p>Clear the Lambda cache with a Custom Job in CodePipeline before the CloudFormation step</p>", "<p>Add a pause of 3 seconds before starting the CloudFormation job. This is an eventual consistency issue due to overwriting PUT</p>", "<p>Enable the SAM Framework option</p>"]}, "correct_response": ["a", "b", "c"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at a leading travel-booking services company is using a CloudFormation template to deploy a Lambda function. The Lambda function code is uploaded into S3 into a file named s3://my-bucket/my-lambda-code.zip by CodePipeline after having passed all the required build checks. CodePipeline then invokes the CloudFormation template to deploy the new code. The team has found that although the CloudFormation template successfully runs, the Lambda function is not updated.\n\nAs a DevOps Engineer, what can you do to quickly fix this issue? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 67357318, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an <code>InsufficientCapabilitiesException</code>.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</strong></p>\n\n<p>With AWS CloudFormation and CodePipeline, you can use continuous delivery to automatically build and test changes to your AWS CloudFormation templates before promoting them to production stacks. For example, you can create a workflow that automatically builds a test stack when you submit an updated template to a code repository. After AWS CloudFormation builds the test stack, you can test it and then decide whether to push the changes to a production stack. Use CodePipeline to build a continuous delivery workflow by building a pipeline for AWS CloudFormation stacks. CodePipeline has built-in integration with AWS CloudFormation, so you can specify AWS CloudFormation-specific actions, such as creating, updating, or deleting a stack, within a pipeline.</p>\n\n<p>You can use IAM with AWS CloudFormation to control what users can do with AWS CloudFormation, such as whether they can view stack templates, create stacks, or delete stacks.</p>\n\n<p>For the given use-case, <code>InsufficientCapabilitiesException</code> means that the CloudFormation stack is trying to create an IAM role but it doesn't have those specified capabilities. As such it must be configured in CodePipeline configuration for the Deploy CloudFormation stage action.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q19-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</strong> - The given exception is not related to the permissions of the user or the CodePipeline IAM Role running the CloudFormation template, so this option is incorrect.</p>\n\n<p><strong>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</strong> - A circular dependency, as the name implies, means that two resources are dependent on each other or that a resource is dependent on itself.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/circular-dependency-sg-group.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p>This option is incorrect as a circular dependency would trigger another error such as this:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/b7eb6c689c037217079766fdb77c3bac3e51cb4c/2019/03/19/CircularDependencyerror.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n\n<p><strong>Increase the service limits for your S3 bucket limits as you've reached it</strong> - This option has been added as a distractor as the exception has nothing to do with service limits for the S3 bucket.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html#using-iam-capabilities</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/\">https://aws.amazon.com/blogs/infrastructure-and-automation/handling-circular-dependency-errors-in-aws-cloudformation/</a></p>\n", "answers": ["<p>Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action</p>", "<p>Update the CodePipeline IAM Role so it has permissions to create all the resources mentioned in the CloudFormation template</p>", "<p>Fix the CloudFormation template as there is circular dependency and CloudFormation does not have that capability</p>", "<p>Increase the service limits for your S3 bucket limits as you've reached it</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at a leading bitcoin wallet and exchange services company is trying to deploy a CloudFormation template that contains a Lambda Function, an S3 bucket, an IAM role, and a DynamoDB table from CodePipeline but the team is getting an InsufficientCapabilitiesException.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you suggest fixing this issue?", "related_lectures": []}, {"_class": "assessment", "id": 67357224, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.</p>\n\n<p>What's the reason and how could this issue be fixed?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</strong></p>\n\n<p>In a CloudFormation template, you can use the AWS::CloudFormation::CustomResource or Custom::String resource type to specify custom resources. Custom resources provide a way for you to write custom provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation, such as when you create, update or delete a stack.</p>\n\n<p>Some resources must be empty before they can be deleted. For example, you must delete all objects in an Amazon S3 bucket or remove all instances in an Amazon EC2 security group before you can delete the bucket or security group.</p>\n\n<p>For this use-case, the issue is that the S3 bucket is not empty before being deleted, therefore you must implement a Custom Resource backed by Lambda which will clean the bucket for you.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q20-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</strong> - CloudFormation can delete resources while they're being used, and a <code>WaitCondition</code> can be attached to EC2 instances and Auto Scaling Groups and NOT to Lambda function. AWS further recommends that for Amazon EC2 and Auto Scaling resources, you use a CreationPolicy attribute instead of wait conditions. Add a CreationPolicy attribute to those resources, and use the cfn-signal helper script to signal when an instance creation process has completed successfully.</p>\n\n<p><strong>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</strong> - This option has been added as a distractor. To clean it, you cannot use a <code>Delete: Force</code> as this is not a feature of CloudFormation.</p>\n\n<p><strong>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</strong> -  A stack policy is a JSON document that defines the update actions that can be performed on designated resources. Stack Policies are only used during CloudFormation stack updates.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html\">https://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket\">https://stackoverflow.com/questions/40383470/can-i-force-cloudformation-to-delete-non-empty-s3-bucket</a></p>\n", "answers": ["<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket</p>", "<p>The Lambda function is still using the S3 bucket and CloudFormation cannot, therefore, delete the S3 bucket. Place a <code>WaitCondition</code> on the Lambda function to fix the issue</p>", "<p>The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Add the property <code>Delete: Force</code> to your CloudFormation template so that the S3 bucket is emptied before being deleted</p>", "<p>A StackPolicy prevents the CloudFormation template to be deleted. Clear the Stack Policy and try again</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "As part of the CICD pipeline, a DevOps Engineer is performing a functional test using a CloudFormation template that will later get deployed to production. That CloudFormation template creates an S3 bucket and a Lambda function which transforms images uploaded into S3 into thumbnails. To test the Lambda function, a few images are automatically uploaded and the thumbnail output is expected from the Lambda function on the S3 bucket. As part of the clean-up of these functional tests, the CloudFormation stack is deleted, but right now the delete fails.\n\nWhat's the reason and how could this issue be fixed?", "related_lectures": []}, {"_class": "assessment", "id": 67357256, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A 3D modeling company would like to deploy applications on Elastic Beanstalk with support for various programming languages with predictable and standardized deployment strategies. Some of these languages are supported (such as Node.js, Java, Golang) but others such as Rust are not supported. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following options would you recommend as the MOST efficient solution for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</strong></p>\n\n<p>Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can also choose your own platform, programming language, and any application dependencies (such as package managers or tools), which typically aren't supported by other platforms. Elastic Beanstalk can deploy a Docker image and source code to EC2 instances running the Elastic Beanstalk Docker platform. The platform offers multi-container (and single-container) support.</p>\n\n<p>A Dockerrun.aws.json file is an Elastic Beanstalk\u2013specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. You can use a Dockerrun.aws.json file for a multi-container Docker environment. Dockerrun.aws.json describes the containers to deploy to each container instance (Amazon EC2 instance that hosts Docker containers) in the environment as well as the data volumes to create on the host instance for the containers to mount.</p>\n\n<p>Here, the most simple solution is to create a Docker container for each application. By using a Multi-Docker container configuration, we will be able to have a standardized deployment system across all the languages that we want to support.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</strong> - Creating custom platforms and packaging applications in S3 will be cumbersome across a wide array of platforms. Using a multi-Docker container configuration is more efficient.</p>\n\n<p><strong>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</strong> - Packaging each application as an AMI might work but it's not going to help you standardize the way applications are deployed.</p>\n\n<p><strong>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</strong> - AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks is a distractor in the question and doesn't have integration with Elastic Beanstalk.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html</a></p>\n", "answers": ["<p>Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR</p>", "<p>Create a custom platform for each language that is not supported. Package each application in S3 before deploying to Elastic Beanstalk</p>", "<p>Package each application as a standalone AMI that contains the OS, the application runtime and the application itself. To update a Beanstalk environment, provide a new AMI</p>", "<p>Run Opsworks on top of Elastic Beanstalk to bring the missing compatibility layer</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A 3D modeling company would like to deploy applications on Elastic Beanstalk with support for various programming languages with predictable and standardized deployment strategies. Some of these languages are supported (such as Node.js, Java, Golang) but others such as Rust are not supported. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following options would you recommend as the MOST efficient solution for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357254, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.</p>\n\n<p>As a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p>Note: Recommended values are applied when you create or update an environment on the Elastic Beanstalk API by a client. For example, the client could be the AWS Management Console, Elastic Beanstalk Command Line Interface (EB CLI), AWS Command Line Interface (AWS CLI), or SDKs. Recommended values are directly set at the API level and have the highest precedence. The configuration setting applied at the API level can't be changed using option_settings, as the API has the highest precedence.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q22-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n\n<p>Configuration changes made to your Elastic Beanstalk environment won't persist if you use the following configuration methods:</p>\n\n<p>Configuring an Elastic Beanstalk resource directly from the console of a specific AWS service.</p>\n\n<p>Installing a package, creating a file, or running a command directly from your Amazon EC2 instance.</p>\n\n<p>For the given use-case, using a <code>.ebextensions</code> file and configuring the rules in the <code>option_settings</code> block is the right option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</strong> - This option has been added as a distractor as you cannot configure CodePipeline to deploy using the EB CLI.</p>\n\n<p><strong>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</strong> - Using the EB CLI on your computer would normally work, but here the question specifies that we don't have the necessary permissions to make direct changes against the Beanstalk environment. We, therefore, have to use CodePipeline.</p>\n\n<p><strong>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</strong> - Using a <code>container_command</code> may work, but it wouldn't be best practice as the EC2 would issue a command to the ALB and therefore the configuration of it would be different from the one specified by Beanstalk itself, and the EC2 instance may not have enough permissions through IAM role to issue that command in the first place. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html</a></p>\n", "answers": ["<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add an <code>option_settings</code> block for which you will specify the Rules for the key <code>aws:elbv2:listener:default</code>. Push your code and let the CodePipeline run</p>", "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Configure CodePipeline to deploy to Elastic Beanstalk using the EB CLI and push the code</p>", "<p>Using the EB CLI, create a <code>.elasticbeanstalk/saved_configs/config.yml</code>, and specify the rules for the key <code>aws:elbv2:listener:default</code>. Run a deploy using the EB CLI from your computer onto the Elastic Beanstalk Environment</p>", "<p>Create a file named <code>.ebextensions/alb.config</code> in your code repository and add a <code>container_commands</code> block for which you will specify a container command that will run in <code>leader_only</code> mode. The EC2 instance will issue an API call to the Load Balancer to add the redirection rule</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at a presentation software company is deploying their flagship application using Elastic Beanstalk. The application is deployed using a Deploy stage in a CodePipeline pipeline. The technical requirements mandate changing the configuration of the Application Load Balancer tied to Elastic Beanstalk by adding an HTTP to HTTPS redirection rule.\n\nAs a DevOps Engineer, you don't have the permissions to directly edit the Elastic Beanstalk environment, how can you proceed?", "related_lectures": []}, {"_class": "assessment", "id": 67357316, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a <code>.ebextensions</code> file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.</p>\n\n<p>Which of the following options would you suggest to address the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use a rolling update with 20% at a time</strong></p>\n\n<p>AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment (you didn't specify the --single option), it uses rolling deployments.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>Comparison of deployment method properties:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p>With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches (for this requirement, we shall use a batch with 20% of the instances) and deploys the new version of the application to one batch at a time. It leaves the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version. Therefore, for the given use-case, we should use a rolling update, which will keep our ASG, our instances, and ensure our application can still serve traffic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q23-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a blue/green deployment and swap CNAMEs</strong> - In a blue/green deployment, you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. A blue/green deployment would create a new load balancer and ASG, but the CNAME swap would allow us to keep the same DNS name. So it does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use immutable</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. So this option does not meet the requirements for the given use-case.</p>\n\n<p><strong>Use in-place</strong> - In-place would not work even though it doesn't create any new resources because your application will be unavailable as all your instances will be updated at the same time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n", "answers": ["<p>Use a rolling update with 20% at a time</p>", "<p>Use a blue/green deployment and swap CNAMEs</p>", "<p>Use immutable</p>", "<p>Use in-place</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at a financial services company is deploying the flagship application in highly available mode using Elastic Beanstalk which has created an ASG and an ALB. The team has also specified a .ebextensions file to create an associated DynamoDB table. As a DevOps Engineer in the team, you would like to perform an update to the application but you need to make sure the DNS name won't change and that no new resources will be created. The application needs to remain available during the update.\n\nWhich of the following options would you suggest to address the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357304, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>What do you recommend for the application to ensure a good performance and address scalability requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong></p>\n\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n\n<p>AWS Elastic Beanstalk enables you to manage all of the resources that run your application as environments. An environment is a collection of AWS resources running an application version. When you launch an Elastic Beanstalk environment, you need to choose an environment tier. An application that serves HTTP requests runs in a web server environment tier. A backend environment that pulls tasks from an Amazon Simple Queue Service (Amazon SQS) queue runs in a worker environment tier.</p>\n\n<p>Elastic Beanstalk Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p>When you create a web server environment, Beanstalk provisions the resources required to run your application. AWS resources created for this type of environment include one elastic load balancer, an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q24-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p>AWS resources created for a worker environment tier include an ASG, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Beanstalk also creates and provisions an SQS queue if you don\u2019t already have one. When you launch a worker environment, Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the ASG. The daemon reads messages from an SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing.</p>\n\n<p>For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the <code>cron.yml</code> file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.</p>\n\n<p>Elastic Beanstalk Worker environment:\n<img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-architecture_worker.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</strong> - As mentioned in the explanation above, the worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The <code>cron.yml</code> file must be defined on the worker tier, it is not supported by the web tier, so this option is incorrect.</p>\n\n<p><strong>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</strong> - The worker tier must be a separate environment from the web tier, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></p>\n", "answers": ["<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>", "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file will send out the emails</p>", "<p>Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>", "<p>Create a separate Beanstalk tier within the same environment that's a worker configuration and processes invoices through an SQS queue. The invoices are directly sent into SQS after being gzipped by the web tier. The workers process these files. A cron job defined using the <code>cron.yml</code> file on the web tier will send out the emails</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at an auditing firm has deployed its flagship application on Elastic Beanstalk that processes invoices uploaded by customers in CSV form. The invoices can be quite big, with up to 10MB and 1,000,000 records total. Processing is CPU intensive which results in slowing down the application. Customers are sent an email when the processing is done, through the use of a cron job. The auditing firm has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhat do you recommend for the application to ensure a good performance and address scalability requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357294, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.</p>\n\n<p>How can you fix this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong></p>\n\n<p>You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the option_settings key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.</p>\n\n<p>You may want to customize and configure the software that your application depends on. You can use the commands key to execute commands on the EC2 instance. The commands run before the application and web server are set up and the application version file is extracted.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>You can use the container_commands key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n\n<p>If you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. Therefore you must use <code>container_commands</code>.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</strong> - As mentioned earlier, if you specify a <code>commands</code> block, every EC2 instance will run it and it does not support the <code>leader_only</code> attribute. So this option is incorrect.</p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p><strong>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</strong></p>\n\n<p>The <code>lock_mode: true</code> attribute has been added as a distractor and it does not exist. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352\">https://stackoverflow.com/questions/35788499/what-is-difference-between-commands-and-container-commands-in-elasticbean-talk/40096352#40096352</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-ec2.html</a></p>\n", "answers": ["<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>", "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>leader_only: true</code> attribute</p>", "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>container_commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>", "<p>Create an <code>.ebextensions/db-migration.config</code> file in your code repository and set a <code>commands</code> block. Set your migration command there and use the <code>lock_mode: true</code> attribute</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "An e-commerce company has deployed a Spring application on Elastic Beanstalk running the Java platform. As a DevOps Engineer at the company, you are referencing an RDS PostgreSQL database through an environment variable so that your application can use it for storing its data. You are using a library to perform a database migration in case the schema changes. Upon deploying updates to Beanstalk, you have seen the migration fail because all the EC2 instances running the new version try to run the migration on the RDS database.\n\nHow can you fix this issue?", "related_lectures": []}, {"_class": "assessment", "id": 67357336, "assessment_type": "multi-select", "prompt": {"question": "<p>The DevOps team at a social media company has created a CodePipeline pipeline and the final step is to use CodeDeploy to update an AWS Lambda function. As a DevOps Engineering Lead at the company, you have decided that for every deployment, the new Lambda function must sustain a small amount of traffic for 10 minutes and then shift all the traffic to the new function. It has also been decided that safety must be put in place to automatically roll-back if the Lambda function experiences too many crashes.</p>\n\n<p>Which of the following recommendations would you provide to address the given use-case? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</strong></p>\n\n<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time. You can monitor metrics such as instance CPU utilization. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.</p>\n\n<p>For the given use-case, the CodeDeploy deployment must be associated with a CloudWatch Alarm for automated rollbacks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p>Configure advanced options for a deployment group:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></strong></p>\n\n<p>A deployment configuration is a set of rules and success and failure conditions used by CodeDeploy during a deployment. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q26-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n\n<p>For canary deployments, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p>\n\n<p>A canary deployment of <code>LambdaCanary10Percent10Minutes</code> means the traffic is 10% on the new function for 10 minutes, and then all the traffic is shifted to the new version after the time has elapsed.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaAllAtOnce</code></strong> - An all at once deployment means all the traffic is shifted to the new function right away and this option does not meet the given requirements.</p>\n\n<p><strong>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></strong> - For linear deployments, traffic is shifted in equal increments with an equal number of minutes between each increment. For example, a linear deployment of <code>LambdaLinear10PercentEvery10Minutes</code> would shift 10 percent of traffic every minute until all traffic is shifted.</p>\n\n<p><strong>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</strong> - The CodeDeploy deployment must be associated with a CloudWatch Alarm and not CloudWatch Event for automated rollbacks to work.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-configure-advanced-options.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p>\n", "answers": ["<p>Choose a deployment configuration of <code>LambdaAllAtOnce</code></p>", "<p>Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment</p>", "<p>Choose a deployment configuration of <code>LambdaCanary10Percent10Minutes</code></p>", "<p>Choose a deployment configuration of <code>LambdaLinear10PercentEvery10Minutes</code></p>", "<p>Create a CloudWatch Event for the Lambda Deployment Monitoring and associate it with the CodeDeploy deployment</p>"]}, "correct_response": ["b", "c"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "The DevOps team at a social media company has created a CodePipeline pipeline and the final step is to use CodeDeploy to update an AWS Lambda function. As a DevOps Engineering Lead at the company, you have decided that for every deployment, the new Lambda function must sustain a small amount of traffic for 10 minutes and then shift all the traffic to the new function. It has also been decided that safety must be put in place to automatically roll-back if the Lambda function experiences too many crashes.\n\nWhich of the following recommendations would you provide to address the given use-case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357326, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</strong></p>\n\n<p>While using CloudFormation, you work with templates and stacks. You create templates to describe your AWS resources and their properties. When you use AWS CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's AWS CloudFormation template.</p>\n\n<p>In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file.</p>\n\n<p>CloudFormation best practices:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q27-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</strong></p>\n\n<p><strong>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</strong></p>\n\n<p>The issue with both these options is that different teams are working on different pieces of the infrastructure with their own timelines, so it's difficult to combine all elements of the infrastructure into a single master template. It's much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks. Nested Stacks can be helpful if a component configuration (such as a Load Balancer) can be reused across many stacks.</p>\n\n<p><strong>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</strong> - Using outputs and exports for individual templates is much better than collaborating via pull requests at code repository level. Using individual templates gives ownership to the contributing team to make sure that the CloudFormation templates are always functional and ready to be referenced in other stacks.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#cross-stack</a></p>\n", "answers": ["<p>Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository</p>", "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template using CloudFormation every-time a nested stack template is updated in version control</p>", "<p>Create one template per logical element of your infrastructure. Create a master stack that contains all the other stacks as a nested template. Deploy the master template once using CloudFormation and then update the nested stacks individually as new CloudFormation code is created</p>", "<p>Create one master template that contains all the stacks in your infrastructure. Collaborate on that template using pull requests and merges to the master branch in your code repository. Deploy the master template every-time it is updated</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A multi-national retail company is in the process of capturing all of its infrastructure as code using CloudFormation. The infrastructure inventory is huge and will contain a networking stack, an application stack, a data stack, and so on. Some teams are ready to move ahead with the process while others are lagging, and there is a desire to keep all the infrastructure version controlled.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this use-case. How would you implement this?", "related_lectures": []}, {"_class": "assessment", "id": 67357194, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An ed-tech company has created a paid-per-use API using API Gateway. This API is available at <code>http://edtech.com/api/v1</code>. The website's static files have been uploaded in S3 and now support a new API route <code>http://edtech.com/api/v1/new-feature</code> if available. Your team has decided it is safer to send a small amount of traffic to that route first and test if the metrics look okay. Your API gateway routes are backed by AWS Lambda.</p>\n\n<p>As a DevOps Engineer, what steps should you take to enable this testing?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using CloudWatch</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API. You can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing. In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. The updated API features are only visible to the canary release. The canary release receives a small percentage of API traffic and the production release takes up the rest.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q28-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n\n<p>For the given use-case, you must deploy API to a new stage called v1, enable canary deployment on this v1 stage and assign a small amount of traffic to this canary stage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using Amazon ES</strong> - API Gateway &amp; AWS Lambda have a direct integration with CloudWatch and NOT with Amazon ES. So this option is incorrect.</p>\n\n<p><strong>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using CloudWatch</strong> - When a new API route is implemented, you must create a new API Gateway stage and NOT a Lambda alias. Lambda aliases are only used to update the behavior of an existing route. Remember that one route in API Gateway is mapped to one AWS Lambda function (or another service).</p>\n\n<p><strong>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using Amazon ES</strong> - When a new API route is implemented, you must create a new API Gateway stage and NOT a Lambda alias. In addition, API Gateway &amp; AWS Lambda have a direct integration with CloudWatch and NOT with Amazon ES.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p>\n", "answers": ["<p>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using CloudWatch</p>", "<p>Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using Amazon ES</p>", "<p>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using CloudWatch</p>", "<p>Create a new Lambda function alias. Enable Canary deployments on the Lambda alias. Deploy the new API to the Lambda alias and assign a small amount of traffic to the canary Lambda version. Enable new route redirection for AWS Lambda and track metrics data using Amazon ES</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "An ed-tech company has created a paid-per-use API using API Gateway. This API is available at http://edtech.com/api/v1. The website's static files have been uploaded in S3 and now support a new API route http://edtech.com/api/v1/new-feature if available. Your team has decided it is safer to send a small amount of traffic to that route first and test if the metrics look okay. Your API gateway routes are backed by AWS Lambda.\n\nAs a DevOps Engineer, what steps should you take to enable this testing?", "related_lectures": []}, {"_class": "assessment", "id": 67357276, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.</p>\n\n<p>As a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</strong></p>\n\n<p>API Gateway APIs can directly invoke an AWS service and pass in a payload. It's a common way to provide a publicly available and secure API for your chosen AWS services.</p>\n\n<p>Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. You create state machines in the Step Functions Console or through the Step Functions API to specify and execute the steps of your application at scale. API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</strong> - AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. So there is no need to use Lambda as an intermediary for this workflow.</p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</strong></p>\n\n<p><strong>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</strong></p>\n\n<p>Lambda functions cannot process the booking workflow as it may take 3 hours, which is more than the 15 minutes max timeout limit that Lambda supports. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n", "answers": ["<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito</p>", "<p>Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with AWS Lambda, which will, in turn, invoke the Step Function workflow. Secure your API using Cognito</p>", "<p>Create the booking workflow in AWS Lambda. Create an API Gateway stage using a service integration with AWS Lambda. The Lambda function will wait for the service provider response and then issue the status back to API Gateway. Secure your API using Cognito</p>", "<p>Create the booking workflow in AWS Lambda. Enable public invocations of the Lambda functions so that clients can start the booking process. The Lambda function will wait for the service provider's response and then issue the status back to the client. Secure the calls using IAM</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "An IT company is creating an online booking system for hotels. The booking workflow that the company has implemented can take over 3 hours to complete as a manual verification step is required by a 3rd party provider to ensure big transactions are not fraudulent.\n\nAs a DevOps Engineer, you need to expose this as a secure API for the end customers. The website must be able to sustain 5000 requests at the same time. How should you implement this in the simplest possible way?", "related_lectures": []}, {"_class": "assessment", "id": 67357192, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A graphics design company is experimenting with a new feature for an API and the objective is to pass the field <code>\"color\"</code> in the JSON payload to enable this feature. The new Lambda function should treat <code>\"color\": \"none\"</code> as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the <code>v1</code> stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</strong></p>\n\n<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway handles tasks such as traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a \"front door\" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Simply creating and developing an API Gateway API doesn't automatically make it callable by your users. To make it callable, you must deploy your API to a stage. A stage is a named reference to a deployment, which is a snapshot of the API.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field <code>\"color\": \"none\"</code>. Newer clients will hit the v2 API and will have that field value included.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q30-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</strong> - If we release two separate Lambda functions (named lambda v1 and lambda v2), then we have to maintain them both and that would be going against the requirements of the given use-case.</p>\n\n<p><strong>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</strong> -  API Gateway mapping templates do not support adding default values for fields as these only support static fields.</p>\n\n<p><strong>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</strong> - You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. For the given use-case, API Gateway caching is a distractor and should be disregarded.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n", "answers": ["<p>Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. Both use the same Lambda function as a backing route for the <code>v1</code> and <code>v2</code> stages. Add a static mapping on the <code>v1</code> route to add <code>\"color\": \"none\"</code> on requests</p>", "<p>Create a new Lambda function version and release it as a separate <code>v2</code> function. Create a new API Gateway Stage and deploy it to the <code>v2</code> stage. The <code>v1</code> API gateway stage points to the <code>v1</code> Lambda function and the <code>v2</code> API Gateway stage to the <code>v2</code> Lambda function. Implement redirection from the Lambda <code>v1</code> function to the Lambda <code>v2</code> function when the request is missing the <code>\"color\"</code> field</p>", "<p>Create a new Lambda function version and release it. Use API Gateway mapping documents to add a default value <code>\"color\": \"none\"</code> to the JSON request being passed on your API Gateway stage</p>", "<p>Enable API Gateway <code>v1</code> API caching and delete the <code>v1</code> AWS Lambda function. Deploy a <code>v2</code> API Gateway backed by a newly released <code>v2</code> AWS Lambda function. Add an API Gateway stage variable to enable the <code>\"color\": \"none\"</code> default value</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A graphics design company is experimenting with a new feature for an API and the objective is to pass the field \"color\" in the JSON payload to enable this feature. The new Lambda function should treat \"color\": \"none\" as a request from an older client. The company would like to only have to manage one Lambda function in the back-end while being able to support both old and new clients. The API Gateway API is currently deployed on the v1 stage. Old clients include Android applications which may take time to be updated. The technical requirements mandate that the solution should support the old clients for years to come.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you recommend as the best fit for the given use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357338, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local <code>Dockerfile</code>, and then pushes to ECR at <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code>. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.</p>\n\n<p>How should you implement a solution to address this issue?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</strong></p>\n\n<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the <code>IMAGEID</code> property, which is the SHA digest for the Docker image used to start the container.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n\n<p>The issue here is that the ECS instances do not detect that a newer image version is available, because the name <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is re-used. Therefore, by specifying the sha256 e.g.: <code>aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app@sha256:94afd1f2e64d908bc90dbca0035a5b567EXAMPLE</code>, we are certain that newer versions of the Docker image will have a different hash value and therefore the ECS cluster will always pull the newest image at the end of our CICD Pipeline.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</strong> - SSM Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. SSM Run Command may work but it's not an elegant solution.</p>\n\n<p><strong>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</strong> - Lambda Functions can't SSH into EC2 instances, so this option is incorrect.</p>\n\n<p><strong>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</strong> - Adding the <code>latest</code> tag won't help because <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app</code> is same as <code>123456789.dkr.ecr.region.amazonaws.com/my-web-app:latest</code>. The <code>latest</code> tag cannot provide visibility and identification to track where container images are deployed.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/\">https://aws.amazon.com/about-aws/whats-new/2019/10/amazon-ecs-now-supports-ecs-image-sha-tracking/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p>\n", "answers": ["<p>When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time</p>", "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an SSM Run Command. That command will clear the local Docker cache and stop the task</p>", "<p>After the deploy step in CodePipeline is done, include a Custom Step using a Lambda function that triggers an AWS Lambda function. That function will SSH onto your ECS instances and clear the local Docker cache and stop the task</p>", "<p>When creating a new task definition for your ECS service, ensure to add the <code>latest</code> tag in the full image name so that ECS pulls the correct image every time</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "As a DevOps Engineer at a social media company, you have implemented a CICD pipeline that takes code from a CodeCommit repository, builds it using CodeBuild thanks to the instructions in the local Dockerfile, and then pushes to ECR at 123456789.dkr.ecr.region.amazonaws.com/my-web-app. The last step of your CICD pipeline is to deploy to the application to your ECS cluster. It seems that while you do so, the application is only partly updated on some ECS instances which are running an older version of your image. You have found that terminating the instance or clearing the local Docker cache fixes the issue, but would like to implement something more robust that provides visibility and identification to track where container images are deployed.\n\nHow should you implement a solution to address this issue?", "related_lectures": []}, {"_class": "assessment", "id": 67357332, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.</p>\n\n<p>Which of the following solutions would you recommend for the given requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the <code>configure</code> hook. So this option is the best fit for the given use-case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q32-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</strong> - As mentioned in the explanation above, the <code>setup</code> hook is only used when an instance is first created, so this option is incorrect.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</strong> - Lifecycle hooks on Auto Scaling Groups may seem like a good idea at first, but using AWS Lambda, the solution is not practicable as SSH'ing into the instance via Lambda will not work.</p>\n\n<p><strong>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</strong> - EC2 user-data scripts are only triggered on an instance's first launch, so this option just acts as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n", "answers": ["<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly</p>", "<p>Manage the EC2 instances using OpsWorks. Include a chef cookbook on the setup lifecycle event that will update the configuration file accordingly</p>", "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an AWS Lambda function. The Lambda function will issue an EC2 <code>DescribeInstances</code> API call and update the configuration file through SSH</p>", "<p>Manage the EC2 instances using an Auto Scaling Group. Include a lifecycle hook for the instance pending and termination that will trigger an EC2 user-data script on the EC2 instances. The script issues an EC2 <code>DescribeInstances</code> API call and update the configuration file locally</p>"]}, "correct_response": ["a"], "section": "Domain 2: Configuration Management and IaC", "question_plain": "A Big Data analytics company is operating a distributed Cassandra cluster on EC2. Each instance in the cluster must have a list of all the other instance's IP to function correctly, store in a configuration file. As a Devops Engineer at the company, you would like this solution to adapt automatically when newer EC2 instances join the cluster, or when some EC2 instances are terminated.\n\nWhich of the following solutions would you recommend for the given requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357252, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Internet-of-Things (IoT) solutions company has decided to release every single application as a Docker container and to use ECS classic (on EC2) as the container orchestration system and ECR as the Docker registry. Part of implementing a monitoring pipeline is to ensure all application logs can be stored in CloudWatch logs.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to provide the simplest possible instructions to accomplish this objective. What are these instructions?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</strong></p>\n\n<p>Here many solutions may work but we're looking for the simplest possible solution. The important thing to remember is that the ECS task definitions can include the <code>awslogs</code> driver and write to CloudWatch Logs natively. But the EC2 instance will be the one writing to CloudWatch, and therefore it must have an EC2 Instance Role with the appropriate permissions to write to CloudWatch. Your Amazon ECS container instances also require logs:CreateLogStream and logs:PutLogEvents permission on the IAM role with which you launch your container instances</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - As mentioned in the explanation above, you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</strong> - This is a roundabout way of getting the container logs to the CloudWatch Logs, so not the best fit for the given use-case.</p>\n\n<p><strong>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application's <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</strong> - Sidecar containers are a common software pattern that has been embraced by engineering organizations. It\u2019s a way to keep server-side architecture easier to understand by building with smaller, modular containers that each serve a simple purpose. Just like an application can be powered by multiple microservices, each microservice can also be powered by multiple containers that work together. A sidecar container is simply a way to move part of the core responsibility of a service out into a containerized module that is deployed alongside a core application container. This again seems to be a roundabout way of getting the container logs to the CloudWatch Logs, but it's not correct for the given use-case. You should note that you need to provide the appropriate permissions to the EC2 Instance Role and not to the IAM task role to write to CloudWatch logs.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html</a></p>\n", "answers": ["<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>", "<p>Create ECS task definitions for your applications, with a mapping of the <code>/var/log</code> directory onto the local filesystem of the EC2 instance. Install the CloudWatch Agent on the EC2 instance using user-data and track the <code>/var/log/containers</code> directory. Create an EC2 instance role with the necessary permissions to write to CloudWatch logs</p>", "<p>Create ECS task definitions that include the <code>awslogs</code> driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs</p>", "<p>Create ECS task definitions for your applications, with a sidecar container which contains the CloudWatch Agent tracking the <code>/var/log/containers</code> directory. Map the application's <code>/var/log</code> directory onto the sidecar filesystem. Set an IAM task role in the task definition with the necessary permissions to write to CloudWatch logs</p>"]}, "correct_response": ["c"], "section": "Domain 4: Monitoring and Logging", "question_plain": "An Internet-of-Things (IoT) solutions company has decided to release every single application as a Docker container and to use ECS classic (on EC2) as the container orchestration system and ECR as the Docker registry. Part of implementing a monitoring pipeline is to ensure all application logs can be stored in CloudWatch logs.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to provide the simplest possible instructions to accomplish this objective. What are these instructions?", "related_lectures": []}, {"_class": "assessment", "id": 67357214, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following would you suggest as the most effective solution?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p>For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail <code>validate-logs</code> command.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q34-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</strong> -  S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as \u201cwrite once read many\u201d (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information on who made the API calls.</p>\n\n<p><strong>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</strong> - AWS Config is used to track resource configuration over time. Although Config has integration with CloudTrail to show who made API calls, Config on its own won't give us the information of who made the API calls. Please note that while having a Glacier Lock Vault policy can help us guarantee that the files cannot be altered, it doesn't provide us the end-to-end guarantee that CloudTrail actually produced those files and then match them against a hash to ascertain that they have remained unaltered.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-cli.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n", "answers": ["<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file</p>", "<p>Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives file right away in Glacier. Implement a Glacier Vault Lock policy</p>", "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the configuration logs into S3 and use the log verification integrity API to verify the log files</p>", "<p>Turn on AWS account configuration tracking using AWS Config. Deliver the logs in an S3 bucket and choose a lifecycle policy that archives the files right away in Glacier. Implement a Glacier Vault Lock policy</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A financial services company has a solution in place to track all the API calls made by users, applications, and SDK within the AWS account. Recently, it has experienced a hack and could find a user amongst the logs that did some compromising API calls. The company wants to know with 100% certainty that the log files represent the correct sequence of events and have not been altered. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following would you suggest as the most effective solution?", "related_lectures": []}, {"_class": "assessment", "id": 67357296, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution to meet these requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</strong></p>\n\n<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p>\n\n<p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore we have to subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q35-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</strong> - The issue with this option is that the target for Kinesis Firehose is set as Amazon ES which is not a serverless service and requires provisioning.</p>\n\n<p><strong>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</strong> - If the log destination is a Lambda function, this could work, but it will be a problem as this Lambda function sends the data to Amazon ES, which is not a serverless service and requires provisioning.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p>\n", "answers": ["<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3</p>", "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift</p>", "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon ES</p>", "<p>Create a log destination in the centralized account, and create a log subscription on that destination. Create a Lambda function on that log subscription, and implement a script to send the data to Amazon ES</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A multi-national retail company is operating a multi-account strategy using AWS Organizations. Each account produces logs to CloudWatch Logs and the company would like to aggregate these logs under a single centralized account for archiving purposes. It needs the solution to be secure and centralized. The target destination for the logs should have little to no provisioning on the storage side.\n\nAs a DevOps Engineer, how would you implement a solution to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357278, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company is capturing metrics for its AWS services and applications using CloudWatch metrics. It needs to be able to go back up to 7 years in time for visualizing these metrics due to regulatory requirements. As a DevOps Engineer at the company, you have been tasked with designing a solution that will help the company comply with the regulations.</p>\n\n<p>Which of the following options requires the minimum development effort to address the given requirements?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</strong></p>\n\n<p>You can use metric streams to continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency. Supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations.</p>\n\n<p>You can create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3.  You can then use tools such as Amazon Athena to get insight into cost optimization, resource performance, and resource utilization for given data ranges. In additon, you can use a QuickSight dashboard to visualize the metrics.</p>\n\n<p>The Kinesis Data Firehose delivery stream must trust CloudWatch through an IAM role that has write permissions to Kinesis Data Firehose. These permissions can be limited to the single Kinesis Data Firehose delivery stream that the CloudWatch metric stream uses. The IAM role must trust the streams.metrics.cloudwatch.amazonaws.com service principal.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</strong></p>\n\n<p>A CloudWatch metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. For example, the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2. The data points themselves can come from any application or business activity from which you collect data.</p>\n\n<p>Metrics cannot be deleted, but they automatically expire after 15 months if no new data is published to them. Data points older than 15 months expire on a rolling basis; as new data points come in, data older than 15 months is dropped.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q36-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p>As the CloudWatch metrics can only be retained for 15 months, we need to use a CloudWatch Event rule and trigger a Lambda function to extract metrics and send them for long term retention to facilitate visual analysis. Here, the only solution that works end-to-end is to send the data to Amazon ES, and use Kibana to create graphs.</p>\n\n<p>Amazon Elasticsearch (ES) Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.</p>\n\n<p>How Amazon ElasticSearch Works:\n<img src=\"https://d1.awsstatic.com/re19/ESS_HIW.db99588614cbf44e2d62ef9c9c173ebfe41e2834.png\">\nvia - <a href=\"https://aws.amazon.com/elasticsearch-service/\">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p>ES is commonly deployed as part of the ELK stack which is an acronym used to describe a stack that comprises three popular open-source projects: Elasticsearch, Logstash, and Kibana. The ELK stack gives you the ability to aggregate logs from all your systems and applications, analyze these logs, and create visualizations for application and infrastructure monitoring, faster troubleshooting, security analytics, and more.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/Elasticsearch/Amazon%20ES%20ELK%20diagram.9d830908067fb7bedb52c6738126f2dfe18b611a.png\">\nvia - <a href=\"https://aws.amazon.com/elasticsearch-service/the-elk-stack/\">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n\n<p>As this option requires you to create a Lambda function that will execute a custom API to export the metrics into an ES cluster, so it's not the best fit for the given requirement as it involves significant development effort.</p>\n\n<p><strong>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable 'Extended Retention' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</strong> - This option has been added as a distractor as CloudWatch metrics do not have an 'Extended Retention' feature.</p>\n\n<p><strong>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</strong> - S3 based data can be integrated easily with QuickSight, however, CloudWatch dashboards can only consume CloudWatch metrics and NOT data/metrics from S3.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metric-Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-metric-streams-trustpolicy.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticsearch-service/\">https://aws.amazon.com/elasticsearch-service/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticsearch-service/the-elk-stack/\">https://aws.amazon.com/elasticsearch-service/the-elk-stack/</a></p>\n", "answers": ["<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in Amazon ES. Create a Kibana dashboard on top to visualize the metrics</p>", "<p>Create a CloudWatch dashboard on top of CloudWatch metrics. Enable 'Extended Retention' on CloudWatch metrics, and implement an AWS Config rule that checks for this setting. If the AWS Config rule is non-compliant, use an Auto Remediation to turn it back on</p>", "<p>Create a CloudWatch event rule to trigger every 15 minutes. The target of the rule should be a Lambda Function that will run an API call to export the metrics and put them in S3. Create a CloudWatch dashboard on top of the metrics in S3</p>", "<p>Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges</p>"]}, "correct_response": ["d"], "section": "Domain 4: Monitoring and Logging", "question_plain": "An analytics company is capturing metrics for its AWS services and applications using CloudWatch metrics. It needs to be able to go back up to 7 years in time for visualizing these metrics due to regulatory requirements. As a DevOps Engineer at the company, you have been tasked with designing a solution that will help the company comply with the regulations.\n\nWhich of the following options requires the minimum development effort to address the given requirements?", "related_lectures": []}, {"_class": "assessment", "id": 67357202, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global financial services company manages over 100 accounts using AWS Organizations and it has recently come to light that several accounts and regions did not have AWS CloudTrail enabled. It also wants to be able to track the compliance of the CloudTrail enablement as a dashboard, and automatically be alerted in case of issues. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How would you go about implementing a solution for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong></p>\n\n<p>CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>For the given use-case, we need to enable CloudTrail and AWS Config in all accounts and all regions. For this, we'll need separate StackSets to create CloudTrail and enable Config in all accounts and all regions. Note that we'll also need an AWS Config aggregator in a centralized account. Finally, compliance breaches would generate CloudWatch events that can be subscribed by a Lambda function to further send out notifications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - The issue with this option is that CloudFormation template is being used only in a centralized account to enable AWS Config, whereas the correct solution must leverage a StackSet to enable Config in all accounts and all regions.</p>\n\n<p><strong>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</strong> - This option has been added as a distractor. There is no such thing as an SQS topic.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n", "answers": ["<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>", "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications</p>", "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create one CloudFormation template in a centralized account to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SNS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>", "<p>Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy it in all your accounts and regions under the AWS organization. Create another StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance across all the other accounts. Create an SQS topic to get notifications when compliance is breached, and subscribe a Lambda function to it, that will send out these notifications</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A global financial services company manages over 100 accounts using AWS Organizations and it has recently come to light that several accounts and regions did not have AWS CloudTrail enabled. It also wants to be able to track the compliance of the CloudTrail enablement as a dashboard, and automatically be alerted in case of issues. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow would you go about implementing a solution for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357266, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.</p>\n\n<p>As a DevOps Engineer, how can you implement a solution for this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p>You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the <code>cloudtrail-enabled</code> Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use <code>cloudtrail-security-trail-enabled</code> Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q38-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</strong></p>\n\n<p><strong>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong></p>\n\n<p>IAM users in a group with a deny policy sounds like a great idea at first, but then you have to remember you can create IAM roles, and they won't have that restriction, and as such you will be able to assume these roles and then issue API calls on CloudTrail to de-activate it. This solution won't work and therefore both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</strong> - You need to have an AWS Config rule to maintain auditability and track compliance over time, as using the Lambda function to trigger an API call would tell you about the CloudTrail status only at that point in time.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/config/faq/\">https://aws.amazon.com/config/faq/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-enabled.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html\">https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-security-trail-enabled.html</a></p>\n", "answers": ["<p>Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail</p>", "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create an AWS Config rule that tracks if every user is in that IAM group. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will add users to the 'everyone' group automatically</p>", "<p>Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>", "<p>Place all your AWS IAM users under an IAM group named 'everyone'. Create an IAM deny policy on that group to prevent users from using the <code>DeleteTrail</code> API. Create a CloudWatch Event rule that will trigger a Lambda function every 5 minutes. That Lambda function will check if CloudTrail is enabled using an API call and enable it back if necessary</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A cyber forensics company would like to ensure that CloudTrail is always enabled in its AWS account. It also needs to have an audit trail of the status for CloudTrail. In the case of compliance breaches, the company would like to automatically resolve them.\n\nAs a DevOps Engineer, how can you implement a solution for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357190, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.</p>\n\n<p>Which of the following options represents the most efficient solution in your opinion?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</strong></p>\n\n<p>You can search and filter the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created.</p>\n\n<p>You can use metric filters to search for and match terms, phrases, or values in your log events. When a metric filter finds one of the terms, phrases, or values in your log events, you can increment the value of a CloudWatch metric. For example, you can create a metric filter to search for and count the occurrence of the word ERROR in your log events.</p>\n\n<p>CloudWatch Logs Metric Filter Concepts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q39-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p>For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - You cannot directly set a CloudWatch Alarm as a target for a CloudWatch Logs Metric Filter. You will first need to create a metric filter which can then be used to create a CloudWatch metric to be eventually used in a CloudWatch Alarm.</p>\n\n<p><strong>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</strong> - The Elastic Beanstalk Health Metrics will not track the errors sent out to a log file, so it does not meet the requirements of the use-case. Besides, CloudWatch alarm cannot be used to work on top of the Elastic Beanstalk Health Metrics.</p>\n\n<p><strong>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</strong> - Implementing custom logic in your Node.js application may seem like a good idea, but then you have to remember that your application can be distributed amongst many servers with Beanstalk, and as such it will not be possible to track the \"100 errors\" across all instances using this methodology.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p>\n", "answers": ["<p>Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS</p>", "<p>Create a CloudWatch Logs Metric Filter with a target being a CloudWatch Alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>", "<p>Use the Elastic Beanstalk Health Metrics to monitor the application health and track the error rates. Create a CloudWatch alarm on top of the metric and use SNS as a target. Create an email subscription on SNS</p>", "<p>Implement custom logic in your Node.js application to track the number of errors it has received in the last 5 minutes. In case the number exceeds the threshold, use the <code>SetAlarmState</code> API to trigger a CloudWatch alarm. Make the CloudWatch Alarm use SNS as a target. Create an email subscription on SNS</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "You are working as a DevOps Engineer at an e-commerce company and have a deployed a Node.js application on Elastic Beanstalk. You would like to track error rates and specifically, you need to ensure by looking at the application log, that you do not have more than 100 errors in a 5 minutes interval. In case you are getting too many errors, you would like to be alerted via email.\n\nWhich of the following options represents the most efficient solution in your opinion?", "related_lectures": []}, {"_class": "assessment", "id": 67357334, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A gaming company would like to be able to receive near real-time notifications when the API call <code>DeleteTable</code> is invoked in DynamoDB.</p>\n\n<p>As a DevOps Engineer at the company, how would you implement this at a minimal cost?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</strong></p>\n\n<p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues.</p>\n\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\">\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. Rules in CloudWatch Events work only in the Region in which they are created. If you configure CloudTrail to track API calls in multiple Regions, and you want a rule-based on CloudTrail to trigger in each of those Regions, you must create a separate rule in each Region that you want to track.</p>\n\n<p>For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q40-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</strong> - A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. DynamoDB Streams do not capture <code>DeleteTable</code> API calls, they only capture item-level events.</p>\n\n<p><strong>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</strong> - Sending CloudTrail Logs to CloudWatch Logs and creating a filter on those will work but will be expensive, as we're streaming all the logs from CloudTrail just to extract a single event.</p>\n\n<p><strong>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</strong> - CloudTrail trails do not have event filters and cannot be directly sent to a Lambda function.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p>\n", "answers": ["<p>Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target</p>", "<p>Enable DynamoDB Streams, and have a Lambda function consuming that stream. Send alerts to SNS whenever a record is being deleted</p>", "<p>Send CloudTrail Logs to CloudWatch Logs and use an AWS Lambda function to be triggered on a CloudWatch Logs metrics filter. Use the Lambda function to send an SNS notification</p>", "<p>Create a CloudTrail event filter and hook it up to a Lambda function. Use the Lambda function to send an SNS notification</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A gaming company would like to be able to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.\n\nAs a DevOps Engineer at the company, how would you implement this at a minimal cost?", "related_lectures": []}, {"_class": "assessment", "id": 67357280, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.</p>\n\n<p>Which of the following options would you suggest to address the use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</strong></p>\n\n<p>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.</p>\n\n<p><img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Config_how-it-works.bd28728a9066c55d7ee69c0a655109001462e25b.png\">\nvia - <a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n\n<p>Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.</p>\n\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags make it easier to manage, search for, and filter resources. You can use tags to categorize your AWS resources in different ways, for example, by purpose, owner, or environment.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p>You can set up the Required-Tag managed rule for Config which requires up to 6 tags with optional values in a single rule. Previously, each rule accepted only a single tag/value combo. Additionally, the Required-Tag managed rule now accepts a comma-separated list of values for each checked tag. This allows for a rule to be compliant if any one of a supplied list of tags is present on the resource.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q41-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</strong> - AWS Service Catalog enabling AWS customers to create and deliver standardized services that provide the necessary control, while still empowering developers to choose the services that best fit their needs. You cannot use Service Catalog to get an inventory of all the resources in your account.</p>\n\n<p><strong>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</strong> - SSM inventory will only help with understanding what is installed on your managed instances. To view Systems Manager Inventory history and change tracking for all of your managed instances, you need to use AWS Config itself.</p>\n\n<p><strong>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</strong> - CloudTrail is used to track API calls, not resources. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/tagging.html\">https://docs.aws.amazon.com/config/latest/developerguide/tagging.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/\">https://aws.amazon.com/blogs/devops/aws-config-checking-for-compliance-with-new-managed-rule-options/</a></p>\n", "answers": ["<p>Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it</p>", "<p>Use AWS Service Catalog to get an inventory of all the resources in your account. Use the integrated dashboard feature to track compliance</p>", "<p>Use SSM to track resource groups without tags. Export that data using SSM inventory into S3, and build a QuickSight dashboard</p>", "<p>Track all your resources with AWS CloudTrail. Output the data in S3 and create a Quicksight dashboard</p>"]}, "correct_response": ["a"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A multi-national retail company has defined tagging guidelines and standard for all its resources in AWS and would like to create a dashboard to visualize the compliance of all the resources with the ability to find out the non-compliant resources. The company has hired you as an AWS Certified DevOps Engineer Professional to develop a solution for this requirement.\n\nWhich of the following options would you suggest to address the use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357310, "assessment_type": "multi-select", "prompt": {"question": "<p>A social media company has multiple EC2 instances that are behind an Auto Scaling group (ASG) and you would like to retrieve all the log files within the instances before they are terminated. You would like to also build a metadata index of all the log files so you can efficiently find them by instance id and date range.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend to address the given requirements? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an ASG launches or terminates them. For example, when a scale-in event occurs, the terminating instance is first deregistered from the load balancer (if the Auto Scaling group is being used with Elastic Load Balancing). Then, a lifecycle hook pauses the instance before it is terminated. While the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>You can use a CloudWatch Events rule to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to CloudWatch Events. The event contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action. Finally, the Lambda function can invoke an SSM Run Command to send the log files from the EC2 instance to S3. SSM Run Command lets you remotely and securely manage the configuration of your managed instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i2.jpg\"></p>\n\n<p><strong>Create a Lambda function that is triggered by S3 events for <code>PUT</code>. Write to the DynamoDB table</strong></p>\n\n<p>You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted. Amazon S3 invokes your function asynchronously with an event that contains details about the object. The Lambda would further write the event information into the DynamoDB table.</p>\n\n<p><strong>Create a DynamoDB table with a primary key of instance-id and a sort key of datetime</strong></p>\n\n<p>When you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. For the given use-case, you need to set the primary key as a combination of partition key of instance-id and a sort key of datetime as we are looking for a specific instance id and a date range.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q42-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to CloudWatch Logs. Create a log subscription to send it to Firehose and then S3</strong> - We must send the log files to S3 directly from the EC2 instance instead of through CloudWatch, as we're doing a one time dump of them. CloudWatch Logs are a good solution for streaming logs as they are created.</p>\n\n<p><strong>Create a Lambda function that is triggered by CloudWatch Events for <code>PUT</code>. Write to the DynamoDB table</strong> - We need to have the Lambda function triggered by S3 events instead of CloudWatch Events, as for CloudWatch Events we would need to also have a CloudTrail trail recording action on the specific S3 bucket.</p>\n\n<p><strong>Create a DynamoDB table with a primary key of datetime and a sort key of instance-id</strong> - As mentioned in the explanation above, since the use-case requires looking up for a specific instance id and a date range, you should use instance-id as the Partition Key and datetime as the Sort Key. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey</a></p>\n", "answers": ["<p>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to S3</p>", "<p>Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to CloudWatch Logs. Create a log subscription to send it to Firehose and then S3</p>", "<p>Create a Lambda function that is triggered by S3 events for <code>PUT</code>. Write to the DynamoDB table</p>", "<p>Create a Lambda function that is triggered by CloudWatch Events for <code>PUT</code>. Write to the DynamoDB table</p>", "<p>Create a DynamoDB table with a primary key of instance-id and a sort key of datetime</p>", "<p>Create a DynamoDB table with a primary key of datetime and a sort key of instance-id</p>"]}, "correct_response": ["a", "c", "e"], "section": "Domain 4: Monitoring and Logging", "question_plain": "A social media company has multiple EC2 instances that are behind an Auto Scaling group (ASG) and you would like to retrieve all the log files within the instances before they are terminated. You would like to also build a metadata index of all the log files so you can efficiently find them by instance id and date range.\n\nAs a DevOps Engineer, which of the following options would you recommend to address the given requirements? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 67357290, "assessment_type": "multi-select", "prompt": {"question": "<p>The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.</p>\n\n<p>Which of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</strong></p>\n\n<p>SSM Agent is an Amazon software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources. The agent processes requests from the Systems Manager service in the AWS Cloud, and then run them as specified in the request. SSM Agent then sends status and execution information back to the Systems Manager service by using the Amazon Message Delivery Service (service prefix: ec2messages).</p>\n\n<p>SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated.</p>\n\n<p>Sample Inventory Cards:\n<img src=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/images/inventory-cards.png\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><strong>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</strong></p>\n\n<p>Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</strong> - SSM does not have any native capability to find out which instances are not currently tracked by the SSM service.</p>\n\n<p><strong>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</strong> - You can use SSM Automation to build Automation workflows to configure and manage instances and AWS resources. You can also create custom workflows or use pre-defined workflows maintained by AWS. For the given requirement, SSM Automation could be used to get the list of packages but it would require a lot of manual work, so it is not the best fit for the given use-case.</p>\n\n<p><strong>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</strong> - Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n", "answers": ["<p>Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3</p>", "<p>Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM</p>", "<p>Use an SSM Run Command to have the SSM service find which instances are not currently tracked by SSM</p>", "<p>Install the SSM agent on the instances. Run an SSM Automation during maintenance windows to get the list of all the packages using <code>yum list installed</code>. Write the output to Amazon S3</p>", "<p>Use AWS Inspector to track the installed package list on your EC2 instances. Visualize the metadata directly in the AWS Inspector Insights console</p>"]}, "correct_response": ["a", "b"], "section": "Domain 4: Monitoring and Logging", "question_plain": "The compliance department at a Wall Street trading firm has hired you as an AWS Certified DevOps Engineer Professional to help with several strategic DevOps initiatives. The department has asked you to regularly generate the list of all the software packages installed on the EC2 instances. The solution needs to be able to extend to future instances in the AWS account and send notifications if the instances are not set up correctly to track their software.\n\nWhich of the following options are the best-fit solutions that require the least effort to meet the given requirements? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357218, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>How would you set up the on-premise server to achieve this objective?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p>AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status.</p>\n\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\">\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n\n<p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account.</p>\n\n<p>To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances.</p>\n\n<p>In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.</p>\n\n<p>After you finish configuring your servers and VMs for Systems Manager, your hybrid machines are listed in the AWS Management Console and described as managed instances. Amazon EC2 instances configured for Systems Manager are also described as managed instances. In the console, however, the IDs of your hybrid instances are distinguished from Amazon EC2 instances with the prefix \"mi-\". Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong> - As mentioned in the explanation earlier, the on-premise instances use the prefix \"mi-\" whereas the Amazon EC2 instance IDs use the prefix \"i-\".</p>\n\n<p><strong>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</strong></p>\n\n<p><strong>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</strong></p>\n\n<p>Both these options suggest using the Access Key ID and the Secret Access Key ID to register your on-premise servers which is considered a bad practice from a security perspective. Instead, you should use an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-managed-instance-activation.html</a></p>\n\n<p><a href=\"#\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p>\n", "answers": ["<p>Create an IAM Service Role for instances to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>", "<p>Create an IAM Service Role for each instance to be able to call the <code>AssumeRole</code> operation on the SSM service. Generate a unique activation code and activation ID for each on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>", "<p>Create an IAM User for each on-premise server to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console</p>", "<p>Create an IAM User for all on-premise servers to be able to call the <code>AssumeRole</code> operation on the SSM service. Using the Access Key ID and the Secret Access Key ID, use the AWS CLI to register your on-premise servers. They will appear with the prefix 'i-' in your SSM console</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "An industrial appliances company would like to take advantage of AWS Systems Manager to manage their on-premise instances and their EC2 instances. This will allow them to run some SSM RunCommand across their hybrid fleet. The company would also like to effectively manage the size of the fleet. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.\n\nHow would you set up the on-premise server to achieve this objective?", "related_lectures": []}, {"_class": "assessment", "id": 67357302, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.</p>\n\n<p>Which of the following options represents the BEST solution to meet this requirement?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong></p>\n\n<p>SSM Patch Manager automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p>Patch Manager provides predefined patch baselines for each of the operating systems supported by Patch Manager. You can use these baselines as they are currently configured (you can't customize them) or you can create your own custom patch baselines. Custom patch baselines allow you greater control over which patches are approved or rejected for your environment.</p>\n\n<p>When you use the default repositories configured on an instance for patching operations, Patch Manager scans for or installs security-related patches. This is the default behavior for Patch Manager.</p>\n\n<p>On Linux systems, however, you can also use Patch Manager to install patches that are not related to security, or that are in a different source repository than the default one configured on the instance. You can specify alternative patch source repositories when you create a custom patch baseline. In each custom patch baseline, you can specify patch source configurations for up to 20 versions of a supported Linux operating system. You can then set up a weekly maintenance window and include the Run Command RunPatchBaseline.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q45-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - SSM Parameter Store is used to store parameter values but cannot write configuration files on EC2 instances (the EC2 instances would have to fetch the value from the Parameter Store instead).</p>\n\n<p><strong>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</strong> - Using chef cookbooks via OpsWorks may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p><strong>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</strong> - Using SSM RunCommand may work for what we need, but the Patch Manager of SSM is a better way of achieving this.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-baselines.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-alt-source-repository.html</a></p>\n", "answers": ["<p>Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>", "<p>Using SSM Parameter Store, configure the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>", "<p>Manage your instances with AWS OpsWorks. Define a maintenance window and define custom chef cookbooks for the 'configure' lifecycle hook that will patch the instances from the internal patch repositories. Schedule the window with a weekly recurrence</p>", "<p>Using SSM, do a RunCommand to install the custom repositories in the OS' internal configuration files. Use the Default Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "An e-commerce company would like to automate the patching of their hybrid fleet and distribute some patches through their internal patch repositories every week. As a DevOps Engineer at the company, you have been tasked to implement this most efficiently.\n\nWhich of the following options represents the BEST solution to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357286, "assessment_type": "multi-select", "prompt": {"question": "<p>An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.</p>\n\n<p>Which of the following options would you recommend for this use-case? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an SSM Automation document to create the AMI in a repeatable manner</strong></p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</strong></p>\n\n<p>The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</strong> - The AMI ID is region-scoped and the AMI must be copied across regions for the solution to work. This option only copies the value of the AMI ID across regions but the AMI itself stays in one region. So this option is incorrect.</p>\n\n<p><strong>Use AWS Inspector to create a patched AMI using the latest working AMI</strong> - AWS Inspector can be leveraged to analyze EC2 instance OS and network vulnerabilities. You cannot use Inspector to create a patched AMI.</p>\n\n<p><strong>Use AWS Lambda to create a patched AMI using the latest working AMI</strong> - AWS Lambda cannot be used to create AMIs, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-ami-automation/</a></p>\n", "answers": ["<p>Create an SSM Automation document to create the AMI in a repeatable manner</p>", "<p>Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions</p>", "<p>Store the AMI ID in the SSM parameter store in one region, and create a Step Function that copies the value of that AMI ID across all the other regions. Use the same parameter store name so it can be re-used across regions</p>", "<p>Use AWS Inspector to create a patched AMI using the latest working AMI</p>", "<p>Use AWS Lambda to create a patched AMI using the latest working AMI</p>"]}, "correct_response": ["a", "b"], "section": "Domain 6: Security and Compliance", "question_plain": "An IT company is deploying a Python Flask based application and would like to ensure that it has a base AMI that contains the necessary Python runtime, as well as OS patches. That AMI must be used able to be referenced programmatically from across all regions in your account in a scalable way. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution to address this requirement.\n\nWhich of the following options would you recommend for this use-case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357306, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.</p>\n\n<p>How should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</strong></p>\n\n<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your EBS volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules.</p>\n\n<p>AWS Config uses Amazon SNS to deliver notifications to subscription endpoints. These notifications provide the delivery status for configuration snapshots and configuration histories, and they provide each configuration item that AWS Config creates when the configurations of recorded AWS resources change. AWS Config also sends notifications that show whether your resources are compliant with your rules. SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and NOT selectively for a given rule.</p>\n\n<p>AWS Config has a managed rule to check for EBS volume encryption. For the given use-case, you need to isolate alerts for this managed rule, so you have to use CloudWatch Events which can then have a specific SNS topic as a target for alerting.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q47-i2.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p><strong>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</strong></p>\n\n<p>As mentioned in the explanation above, SNS topics in Config can only be used to stream all the notifications and configuration changes. To isolate alerts for a single rule, you have to use CloudWatch Events. Therefore both these options are incorrect.</p>\n\n<p><strong>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</strong> - Using AWS Lambda may work, but it will not provide you the auditing capability that AWS Config provides (a timeline dashboard with compliance over time).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html\">https://docs.aws.amazon.com/config/latest/developerguide/encrypted-volumes.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/\">https://aws.amazon.com/premiumsupport/knowledge-center/config-resource-non-compliant/</a></p>\n", "answers": ["<p>Create an AWS Config managed rule checking for EBS volume encryption. Connect the rule to an SNS topic to provide alerting</p>", "<p>Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting</p>", "<p>Create an AWS Config custom rule checking for the EC2 instances, and their EBS attachments. Connect the rule to an SNS topic to provide alerting</p>", "<p>Create an AWS Lambda Function that is triggered by a CloudWatch Event rule. The rule is monitoring for new EBS volumes being created. The Lambda function should send a notification to SNS in case of a compliance check</p>"]}, "correct_response": ["b"], "section": "Domain 6: Security and Compliance", "question_plain": "A health-care services company has strong regulatory requirements and it has come to light recently that some of the EBS volumes have not been encrypted. It is necessary for the company to monitor and audit compliance over time and alert the corresponding teams if unencrypted EBS volumes are detected.\n\nHow should a DevOps Engineer implement an alert for the unencrypted EBS volumes with the least administrative overhead?", "related_lectures": []}, {"_class": "assessment", "id": 67357228, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.</p>\n\n<p>As a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</strong></p>\n\n<p>AWS Service Catalog allows IT administrators to create, manage, and distribute catalogs of approved products to end-users, who can then access the products they need in a personalized portal. Administrators can control which users have access to each product to enforce compliance with organizational business policies.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p>A product is a service or application for end-users. A portfolio is a collection of products, with configuration information that determines who can use those products and how they can use them. A catalog is a collection of products that the administrator creates, adds to portfolios, and provides updates for using AWS Service Catalog. To create a Service Catalog product, you first need to create an AWS CloudFormation template by using an existing AWS CloudFormation template or creating a custom template. Then you can use the AWS Service Catalog console to upload the template and create the product.</p>\n\n<p>Therefore, for the given use-case, we need to use Service Catalog as it was precisely designed for that purpose and give users only access to the stack they should be able to create in Service Catalog.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q48-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</strong> - If you let IAM users use the CloudFormation service directly, they will have the power to create any resource through their permissions. You cannot restrict templates using IAM policies in CloudFormation.</p>\n\n<p><strong>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</strong> - AWS Config Rules would be a way to \"monitor\" the situation but not prevent resources from being created the wrong way.</p>\n\n<p><strong>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</strong> - An IAM policy cannot have a \"conditional approval\", so this option is a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/servicecatalog/faqs/\">https://aws.amazon.com/servicecatalog/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/\">https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/</a></p>\n", "answers": ["<p>Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services</p>", "<p>Define commonly used architectures as CloudFormation templates. Place the IAM users into a beginner group and allow the users to only launch stacks from these CloudFormation stacks, while restricting any write access to other services</p>", "<p>Create AWS Config custom rules that will check for the compliance of your company's resources thanks to a Lambda Function. Update the Lambda Function over time while your company improves its architectural and tagging rules. Provide IAM users full access to AWS</p>", "<p>Place the beginner IAM users into a group and create an IAM policy that requires conditional approvals from senior DevOps engineers upon resource creation. Hook an SNS topic into the IAM approval channel</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "A retail company is finishing its migration to AWS and realizes that while some employees have passed the AWS Certified DevOps Engineer Professional certification and know AWS very well, other ones are still beginning and haven't passed their Associate-level certifications yet. The company has established architectural and tagging specific internal rules and would like to minimize the risk of the AWS-beginner employees launching uncompliant resources.\n\nAs a DevOps Engineer, how should you implement this requirement while allowing the employees to create the resources they need?", "related_lectures": []}, {"_class": "assessment", "id": 67357292, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company is using security-hardened AMI due to strong regulatory compliance requirements. The company must be able to check every day for AMI vulnerabilities based on the newly disclosed ones through the common vulnerabilities and exposures (CVEs) program. Currently, all the instances are launched through an Auto Scaling group (ASG) leveraging the latest security-hardened AMI.</p>\n\n<p>As a DevOps Engineer, how can you implement this while minimizing cost and application disruption?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule, the target being a Step Function. The Step Function should launch an EC2 instance from the AMI and tag it with CheckVulnerabilities: True. The Step Function then starts an AMI assessment template using AWS Inspector and the above tag. Terminate the instance afterward</strong></p>\n\n<p>AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>A golden AMI is an AMI that contains the latest security patches, software, configuration, and software agents that you need to install for logging, security maintenance, and performance monitoring. A security best practice is to perform routine vulnerability assessments of your golden AMIs to identify if newly found vulnerabilities apply to them. If you identify a vulnerability, you can update your golden AMIs with the appropriate security patches, test the AMIs, and deploy the patched AMIs in your environment.</p>\n\n<p>You can create an EC2 instance from the golden AMI and then run an Amazon Inspector security assessment on the created instance. Amazon Inspector performs security assessments of Amazon EC2 instances by using AWS managed rules packages such as the Common Vulnerabilities and Exposures (CVEs) package.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q49-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/\">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p>\n\n<p>So to summarize, the most cost-effective and the least disruptive way to do an assessment is to create an EC2 instance from an AMI for that very purpose, run the assessment and then finally terminate the instance. Step Functions are perfect to orchestrate that workflow by targeting the instances tagged with CheckVulnerabilities: True.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule. Invoke a Lambda Function that will start an AWS Inspector Run directly from the AMI reference in the API call. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</strong> - AWS Inspector cannot run an assessment directly on an AMI, it will not launch an EC2 instance for you. Therefore, you need to make sure an EC2 instance is created in advance from that AMI, with the proper tag on the EC2 instance to match the assessment target.</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule. Make the target of the rule being AWS Inspector and pass some extra data in the rule using the AMI ID to inspect. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</strong> - AWS Inspector cannot run an assessment directly on an AMI, it will not launch an EC2 instance for you. Therefore, you need to make sure an EC2 instance is created in advance from that AMI, with the proper tag on the EC2 instance to match the assessment target.</p>\n\n<p><strong>Create a CloudWatch Event with a daily schedule, the target being a Lambda Function. Tag all the instances in your ASG with <code>CheckVulnerabilities: True</code>. The Lambda function should start an assessment in AWS Inspector targeting all instances having the tag</strong> - If you launch an assessment on all the instances in an ASG, it will be problematic from a cost perspective as you will be testing the same AMI for as many instances that are part of the ASG. This will also incur extra AWS Inspector charges.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/\">https://aws.amazon.com/blogs/security/how-to-set-up-continuous-golden-ami-vulnerability-assessments-with-amazon-inspector/</a></p>\n\n<p><a href=\"https://aws.amazon.com/step-functions/faqs/\">https://aws.amazon.com/step-functions/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cves.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_cves.html</a></p>\n", "answers": ["<p>Create a CloudWatch Event with a daily schedule, the target being a Step Function. The Step Function should launch an EC2 instance from the AMI and tag it with CheckVulnerabilities: True. The Step Function then starts an AMI assessment template using AWS Inspector and the above tag. Terminate the instance afterward</p>", "<p>Create a CloudWatch Event with a daily schedule. Invoke a Lambda Function that will start an AWS Inspector Run directly from the AMI reference in the API call. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</p>", "<p>Create a CloudWatch Event with a daily schedule. Make the target of the rule being AWS Inspector and pass some extra data in the rule using the AMI ID to inspect. AWS Inspector will automatically launch an instance and terminate it upon assessment completion</p>", "<p>Create a CloudWatch Event with a daily schedule, the target being a Lambda Function. Tag all the instances in your ASG with <code>CheckVulnerabilities: True</code>. The Lambda function should start an assessment in AWS Inspector targeting all instances having the tag</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "A financial services company is using security-hardened AMI due to strong regulatory compliance requirements. The company must be able to check every day for AMI vulnerabilities based on the newly disclosed ones through the common vulnerabilities and exposures (CVEs) program. Currently, all the instances are launched through an Auto Scaling group (ASG) leveraging the latest security-hardened AMI.\n\nAs a DevOps Engineer, how can you implement this while minimizing cost and application disruption?", "related_lectures": []}, {"_class": "assessment", "id": 67357284, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The technology team at a leading bank is using software that has a license type that gets billed based on the number of CPU sockets that are being used. The team would like to ensure that they are using the most appropriate EC2 launch mode and create a compliance dashboard that highlights any violation of that decision. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend as the best fit?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong></p>\n\n<p>An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated for your use. When you bring your own licenses to Amazon EC2 Dedicated Hosts, you can let AWS take care of all these administrative tasks on your behalf. AWS gives administrators the option to perform a one-time onboarding set up in AWS License Manager.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/dedicated-hosts/faqs/\">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p>To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts. Reserved Instances are here to save cost on a yearly utilization of EC2. Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone.</p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time. An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule that will check the application tag and ensure the instance is launched as a Dedicated Host.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q50-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> -  Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</strong> - Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p><strong>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</strong> - Service Catalog is used to create stacks backed by CloudFormation through a service portal. To track compliance over time, you must use AWS Config. Besides, Reserved Instances can only be used to save cost on a yearly utilization of EC2 for example. To get access to the CPU sockets for billing purposes, you need to use EC2 Dedicated Hosts.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/dedicated-hosts/faqs/\">https://aws.amazon.com/ec2/dedicated-hosts/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n", "answers": ["<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>", "<p>Launch the EC2 instances on Reserved Instances and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode</p>", "<p>Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>", "<p>Launch the EC2 instances on Reserved Instance and create a tag for the application. Deploy an AWS Service Catalog rule backed by a Lambda function to track that the application is always launched on an EC2 instance with the correct mode</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "The technology team at a leading bank is using software that has a license type that gets billed based on the number of CPU sockets that are being used. The team would like to ensure that they are using the most appropriate EC2 launch mode and create a compliance dashboard that highlights any violation of that decision. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following solutions would you recommend as the best fit?", "related_lectures": []}, {"_class": "assessment", "id": 67357210, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data analytics company would like to create an automated solution to be alerted in case of EC2 instances being under-utilized for over 24 hours in order to save some costs. The solution should require a manual intervention of an operator validating the assessment before proceeding for instance termination.</p>\n\n<p>As a DevOps Engineer, how would you implement a solution with the LEAST development effort?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Create a CloudWatch Event that tracks the events created by Trusted Advisor and use a Lambda Function as a target for that event. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong></p>\n\n<p>Trusted Advisor inspects your AWS infrastructure across all AWS Regions, and then presents a summary of check results. It recommends stopping or terminating EC2 instances with low utilization. You can also choose to scale your instances using Amazon EC2 Auto Scaling.</p>\n\n<p>Trusted Advisor cost optimization check allows you to check EC2 instances that were running at any time during the last 14 days and alerts you if the daily CPU utilization was 10% or less and network I/O was 5 MB or less on 4 or more days. Running instances generate hourly usage charges. Estimated monthly savings are calculated by using the current usage rate for On-Demand Instances and the estimated number of days the instance might be underutilized.</p>\n\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions. Finally, SSM Automation can have a manual approval step and terminate instances.</p>\n\n<p>Monitoring Trusted Advisor check results with Amazon CloudWatch Events:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q51-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p>\n\n<p>Sample CloudWatch Event for Trusted Advisor check for Low Utilization Amazon EC2 Instances:</p>\n\n<pre><code>{\n  \"version\": \"0\",\n  \"id\": \"8dee56b0-b19f-441a-a05c-aa26e583c6c4\",\n  \"detail-type\": \"Trusted Advisor Check Item Refresh Notification\",\n  \"source\": \"aws.trustedadvisor\",\n  \"account\": \"123456789012\",\n  \"time\": \"2016-11-13T13:31:34Z\",\n  \"region\": \"us-east-1\",\n  \"resources\": [],\n  \"detail\": {\n    \"check-name\": \"Low Utilization Amazon EC2 Instances\",\n    \"check-item-detail\": {\n      \"Day 1\": \"0.0%  0.00MB\",\n      \"Day 2\": \"0.0%  0.00MB\",\n      \"Day 3\": \"0.0%  0.00MB\",\n      \"Region/AZ\": \"eu-central-1a\",\n      \"Estimated Monthly Savings\": \"$10.80\",\n      \"14-Day Average CPU Utilization\": \"0.0%\",\n      \"Day 14\": \"0.0%  0.00MB\",\n      \"Day 13\": \"0.0%  0.00MB\",\n      \"Day 12\": \"0.0%  0.00MB\",\n      \"Day 11\": \"0.0%  0.00MB\",\n      \"Day 10\": \"0.0%  0.00MB\",\n      \"14-Day Average Network I/O\": \"0.00MB\",\n      \"Number of Days Low Utilization\": \"14 days\",\n      \"Instance Type\": \"t2.micro\",\n      \"Instance ID\": \"i-917b1a5f\",\n      \"Day 8\": \"0.0%  0.00MB\",\n      \"Instance Name\": null,\n      \"Day 9\": \"0.0%  0.00MB\",\n      \"Day 4\": \"0.0%  0.00MB\",\n      \"Day 5\": \"0.0%  0.00MB\",\n      \"Day 6\": \"0.0%  0.00MB\",\n      \"Day 7\": \"0.0%  0.00MB\"\n    },\n    \"status\": \"WARN\",\n    \"resource_id\": \"arn:aws:ec2:eu-central-1:123456789012:instance/i-917b1a5f\",\n    \"uuid\": \"6ba6d96a-d3dd-4fca-8020-350bbee4719c\"\n  }\n}\n</code></pre>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Connect Trusted Advisor to an SNS topic for that check and use a Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - As mentioned in the explanation above, you need to use CloudWatch Events to track the events for a particular rule and NOT SNS.</p>\n\n<p><strong>Create a CloudWatch Event rule that triggers every 5 minutes and use a Lambda function as a target. The Lambda function should issue API calls to AWS CloudWatch Metrics and store the information in DynamoDB. Use a DynamoDB Stream to detect a stream of the low-utilized event for a period of 24 hours and trigger a Lambda function. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - The workflow using Lambda as described in this option will involve significant development effort. Also, this option uses resources such as DynamoDB streams which are really not required to build a solution.</p>\n\n<p><strong>Create a CloudWatch Alarm tracking the minimal CPU utilization across all your EC2 instances. Connect the CloudWatch Alarm to an SNS topic and use the Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</strong> - CloudWatch Alarm won't work as it won't allow you to track the CPU utilization of each individual instance if you create one aggregated one tracking the minimal CPU utilization. Side note, it'll be very expensive to create an Alarm for each EC2 instance as well.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n", "answers": ["<p>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Create a CloudWatch Event that tracks the events created by Trusted Advisor and use a Lambda Function as a target for that event. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>", "<p>Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Connect Trusted Advisor to an SNS topic for that check and use a Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>", "<p>Create a CloudWatch Event rule that triggers every 5 minutes and use a Lambda function as a target. The Lambda function should issue API calls to AWS CloudWatch Metrics and store the information in DynamoDB. Use a DynamoDB Stream to detect a stream of the low-utilized event for a period of 24 hours and trigger a Lambda function. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>", "<p>Create a CloudWatch Alarm tracking the minimal CPU utilization across all your EC2 instances. Connect the CloudWatch Alarm to an SNS topic and use the Lambda Function as a subscriber to the SNS topic. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "A data analytics company would like to create an automated solution to be alerted in case of EC2 instances being under-utilized for over 24 hours in order to save some costs. The solution should require a manual intervention of an operator validating the assessment before proceeding for instance termination.\n\nAs a DevOps Engineer, how would you implement a solution with the LEAST development effort?", "related_lectures": []}, {"_class": "assessment", "id": 67357208, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.</p>\n\n<p>How can you do this most securely?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</strong></p>\n\n<p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.</p>\n\n<p>You can use Secrets Manager to natively rotate credentials for Amazon Relational Database Service (RDS), Amazon DocumentDB, and Amazon Redshift. You can extend Secrets Manager to rotate other secrets, such as credentials for Oracle databases hosted on EC2 or OAuth refresh tokens, by modifying sample AWS Lambda functions available in the Secrets Manager documentation.</p>\n\n<p>To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective. Access to Secrets Manager itself is regulated using an IAM role with the requisite policy. You must write this IAM policy permitting your application on EC2 instances to access specific secrets. Then, in the application source code, you can replace secrets in plain text with code to retrieve these secrets programmatically using the Secrets Manager APIs. To access the DynamoDB table, you should also add the appropriate policy to this IAM role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - As mentioned in the explanation above, Secrets Manager does NOT support DynamoDB, so this option is incorrect.</p>\n\n<p><strong>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</strong> - It's not recommended to store RDS credentials in a DynamoDB, as it can be accessed by everyone who has access to the underlying table. This constitutes a serious security threat.</p>\n\n<p><strong>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</strong> - Storing IAM user credentials in Secrets Manager is a distractor as IAM user credentials are not required to build a solution for this use-case. You can just use an IAM instance role for EC2 to access Secrets Manager.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/secrets-manager/faqs/\">https://aws.amazon.com/secrets-manager/faqs/</a></p>\n", "answers": ["<p>Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB</p>", "<p>Store the RDS credentials &amp; DynamoDB credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>", "<p>Store the RDS credentials in a DynamoDB table and create an IAM instance role for EC2 to access DynamoDB</p>", "<p>Store IAM user credentials &amp; RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager</p>"]}, "correct_response": ["a"], "section": "Domain 6: Security and Compliance", "question_plain": "The DevOps team at a retail company has deployed its flagship application on EC2 instances using CodeDeploy and uses an RDS PostgreSQL database to store the data, while it uses DynamoDB to store the user sessions. As the Lead DevOps Engineer at the company, you would like the application to securely access RDS &amp; DynamoDB.\n\nHow can you do this most securely?", "related_lectures": []}, {"_class": "assessment", "id": 67357244, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>An e-commerce company is managing its entire application stack and infrastructure using AWS OpsWorks Stacks. The DevOps team at the company has noticed that a lot of instances have been automatically replaced in the stack and the team would henceforth like to be notified via Slack notifications when these events happen.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following options would you implement to meet this requirement?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</strong></p>\n\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n\n<p>A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p>\n\n<p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes.</p>\n\n<p>OpsWorks Stacks provides an integrated management experience that spans the entire application lifecycle including resource provisioning, EBS volume setup, configuration management, application deployment, monitoring, and access control. You can send state changes in OpsWorks Stacks, such as instance stopped or deployment failed, to CloudWatch Events. The initiated_by field is only populated when the instance is in the requested, terminating, or stopping states. The initiated_by field can contain one of the following values.</p>\n\n<p>user - A user requested the instance state change by using either the API or AWS Management Console.</p>\n\n<p>auto-scaling - The AWS OpsWorks Stacks automatic scaling feature initiated the instance state change.</p>\n\n<p>auto-healing - The AWS OpsWorks Stacks automatic healing feature initiated the instance state change.</p>\n\n<p>For the given use-case, you need to use CloudWatch Events, and the value of <code>initiated_by</code> must be <code>auto-healing</code>. CloudWatch Events does not have a Slack integration, so you need to configure a Lambda function as the target for the CloudWatch rule which would in turn send the Slack notification.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q53-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</strong> -  This option is incorrect as <code>auto-scaling</code> is just a supported value but it's not meant for the healing events, instead, it is used for the scaling events.</p>\n\n<p><strong>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</strong> - Opsworks does not send notifications to SNS directly for auto-healing, so this option is incorrect.</p>\n\n<p><strong>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</strong> - <code>auto-scaling</code> is just a supported value but it's not meant for the healing events, instead, it is used for the scaling events. Besides, CloudWatch Events does not have a direct integration with Slack , so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/mt/how-to-set-up-aws-opsworks-stacks-auto-healing-notifications-in-amazon-cloudwatch-events/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-healing</code>. Target a Lambda function that will send notifications out to the Slack channel</p>", "<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Target a Lambda function that will send notifications out to the Slack channel</p>", "<p>Subscribe your OpsWorks auto-healing notifications to an SNS topic. Subscribe a Lambda function that will send notifications out to the Slack channel</p>", "<p>Create a CloudWatch Events rule for <code>aws.opsworks</code> and set the <code>initiated_by</code> field to <code>auto-scaling</code>. Enable the CloudWatch Event Slack integration for sending out the notifications</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "An e-commerce company is managing its entire application stack and infrastructure using AWS OpsWorks Stacks. The DevOps team at the company has noticed that a lot of instances have been automatically replaced in the stack and the team would henceforth like to be notified via Slack notifications when these events happen.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following options would you implement to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357226, "assessment_type": "multi-select", "prompt": {"question": "<p>A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.</p>\n\n<p>As a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></strong></p>\n\n<p>In a typical Kinesis Data Streams architecture, you have producers that continually push data to Kinesis Data Streams, and the consumers process the data in real-time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>Key concepts for Kinesis Data Streams:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p>The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data.</p>\n\n<p>For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric <code>MillisBehindLatest</code>, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q54-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n\n<p><strong>Increase the stream data retention period</strong></p>\n\n<p>The retention period is the length of time that data records are accessible after they are added to the stream. A stream\u2019s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to AWS Lambda</strong> - Migrating the application to AWS Lambda will not help with the processing time, as eventually, the same processing code would run under EC2 or Lambda.</p>\n\n<p><strong>Increase the number of shards in Kinesis to increase throughput</strong> - Increasing the number of shards in Kinesis can increase the total throughput of the stream, but this does not impact the processing performance of your processes (which is bound by what you do with the messages). Increasing the number of shards though would help you increase the number of processing processes in KCL if that was already an upper bound (but currently we only have one KCL process running so it's not running at capacity).</p>\n\n<p><strong>Decrease the numbers of shards in Kinesis to decrease the load</strong> - Decrease the number of shards would decrease the throughput but again would have no effect on processing applications regarding their performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html\">https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-kcl.html</a></p>\n", "answers": ["<p>Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric <code>MillisBehindLatest</code></p>", "<p>Increase the stream data retention period</p>", "<p>Migrate the application to AWS Lambda</p>", "<p>Increase the number of shards in Kinesis to increase throughput</p>", "<p>Decrease the numbers of shards in Kinesis to decrease the load</p>"]}, "correct_response": ["a", "b"], "section": "Domain 5: Incident and Event Response", "question_plain": "A Big Data analytics company has deployed a stream processing application using KCL to read records from Kinesis Data Streams configured with multiple shards. The application is running on one EC2 instance. It seems that the consuming application is lagging under a large load and therefore records are not processed in time and eventually dropped from the stream.\n\nAs a DevOps Engineer, you have been tasked with improving the reliability of this application with minimal changes, what should you do? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357234, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A retail company is storing the users' information along with their purchase history in a DynamoDB table and it has also enabled the DynamoDB Streams. Three use cases are implemented for this table: a Lambda function reads the stream to send emails for new users subscriptions, another Lambda function which sends an email after a user has done their first purchase and finally the last Lambda function which awards discounts to users every 10 purchase. When there is a high volume of data on your DynamoDB table, the Lambda functions are experiencing a throttling issue. As you plan on adding future Lambda functions to read from that stream, you need to update the existing solution.</p>\n\n<p>As a DevOps Engineer, which of the following options would you recommend?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a new Lambda function that will read from the stream and pass on the payload to SNS. Have the other three and upcoming Lambda functions directly read from the SNS topic</strong></p>\n\n<p>DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time. A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q55-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q55-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p>DynamoDB is integrated with AWS Lambda so that you can create triggers\u2014pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p>\n\n<p>If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p>\n\n<p>No more than two processes at most should be reading from the same streams shard at the same time. Having more than two readers per shard can result in throttling. Therefore, you need to use a fan-out pattern for this, SNS being perfect for that.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the RCUs on your DynamoDB table to avoid throttling issues</strong> - DynamoDB Streams operates asynchronously, so there is no performance impact on a table if you enable a stream. So, RCUs have no bearing on throttling issues and this option just acts as a distractor.</p>\n\n<p><strong>Create a DynamoDB DAX cluster to cache the reads</strong> - DAX won't help here, it's meant to improve reads on your DynamoDB table through a cache, and NOT for DynamoDB Streams.</p>\n\n<p><strong>Increase the memory on the Lambda function so that they have an increased vCPU allocation and process the data faster while making fewer requests to DynamoDB</strong> - The Lambda function memory won't help, the issue is that too many processes are reading from the same shard.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a new Lambda function that will read from the stream and pass on the payload to SNS. Have the other three and upcoming Lambda functions directly read from the SNS topic</p>", "<p>Increase the RCUs on your DynamoDB table to avoid throttling issues</p>", "<p>Create a DynamoDB DAX cluster to cache the reads</p>", "<p>Increase the memory on the Lambda function so that they have an increased vCPU allocation and process the data faster while making fewer requests to DynamoDB</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A retail company is storing the users' information along with their purchase history in a DynamoDB table and it has also enabled the DynamoDB Streams. Three use cases are implemented for this table: a Lambda function reads the stream to send emails for new users subscriptions, another Lambda function which sends an email after a user has done their first purchase and finally the last Lambda function which awards discounts to users every 10 purchase. When there is a high volume of data on your DynamoDB table, the Lambda functions are experiencing a throttling issue. As you plan on adding future Lambda functions to read from that stream, you need to update the existing solution.\n\nAs a DevOps Engineer, which of the following options would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 67357230, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>How can you improve the instance utilization while reducing cost and maintaining application availability?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</strong></p>\n\n<p>With target tracking scaling policies, you choose a scaling metric and set a target value. Application Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.</p>\n\n<p>Target tracking scaling policies for Amazon EC2 Auto Scaling:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action. For the given use-case, you can create two separate scheduled actions that take care of the required minimum capacity during both peak and off-peak times.</p>\n\n<p>Here, we need a scaling policy that tracks a good CPU usage of 75% and adjusts the minimum desired capacity through scheduled actions so it doesn't disrupt the number of EC2 instances negatively at any time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q56-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</strong></p>\n\n<p><strong>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</strong></p>\n\n<p>If a Lambda function terminates 9 instances because they're in an ASG, the desired capacity won't have changed and the ASG will re-create instances automatically. Therefore both these options are incorrect.</p>\n\n<p><strong>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</strong> - UpdatePolicy for CloudFormation cannot help define Scheduled Actions. There's a special ScheduledActions property for that.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that increases the number of minimum instances to 6 during peak times and a second scheduled action that reduces the number of minimum instances to 3 off-peak times</p>", "<p>Create a Lambda function that terminates 9 instances at the end of business hours. Create a second Lambda function that creates instances when peak time starts. Schedule the functions using CloudWatch Events</p>", "<p>Create a scaling policy that tracks the CPU utilization with a target of 75%. Create a scheduled action that invokes a Lambda function which will terminate 9 instances after peak times</p>", "<p>Use a CloudFormation UpdatePolicy to define how the Auto Scaling Group should behave off and on peaks. Ensure the ASG invokes the CloudFormation using SNS notifications relay</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A social media company is running its flagship application via an Auto-Scaling group (ASG) which has 15 EC2 instances spanning across 3 Availability Zones (AZs). The current average CPU utilization of the group sits at 15% off-peak time. During peak time, it goes all the way to 45%, and these peak times happen predictably during business hours. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nHow can you improve the instance utilization while reducing cost and maintaining application availability?", "related_lectures": []}, {"_class": "assessment", "id": 67357238, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As a DevOps Engineer at an e-commerce company, you have deployed a web application in an Auto Scaling group (ASG)  that is being distributed by an Application Load Balancer (ALB). The web application is using RDS Multi-AZ as a back-end and has been experiencing some issues to connect to the database. The health check implemented in the application currently returns an un-healthy status if the application cannot connect to the database. The ALB / ASG health check integration has been enabled, and therefore the ASG keeps on terminating instances right after they're done booting up.</p>\n\n<p>You need to be able to isolate one instance for troubleshooting for an undetermined amount of time, how should you proceed?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Set an instance in Standby right after it has launched</strong></p>\n\n<p>The Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered.</p>\n\n<p>The default health checks for an Auto Scaling group are EC2 status checks only. If you configure the Auto Scaling group to use ELB health checks, it considers the instance unhealthy if it fails either the EC2 status checks or the ELB health checks.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q57-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p><img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/auto_scaling_lifecycle.png\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p>\n\n<p>You can put an instance that is in the InService state into the Standby state, update or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q57-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Suspend the Launch process</strong> - Suspending the Launch process would prevent instances from being created, which wouldn't work here. Please note that suspending the terminate or health check processes may help the situation (but they're not options in this question)</p>\n\n<p><strong>Create an autoscaling hook for instance termination. Troubleshoot the instance while it is in the Terminating:Wait state</strong> - Auto Scaling Hooks may work but they come with a one-hour default timeout and therefore we may not get enough time to perform all the troubleshooting we need.</p>\n\n<p><strong>Enable termination protection for EC2</strong> - Termination protection prevents users from terminating an instance but doesn't prevent the ASG from terminating instances. For the instances in an Auto Scaling group, use Amazon EC2 Auto Scaling features to protect an instance when a scale-in event occurs. If you want to protect your instance from being accidentally terminated, use Amazon EC2 termination protection.</p>\n\n<p><img src=\"https://media.amazonwebservices.com/blog/2015/ec2_console_menu_set_scale_in_protection_1.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/\">https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroupLifecycle.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-enter-exit-standby.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/\">https://aws.amazon.com/blogs/aws/new-instance-protection-for-auto-scaling/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Set an instance in Standby right after it has launched</p>", "<p>Suspend the Launch process</p>", "<p>Create an autoscaling hook for instance termination. Troubleshoot the instance while it is in the Terminating:Wait state</p>", "<p>Enable termination protection for EC2</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "As a DevOps Engineer at an e-commerce company, you have deployed a web application in an Auto Scaling group (ASG)  that is being distributed by an Application Load Balancer (ALB). The web application is using RDS Multi-AZ as a back-end and has been experiencing some issues to connect to the database. The health check implemented in the application currently returns an un-healthy status if the application cannot connect to the database. The ALB / ASG health check integration has been enabled, and therefore the ASG keeps on terminating instances right after they're done booting up.\n\nYou need to be able to isolate one instance for troubleshooting for an undetermined amount of time, how should you proceed?", "related_lectures": []}, {"_class": "assessment", "id": 67357206, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The DevOps team at an analytics company is deploying an Apache Kafka cluster that contains 6 instances and is distributed across 3 Availability Zones (AZs). Apache Kafka is a stateful service and needs to store its data in an EBS volume. Therefore each instance must have the auto-healing capability and always attach the correct EBS volumes.</p>\n\n<p>As an AWS Certified DevOps Engineer Professional, which of the following solutions would you suggest for the given requirement?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFormation template with an ASG of min/max capacity of 1, and an EBS volume. Tag the ASG and EBS volume. Create a User Data script that will acquire the EBS volume at boot time. Use a master CloudFormation template and reference the nested template 6 times</strong></p>\n\n<p>You can use CloudFormation to create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.</p>\n\n<p>Auto Scaling group enables you to automatically scale Amazon EC2 instances, either with scaling policies or with scheduled scaling. Auto Scaling groups are collections of Amazon EC2 instances that enable automatic scaling and fleet management features, such as health checks and integration with Elastic Load Balancing.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q58-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html</a></p>\n\n<p>For the given use-case, you need to leverage CloudFormation to set up 6 ASGs of 1 instance each and EBS volumes with the appropriate tags and then use an EC2 user data script to attach the corresponding EBS volumes correctly.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group in CloudFormation with a min/max desired capacity of 6 instances spread across 3 AZs, and 6 EBS volumes also across the 3 AZs. Create a user data script so that instances launching from the ASG automatically acquire an available EBS volume in the corresponding AZ</strong> - If you use an ASG of 6 instances, this may seem like a good idea but then you may get into a situation where an AZ is down and 3 instances are created in the other 2 AZ. EBS volumes cannot transfer cross regions and you'll be stuck.</p>\n\n<p><strong>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, it will be automatically re-created by CloudFormation with the correct EBS attachment</strong> - If you define 6 instances and attachments in CloudFormation, in case an instance is terminated it won't come back automatically.</p>\n\n<p><strong>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, launch a drift detection in CloudFormation and then use CloudFormation remediation</strong> - If you define 6 instances and attachments in CloudFormation, in case an instance is terminated it won't come back automatically. Drift detection will allow you to see what has changed, but it will not allow you to fix it through CloudFormation.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-autoscaling.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudFormation template with an ASG of min/max capacity of 1, and an EBS volume. Tag the ASG and EBS volume. Create a User Data script that will acquire the EBS volume at boot time. Use a master CloudFormation template and reference the nested template 6 times</p>", "<p>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, it will be automatically re-created by CloudFormation with the correct EBS attachment</p>", "<p>Create an Auto Scaling Group in CloudFormation with a min/max desired capacity of 6 instances spread across 3 AZs, and 6 EBS volumes also across the 3 AZs. Create a user data script so that instances launching from the ASG automatically acquire an available EBS volume in the corresponding AZ</p>", "<p>Create 6 EC2 instances using CloudFormation with EBS volumes. Define the attachments in the CloudFormation template. If the EC2 instance is terminated, launch a drift detection in CloudFormation and then use CloudFormation remediation</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "The DevOps team at an analytics company is deploying an Apache Kafka cluster that contains 6 instances and is distributed across 3 Availability Zones (AZs). Apache Kafka is a stateful service and needs to store its data in an EBS volume. Therefore each instance must have the auto-healing capability and always attach the correct EBS volumes.\n\nAs an AWS Certified DevOps Engineer Professional, which of the following solutions would you suggest for the given requirement?", "related_lectures": []}, {"_class": "assessment", "id": 67357204, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with <code>AWS::RDS::DBInstance</code> and setup using Multi-AZ.</p>\n\n<p>You have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong></p>\n\n<p>You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn\u2019t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property <code>EngineVersion</code>, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p>You should also note that a Multi-AZ configuration does not prevent downtime during an upgrade. Multi-AZ is only recommended for a high availability use-case. However, in the case of a MySQL or MariaDB engine upgrade, Multi-AZ doesn\u2019t eliminate downtime. The slow shutdown and the physical changes made on the active server by the mysql_upgrade program require this downtime.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</strong> - You should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p><strong>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - If you update the <code>EngineVersion</code> property of an AWS::RDS::DBInstance resource type, AWS CloudFormation creates a new resource and replaces the current DB instance resource with the new one, so this option is incorrect.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q59-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n\n<p><strong>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</strong> - Also, you should remember that the property is <code>EngineVersion</code>, not <code>DBEngineVersion</code>, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/\">https://aws.amazon.com/blogs/database/best-practices-for-upgrading-amazon-rds-for-mysql-and-amazon-rds-for-mariadb/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.MySQL.html#USER_UpgradeDBInstance.MySQL.ReducedDowntime</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-engineversion</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>EngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>", "<p>Create an RDS Read Replica in a CloudFormation template by specifying <code>SourceDBInstanceIdentifier</code> and wait for it to be created. Afterward, upgrade the RDS Read Replica <code>DBEngineVersion</code> to the next major version. Then promote the Read Replica and use it as your new master database</p>", "<p>Upgrade the RDS database by updating the <code>EngineVersion</code> to the next major version, then run an UpdateStack Operation</p>", "<p>Upgrade the RDS database by updating the <code>DBEngineVersion</code> to the next major version, then run an UpdateStack Operation</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "As the Lead DevOps Engineer at an e-commerce company, you would like to upgrade the major version of your MySQL database, which is managed by CloudFormation with AWS::RDS::DBInstance and setup using Multi-AZ.\n\nYou have a requirement to minimize the downtime as much as possible, what steps should you take to achieve this?", "related_lectures": []}, {"_class": "assessment", "id": 67357298, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The development team at a social media company is using AWS CodeCommit to store code. As a Lead DevOps Engineer at the company, you have defined a company-wide rule so that the team should not be able to push to the master branch. You have added all the developers in an IAM group <code>developers</code> and attached the AWS managed IAM policy <code>arn:aws:iam::aws:policy/AWSCodeCommitPowerUser</code> to the group. This policy provides full access to AWS CodeCommit repositories but does not allow repository deletion, however, your developers can still push to the master branch.</p>\n\n<p>How should you prevent the developers from pushing to the master branch?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong></p>\n\n<p>Any CodeCommit repository user who has sufficient permissions to push code to the repository can contribute to any branch in that repository. You can configure a branch so that only some repository users can push or merge code to that branch. For example, you might want to configure a branch used for production code so that only a subset of senior developers can push or merge changes to that branch. Other developers can still pull from the branch, make their own branches, and create pull requests, but they cannot push or merge changes to that branch. You can configure this access by creating a conditional policy that uses a context key for one or more branches in IAM.</p>\n\n<p>For the given use-case, you need to add an extra policy with an explicit Deny. Please note an Explicit Deny always has priority over anything else.</p>\n\n<p>Limit pushes and merges to branches in AWS CodeCommit:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q60-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></strong> - This option has been added as a distractor since CodeCommit repository policies do not exist.</p>\n\n<p><strong>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</strong> - You cannot modify an AWS managed IAM policy, so this option is incorrect.</p>\n\n<p><strong>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</strong> - Although it would be cool, CodeCommit still does not have a pre-hook feature to integrate with Lambda.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Add a new IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>", "<p>Include a CodeCommit repository policy on each repository with an explicit Deny for <code>codecommit:GitPush</code></p>", "<p>Modify the AWS managed IAM policy attached to the group to Deny <code>codecommit:GitPush</code> with a condition on the master branch</p>", "<p>Include a git commit pre-hook that invokes a Lambda function and checks if the push is done to master</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "The development team at a social media company is using AWS CodeCommit to store code. As a Lead DevOps Engineer at the company, you have defined a company-wide rule so that the team should not be able to push to the master branch. You have added all the developers in an IAM group developers and attached the AWS managed IAM policy arn:aws:iam::aws:policy/AWSCodeCommitPowerUser to the group. This policy provides full access to AWS CodeCommit repositories but does not allow repository deletion, however, your developers can still push to the master branch.\n\nHow should you prevent the developers from pushing to the master branch?", "related_lectures": []}, {"_class": "assessment", "id": 67357322, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A cyber-security company has had a dubious distinction of their own AWS account credentials being put in public GitHub repositories. The company wants to implement a workflow to be alerted in case credentials are leaked, generate a report of API calls made recently using the credentials, and de-activate the credentials. All executions of the workflow must be auditable.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a robust solution for this requirement. Which of the following solutions would you implement?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>AWS monitors popular code repository sites for IAM access keys that have been publicly exposed. AWS Health generates an AWS_RISK_CREDENTIALS_EXPOSED event when an IAM access key has been publicly exposed on GitHub. A CloudWatch Events rule further detects this event and invokes a Step Function that orchestrates the automated workflow to delete the exposed IAM access key, and summarize the recent API activity for the exposed key. The workflow will also issue API calls to IAM, CloudTrail, and SNS. The AWS_RISK_CREDENTIALS_EXPOSED is exposed by the Personal Health Dashboard service.</p>\n\n<p>Mitigating security events using AWS Health and CloudTrail:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q61-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong></p>\n\n<p>As the way to react to that event is complex and may have retries, and you want to have a full audit trail of each workflow, you should use a Step Function instead of an AWS Lambda function. So both these options are incorrect.</p>\n\n<p><strong>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</strong> - AWS_RISK_CREDENTIALS_EXPOSED event is generated by AWS Health service and NOT CloudTrail, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html\">https://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>", "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>", "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Lambda Function that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>", "<p>Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the CloudTrail Service. Trigger a Lambda Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A cyber-security company has had a dubious distinction of their own AWS account credentials being put in public GitHub repositories. The company wants to implement a workflow to be alerted in case credentials are leaked, generate a report of API calls made recently using the credentials, and de-activate the credentials. All executions of the workflow must be auditable.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a robust solution for this requirement. Which of the following solutions would you implement?", "related_lectures": []}, {"_class": "assessment", "id": 67357328, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.</p>\n\n<p>How can you implement this efficiently and in a fail-safe way?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</strong></p>\n\n<p>Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.</p>\n\n<p>How Step Functions Work:\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - <a href=\"https://aws.amazon.com/step-functions/\">https://aws.amazon.com/step-functions/</a></p>\n\n<p>For the given use-case, you need to combine Step Functions, Lambda and CloudWatch Events into a single coherent solution. You can use the Step Functions to coordinate the business logic to automate the snapshot management flow with error handling, retry logic, and workflow logic all baked into the Step Functions definition. CloudWatch Events integrates with Step Functions and Lambda to let you execute your custom code when relevant events occur.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q62-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>For a deep-dive on this solution, highly recommend the following reference material:\n<a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</strong> - Creating an EC2 instance may work, but if it gets terminated we have to re-create a new one. Failure scenarios may be tough to analyze and having the audit trail in DynamoDB probably won't be easy to use.</p>\n\n<p><strong>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</strong> - Creating a CW event rule + Lambda function may work, but the Lambda function may have a timeout issue if the backup is taking longer than 15 minutes, and AWS Config cannot store the history of the execution. AWS Config only provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><strong>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</strong> - An SSM automation cannot contain complex logic to handle failures, although it would provide an execution history.</p>\n\n<p>An SSM Automation document defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs. A document contains one or more steps that run in sequential order. Each step is built around a single action. The output from one step can be used as input in a later step. The process of running these actions and their steps is called the automation workflow.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases</p>", "<p>Create an EC2 instance in the region where the EBS volume is. Create a CRON script that will invoke a Python script that performs all the steps and logic outlined above. For each step completion, write metadata to a DynamoDB table</p>", "<p>Create a CloudWatch Event rule that gets triggered every day. It triggers a Lambda function written in Python that performs all the steps and logic outlined above. Analyze the history of execution using AWS Config</p>", "<p>Create an SSM Automation that will perform each action. Add failure logic between steps to deal with conditional cases</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "As a DevOps Engineer at an IT company, you are looking to create a daily EBS backup workflow. That workflow must take an EBS volume, and create a snapshot from it. When the snapshot is created, it must be copied to another region. In case the other region is unavailable because of a disaster, then that backup should be copied to a third region. An email address must be notified of the final result. There's a requirement to keep an audit trail of all executions as well.\n\nHow can you implement this efficiently and in a fail-safe way?", "related_lectures": []}, {"_class": "assessment", "id": 67357250, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>A financial planning company runs a tax optimization application that allows people to enter their personal financial information and get recommendations. The company is committed to the maximum security for the Personally identifiable information (PII) data in S3 buckets, and as part of compliance requirements, it needs to implement a solution to be alerted in case of new PII and its access in S3.</p>\n\n<p>As an AWS Certified DevOps Engineer, which solution would you recommend such that it needs MINIMUM development effort?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</strong></p>\n\n<p>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3.</p>\n\n<p>How Macie Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/macie/Product-Page-Diagram_AWS-Macie@2x.369dcc5a001e7a44b121d65637ff82b60b809148.png\">\nvia - <a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n\n<p>For the given use-case, you can enable Macie on specific S3 buckets and then configure SNS notifications via CloudWatch events for Macie alerts.\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q63-i1.jpg\"></p>\n\n<p>For a deep-dive on how to query PII data using Macie, please refer to this excellent blog:\n<a href=\"https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/\">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.</p>\n\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><strong>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</strong> - Amazon Lambda + Sagemager might work but it requires significant development effort and probably won't yield excellent results.</p>\n\n<p><strong>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</strong> - S3 bucket policies cannot be used to analyze the data payload in a request. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/\">https://aws.amazon.com/blogs/security/how-to-query-personally-identifiable-information-with-amazon-macie/</a></p>\n\n<p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events</p>", "<p>Enable Amazon GuardDuty on the select S3 buckets. Setup alerting using CloudWatch Alarms</p>", "<p>Create an Amazon Lambda function that is integrated with Amazon Sagemaker to detect PII data. Integrate the Lambda function with S3 events for PUT requests</p>", "<p>Set up an S3 bucket policy that filters requests containing PII data using a conditional statement</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "A financial planning company runs a tax optimization application that allows people to enter their personal financial information and get recommendations. The company is committed to the maximum security for the Personally identifiable information (PII) data in S3 buckets, and as part of compliance requirements, it needs to implement a solution to be alerted in case of new PII and its access in S3.\n\nAs an AWS Certified DevOps Engineer, which solution would you recommend such that it needs MINIMUM development effort?", "related_lectures": []}, {"_class": "assessment", "id": 67357236, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>As the Lead DevOps Engineer at an analytics company, you are deploying a global application using a CICD pipeline comprising of AWS CodeCommit, CodeBuild, CodeDeploy and orchestrated by AWS CodePipeline. Your pipeline is currently setup in eu-west-1 and you would like to extend the pipeline to deploy your application in us-east-2. This will require a multi-step CodePipeline to be created there and invoked.</p>\n\n<p>How would you implement a solution to address this use-case?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include an S3 step to copy the artifacts being used by CodeDeploy to an S3 bucket in us-east-2. Make the CodePipeline in us-east-2 source files from S3</strong></p>\n\n<p>AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.</p>\n\n<p>CodePipeline Overview:\n<img src=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/images/PipelineFlow.png\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p>CodePipeline Key Concepts:</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts.html</a></p>\n\n<p>For the given use-case, you can use an S3 deploy step to copy artifacts into another bucket. Then CodePipeline in the other region will respond to an event and source the files from the other bucket and kickstart the deployment pipeline there.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q64-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodeDeploy step to deploy the application to the CodePipeline in us-east-2</strong> - CodeDeploy cannot deploy to AWS CodePipeline. CodeDeploy can only deploy to EC2, on-premise, Lambda, and ECS.</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodeCommit step to push the changes to the code into the master branch of another CodeCommit repository in us-east-2. Make the CodePipeline in us-east-2 source files from CodeCommit</strong> - CodePipeline can only source from CodeCommit, it cannot push commits to it.</p>\n\n<p><strong>At the end of the pipeline in eu-west-1, include a CodePipeline step to invoke the CodePipeline in us-east-2. Ensure the CodePipeline in us-east-2 has the necessary IAM permission to read the artifacts in S3 in eu-west-1</strong> - CodePipeline cannot invoke another CodePipeline directly. This is something you might be able to achieve using a Custom Action and a Lambda function, but you would need to make sure artifacts are copied locally as well.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/welcome-introducing.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html</a></p>\n", "relatedLectureIds": "", "answers": ["<p>At the end of the pipeline in eu-west-1, include an S3 step to copy the artifacts being used by CodeDeploy to an S3 bucket in us-east-2. Make the CodePipeline in us-east-2 source files from S3</p>", "<p>At the end of the pipeline in eu-west-1, include a CodeDeploy step to deploy the application to the CodePipeline in us-east-2</p>", "<p>At the end of the pipeline in eu-west-1, include a CodeCommit step to push the changes to the code into the master branch of another CodeCommit repository in us-east-2. Make the CodePipeline in us-east-2 source files from CodeCommit</p>", "<p>At the end of the pipeline in eu-west-1, include a CodePipeline step to invoke the CodePipeline in us-east-2. Ensure the CodePipeline in us-east-2 has the necessary IAM permission to read the artifacts in S3 in eu-west-1</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "As the Lead DevOps Engineer at an analytics company, you are deploying a global application using a CICD pipeline comprising of AWS CodeCommit, CodeBuild, CodeDeploy and orchestrated by AWS CodePipeline. Your pipeline is currently setup in eu-west-1 and you would like to extend the pipeline to deploy your application in us-east-2. This will require a multi-step CodePipeline to be created there and invoked.\n\nHow would you implement a solution to address this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357274, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "question": "<p>The engineering team at a multi-national retail company is deploying its flagship web application onto an Auto Scaling Group using CodeDeploy. The team has chosen a strategy of a rolling update so that instances are updated in small batches in the ASG. The ASG has five instances running. At the end of the deployment, it seems that three instances are running the new version of the application, while the other two are running the old version. CodeDeploy is reporting a successful deployment.</p>\n\n<p>As a DevOps Engineer, what is the most likely reason that you would attribute for this issue?</p>\n", "explanation": "<p>Correct option:</p>\n\n<p><strong>Two new instances were created during the deployment</strong></p>\n\n<p>If an Amazon EC2 Auto Scaling scale-up event occurs while a deployment is underway, the new instances will be updated with the application revision that was most recently deployed, not the application revision that is currently being deployed. If the deployment succeeds, the old instances and the newly scaled-up instances will be hosting different application revisions.</p>\n\n<p>To resolve this problem after it occurs, you can redeploy the newer application revision to the affected deployment groups.</p>\n\n<p>To avoid this problem, AWS recommends suspending the Amazon EC2 Auto Scaling scale-up processes while deployments are taking place. You can do this through a setting in the common_functions.sh script that is used for load balancing with CodeDeploy. If HANDLE_PROCS=true, the following Amazon EC2 Auto Scaling events are suspended automatically during the deployment process:</p>\n\n<p>AZRebalance</p>\n\n<p>AlarmNotification</p>\n\n<p>ScheduledActions</p>\n\n<p>ReplaceUnhealthy</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Two instances are having an IAM permissions issue and cannot download the new code revision from S3</strong> - IAM permissions issue would result in the overall deployment status being returned as a failure, but CodeDeploy does report the status as a success. This option is just a distractor.</p>\n\n<p><strong>The auto-scaling group launch configuration has not been updated</strong> - Launch configuration would affect all instances in the same way and not just 2 instances. So this option is incorrect.</p>\n\n<p><strong>A CloudWatch alarm has been triggered during the deployment</strong> - This is another distractor added to the mix of options. CloudWatch alarm would have no bearing on the version of the CodeDeploy application deployed to the instances.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html#integrations-aws-auto-scaling-behaviors\">https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html#integrations-aws-auto-scaling-behaviors</a></p>\n", "relatedLectureIds": "", "answers": ["<p>Two new instances were created during the deployment</p>", "<p>Two instances are having an IAM permissions issue and cannot download the new code revision from S3</p>", "<p>The auto-scaling group launch configuration has not been updated</p>", "<p>A CloudWatch alarm has been triggered during the deployment</p>"]}, "correct_response": ["a"], "section": "Domain 5: Incident and Event Response", "question_plain": "The engineering team at a multi-national retail company is deploying its flagship web application onto an Auto Scaling Group using CodeDeploy. The team has chosen a strategy of a rolling update so that instances are updated in small batches in the ASG. The ASG has five instances running. At the end of the deployment, it seems that three instances are running the new version of the application, while the other two are running the old version. CodeDeploy is reporting a successful deployment.\n\nAs a DevOps Engineer, what is the most likely reason that you would attribute for this issue?", "related_lectures": []}, {"_class": "assessment", "id": 67357258, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.</p>\n\n<p>What can be done to improve the performance of the setup?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong></p>\n\n<p>A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI.</p>\n\n<p>Once you create a golden AMI for a product (a product can be a standardized OS-AMI that you want to distribute to accounts in your organization or an application-specific AMI you want to let your business unit(s) deploy in their environment), you can validate whether the AMI meets your expectations, and choose to approve or reject the AMI.</p>\n\n<p>About the golden AMI pipeline:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q66-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</strong> - The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time. So creating an AMI with a copy of the local data cache just serves as a distractor.</p>\n\n<p><strong>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</strong> - You cannot migrate from ElastiCache to DynamoDB for the given use-case, as it's primarily a NoSQL database and not a caching solution (You could use DAX as a caching solution with DynamoDB). Besides, the existing database is RDS MySQL which is a relational database, so DynamoDB does not really fit into this mix.</p>\n\n<p><strong>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</strong> - You cannot use Lambda to configure the instance local cache at boot time as caching is dynamic and data may change over time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n", "answers": ["<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>", "<p>Create an AMI that contains the web application and a copy of the local data cache. Configure the dynamic part at runtime an EC2 User Data script</p>", "<p>Migrate from ElastiCache to DynamoDB. Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script</p>", "<p>Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script. Use AWS Lambda to configure the instance local cache at boot time</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "As a DevOps Engineer at a data analytics company, you're deploying a web application on EC2 using an Auto Scaling group. The data is stored in RDS MySQL Multi-AZ, and a caching layer using ElastiCache. The application configuration takes time and currently needs over 20 minutes to warm up. 10 of those minutes are spent installing and configuring the web application, and another 10 minutes are spent warming up the local instance data cache.\n\nWhat can be done to improve the performance of the setup?", "related_lectures": []}, {"_class": "assessment", "id": 67357268, "assessment_type": "multi-select", "prompt": {"question": "<p>The DevOps team at a multi-national financial services company manages hundreds of accounts through AWS Organizations. As part of the security compliance requirements, the team must enforce the use of a security-hardened AMI in each AWS account. When a new AMI is created, the team wants to make sure new EC2 instances cannot be instantiated from the old AMI. Additionally, the team also wants to track and audit compliance of AMI usage across all the accounts.</p>\n\n<p>The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What do you recommend? (Select two)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</strong></p>\n\n<p>The DevOps team needs to provide approved AMIs that include the latest operating system updates, hardening requirements, and required\nthird-party software agents thereby enabling a repeatable, scalable, and approved application stack factory that increases innovation velocity and reduces effort. This solution uses Amazon EC2 Systems Manager Automation to drive the workflow.</p>\n\n<p>AMI hardening process:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i1.jpg\"></p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i2.jpg\"></p>\n\n<p>via - <a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p>After you have an approved AMI, you can distribute the AMI across AWS Regions, and then share it with any other AWS accounts. To do this, you use an Amazon EC2 Systems Manager Automation document that uses an AWS Lambda function to copy the AMIs across a specified list of regions, and then another Lambda function to share this copied AMI with the other accounts. The resulting AMI IDs can be stored in the SSM Parameter Store or Amazon DynamoDB for later consumption.</p>\n\n<p>Copying and sharing across AWS Regions and accounts:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i3.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><strong>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation</strong></p>\n\n<p>AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i4.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html\">https://docs.aws.amazon.com/config/latest/developerguide/how-does-config-work.html</a></p>\n\n<p>An AWS Config rule represents your desired configuration settings for specific AWS resources or an entire AWS account. AWS Config provides customizable, predefined rules to help you get started. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant, and AWS Config notifies you through Amazon SNS.</p>\n\n<p>For the given use-case, you need to create a Config custom rule to check that only the new AMI is being used and then report the rule's result using an AWS Config aggregation.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q67-i5.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p>\n\n<p>Multiple accounts and multiple regions.</p>\n\n<p>Single account and multiple regions.</p>\n\n<p>An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/config/latest/developerguide/images/Aggregate_Data_Landing_Page_Diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n\n<p>So to summarize, the key is to enforce AMI usage. As such, you don't want the AMI to be created or copied locally onto the other accounts, you want it to be available only in a central account and \"shared\" with other accounts. This way, if you have a new AMI, you unshare the previous one and share the new one. Finally, to monitor the EC2 instances and their AMI ID over time, an AWS Config custom rule is perfect for that.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</strong> - You don't want the AMI to be created in a master account and then copied locally onto the other accounts, you want it to be available only in a central account and \"shared\" with other accounts.</p>\n\n<p><strong>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</strong> - You can't create the AMI in a master account using AWS Automation document and then deploy it to all the accounts using AWS CloudFormation StackSets, rather you want it to be available only in a central account and then \"share\" it with other accounts.</p>\n\n<p><strong>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</strong> - You could use the Lambda function in all accounts to check the AMI id of all the EC2 instances in the account, but it would not allow you to track as well as audit the compliance of AMI usage across all the accounts.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf\">https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-concepts.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html\">https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html</a></p>\n", "answers": ["<p>Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one</p>", "<p>Create an AWS Automation document to create that AMI in a master account and copy the AMI into the other accounts. When a new AMI is created, copy it as well</p>", "<p>Create an AWS Automation document to create that AMI and deploy it to all the accounts using AWS CloudFormation StackSets. Run the Automation in all the accounts to have the AMI created locally</p>", "<p>Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation</p>", "<p>Create an AWS Lambda function in all the accounts using CloudFormation StackSets, which will check the AMI id of all the EC2 instances in the account. Give it an IAM role that allows it to publish messages to an SNS topic in the master account</p>"]}, "correct_response": ["a", "d"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "The DevOps team at a multi-national financial services company manages hundreds of accounts through AWS Organizations. As part of the security compliance requirements, the team must enforce the use of a security-hardened AMI in each AWS account. When a new AMI is created, the team wants to make sure new EC2 instances cannot be instantiated from the old AMI. Additionally, the team also wants to track and audit compliance of AMI usage across all the accounts.\n\nThe company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What do you recommend? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 67357272, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.</p>\n\n<p>How should a DevOps engineer implement a solution for this use-case?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. You can use these metrics to verify that your system is performing as expected.</p>\n\n<p>Using custom metrics for your Auto Scaling groups and instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n\n<p>RPO and RTO explained:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q68-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p>For the given use-case, we need to create a custom metric via the application that captures the lag in file reads and then uses it for scaling the ASG managing the EC2 instances to replicate the source EFS cluster into S3. Use another ASG to copy data from S3 into EFS in the target AWS Region. Here we want minimum RPO so we want continuous replication, and minimum RTO so we want a hot EFS system ready to go. Please note that because the RPO and RTO are low, the cost of the solution will be very high.</p>\n\n<p>Side note (for your knowledge) the AWS DataSync service (not covered in the exam) can achieve EFS to EFS replication in a much more native way.</p>\n\n<p>Note: With this solution, as the files are copied to S3, the file Linux permissions would not be replicated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</strong> - As the VPCs are not peered, it's not possible to mount the EFS of two different regions onto the same EC2 cluster. We need to go through S3 for the replication.</p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</strong></p>\n\n<p><strong>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</strong></p>\n\n<p>As the target, EFS needs to have a hot copy of the data, so both these options are ruled out since there is a delay of an hour.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/recovery-time-objective-rto-and-recovery-point-objective-rpo.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p>\n", "answers": ["<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>", "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create a Lambda Function in ap-southeast-2 for PUT on Amazon S3 and triggers an SSM Run Command to copy the files from S3 into EFS</p>", "<p>Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Create a standby EFS cluster in ap-southeast-2 and mount it on the same EC2 cluster. Let the replication software perform EFS to EFS replication</p>", "<p>Create a CloudWatch Event hourly rule that triggers an AWS Batch cluster in eu-west-1 to perform an incremental replication. Replicate the data into Amazon S3 in another region. Create an EC2 replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A global health-care company has an EFS filesystem being used in eu-west-1. The company would like to plan for a disaster recovery strategy and backup that EFS file system in ap-southeast-2. It needs to have a hot copy of the data so that the applications can be re-deployed in ap-southeast-2 with a minimum RPO and RTO. The VPCs in each region are not peered with each other.\n\nHow should a DevOps engineer implement a solution for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 67357248, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.</p>\n\n<p>How can you improve the user experience with the least effort?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enhance the health check so that the return status code corresponds to the connectivity to the database</strong></p>\n\n<p>Configuring health checks for the Application Load Balancer (ALB) is an important step to ensure that your AWS Cloud application runs smoothly. The ALB Health Check is configured with a protocol and port number to call on the target instances. A healthy EC2 instance is one that issues a response to a health check call with an HTTP 200 response code. Instances that return a status code that is other than the 2XX range or which time out are designated as being unhealthy and will not receive traffic from the ELB.</p>\n\n<p>Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q69-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p>You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</strong> - Migrating to Beanstalk would require significant effort and even then it won't help gather detailed database-specific health checks.</p>\n\n<p><strong>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</strong> - Health Checks for the ALB are pretty basic and only work with the HTTP return status code, not the payload itself.</p>\n\n<p><strong>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</strong> - Route53 health checks can only be used to prevent DNS records from being returned from a DNS query, so it won't help for routing to specific instances behind an ALB (that's why we have health checks at the ALB level).</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf\">https://d1.awsstatic.com/builderslibrary/pdfs/implementing-health-checks.pdf</a></p>\n", "answers": ["<p>Enhance the health check so that the return status code corresponds to the connectivity to the database</p>", "<p>Migrate the application to Elastic Beanstalk and enable advanced health monitoring</p>", "<p>Enhance the Health Check to report a JSON document that contains the health status of the connectivity to the database. Tune the ALB health check to look for a specific string in the health check result using a RegEx</p>", "<p>Include the health check in a Route 53 record so that users going through the ALB are not routed to the unhealthy instances</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "As a DevOps Engineer at an IT company, you have deployed a web application with a health check that currently checks if the application is running actively. The application is running in an ASG and the ALB health check integration is turned on. Recently your application has had issues with connecting to a backend database and as such the users of your website were experiencing issues accessing your website through the faulty instances.\n\nHow can you improve the user experience with the least effort?", "related_lectures": []}, {"_class": "assessment", "id": 67357242, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company has deployed its flagship application in two Auto Scaling groups (ASGs) and two Application Load Balancers (ALBs). You have a Route 53 record that points to the ALB+ASG group where the application has been the most recently deployed. Deployments are alternating between the two groups, and every time a deployment happens it is done on the non-active ALB+ASG group. Finally, the Route53 record is updated. It turns out that some of your clients are not behaving correctly towards the DNS record and thus making requests to the inactive ALB+ASG group.</p>\n\n<p>The company would like to improve this behavior at a minimal cost and also reduce the complexity of the solution. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What of the following would you suggest?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Remove one ALB and keep the two ASG. When new deployments happen, deploy to the older ASG, and then swap the target group in the ALB rule. Keep the Route53 record pointing to the ALB</strong></p>\n\n<p>An ALB distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes the requests to its registered targets. Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n\n<p>The issue is because of using the second load balancer for the second application stack and then changing the DNS route to direct the traffic to the other stack when required. The correct solution is to replace only the infrastructure behind the load balancer. To summarize, we can migrate to one ALB only and then just use one target group at a time behind each ASG for correct routing. This will have the added benefit that we won't need to pre-warm each ALB at each deployment.</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/10/06/2019-10-06_16-01-07-1.png\"></p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2019/10/06/2019-10-06_17-50-17.png\"></p>\n\n<p>via - <a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy a set of NGINX proxy onto each application instance so that if requests are made through the inactive ALB, they are proxied onto the correct ALB</strong> - Deploying an NGINX proxy will work but will be tedious to manage and will complicate the deployments.</p>\n\n<p><strong>Change the TTL of the Route53 to 1 minute before doing a deployment. Do the deployment and then increase the TTL back to the old value</strong> - Changing the TTL won't help as the clients are misbehaving already regarding the way they handle DNS records.</p>\n\n<p><strong>Deploy the application to Elastic Beanstalk under two environments. To do a deployment, deploy to the older environment, then perform a CNAME swap</strong> - Migrating to Elastic Beanstalk will not help either as CNAME swap is a DNS record change and clients do not seem to respect the DNS responses.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-simplifies-deployment-with-weighted-target-groups/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p>\n", "answers": ["<p>Remove one ALB and keep the two ASG. When new deployments happen, deploy to the older ASG, and then swap the target group in the ALB rule. Keep the Route53 record pointing to the ALB</p>", "<p>Deploy a set of NGINX proxy onto each application instance so that if requests are made through the inactive ALB, they are proxied onto the correct ALB</p>", "<p>Change the TTL of the Route53 to 1 minute before doing a deployment. Do the deployment and then increase the TTL back to the old value</p>", "<p>Deploy the application to Elastic Beanstalk under two environments. To do a deployment, deploy to the older environment, then perform a CNAME swap</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "An e-commerce company has deployed its flagship application in two Auto Scaling groups (ASGs) and two Application Load Balancers (ALBs). You have a Route 53 record that points to the ALB+ASG group where the application has been the most recently deployed. Deployments are alternating between the two groups, and every time a deployment happens it is done on the non-active ALB+ASG group. Finally, the Route53 record is updated. It turns out that some of your clients are not behaving correctly towards the DNS record and thus making requests to the inactive ALB+ASG group.\n\nThe company would like to improve this behavior at a minimal cost and also reduce the complexity of the solution. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement. What of the following would you suggest?", "related_lectures": []}, {"_class": "assessment", "id": 67357324, "assessment_type": "multi-select", "prompt": {"question": "<p>As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.</p>\n\n<p>Which of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</strong></p>\n\n<p>Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. When a scale-in event occurs, the terminating instance is first deregistered from the load balancer and while the instance is in the wait state, you can, for example, connect to the instance and download logs or other data before the instance is fully terminated.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q71-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p>For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3.</p>\n\n<p><strong>Enable Access Logs at the Application Load Balancer level</strong></p>\n\n<p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p>\n\n<p><strong>Analyze the logs using AWS Athena</strong></p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3. You can analyze the access logs stored in S3 via Athena.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</strong> - CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.</p>\n\n<p><strong>Enable Access Logs at the Target Group level</strong> - Please note that access logs are enabled at the ALB level and NOT at the target group level.</p>\n\n<p><strong>Analyze the logs using an EMR cluster</strong> - Analyzing logs at a low cost and in a serverless fashion should be done using AWS Athena. EMR clusters are usually long-running and cost a lot of money, and don't have serverless scaling capabilities.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n", "answers": ["<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3</p>", "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to install the CloudWatch Logs Agent and push the applications logs in S3</p>", "<p>Enable Access Logs at the Application Load Balancer level</p>", "<p>Enable Access Logs at the Target Group level</p>", "<p>Analyze the logs using AWS Athena</p>", "<p>Analyze the logs using an EMR cluster</p>"]}, "correct_response": ["a", "c", "e"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "As the Lead DevOps Engineer at a retail company, you have a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. You must collect the logs before an instance is terminated to perform log analytics later on. It's also necessary to collect all the access logs. The analysis of these logs should be performed at a minimal cost, and only need to be run from time to time.\n\nWhich of the following options would you suggest to implement the MOST cost-optimal solution for this requirement? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 67357216, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.</p>\n\n<p>Which configuration should you use in the CloudFormation template?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>AutoScalingRollingUpdate</strong></p>\n\n<p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-dop-pt/assets/pt1-q72-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AutoScalingReplacingUpdate</strong> - To specify how AWS CloudFormation handles replacement updates for an Auto Scaling group, you should use the AutoScalingReplacingUpdate policy. This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. This option will create a new ASG entirely, so this is ruled out.</p>\n\n<p><strong>AutoScalingLaunchTemplateUpdate</strong></p>\n\n<p><strong>AutoScalingLaunchConfigurationUpdate</strong></p>\n\n<p>AutoScalingLaunchTemplateUpdate and AutoScalingLaunchConfigurationUpdate do not exist, so both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p>\n", "answers": ["<p>AutoScalingRollingUpdate</p>", "<p>AutoScalingReplacingUpdate</p>", "<p>AutoScalingLaunchTemplateUpdate</p>", "<p>AutoScalingLaunchConfigurationUpdate</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "As a DevOps Engineer at a social media company, you have deployed your application in an Auto Scaling group (ASG) using CloudFormation. You would like to update the Auto Scaling Group to have all the instances reference the newly created launch configuration, which upgrades the instance type. Your ASG currently contains 6 instances and you need at least 4 instances to be up at all times.\n\nWhich configuration should you use in the CloudFormation template?", "related_lectures": []}, {"_class": "assessment", "id": 67357200, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The technology team at a health-care solutions company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon doing some analytics, it's found that 85% of the read requests are shared across all users.</p>\n\n<p>As a DevOps Engineer, how can you improve the application performance while decreasing the cost?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement\u2014from milliseconds to microseconds\u2014even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB\u2014you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they\u2019ve requested isn\u2019t yet cached, CloudFront retrieves it from your origin \u2013 for example, the S3 bucket where you\u2019ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although, you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n", "answers": ["<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>", "<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>", "<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>", "<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "The technology team at a health-care solutions company has developed a REST API which is deployed in an Auto Scaling Group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. Upon doing some analytics, it's found that 85% of the read requests are shared across all users.\n\nAs a DevOps Engineer, how can you improve the application performance while decreasing the cost?", "related_lectures": []}, {"_class": "assessment", "id": 67357330, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media streaming solutions company has deployed an application that allows its customers to view movies in real-time. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the <code>movies</code> table to be accessible globally but needs the <code>users</code> and <code>movies_watched</code> table to be regional only.</p>\n\n<p>As a DevOps Engineer, how would you implement this with minimal application refactoring?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (movies table) and the other one for the local tables (users and movies_watched tables).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</strong></p>\n\n<p>Here, we want minimal application refactoring. DynamoDB and Aurora have a completely different API, due to Aurora being SQL and DynamoDB being NoSQL. So all three options are incorrect, as they have DynamoDB as one of the components.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n", "answers": ["<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>", "<p>Use an Amazon Aurora Global Database for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>", "<p>Use a DynamoDB Global Table for the <code>movies</code> table and use Amazon Aurora for the <code>users</code> and <code>movies_watched</code> tables</p>", "<p>Use a DynamoDB Global Table for the <code>movies</code> table and use DynamoDB for the <code>users</code> and <code>movies_watched</code> tables</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A media streaming solutions company has deployed an application that allows its customers to view movies in real-time. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the movies table to be accessible globally but needs the users and movies_watched table to be regional only.\n\nAs a DevOps Engineer, how would you implement this with minimal application refactoring?", "related_lectures": []}, {"_class": "assessment", "id": 67357196, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national retail company is planning for disaster recovery and needs the data to be stored in Amazon S3 in two different regions that are in different continents. The data is written at a high rate of 10000 objects per second. For regulatory reasons, the data also needs to be encrypted in transit and at rest. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.</p>\n\n<p>Which of the following solutions would you recommend?</p>\n", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p>\n\n<p><em>*Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"false\"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication *</em></p>\n\n<p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the requirements, confirm that your bucket policies explicitly deny access to HTTP requests. Bucket policies that allow HTTPS requests without explicitly denying HTTP requests might not comply with the rule.</p>\n\n<p>To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key \"aws:SecureTransport\". When this key is true, this means that the request is sent through HTTPS. Create a bucket policy that explicitly denies access when the request meets the condition \"aws:SecureTransport\": \"false\". This policy explicitly denies access to HTTP requests.</p>\n\n<p>Finally, if we encrypt using KMS, we may get throttled at 10000 objects per second. SSE-S3 is a better choice in this case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"true\"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</strong> - As mentioned in the explanation above, you need to set the condition \"aws:SecureTransport\": \"false\" for the solution to work.</p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"false\"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</strong></p>\n\n<p><strong>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"true\"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</strong></p>\n\n<p>If we encrypt using KMS, we may get throttled at 10000 objects per second. So both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/\">https://aws.amazon.com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/resource-limits.html\">https://docs.aws.amazon.com/kms/latest/developerguide/resource-limits.html</a></p>\n", "answers": ["<p>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"false\"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</p>", "<p>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"true\"</code>. Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication</p>", "<p>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"false\"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</p>", "<p>Create a bucket policy to create a condition for Denying any request that is <code>\"aws:SecureTransport\": \"true\"</code>. Encrypt the objects at rest using SSE-KMS. Setup Cross-Region Replication</p>"]}, "correct_response": ["a"], "section": "Domain 3: Resilient Cloud Solutions", "question_plain": "A multi-national retail company is planning for disaster recovery and needs the data to be stored in Amazon S3 in two different regions that are in different continents. The data is written at a high rate of 10000 objects per second. For regulatory reasons, the data also needs to be encrypted in transit and at rest. The company has hired you as an AWS Certified DevOps Engineer Professional to build a solution for this requirement.\n\nWhich of the following solutions would you recommend?", "related_lectures": []}]}
5794160
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 69164380, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer launched an Amazon EC2 instance in an Amazon VPC. The instance must download an object from a restricted Amazon S3 bucket. When trying to download the object, a 403 Access Denied error was received.</p><p>What are two possible causes for this error? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The most likely cause of this error message is that either the S3 bucket policy or IAM role configuration has an error or simply does not grant the required permissions. The DevOps engineer should review the bucket policy or associated IAM user policies for any statements that might be denying access.</p><p>The engineer should also verify that the requests to the bucket meet any conditions in the bucket policy or IAM policies, checking for any incorrect deny statements, missing actions, or incorrect spacing in the policy.</p><p><strong>CORRECT: </strong>\"The bucket policy does not grant permission\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"There is an issue with the IAM role configuration\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Default encryption is enabled on the S3 bucket\" is incorrect.</p><p>This would not affect access to the object as the object would be automatically decrypted by S3.</p><p><strong>INCORRECT:</strong> \"The object has been moved to Amazon Glacier\" is incorrect.</p><p>With Glacier an AccessDeniedException error would be received if permission is not granted.</p><p><strong>INCORRECT:</strong> \"S3 versioning is enabled on the S3 bucket\" is incorrect.</p><p>Versioning does not affect access permissions to an object.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>", "answers": ["<p>Default encryption is enabled on the S3 bucket.</p>", "<p>The object has been moved to Amazon Glacier.</p>", "<p>The bucket policy does not grant permission.</p>", "<p>S3 versioning is enabled on the S3 bucket.</p>", "<p>There is an issue with the IAM role configuration.</p>"]}, "correct_response": ["c", "e"], "section": "AWS Storage", "question_plain": "A DevOps engineer launched an Amazon EC2 instance in an Amazon VPC. The instance must download an object from a restricted Amazon S3 bucket. When trying to download the object, a 403 Access Denied error was received.What are two possible causes for this error? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69164382, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company requires an automated solution that terminates Amazon EC2 instances that have been logged into manually within 24 hours of the login event. The applications running in the account are launched using Auto Scaling groups and the CloudWatch Logs agent is configured on all instances.</p><p>How should a DevOps engineer build the automation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to Kinesis, Lambda, or Kinesis Data Firehose. When log events are sent to the receiving service, they are base64 encoded and compressed with the gzip format.</p><p>The Lambda function that receives the log events can process the log files looking for entries that indicate that a manual login event occurred and add a tag. Another Lambda function that runs on a schedule can then look for instances that have been tagged and terminate them.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged\" is incorrect.</p><p>You cannot configure Step Functions to receive the events.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event\" is incorrect.</p><p>The API calls logged via CloudTrail will not show that a manual login event occurred as that does not require an AWS API call to be made. The manual login data would be included in the logs delivered to CloudWatch Logs by the CloudWatch agent.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours\" is incorrect.</p><p>You can configure a KDS stream to receive the events but would then need a record processor (KCL worker) to process the events, you cannot send directly to an SNS topic. Also, requiring the operations team to process the terminations is not automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Create a CloudWatch alarm that will trigger on AWS API call events in CloudTrail. Configure the alarm to send a message to an Amazon SQS queue. Create an AWS Lambda function that processes messages from the queue and terminates the instances that produced the login event.</p>", "<p>Create a CloudWatch alarm that will trigger on login events. Send the notification to a Kinesis Data Firehose stream. Configure the stream to send notifications to an SNS topic and instruct the operations team to subscribe to the topic and terminate EC2 instances that produced login events within 24 hours.</p>", "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Lambda function. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>", "<p>Create a CloudWatch Logs subscription filter that delivers logs to an AWS Step Functions state machine. Configure the function to tag the resources that produced the login event. Create a CloudWatch Events rule that triggers another Lambda function daily that terminates all instances that were tagged.</p>"]}, "correct_response": ["c"], "section": "AWS Compute", "question_plain": "A company requires an automated solution that terminates Amazon EC2 instances that have been logged into manually within 24 hours of the login event. The applications running in the account are launched using Auto Scaling groups and the CloudWatch Logs agent is configured on all instances.How should a DevOps engineer build the automation?", "related_lectures": []}, {"_class": "assessment", "id": 69164384, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has deployed AWS Single Sign-On (AWS SSO) and needs to ensure that user accounts are not created within AWS Identity and Access Management (AWS IAM). A DevOps engineer must create an automated solution for immediately disabling credentials of any new IAM user that is created. The security team must be notified when user creation events take place.</p><p>Which combination of steps should the DevOps engineer take to meet these requirements? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The company is using AWS SSO and we can presume have an identity source that is outside of AWS IAM. They therefore want to control creation of IAM users. The solution uses an EventBridge rule that monitors for CreateUser API calls in AWS CloudTrail. This will pick up all user creation events.</p><p>Then, an AWS Lambda function will disable both the access keys (if created) and login profile (if created) that are associated with the newly created user account. Then, an SNS notification will be sent to the security team.</p><p>This solution meets all the stated requirements.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail\" is incorrect.</p><p>This would only be triggered when user accounts with console (password) access are created. A user with programmatic access does not have a login profile unless you create a password for the user to access the AWS Management Console. Therefore, this would miss users that are created with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule\" is incorrect.</p><p>As above, login profiles are associated with console-based password access only, they do not apply to users with programmatic-only access.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified\" is incorrect.</p><p>The DevOps engineer should directly configure Amazon SNS to be triggered by EventBridge. There is no need to send notifications related to user modifications, only creation events.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html\">https://docs.aws.amazon.com/IAM/latest/APIReference/API_GetLoginProfile.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>", "answers": ["<p>Create an Amazon EventBridge rule that is triggered by IAM CreateUser API calls in AWS CloudTrail.</p>", "<p>Create an Amazon EventBridge rule that is triggered by IAM GetLoginProfile API calls in AWS CloudTrail.</p>", "<p>Create an AWS Lambda function that disables the access keys and deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>", "<p>Create an AWS Lambda function that deletes the login profiles associated with new IAM users. Configure the function as a target of the EventBridge rule.</p>", "<p>Create an Amazon SNS topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.</p>", "<p>Create an AWS Config rule that sends a notification to the security team using Amazon SNS when user accounts are modified.</p>"]}, "correct_response": ["a", "c", "e"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company has deployed AWS Single Sign-On (AWS SSO) and needs to ensure that user accounts are not created within AWS Identity and Access Management (AWS IAM). A DevOps engineer must create an automated solution for immediately disabling credentials of any new IAM user that is created. The security team must be notified when user creation events take place.Which combination of steps should the DevOps engineer take to meet these requirements? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69164386, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses a single AWS account and Region for development activities with multiple teams working on independent projects. A DevOps engineer must implement a mechanism to notify the operations manager when the creation of resources approaches the service limits for the AWS account.</p><p>Which solution will accomplish this with the LEAST amount of development effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor evaluates your account by using checks. Some checks are refreshed automatically whilst others do require you to call the RefreshTrustedAdvisorCheck API action. Checks include evaluating the service limit usage by AWS service in a Region.</p><p>The solution presented here uses AWS Lambda to refresh the service limit checks and EventBridge to trigger the Lambda function execution. Then, EventBridge is used again to look for event patterns that indicate service limits have been reached and then send an SNS notification to the operations manager. This is the simplest solution for meeting these requirements.</p><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that refreshes AWS Trusted Advisor checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that evaluates the deployed resources in the account and evaluates them against resource limits on the account. Create an Amazon EventBridge rule to run the Lambda function regularly and configure an Amazon SNS topic that notifies the operations manager\" is incorrect.</p><p>This would require complex development and is much more difficult compared to using AWS Trusted Advisor which already has all the service limit checks built in.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that refreshes AWS Personal Health Dashboard checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager\" is incorrect.</p><p>Personal Health Dashboard does not monitor service limits, it is a place to learn about the availability and operations of AWS services.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config custom rule that runs regularly and checks the AWS service limit status and sends notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the operations manager and subscribe the Lambda function to the SNS topic\" is incorrect.</p><p>This would be complex to implement and again it would be better to use AWS Trusted Advisor which already has the service limit checks built in.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-trusted-advisor/\">https://digitalcloud.training/aws-trusted-advisor/</a></p>", "answers": ["<p>Create an AWS Lambda function that evaluates the deployed resources in the account and evaluates them against resource limits on the account. Create an Amazon EventBridge rule to run the Lambda function regularly and configure an Amazon SNS topic that notifies the operations manager.</p>", "<p>Create an AWS Lambda function that refreshes AWS Personal Health Dashboard checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager.</p>", "<p>Create an AWS Config custom rule that runs regularly and checks the AWS service limit status and sends notifications to an Amazon SNS topic. Deploy an AWS Lambda function that notifies the operations manager and subscribe the Lambda function to the SNS topic.</p>", "<p>Create an AWS Lambda function that refreshes AWS Trusted Advisor checks and use an Amazon EventBridge rule to run the Lambda function regularly. Create another EventBridge rule with an event pattern matching Trusted Advisor checks and configure an Amazon SNS topic that notifies the operations manager.</p>"]}, "correct_response": ["d"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company uses a single AWS account and Region for development activities with multiple teams working on independent projects. A DevOps engineer must implement a mechanism to notify the operations manager when the creation of resources approaches the service limits for the AWS account.Which solution will accomplish this with the LEAST amount of development effort?", "related_lectures": []}, {"_class": "assessment", "id": 69164388, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An eCommerce company has operations in several countries around the world. The company runs an application in co-location facilities that uses Linux servers and a relational database running on MySQL. The application will be migrated to AWS and will include Amazon EC2 instances behind an Application Load Balancer in multiple AWS Regions. The database configuration has not yet been finalized.</p><p>A DevOps engineer has been asked to assist with determining the best solution for the database. The data includes product catalog information which must be served with low latency and customer purchase information which should be kept within each Region for compliance purposes.</p><p>Which database solution should the DevOps engineer recommend to meet these requirements with the LEAST changes to the application?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The current database runs on MySQL which is a relational database. Therefore, to meet the requirement to minimize changes to the application the cloud solution for the database should also be a relational database. Otherwise significant changes to the application may be required. This rules out all answers that include DynamoDB as that service is a non-relational database.</p><p>Amazon Aurora provides read replicas for scaling read performance horizontally. These replicas can be within a Region or across Regions. The product catalog data can be provided at low latency within each AWS Region using cross-Region read replicas.</p><p>For the customer purchase data, this can be kept within each Region by implementing local Aurora database instances which can additionally have read replicas within the Region if additional read performance is required.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_22-38-09-545c686c23e14af309338aad4a3b523c.jpg\"><p><strong>CORRECT: </strong>\"Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data\" is incorrect.</p><p>RedShift is used for online analytics processing (OLAP) use cases as it is a data warehouse solution. In this case the solution calls for an online transaction processing (OLTP) type of database as it is processing transactions.</p><p><strong>INCORRECT:</strong> \"Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data\" is incorrect.</p><p>DynamoDB is a non-relational database, and the application is designed to work with a relational database. Using DynamoDB would require significant changes to the application.</p><p><strong>INCORRECT:</strong> \"Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data\" is incorrect.</p><p>As above, DynamoDB should not be used for this solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>", "answers": ["<p>Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer purchase data.</p>", "<p>Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer purchase data.</p>", "<p>Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer purchase data.</p>", "<p>Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer purchase data.</p>"]}, "correct_response": ["c"], "section": "AWS Database", "question_plain": "An eCommerce company has operations in several countries around the world. The company runs an application in co-location facilities that uses Linux servers and a relational database running on MySQL. The application will be migrated to AWS and will include Amazon EC2 instances behind an Application Load Balancer in multiple AWS Regions. The database configuration has not yet been finalized.A DevOps engineer has been asked to assist with determining the best solution for the database. The data includes product catalog information which must be served with low latency and customer purchase information which should be kept within each Region for compliance purposes.Which database solution should the DevOps engineer recommend to meet these requirements with the LEAST changes to the application?", "related_lectures": []}, {"_class": "assessment", "id": 69164390, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is deploying a three-tier application on AWS that includes an Application Load Balancer in front of the Amazon ECS web tier, an Amazon ECS application tier, and an Amazon RDS database. The load balancer should only distribute traffic to the web tier if the web tier can successfully communicate with the application and database tiers.</p><p>How can this validation be implemented?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The requirement is that the web tier must be able to communicate with the application and database tiers and this should be validated by the load balancer. You can configure health checks in the load balancer for registered instances/tasks. However, that would not check each tier and the database.</p><p>A solution is to create an endpoint within the web tier that performs a connectivity/liveness check on the application and database tiers. Then, the load balancer simply needs to be configured with this URL as the endpoint for health checks for the target group associated with the web tier.</p><p>If the connectivity exists the health check returns a 200 (success) status code, and the load balancer considers the instance/task as being healthy.</p><p><strong>CORRECT: </strong>\"Create a health check endpoint in the web application that tests connectivity to the application and database tiers. Configure the endpoint as the health check URL for the target group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an Amazon RDS active connection count and an Amazon CloudWatch ELB metric to alarm on a significant change to the number of open connections. Configure the load balancer to consider targets unhealthy based on the alarm status\" is incorrect.</p><p>This is not a reliable way to test the connectivity between the tiers and you cannot configure the load balancer to consider targets unhealthy based on CloudWatch alarm status.</p><p><strong>INCORRECT:</strong> \"Use Amazon Route 53 health checks to detect issues with the web service and use the Application Auto Scaling service to automatically terminate unhealthy ECS and RDS instances\" is incorrect.</p><p>You cannot use Application Auto Scaling to terminate RDS instances and you wouldn\u2019t want to do that. The solution does not call for terminating only validating connectivity.</p><p><strong>INCORRECT:</strong> \"Register the ECS tasks and RDS database instances in the target group for the load balancer. Create health checks that test the liveness of the ECS tasks and database instances\" is incorrect.</p><p>You cannot register RDS database instances in a target group.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Register the ECS tasks and RDS database instances in the target group for the load balancer. Create health checks that test the liveness of the ECS tasks and database instances.</p>", "<p>Use an Amazon RDS active connection count and an Amazon CloudWatch ELB metric to alarm on a significant change to the number of open connections. Configure the load balancer to consider targets unhealthy based on the alarm status.</p>", "<p>Use Amazon Route 53 health checks to detect issues with the web service and use the Application Auto Scaling service to automatically terminate unhealthy ECS and RDS instances.</p>", "<p>Create a health check endpoint in the web application that tests connectivity to the application and database tiers. Configure the endpoint as the health check URL for the target group.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "A DevOps engineer is deploying a three-tier application on AWS that includes an Application Load Balancer in front of the Amazon ECS web tier, an Amazon ECS application tier, and an Amazon RDS database. The load balancer should only distribute traffic to the web tier if the web tier can successfully communicate with the application and database tiers.How can this validation be implemented?", "related_lectures": []}, {"_class": "assessment", "id": 69164392, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a large development team who run many projects in a single shared AWS account. The company wants to optimize costs by automatically stopping EBS-backed Amazon EC2 instances if resources are idle.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Trusted Advisor can be run to check if resources are idle. One of the core areas of optimization in AWS Trusted Advisor is cost optimization and one of the checks in this category is the \u201cunderutilized Amazon EBS Volumes\u201d check.</p><p>This check will check Amazon EBS volume configurations and warns when volumes appear to be underutilized. If a volume remains unattached or has very low write activity (excluding boot volumes) for a period, the volume is underutilized.</p><p>In this scenario the Lambda function can be configured to automatically stop any EC2 instances that have attached EBS volumes that are underutilized.</p><p><strong>CORRECT: </strong>\"Use a scheduled Amazon CloudWatch Events rule to target a custom AWS Lambda function that runs AWS Trusted Advisor checks. Create a second CloudWatch Events rule to filter events from Trusted Advisor to trigger a Lambda function to stop the idle instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a scheduled Amazon CloudWatch Events rule to filter for Amazon EC2 instance status checks and identify idle EC2 instances. Use the CloudWatch Events rule to target an AWS Lambda function to stop the idle instances\" is incorrect.</p><p>Instance status checks do not check for the utilization levels of the instances. They check for things like network reachability and aliveness.</p><p><strong>INCORRECT:</strong> \"Use a scheduled Amazon CloudWatch Events rule to filter AWS Systems Manager events and identify idle EC2 instances and resources. Use the CloudWatch Events rule to trigger an AWS Lambda function to stop the idle instances\" is incorrect.</p><p>Systems Manager also does not check the utilization status of Amazon EC2 instances or attached resources.</p><p><strong>INCORRECT:</strong> \"Use a scheduled Amazon CloudWatch Events rule to target Amazon Inspector events for idle EC2 instances. Use the CloudWatch Events rule to trigger an AWS Lambda function to stop the idle instances\" is incorrect.</p><p>Inspector does not check for cost optimization issues like underutilized resources. Instead, AWS Trusted Advisor should be used as that is a feature it performs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#underutilized-amazon-ebs-volumes\">https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#underutilized-amazon-ebs-volumes</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-trusted-advisor/\">https://digitalcloud.training/aws-trusted-advisor/</a></p>", "answers": ["<p>Use a scheduled Amazon CloudWatch Events rule to filter for Amazon EC2 instance status checks and identify idle EC2 instances. Use the CloudWatch Events rule to target an AWS Lambda function to stop the idle instances.</p>", "<p>Use a scheduled Amazon CloudWatch Events rule to target a custom AWS Lambda function that runs AWS Trusted Advisor checks. Create a second CloudWatch Events rule to filter events from Trusted Advisor to trigger a Lambda function to stop the idle instances.</p>", "<p>Use a scheduled Amazon CloudWatch Events rule to filter AWS Systems Manager events and identify idle EC2 instances and resources. Use the CloudWatch Events rule to trigger an AWS Lambda function to stop the idle instances.</p>", "<p>Use a scheduled Amazon CloudWatch Events rule to target Amazon Inspector events for idle EC2 instances. Use the CloudWatch Events rule to trigger an AWS Lambda function to stop the idle instances.</p>"]}, "correct_response": ["b"], "section": "AWS Management & Governance", "question_plain": "A company has a large development team who run many projects in a single shared AWS account. The company wants to optimize costs by automatically stopping EBS-backed Amazon EC2 instances if resources are idle.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69164394, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer manages an application that stores logs in Amazon CloudWatch Logs. The engineer needs to archive the logs in an Amazon S3 bucket. The log files are rarely accessed after 90 days and for compliance reasons must be retained for 10 years. The solution should run in an automated fashion.</p><p>Which combination of steps should the DevOps engineer take to meet the requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The best solution to meet these requirements is to use an CloudWatch Logs subscription filter to automatically stream the log files via Amazon Kinesis Data Firehose to the S3 bucket. You can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.</p><p>The S3 bucket can be configured with a bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and then expires the log files after 10 years (3,650 days). Glacier is the most cost-effective option for storing objects that are rarely accessed.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Logs subscription filter that uses Amazon Kinesis Data Firehose to stream all logs to an S3 bucket\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an S3 bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and expires the log files after 3,650 days\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs subscription filter that uses AWS DataSync to synchronize all logs to an S3 bucket\" is incorrect.</p><p>You cannot use DataSync with a CloudWatch Logs subscription filter. You can use a subscription filter with Kinesis, Lambda, or Kinesis Data Firehose.</p><p><strong>INCORRECT:</strong> \"Create a CloudWatch Logs export to save the log files to a logging bucket and then import all logs to the destination S3 bucket\" is incorrect.</p><p>This is not an automated solution, and the logging bucket is not required in this case.</p><p><strong>INCORRECT:</strong> \"Create an S3 bucket lifecycle policy that transitions the log files to Reduced Redundancy after 90 days and expires the log files after 3,650 days\" is incorrect.</p><p>The most cost-effective solution for rarely accessed files is Glacier.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Create a CloudWatch Logs subscription filter that uses Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.</p>", "<p>Create a CloudWatch Logs subscription filter that uses AWS DataSync to synchronize all logs to an S3 bucket.</p>", "<p>Create a CloudWatch Logs export to save the log files to a logging bucket and then import all logs to the destination S3 bucket.</p>", "<p>Create an S3 bucket lifecycle policy that transitions the log files to Reduced Redundancy after 90 days and expires the log files after 3,650 days.</p>", "<p>Create an S3 bucket lifecycle policy that transitions the log files to S3 Glacier after 90 days and expires the log files after 3,650 days.</p>"]}, "correct_response": ["a", "e"], "section": "AWS Storage", "question_plain": "A DevOps engineer manages an application that stores logs in Amazon CloudWatch Logs. The engineer needs to archive the logs in an Amazon S3 bucket. The log files are rarely accessed after 90 days and for compliance reasons must be retained for 10 years. The solution should run in an automated fashion.Which combination of steps should the DevOps engineer take to meet the requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69164396, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A critical production application running on AWS uses automatic scaling. The operations team must run updates on the application that affect only one instance at a time. The deployment process must ensure all remaining instances continue to serve traffic. The deployment must roll back if the update causes the CPU utilization of the updated instance to exceed 85%.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.</p><p>Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period. You can monitor metrics such as instance CPU utilization.</p><p>If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).</p><p>For this scenario the deployment can use the CodeDeployDefault.OneAtAtime strategy which ensures that only one instance will be taken out of action at a time, and automatic rollbacks if the alarm threshold is exceeded. This meets all requirements.</p><p><strong>CORRECT: </strong>\"Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group\" is incorrect.</p><p>CodeDeploy is a much better solution as it is designed for this purpose. This answer pieces together several services in a more complex solution.</p><p><strong>INCORRECT:</strong> \"Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Elastic Beanstalk uses ELB health checks (basic health), or with enhanced health it monitors application logs and the state of your environment's other resources. It does not use CloudWatch alarms.</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded\" is incorrect.</p><p>Systems Manager cannot perform blue/green updates though it can be used with other AWS Developer Tools as part of a solution for deploying updates in this manner.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/monitoring-create-alarms.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Configure AWS CodeDeploy with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Use the CodeDeployDefault.OneAtAtime configuration for deployment. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>", "<p>Configure AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group.</p>", "<p>Configure AWS Elastic Beanstalk with a load balancer and use AWS Auto Scaling. Create an alarm based on the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>", "<p>Configure AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Create an alarm based on the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks to roll back the deployment if the alarm thresholds are exceeded.</p>"]}, "correct_response": ["a"], "section": "AWS Management & Governance", "question_plain": "A critical production application running on AWS uses automatic scaling. The operations team must run updates on the application that affect only one instance at a time. The deployment process must ensure all remaining instances continue to serve traffic. The deployment must roll back if the update causes the CPU utilization of the updated instance to exceed 85%.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69164398, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>With AWS Organizations Service Control Policies (SCPs) you can restrict access to specific resources or define conditions for when SCPs are in effect. The condition element is supported when the Effect element has a value of Deny.</p><p>The StringNotLike condition will check the exception list for the aws:username that made the request. If the username is not present the request will be denied.</p><p><strong>CORRECT: </strong>\"Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list\" is incorrect.</p><p>The API action iam:CreateLoginProfile creates a password for the specified IAM user. This is not used to create the user account.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The API action iam:GetUser retrieves information about the specified IAM user, including the user's creation date, path, unique ID, and ARN.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed\" is incorrect.</p><p>The function should be used to validate that the iam:CreateUser request is being initiated by a user on the exception list. It should not delete the requesting user account.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>", "answers": ["<p>Attach an Organizations SCP with an explicit deny for all iam:CreateLoginProfile actions with a condition that excludes StringEquals for aws:username with a value of the exception list.</p>", "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:GetUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>", "<p>Attach an Organizations SCP with an explicit deny for all iam:CreateUser actions with a condition that includes StringNotLike for aws:username with a value of the exception list.</p>", "<p>Create an Amazon EventBridge rule with a pattern that matches the iam:CreateUser action and an AWS Lambda function target. Use the function to check the user\u2019s name against the exception list and delete the user account if it is not listed.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "A legacy application uses IAM user credentials to access resources in the company's AWS Organizations organization. It should not be possible to create IAM users unless the user account making the requires is specific on an exception list. A DevOps engineer must apply these restrictions.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69164400, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A web application runs on Amazon EC2 instances in an EC2 Auto Scaling group behind an Application Load Balancer (ALB). A DevOps engineer needs to implement a strategy for deploying updates that meets the following requirements:</p><p>\u00b7 Automatically launches the new version of the application on a second set of instances with the same capacity as the old version of the application.</p><p>\u00b7 Maintains the old version unchanged while the new version is launched.</p><p>\u00b7 Shifts traffic to the new version when the instances are fully deployed.</p><p>\u00b7 Terminates the old fleet of instances automatically 1 hour after shifting traffic.</p><p>Which solution will satisfy these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you use a blue/green deployment strategy with AWS CodeDeploy it can be configured to automatically copy the auto scaling group and launch instances running the new version of the application in a new environment. The capacity of the new auto scaling group can be the same and the original ASG can be maintained until the launch is complete.</p><p>The instances in the new environment can be a new target group behind the ALB. The traffic can be shifted across to the new environment when the launch is complete by modifying the target group setting in the ALB. This is performed automatically by CodeDeploy.</p><p>The BlueInstanceTerminationOption can be used to configure whether instances in the original environment are terminated when a blue/green deployment is successful. The options are:</p><ul><li><p>TERMINATE: Instances are terminated after a specified wait time.</p></li><li><p>KEEP_ALIVE: Instances are left running after they are deregistered from the load balancer and removed from the deployment group.</p></li></ul><p><strong>CORRECT: </strong>\"Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB\" is incorrect.</p><p>A CloudFormation retention policy configures which resources should be retained when a stack is deleted. Retention policies are not suitable for implementing blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour\" is incorrect.</p><p>A lifecycle policy tells Elastic Beanstalk to delete application versions that are old, or to delete application versions when the total number of versions for an application exceeds a specified number. This is not used for blue/green deployments.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application\" is incorrect.</p><p>You can use the Resources key in a configuration file to create and customize AWS resources in your environment. However this is not suitable for implementing a blue/green deployment strategy and the ALB does not need to be deleted. Instead, traffic simply needs to be shifted from one target group to another (when using the correct deployment configuration).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html\">https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_BlueInstanceTerminationOption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Create an AWS CloudFormation template and configure a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to redirect traffic to the new ALB.</p>", "<p>Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from old set of instances to the new one. Create an application version lifecycle policy that terminates the original environment after 1 hour.</p>", "<p>Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour and deploy the application.</p>", "<p>Use AWS CodeDeploy and create a deployment group that uses a blue/green deployment configuration. Use the BlueInstanceTerminationOption to terminate the instances in the blue environment after 1 hour.</p>"]}, "correct_response": ["d"], "section": "AWS Developer Tools", "question_plain": "A web application runs on Amazon EC2 instances in an EC2 Auto Scaling group behind an Application Load Balancer (ALB). A DevOps engineer needs to implement a strategy for deploying updates that meets the following requirements:\u00b7 Automatically launches the new version of the application on a second set of instances with the same capacity as the old version of the application.\u00b7 Maintains the old version unchanged while the new version is launched.\u00b7 Shifts traffic to the new version when the instances are fully deployed.\u00b7 Terminates the old fleet of instances automatically 1 hour after shifting traffic.Which solution will satisfy these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69164402, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Users on the internet have reported performance issues with the application. A DevOps engineer must investigate the reports and identify the processing latencies for requests.</p><p>How can the engineer obtain this information?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p><p>The access logs include the following entries (relating to processing latency):</p><p>\u00b7 Request_processing_time - The total time elapsed from the time the load balancer received the request until the time it sent the request to a target.</p><p>\u00b7 Target_processing_time - The total time elapsed from the time the load balancer sent the request to a target until the target started to send the response headers.</p><p>\u00b7 Response_processing_time - The total time elapsed from the time the load balancer received the response header from the target until it started to send the response to the client.</p><p><strong>CORRECT: </strong>\"Enable access logs on the load balancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CIoudTrail to identify API requests\" is incorrect.</p><p>API requests do not include any application metrics.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent\" is incorrect.</p><p>The CloudWatch agent does capture additional metrics, but this does not include application processing times.</p><p><strong>INCORRECT:</strong> \"Review Amazon CloudWatch metrics\" is incorrect.</p><p>CloudWatch does not capture this information in standard metrics. You would need to create custom metrics.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html#access-log-entry-format</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Use AWS CIoudTrail to identify API requests.</p>", "<p>Enable access logs on the load balancer.</p>", "<p>Install the Amazon CloudWatch agent.</p>", "<p>Review Amazon CloudWatch metrics.</p>"]}, "correct_response": ["b"], "section": "AWS Networking & Content Delivery", "question_plain": "An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). Users on the internet have reported performance issues with the application. A DevOps engineer must investigate the reports and identify the processing latencies for requests.How can the engineer obtain this information?", "related_lectures": []}, {"_class": "assessment", "id": 69164404, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The launch template that is used by an Auto Scaling group has been modified to use a new instance type and AMI. The Auto Scaling group is deployed using AWS CloudFormation. There are 8 production EC2 instances running in the Auto Scaling group.</p><p>A DevOps engineer needs to modify the Auto Scaling group to use the new template version without causing any interruption to the application and must ensure that at least 4 instances are always running.</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use the UpdatePolicy attribute to specify how AWS CloudFormation handles updates to several types of resource including Auto Scaling groups. This attribute can be configured with several properties to determine how the update process works. This is important to ensure that the correct number of instances are still available to service requests whilst the update process occurs.</p><p>The MinInstancesInService property specifies the minimum number of instances that must be in service within the Auto Scaling group while CloudFormation updates old instances. You can also specify the MaxBatchSize which specifies the maximum number of instances that CloudFormation updates.</p><p><strong>CORRECT: </strong>\"Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingScheduledAction attribute with the MaxBatchSize property\" is incorrect.</p><p>This is the wrong attribute to use; the engineer must use the AutoScalingRollingUpdate attribute instead.</p><p><strong>INCORRECT:</strong> \"Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties\" is incorrect.</p><p>A rolling update should be used to specify the number of instances to update in a batch. The replacing update either replaces all instances or creates a new ASG. The properties specified are also inappropriate for the situation (check the link below for more detail).</p><p><strong>INCORRECT:</strong> \"Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties\" is incorrect.</p><p>This attribute is used to retain or, in some cases, backup the existing physical instance of a resource when it's replaced during a stack update operation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Use the AutoScalingScheduledAction attribute with the MaxBatchSize property.</p>", "<p>Use the AutoScalingReplacingUpdate attribute and specify the WaitOnResourceSignals and PauseTime properties.</p>", "<p>Use the UpdateReplacePolicy attribute and specify the MinSuccessfulInstancesPercent and MaxBatchSize properties.</p>", "<p>Use the AutoScalingRollingUpdate attribute with the MinInstancesInService property.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "The launch template that is used by an Auto Scaling group has been modified to use a new instance type and AMI. The Auto Scaling group is deployed using AWS CloudFormation. There are 8 production EC2 instances running in the Auto Scaling group.A DevOps engineer needs to modify the Auto Scaling group to use the new template version without causing any interruption to the application and must ensure that at least 4 instances are always running.", "related_lectures": []}, {"_class": "assessment", "id": 69164406, "assessment_type": "multi-select", "prompt": {"question": "<p>A company runs many different workloads across hundreds of Amazon EC2 instances. The DevOps team requires that all instances have standard configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.</p><p>Which combination of actions meets these requirements with the most operational efficiency? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The most operationally efficient solution is to use AWS Systems Manager patch manager for patching and Amazon Inspector for assessing the security status of the instances. The Amazon CloudWatch agent can also be installed on the instances to get enhanced metrics and logging.</p><p>Systems Manager can be used to install and manage the other components. You must have the Systems Manager agent installed on the instances and the instance profile attached must have permissions to Systems Manager.</p><p>The patching process can be implemented by using Systems Manager Run Command to schedule tasks that deploy the updates using Systems Manager Patch Manager. EventBridge is ideal for scheduling the Amazon Inspector assessment runs.</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances\" is incorrect.</p><p>Inspector is not a service that can be used to install anything, it simply runs assessments against your instances and lets you know if there are any security concerns.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs\" is incorrect.</p><p>OpsWorks would be less operationally efficient for this purpose. Systems Manager Patch Manager is a better way to deploy patches as it is designed for this purpose and provides many features to simplify and optimize the patching process.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs\" is incorrect.</p><p>You cannot deploy patched AMIs to existing instances. AMIs are used to deploy the instance initially but you cannot redeploy the AMI without wiping the instance state. Config also cannot be used to enforce an Amazon Inspector assessment run.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>", "answers": ["<p>Use AWS Systems Manager to install and manage Amazon Inspector, Systems Manager Patch Manager, and the Amazon CloudWatch agent on all instances.</p>", "<p>Use Amazon Inspector to install and manage AWS Systems Manager, AWS OpsWorks, and the Amazon CloudWatch agent on all instances.</p>", "<p>Use AWS OpsWorks and execute custom recipes to patch the EC2 instances. Use AWS Systems Manager Run Command to initiate regular Amazon Inspector assessment runs.</p>", "<p>Use AWS CloudFormation with change sets to deploy AMIs patched by Systems Manager Patch Manager to existing EC2 instances. Use AWS Config to require regular Amazon Inspector assessment runs.</p>", "<p>Use AWS Systems Manager maintenance windows with Systems Manager Run Command to schedule Systems Manager Patch Manager tasks. Use Amazon EventBridge to schedule Amazon Inspector assessment runs.</p>"]}, "correct_response": ["a", "e"], "section": "AWS Management & Governance", "question_plain": "A company runs many different workloads across hundreds of Amazon EC2 instances. The DevOps team requires that all instances have standard configurations. These configurations include standard logging, metrics, security assessments, and weekly patching.Which combination of actions meets these requirements with the most operational efficiency? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69164408, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company manages several legacy applications that all generate different log formats. The logs need to be standardized so they can be queried and analyzed. A DevOps engineer needs a solution for standardizing the log formats before writing them to<br>an Amazon S3 bucket.</p><p>How can this requirement be met at the LOWEST cost?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Kinesis Data Firehose can invoke an AWS Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.</p><p>Each server can run the Kinesis agent and upload logs to the Kinesis Data Firehose delivery stream. Then the Lambda function will normalize the log files before they are loaded to the S3 bucket.</p><p><strong>CORRECT: </strong>\"Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3\" is incorrect.</p><p>The Amazon OpenSearch service is not a suitable solution for this scenario as it is a service that is used for searching and analyzing data sets. In this case the data simply needs to be transformed (normalized) before loading it to S3 so there is no need to involve OpenSearch.</p><p><strong>INCORRECT:</strong> \"Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight\" is incorrect.</p><p>QuickSight is used for analysis but in this scenario we simply need a solution for normalizing the log files before loading to S3.</p><p><strong>INCORRECT:</strong> \"Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place\" is incorrect.</p><p>To use RedShift Spectrum you must have a RedShift cluster in place and these run-on Amazon EC2 instances. Therefore, this solution is unlikely to be the lowest cost option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\">https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>", "answers": ["<p>Configure the application to send its logs to an Amazon OpenSearch cluster and use Lambda to normalize the logs and export to S3.</p>", "<p>Configure the application to send its logs to Amazon QuickSight, then use the Amazon QuickSight SPICE engine to normalize the logs. Do the analysis directly from Amazon QuickSight.</p>", "<p>Configure the application to send the log files to an Amazon S3 bucket and use Amazon Redshift Spectrum to normalize the logs in place.</p>", "<p>Configure the Amazon Kinesis Agent to upload the logs to Amazon Kinesis Data Firehose and use an AWS Lambda function to normalize the log files before they are loaded to Amazon S3.</p>"]}, "correct_response": ["d"], "section": "AWS Analytics", "question_plain": "A company manages several legacy applications that all generate different log formats. The logs need to be standardized so they can be queried and analyzed. A DevOps engineer needs a solution for standardizing the log formats before writing them toan Amazon S3 bucket.How can this requirement be met at the LOWEST cost?", "related_lectures": []}, {"_class": "assessment", "id": 69164410, "assessment_type": "multi-select", "prompt": {"question": "<p>A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.</p><p>A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.</p><p>Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. The IMAGEID property is the SHA digest for the Docker image used to start the container.</p><p>Ideally, a container image is intended to be a complete snapshot of everything that the application requires to function. With a complete container image, the application could be run by downloading one container image from one place.</p><p>There is no need to download several separate pieces from different locations. Therefore, as a best practice, store all application dependencies as static files inside the container image. This will improve performance and start up time.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-00-17-d1b2e063cff66b89f7caee81435b0971.jpg\"><p><strong>CORRECT: </strong>\"Move all the dependencies into a single image and pull them from a single container registry\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time\" is incorrect.</p><p>The exact image tag needs to be pulled because the latest tag will only pull the image that was pushed last.</p><p><strong>INCORRECT:</strong> \"After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task\" is incorrect.</p><p>Using Lambda to SSH onto instances is not a workable solution. SSM Run Command may be better but it\u2019s also not ideal.</p><p><strong>INCORRECT:</strong> \"Move secondary dependencies to be downloaded at application startup rather than including them within a static container image\" is incorrect.</p><p>This would delay the image spin up time rather than improving it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html\">https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/application.html</a></p><p><br></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-metadata.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Move all the dependencies into a single image and pull them from a single container registry.</p>", "<p>When creating a new task definition for the ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time.</p>", "<p>When creating a new task definition for the ECS service, ensure to add the latest tag in the full image name so that ECS pulls the correct image every time.</p>", "<p>After the deploy step in CodePipeline is done, include a Custom Step that triggers an AWS Lambda. The function will SSH onto the ECS instances and clear the local Docker cache and restart the task.</p>", "<p>Move secondary dependencies to be downloaded at application startup rather than including them within a static container image.</p>"]}, "correct_response": ["a", "b"], "section": "AWS Developer Tools", "question_plain": "A data intelligence and analytics company has implemented a CI/CD pipeline using AWS CodePipeline which takes code from an AWS CodeCommit repository and then builds it using AWS CodeBuild. During the deploy stage, the application is deployed onto an Amazon ECS cluster. During deployment, the application is only partly updated on some ECS tasks which are running an older version of the image.A DevOps engineer investigated and found that terminating the task or clearing the local Docker cache fixes the issue, but a more robust solution is required that provides visibility and identification to track where container images are deployed. Also, the start-up time of the containers needs to be optimized.Which actions should the DevOps engineer take to achieve these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69164412, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at a global retail company wants to deploy the latest application code to through build, staging, beta &amp; prod environments. While doing the staging deployment, an automated functional test suite needs to be executed which runs for approximately two hours to complete regression testing. The code is managed via AWS CodeCommit.</p><p>How can a DevOps engineer optimize the configuration and automate the pipeline?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The solution to these requirements uses an AWS CodePipeline pipeline to automate the entire CI/CD pipeline. When code is committed it will be picked up as a change and the pipeline execution commences. This will automate the testing suites and deployment into production using CodeDeploy.</p><p>Lambda should be ruled out for running the test suite as the maximum timeout for a Lambda function is 15 minutes, so it will not support the given use-case since the functional test suite runs for over two hours.</p><p><strong>CORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn\u2019t fail, the last stage will deploy the application to production\" is incorrect.</p><p><strong>INCORRECT: </strong>\"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production\" is incorrect.</p><p>Since CodeBuild Test can\u2019t be as a stage prior to deploying in the staging environment, so this option is incorrect.</p><p><strong>INCORRECT:</strong> \"Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn\u2019t fail, the last stage will deploy the application to production\" is incorrect.</p><p>AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services. Workflows manage failures, retries, parallelization, service integrations, and observability so developers can focus on higher-value business logic. This is not an ideal use case for Step Functions.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/\">https://aws.amazon.com/blogs/devops/test-reports-with-aws-codebuild/</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a custom stage using a Lambda function that will run the test suite. If the stage succeeds, the last stage will deploy the application to production.</p>", "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and as a first stage run a CodeBuild build that will run the test suite against the staging environment. Upon passing, deploy to staging using CodeDeploy and if it succeeds, deploy to production.</p>", "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn\u2019t fail, the last stage will deploy the application to production.</p>", "<p>Create a CodePipeline pointing to the master branch of the CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a Step Function which will run the test suite. Create a CloudWatch Event Rule on the execution termination of the Step Function to invoke a Lambda function and signal CodePipeline the success or failure. If the stage doesn\u2019t fail, the last stage will deploy the application to production.</p>"]}, "correct_response": ["c"], "section": "AWS Developer Tools", "question_plain": "The DevOps team at a global retail company wants to deploy the latest application code to through build, staging, beta &amp; prod environments. While doing the staging deployment, an automated functional test suite needs to be executed which runs for approximately two hours to complete regression testing. The code is managed via AWS CodeCommit.How can a DevOps engineer optimize the configuration and automate the pipeline?", "related_lectures": []}, {"_class": "assessment", "id": 69164414, "assessment_type": "multi-select", "prompt": {"question": "<p>An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.</p><p>What steps should be taken to secure the web application? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To secure the traffic in transit an SSL/TLS certificate should be attached to a secure listener on the ALB. This will not affect the performance of the EC2 instances as the encryption takes place only between the client and the ALB. The certificate can be issued through AWS Certificate Manager (ACM).</p><p>The AWS Web Application Firewall (AWS WAF) protects against common web exploits. The company can create a web ACL with a rule and action and then attach it to the ALB. This will protect against web exploits.</p><p><strong>CORRECT: </strong>\"Add an SSL/TLS certificate to a secure listener on the ALB\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL and attach it to the ALB\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install SSL/TLS certificates on the EC2 instances\" is incorrect.</p><p>Encryption on the EC2 instances would impact the performance of those instances.</p><p><strong>INCORRECT:</strong> \"Configure Server-Side Encryption with KMS managed keys\" is incorrect.</p><p>This is not relevant to in transit encryption, this is used to encrypt data at rest on services such as Amazon S3.</p><p><strong>INCORRECT:</strong> \"Enable EBS encryption for the EC2 volumes to encrypt all traffic\" is incorrect.</p><p>EBS encryption is used for encrypting data at rest. The question requires encryption using SSL/TLS certificates which is encryption in transit.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p><p><a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\">https://aws.amazon.com/elasticloadbalancing/application-load-balancer/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Create an AWS WAF web ACL and attach it to the ALB.</p>", "<p>Add an SSL/TLS certificate to a secure listener on the ALB.</p>", "<p>Install SSL/TLS certificates on the EC2 instances.</p>", "<p>Configure Server-Side Encryption with KMS managed keys.</p>", "<p>Enable EBS encryption for the EC2 volumes to encrypt all traffic.</p>"]}, "correct_response": ["a", "b"], "section": "AWS Security, Identity, & Compliance", "question_plain": "An application is being deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The security team requires that the traffic is secured with SSL/TLS certificates. Protection against common web exploits must also be implemented. The solution should not have a performance impact on the EC2 instances.What steps should be taken to secure the web application? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69164416, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.</p><p>Which actions should the engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use Amazon S3 to host a static website. On a <em>static</em> website, individual webpages include static content. They might also contain client-side scripts.</p><p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.</p><p>To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website.</p><p>AWS Web Application Firewall (WAF) can be used to protect the static website from web exploits. To do this, the engineer should create an AWS WAF web ACL.</p><p><strong>CORRECT: </strong>\"Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield\" is incorrect.</p><p>Both EC2 and ElastiCache use EC2 instances so they are not serverless services. AWS Shield protects against DDoS attacks but AWS WAF should be used for web exploits.</p><p><strong>INCORRECT:</strong> \"Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL\" is incorrect.</p><p>ECS with Fargate is serverless but you cannot use a Redis cluster to cache content served by a task running on Amazon ECS. ElastiCache is used for caching database content.</p><p><strong>INCORRECT:</strong> \"Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty\" is incorrect.</p><p>Lambda with a REST API as the frontend is definitely possible, however the static website on S3 would be easier to deploy and manage. AWS GuardDuty does not protect against web exploits.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>", "answers": ["<p>Deploy the website using a static website running on Amazon S3. Create an Amazon CloudFront distribution and an AWS WAF web ACL.</p>", "<p>Deploy the website on an Amazon EC2 instance. Create an Amazon ElastiCache Redis cluster and enable AWS Shield.</p>", "<p>Deploy the website on an Amazon ECS cluster with the Fargate launch type. Create an Amazon ElastiCache Redis cluster and an AWS WAF web ACL.</p>", "<p>Deploy the website using an AWS Lambda function with an Amazon API Gateway front end. Enable AWS GuardDuty.</p>"]}, "correct_response": ["a"], "section": "AWS Storage", "question_plain": "A DevOps engineer must deploy a serverless website on AWS. The website will host content that includes HTML files, images, videos, and JavaScript (client-side scripts). The website must be optimized for global performance and be protected against web exploits.Which actions should the engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 69164418, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A law firm is planning to migrate existing applications to AWS. These applications are hosted in an on-premises data center and are complex in nature. The applications could take many months to migrate. While the migration is underway, the application development team implemented a tactical solution using Amazon CloudFront with a custom origin pointing to the SSL endpoint URL for the legacy web application.</p><p>The ad-hoc solution worked for several weeks; however, all browser connections recently began showing an HTTP 502 Bad Gateway error with the header \"X-Cache: Error from CloudFront\". Network monitoring services show that the HTTPS port 443 on the legacy web application is open and responding to requests.</p><p>Which option could be the reason for the error and how can it be solved?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-04-44-3acbb19734df50fc3c67eb1acc619f2b.jpg\"><p><strong>CORRECT: </strong>\"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>You can't export an Amazon Issued ACM public certificate for use on an EC2 instance or another custom web server because ACM manages the private key.</p><p><strong>INCORRECT:</strong> \"The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region\" is incorrect.</p><p>If you\u2019re using certificates that you get from a third-party certificate authority (CA), you must monitor certificate expiration dates and renew the certificates that you import into AWS Certificate Manager (ACM) or upload to the AWS Identity and Access Management certificate store before they expire.</p><p><strong>INCORRECT</strong>: \"The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server\" is incorrect.</p><p>A self-signed certificate is a security certificate that is not signed by a certificate authority (CA). You can't use a self-signed certificate for HTTPS communication between CloudFront and your origin, so this option is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html#https-requirements-cert-expiration</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>", "answers": ["<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server via the AWS Certificate Manager (ACM) in the hosting region.</p>", "<p>The SSL certificate on the CloudFront distribution has expired. Reissue the SSL certificate on the CloudFront distribution via the AWS Certificate Manager (ACM) in the hosting region.</p>", "<p>The SSL certificate on the legacy web application server has expired. Reissue the SSL certificate on the web server that is signed by a globally recognized certificate authority (CA). Install the full certificate chain onto the legacy web application server.</p>", "<p>The SSL certificate on the legacy web application server has expired. Install a new self-signed certificate along with the full certificate chain onto the legacy web application server.</p>"]}, "correct_response": ["c"], "section": "AWS Networking & Content Delivery", "question_plain": "A law firm is planning to migrate existing applications to AWS. These applications are hosted in an on-premises data center and are complex in nature. The applications could take many months to migrate. While the migration is underway, the application development team implemented a tactical solution using Amazon CloudFront with a custom origin pointing to the SSL endpoint URL for the legacy web application.The ad-hoc solution worked for several weeks; however, all browser connections recently began showing an HTTP 502 Bad Gateway error with the header \"X-Cache: Error from CloudFront\". Network monitoring services show that the HTTPS port 443 on the legacy web application is open and responding to requests.Which option could be the reason for the error and how can it be solved?", "related_lectures": []}, {"_class": "assessment", "id": 69164420, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company plans to deploy a high-performance computing (HPC) workload on Amazon EC2 instances in a shared Amazon VPC. Developers in multiple participant accounts must be granted access to the cluster to perform analytics. The cluster requires a shared file system that supports file-based access to objects stored in Amazon S3 buckets.</p><p>Which deployment steps should be implemented to support the required features and access control?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon FSx for Lustre is the only option that can present objects stored in S3 buckets as files. The question specifically asks for file-based access for objects stored in buckets, so this requirement is satisfied by using FSx for Lustre. The access control requirements can be implemented through an AWS IAM role that can be assumed by cross-account participants. Permissions should be assigned through an identity-based permissions policy assigned to the role.</p><p><strong>CORRECT: </strong>\"Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. You also cannot use resource based policies with FSx file systems and access keys would be unsuitable for cross-account access.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects. This file system should be used with Windows servers that need an NTFS file system.</p><p><strong>INCORRECT:</strong> \"Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access\" is incorrect.</p><p>This FSx file system does not allow file-based access to S3 objects.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/security_iam_service-with-iam.html</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/faqs/\">https://aws.amazon.com/fsx/lustre/faqs/</a></p>", "answers": ["<p>Deploy an Amazon FSx for Lustre file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>", "<p>Deploy an Amazon FSx for NetApp ONTAP file system with NFS access. Create a resource based policy that allows cross-account access for members of the participant accounts. Use access keys to authenticate users and use security groups to enable file system access.</p>", "<p>Deploy an Amazon FSx for Windows file server file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>", "<p>Deploy and Amazon FSx for OpenZFS file system. Create an IAM role that can be assumed by members of the participant accounts and provide permissions through an identity based policy assigned to the role. Use security groups to enable file system access.</p>"]}, "correct_response": ["a"], "section": "AWS Storage", "question_plain": "A company plans to deploy a high-performance computing (HPC) workload on Amazon EC2 instances in a shared Amazon VPC. Developers in multiple participant accounts must be granted access to the cluster to perform analytics. The cluster requires a shared file system that supports file-based access to objects stored in Amazon S3 buckets.Which deployment steps should be implemented to support the required features and access control?", "related_lectures": []}, {"_class": "assessment", "id": 69164422, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A development team run a fleet of Amazon EC2 instances in a dev/test environment. A manager is concerned about the costs of the workloads. While the developers are unable to determine the exact resource requirements for each workload, they also cannot terminate any instances as all are currently in operational use.</p><p>What is the best way to ensure the most efficient use of the underlying hardware?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Compute Optimizer helps you identify the optimal AWS resource configurations, such as Amazon Elastic Compute Cloud (EC2) instance types, Amazon Elastic Block Store (EBS) volume configurations, task sizes of Amazon Elastic Container Service (ECS) services on AWS Fargate, and AWS Lambda function memory sizes, using machine learning to analyze historical utilization metrics.</p><p>The recommendations provided by AWS Computer Optimizer can be used by the development team to right-size the workloads which will ensure cost-efficiency.</p><p><strong>CORRECT: </strong>\"Use AWS Computer Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder\" is incorrect.</p><p>Control Tower is used for managing multiple AWS accounts and applying best practice baselines and guardrails. It is not used for monitoring instance utilization metrics. EC2 Image Builder is used for automating the image management processes for AMIs.</p><p><strong>INCORRECT:</strong> \"Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function\" is incorrect.</p><p>Amazon Detective is a security service that runs statistical analysis on log data using machine learning algorithms. It is used to identify security issues and does not provide performance recommendations for right-sizing workloads.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents\" is incorrect.</p><p>Amazon CloudWatch is used for performance monitoring. However, it would be more efficient for the development team to leverage AWS Computer Optimizer for the purpose of right-sizing workloads as it is a tool that is designed for exactly that purpose.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/compute-optimizer/faqs/\">https://aws.amazon.com/compute-optimizer/faqs/</a></p>", "answers": ["<p>Use AWS Control Tower to inspect the utilization metrics of the workloads and use the reports to right-size the instance types to the workloads. Implement the changes using EC2 Image Builder.</p>", "<p>Use Amazon Detective to run statistical analysis on the log data collected from EC2 instances to determine services that are underutilized. Disable unnecessary services automatically using an AWS Lambda function.</p>", "<p>Use AWS Compute Optimizer to report on resource utilization on EC2 instances and use the recommendations to manually reconfigure resources for improved utilization.</p>", "<p>Use AWS CloudWatch to collect performance metrics and create a dashboard displaying utilization metrics across the EC2 instances. Auto remediate instance types with AWS Systems Manager automation documents.</p>"]}, "correct_response": ["c"], "section": "AWS Compute", "question_plain": "A development team run a fleet of Amazon EC2 instances in a dev/test environment. A manager is concerned about the costs of the workloads. While the developers are unable to determine the exact resource requirements for each workload, they also cannot terminate any instances as all are currently in operational use.What is the best way to ensure the most efficient use of the underlying hardware?", "related_lectures": []}, {"_class": "assessment", "id": 69164424, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.</p><p>Which deployment will satisfy the requirements with the most operational efficiency?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Transit Gateway can be configured to enable a fully meshed network topology which allows transitive routing between all the VPCs. With AWS Network Firewall, you can define firewall rules that provide fine-grained control over network traffic.</p><p>AWS Network Firewall works together with AWS Firewall Manager so you can build policies based on Network Firewall rules and then centrally apply those policies across your virtual private clouds (VPCs) and accounts.</p><p><strong>CORRECT: </strong>\"Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Network Firewall to centrally deploy and manage security policies across the VPCs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs\" is incorrect.</p><p>VPC peering requires creating complex peering relationships and does not support transitive routing (though this can be achieved through a mesh of peering connections). VPC peering is less operationally efficient compared to using a Transit Gateway. AWS WAF is not the best solution for enforcing centralized network access controls, it is used for preventing web based attacks.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs\" is incorrect.</p><p>You cannot connect Amazon VPCs using AWS S2S VPNs. You can only use an AWS VPN to connect on-premises networks.</p><p><strong>INCORRECT:</strong> \"Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPC\" is incorrect.</p><p>PrivateLink is not used for creating this kind of network deployment. It is used for private access to AWS services using private IP addresses.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/network-firewall/\">https://aws.amazon.com/network-firewall/</a></p><p><a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>", "answers": ["<p>Deploy AWS Transit Gateway to create a fully meshed network topology with transitive routing. Use AWS Network Firewall to centrally deploy and manage security policies across the VPCs.</p>", "<p>Create VPC peering connections between the VPCs and configure a fully meshed network topology. Use AWS Web Application Firewall (WAF) to centrally deploy and manage WebACLs across the VPCs.</p>", "<p>Deploy an AWS Site-to-Site VPN between each VPC and configure route tables to enable fully meshed routing. Use AWS Firewall Manager to centrally deploy and manage security policies across the VPCs.</p>", "<p>Create an AWS PrivateLink connection between each VPC and configure VPC endpoints to enable fully meshed connectivity. Use AWS Security Hub to centrally deploy and manage security policies across the VPCs.</p>"]}, "correct_response": ["a"], "section": "AWS Networking & Content Delivery", "question_plain": "A development team is running a project that will involve deploying applications across several Amazon VPCs. The applications will require fully meshed network connectivity to enable transitive routing between VPCs. The development lead is concerned about security and has requested centralized control over network access controls.Which deployment will satisfy the requirements with the most operational efficiency?", "related_lectures": []}, {"_class": "assessment", "id": 69164426, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.</p><p>Which solution can a DevOps engineer use to meet all these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>App2Container (A2C) is a command-line tool for modernizing .NET and Java applications into containerized applications. A2C analyzes and builds an inventory of all applications running in virtual machines on-premises or in the cloud.</p><p>With A2C you can simply select the application you want to containerize, and A2C packages the application artifact and identified dependencies into container images, configures the network ports, and generates the ECS task and Kubernetes pod definitions.</p><p>App2Container creates CI/CD pipelines for Amazon DevOps services such as CodeBuild and CodeDeploy to build and deploy containers. If you have existing CI/CD tooling (for example, Azure DevOps and Jenkins), then you can integrate A2C provided artifacts \u2013 dockerfile, ECS task definition, EKS deployment YAML - into your existing CI/CD workflows.</p><p><strong>CORRECT: </strong>\"Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates\" is incorrect.</p><p>AWS Copilot is a command line interface (CLI) that you can use to quickly launch and manage containerized applications on AWS. It simplifies running applications on Amazon Elastic Container Service (ECS), AWS Fargate, and AWS App Runner. It is not able to inventory and analyze existing applications or migrate them to containers.</p><p><strong>INCORRECT:</strong> \"Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository\" is incorrect.</p><p>AWS Proton is used to create application stack templates and build infrastructure for applications using the CLI, API, or UI. It is not used for refactoring applications or migrating them to containerized deployments.</p><p><strong>INCORRECT:</strong> \"Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy\" is incorrect.</p><p>Amazon AppFlow is used to automate bi-directional data flows between SaaS applications and AWS services. It is not used to migrate applications between on-premises deployments and Amazon ECS.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/app2container/features/\">https://aws.amazon.com/app2container/features/</a></p>", "answers": ["<p>Use AWS Copilot to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure Copilot to generate a CI/CD pipeline using AWS CloudFormation templates.</p>", "<p>Use AWS App2Container (A2C) to analyze and inventory the on-premises application and then containerize it on Amazon ECS. Configure A2C to build a CI/CD pipeline using CodeBuild and CodeDeploy.</p>", "<p>Use AWS Proton to build a service template for refactoring the on-premises application to a container running on Amazon EKS Distro. Build a CI/CD pipeline by deploying template updates through a Git repository.</p>", "<p>Use Amazon AppFlow to build a data flow between the on-premises application and Amazon ECS. Map and merge the Java application code to an ECS task through a CI/CD pipeline using CodeBuild and CodeDeploy.</p>"]}, "correct_response": ["b"], "section": "AWS Compute", "question_plain": "A company runs a Java application in an on-premises data center. The application is not currently containerized but the company plans to migrate it to the AWS Cloud and modernize the application into a containerized deployment. A CI/CD pipeline should also be created to automate application updates.Which solution can a DevOps engineer use to meet all these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69164428, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is building a web application that will use federated access to a SAML identity provider (IdP). The web application requires sign up and sign in functionality using a custom webpage with authenticated access to AWS services.</p><p>Which steps should the DevOps engineer take to implement the authentication and access control solution for the web application?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The DevOps engineer should use Amazon Cognito with a user pool to create a custom webpage offering sign in and sign up functionality. The user pool can be integrated with a SAML IdP for federated access.</p><p>The process for configuring this federation is as follows:</p><ol><li><p>Create or select a user pool.</p></li><li><p>Choose the Sign-in experience tab. Locate Federated sign-in and select Add an identity provider.</p></li><li><p>Choose a SAML social identity provider.</p></li><li><p>Enter <strong>Identifiers</strong> separated by commas. An identifier tells Amazon Cognito it should check the email address a user enters when they sign in, and then direct them to the provider that corresponds to their domain.</p></li></ol><p>When a user signs into the app, Amazon Cognito verifies the login information. If the login is successful, Amazon Cognito creates a session and returns an ID, access, and refresh token for the authenticated user. The tokens can be used to grant users access to server-side resources or to the Amazon API Gateway. Or they can be exchanged for temporary AWS credentials to access other AWS services.</p><p><strong>CORRECT: </strong>\"Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services\" is incorrect.</p><p>Though you can use an identity pool for federated sign in you should use a user pool for creating a custom webpage offering sign in and sign up functionality. Also the API action provided is not suitable for SAML \u2013 AssumeRoleWith SAML should be used instead.</p><p><strong>INCORRECT:</strong> \"Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies\" is incorrect.</p><p>API Gateway is not a suitable web endpoint for providing a custom webpage with the required sign in and sign up functionality.</p><p><strong>INCORRECT:</strong> \"Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application\" is incorrect.</p><p>You cannot configure trust relationships with any other directory when using Simple AD. Amazon CloudFront also does not offer sign up and sign in functionality.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-configuring-federation-with-saml-2-0-idp.html</a></p>", "answers": ["<p>Use Amazon Cognito and create an identity pool for federated sign-in. Configure the SAML IdP to add a relying party trust between the IdP and AWS. Use the AssumeRoleWithWebIdentity API to call the AWS Security Token Service (STS) and generate temporary security credentials providing access to the appropriate AWS services.</p>", "<p>Use an Amazon API Gateway REST API to provide a web endpoint for the application. Use an AWS Lambda authorizer to control access to the API with a bearer token authentication strategy utilizing the SAML IdP. Authorize access to the backend services using identity-based policies.</p>", "<p>Use AWS Directory Service to create a Simple AD and configure a trust relationship with the SAML IdP. Create a custom webpage on an Amazon S3 static website and use Amazon CloudFront for sign up and sign in functionality. Use role-based access control to access AWS services through the web application.</p>", "<p>Use Amazon Cognito and create a user pool for federated sign-in. Add a SAML IdP and enter identifiers to map the sign-in email addresses to the relevant provider. Generate access tokens and exchange them for temporary security credentials providing access to the appropriate AWS services.</p>"]}, "correct_response": ["d"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A DevOps engineer is building a web application that will use federated access to a SAML identity provider (IdP). The web application requires sign up and sign in functionality using a custom webpage with authenticated access to AWS services.Which steps should the DevOps engineer take to implement the authentication and access control solution for the web application?", "related_lectures": []}]}
5794162
~~~
{"count": 25, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 57188352, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is running an eCommerce application on a fleet of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. There have been issues occurring occasionally where instances fail to launch successfully, and the support team wants to be notified whenever this occurs.</p><p>Which configuration update will achieve these requirements?</p><p>Which action will accomplish this?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can be notified when Amazon EC2 Auto Scaling is launching or terminating the EC2 instances in your Auto Scaling group. You manage notifications using Amazon Simple Notification Service (Amazon SNS).</p><p>Amazon EC2 Auto Scaling supports sending Amazon SNS notifications when the following events occur:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-25-53-89c65e690caba055c07c38d0fb8c649a.jpg\"><p>The \u201cautoscaling:EC2_INSTANCE_LAUNCH_ERROR\u201d would be the correct event to monitor as this indicates if failed instance launch events have occurred. SNS can then send a notification to the support team to let them know what has happened.</p><p><strong>CORRECT: </strong>\"Configure an Amazon SNS topic for the Auto Scaling group that sends a notification whenever a failed instance launch occurs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that sends an Amazon SNS notification when a failed SetInstanceHealth API call is made\" is incorrect.</p><p>This API action is used to set the health of an instance and does not monitor failed instance launch events.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm that sends a notification when an Amazon EC2 status check fails\" is incorrect.</p><p>Status checks will show issues with instances that are already running rather than issues with launch events.</p><p><strong>INCORRECT:</strong> \"Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired\" is incorrect.</p><p>Health checks can be used to check the status of an instance, but the instance is already running. This does not help if the instance failed to launch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html#auto-scaling-sns-notifications</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Create an Amazon CloudWatch alarm that sends an Amazon SNS notification when a failed SetInstanceHealth API call is made.</p>", "<p>Create an Amazon CloudWatch alarm that sends a notification when an Amazon EC2 status check fails.</p>", "<p>Configure an Amazon SNS topic for the Auto Scaling group that sends a notification whenever a failed instance launch occurs.</p>", "<p>Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.</p>"]}, "correct_response": ["c"], "section": "AWS Compute", "question_plain": "A company is running an eCommerce application on a fleet of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. There have been issues occurring occasionally where instances fail to launch successfully, and the support team wants to be notified whenever this occurs.Which configuration update will achieve these requirements?Which action will accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 57188354, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A custom application processes data associated with customer purchase activity using a multistep sequential action. Each step completes within 5 minutes. If a single step fails the entire process must be restarted. The current solution uses Amazon EC2 instances in an Auto Scaling group.</p><p>The company wants to update the application architecture so that if a single step fails only that step should be reprocessed.</p><p>What is the MOST operationally efficient solution that meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Step Functions is the ideal solution for rearchitecting this application. With Step Functions you can configure separate tasks with separate Lambda functions for processing each step of the application. The application logic can be configured as part of the state machine. If a single step fails the logic can require that only that step must rerun to reprocess the data.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-29-43-3ff9d4d6bdb94da71f59bd936642bd8b.jpg\"><p><strong>CORRECT: </strong>\"Create a web application to pass the data to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a web application to pass the data to separate Amazon SQS queues for each step. Process each step by using separate Lambda functions with the SQS queues\" is incorrect.</p><p>This is more operationally complex and there is no overarching logic to orchestrate the different components of the application and coordinate the reprocessing of failed steps.</p><p><strong>INCORRECT:</strong> \"Create a web application to write the data to an Amazon S3 bucket. Use S3 Event Notifications to publish to separate SQS queues for each step. Use EC2 instances to process the data in each SQS queue\" is incorrect.</p><p>There is no way that S3 can know which queue to place the data in as this answer states that all data is written to the same S3 bucket. This is also operationally complex and more expensive than using Lambda.</p><p><strong>INCORRECT:</strong> \"Create a REST API in Amazon API Gateway and configure the API as the endpoint for a web application to pass the data for processing. Use a single AWS Lambda function to process the data\" is incorrect.</p><p>Using a single Lambda function will not assist with decoupling the application so only a single step must be rerun if processing fails.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html\">https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>", "answers": ["<p>Create a web application to pass the data to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.</p>", "<p>Create a web application to write the data to an Amazon S3 bucket. Use S3 Event Notifications to publish to separate SQS queues for each step. Use EC2 instances to process the data in each SQS queue.</p>", "<p>Create a web application to pass the data to separate Amazon SQS queues for each step. Process each step by using separate Lambda functions with the SQS queues.</p>", "<p>Create a REST API in Amazon API Gateway and configure the API as the endpoint for a web application to pass the data for processing. Use a single AWS Lambda function to process the data.</p>"]}, "correct_response": ["a"], "section": "AWS Application Integration", "question_plain": "A custom application processes data associated with customer purchase activity using a multistep sequential action. Each step completes within 5 minutes. If a single step fails the entire process must be restarted. The current solution uses Amazon EC2 instances in an Auto Scaling group.The company wants to update the application architecture so that if a single step fails only that step should be reprocessed.What is the MOST operationally efficient solution that meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188356, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company allows DevOps engineers to assume an administrator IAM role when they need more permissions within an AWS account. The security team would like to be able to track usage of the administrator role and receive a notification when the administrator IAM role is assumed.</p><p>How should this be accomplished?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use Amazon EventBridge rules to react to API calls made by an AWS service that are recorded by AWS CloudTrail. The \u201cAWS API call via CloudTrail\u201d event pattern can be configured to specifically trigger when the API calls are made to assume the administrator IAM role.</p><p>Then, an AWS Lambda function can be triggered that processes the information received in the event and publishes this information to an Amazon SNS topic. The security team will be subscribers to the SNS topic and so will receive the notifications in their email inbox.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge events rule with an AWS API call via CloudTrail event pattern that triggers an AWS Lambda function that publishes a notification to an Amazon SNS topic if the administrator IAM role is assumed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge events rule with an AWS Management Console sign-in events event pattern that triggers an AWS Lambda function that publishes a notification to an Amazon SNS topic if the administrator IAM role is assumed\" is incorrect.</p><p>Management console sign-in events are different to assuming a role. In this case the users will be signing in with their own accounts and then assuming the administrator IAM role so this would not detect the assuming of the role.</p><p><strong>INCORRECT:</strong> \"Configure AWS Config to save logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and look for AWS STS sign-in events. Publish a notification to Amazon Kinesis Firehose if the administrator IAM role is assumed\" is incorrect.</p><p>AWS Config does not log STS sign-in events. These would be logged as API calls in the AWS CloudTrail service and would need to be picked up there. There is also no mechanism for alerting that would work here as Firehose would simply load the data to a datastore and would not notify anyone.</p><p><strong>INCORRECT:</strong> \"Configure Amazon GuardDuty to monitor the account and alert on any anomalous usage of the administrator IAM role. When detected, publish a notification to an AWS CloudTrail trail\" is incorrect.</p><p>GuardDuty cannot be configured to alert when a specific role is assumed. Also, you cannot publish a notification to an AWS CloudTrail trail, use SNS instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudtrail/\">https://digitalcloud.training/aws-cloudtrail/</a></p>", "answers": ["<p>Create an Amazon EventBridge events rule with an AWS Management Console sign-in events event pattern that triggers an AWS Lambda function that publishes a notification to an Amazon SNS topic if the administrator IAM role is assumed.</p>", "<p>Configure AWS Config to save logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and look for AWS STS sign-in events. Publish a notification to Amazon Kinesis Firehose if the administrator IAM role is assumed.</p>", "<p>Create an Amazon EventBridge events rule with an AWS API call via CloudTrail event pattern that triggers an AWS Lambda function that publishes a notification to an Amazon SNS topic if the administrator IAM role is assumed.</p>", "<p>Configure Amazon GuardDuty to monitor the account and alert on any anomalous usage of the administrator IAM role. When detected, publish a notification to an AWS CloudTrail trail.</p>"]}, "correct_response": ["c"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company allows DevOps engineers to assume an administrator IAM role when they need more permissions within an AWS account. The security team would like to be able to track usage of the administrator role and receive a notification when the administrator IAM role is assumed.How should this be accomplished?", "related_lectures": []}, {"_class": "assessment", "id": 57188358, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is using AWS CodeCommit for version control and AWS CodePipeline for orchestration of software deployments. The development team are using a remote main branch as the trigger for the pipeline. A developer noticed that the CodePipeline pipeline was not triggered after the developer pushed code changes to the CodeCommit repository.</p><p>Which of the following actions should be taken to troubleshoot this issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An Amazon CloudWatch Events rule must be created to trigger the pipeline when changes are committed to the CodeCommit repository. If you use the console to create or edit your pipeline, the CloudWatch Events rule is created for you. In this case, the developer should check to make sure that the rule has been created and is correctly configured.</p><p>The following is a sample CodeCommit event pattern for a MyTestRepo repository with a branch named master:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-34-08-dc82125abf42153048e88229777e35c5.jpg\"><p><strong>CORRECT: </strong>\"Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check that the CodePipeline service role has permission to access the CodeCommit repository\" is incorrect.</p><p>The issue is that the pipeline was not triggered. If the service role does not have permissions the pipeline should still be triggered by the CloudWatch Events rule but then an error would be generated if insufficient permissions are assigned for accessing the CodeCommit repository.</p><p><strong>INCORRECT:</strong> \"Check that the developer's IAM role has permission to push to the CodeCommit repository\" is incorrect.</p><p>The developer already committed the code to the repository and did not experience any errors.</p><p><strong>INCORRECT:</strong> \"Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline\" is incorrect.</p><p>An AWS Lambda function is not used to check for commits or to trigger the pipeline. A CloudWatch Events rule must be created for this purpose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Check that the developer's IAM role has permission to push to the CodeCommit repository.</p>", "<p>Check that an AWS Lambda function has been created to check for code commits and trigger the pipeline.</p>", "<p>Check that the CodePipeline service role has permission to access the CodeCommit repository.</p>", "<p>Check that an Amazon CloudWatch Events rule has been created for the main branch to trigger the pipeline.</p>"]}, "correct_response": ["d"], "section": "AWS Developer Tools", "question_plain": "A company is using AWS CodeCommit for version control and AWS CodePipeline for orchestration of software deployments. The development team are using a remote main branch as the trigger for the pipeline. A developer noticed that the CodePipeline pipeline was not triggered after the developer pushed code changes to the CodeCommit repository.Which of the following actions should be taken to troubleshoot this issue?", "related_lectures": []}, {"_class": "assessment", "id": 57188360, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company stores sensitive data in Amazon S3 buckets. Each day the development team create new buckets for projects they are working on, and all existing and future buckets must be secured. The security team requires encryption, logging, and versioning to be enabled. It is also required that buckets should not be publicly accessible.</p><p>What should a DevOps engineer do to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use the AWS Config Auto Remediation feature to auto remediate any non-compliant S3 buckets using the AWS Config rules. There are several pre-built rules you can leverage for various use cases. For example, the following rules can be used to meet the requirements specified in this question:</p><ul><li><p>s3-bucket-logging-enabled</p></li><li><p>s3-bucket-server-side-encryption-enabled</p></li><li><p>s3-bucket-public-read-prohibited</p></li><li><p>s3-bucket-public-write-prohibited</p></li></ul><p>These rules act as controls to prevent any non-compliant S3 activities. AWS Config uses AWS Systems Manager to implement the remediations and the rules are automation documents that Systems Manager runs.</p><p><strong>CORRECT: </strong>\"Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events\" is incorrect.</p><p>Trusted Advisor will not discover these specific compliance events and CloudWatch Events is not able to remediate them.</p><p><strong>INCORRECT:</strong> \"Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents\" is incorrect.</p><p>Systems Manager automation documents are used for remediation, but Systems Manager is unable to discover the specific compliance events for this scenario. AWS Config should be used with Systems Manager to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Enable AWS CloudTrail and configure automatic remediation using AWS Lambda\" is incorrect.</p><p>CloudTrail can only detect API actions rather than audit compliance with configuration requirements. Though it is possible to use CloudTrail to detect some configuration changes this would be complex to implement.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p>", "answers": ["<p>Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.</p>", "<p>Enable AWS Trusted Advisor and configure automatic remediation using Amazon CloudWatch Events.</p>", "<p>Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.</p>", "<p>Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "A company stores sensitive data in Amazon S3 buckets. Each day the development team create new buckets for projects they are working on, and all existing and future buckets must be secured. The security team requires encryption, logging, and versioning to be enabled. It is also required that buckets should not be publicly accessible.What should a DevOps engineer do to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188362, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer needs to enable cross-Region replication for all the objects in an Amazon S3 bucket. The destination bucket will also be created in a separate AWS account. The objects must be automatically replicated between the source and target buckets across Regions and accounts.</p><p>Which combination of actions should be performed to enable this replication? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The correct configuration for this solution is to create the IAM role in the source AWS account and apply permissions to the source and target buckets. Then, the engineer must add statements to the target bucket policy that allow the replication IAM role in the source account to replicate the objects into the target bucket. Finally, the engineer must create the replication rule in the source bucket to enable replication for all objects.</p><p><strong>CORRECT: </strong>\"Create an IAM role in the source account that has permissions to the source and destination buckets\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add statements to the target bucket policy allowing the replication IAM role to replicate objects\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a replication rule in the source bucket to enable replication for all the objects in the bucket\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the target account that has permissions to the source and destination buckets\" is incorrect \u2013 please refer to the correct process which is provided above.</p><p><strong>INCORRECT:</strong> \"Add statements to the source bucket policy allowing the replication IAM role to replicate objects\" is incorrect \u2013 please refer to the correct process which is provided above.</p><p><strong>INCORRECT:</strong> \"Create a replication rule in the target bucket to enable replication for all the objects in the source bucket\" is incorrect \u2013 please refer to the correct process which is provided above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-s3-and-glacier/\">https://digitalcloud.training/amazon-s3-and-glacier/</a></p>", "answers": ["<p>Create an IAM role in the source account that has permissions to the source and destination buckets.</p>", "<p>Create an IAM role in the target account that has permissions to the source and destination buckets.</p>", "<p>Add statements to the source bucket policy allowing the replication IAM role to replicate objects.</p>", "<p>Add statements to the target bucket policy allowing the replication IAM role to replicate objects.</p>", "<p>Create a replication rule in the source bucket to enable replication for all the objects in the bucket.</p>", "<p>Create a replication rule in the target bucket to enable replication for all the objects in the source bucket.</p>"]}, "correct_response": ["a", "d", "e"], "section": "AWS Storage", "question_plain": "A DevOps engineer needs to enable cross-Region replication for all the objects in an Amazon S3 bucket. The destination bucket will also be created in a separate AWS account. The objects must be automatically replicated between the source and target buckets across Regions and accounts.Which combination of actions should be performed to enable this replication? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 57188364, "assessment_type": "multi-select", "prompt": {"question": "<p>A company needs is deploying a new application in AWS and requires a CI/CD pipeline to automate process. The company requires that the entire CI/CD pipeline can be re-provisioned in different AWS accounts or Regions within minutes.</p><p>The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.</p><p>Which combination of actions should the DevOps engineer take to meet these requirements? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The DevOps engineer should create a pipeline using AWS CodePipeline to automate the entire deployment. The CodeCommit repository can be used as the source. The combinations of CodeBuild with Elastic Beanstalk provides a way to build, test, and deploy with automatic rollback upon failure.</p><p>The question also requires that the entire CI/CD pipeline can be recreated in different accounts and Regions. For this reason the pipeline should be deployed using AWS CloudFormation. The templates can then be easily used to recreate the entire stack.</p><p><strong>CORRECT: </strong>\"Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Provision all resources using AWS CloudFormation\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Copy the build artifact from CodeCommit to Amazon S3\" is incorrect.</p><p>There is no need to do this, CodeCommit can be used directly as a source for source code and build artifacts.</p><p><strong>INCORRECT:</strong> Implement an Amazon SQS queue to decouple the pipeline components\" is incorrect.</p><p>CodePipeline has its own built-in capabilities for passing information durably between stages and does not require decoupling using Amazon SQS.</p><p><strong>INCORRECT:</strong> \"Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline\" is incorrect.</p><p>This solution would not provide the automatic rollback upon failure requested. Automatic rollback can be implemented when using Elastic Beanstalk with CodeBuild. Otherwise, you would need CodeDeploy.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts-continuous-delivery-integration.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Configure an AWS CodePipeline pipeline with a build stage using AWS CodeBuild.</p>", "<p>Copy the build artifact from CodeCommit to Amazon S3.</p>", "<p>Launch Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) and set the ALB as the deployment target in AWS CodePipeline.</p>", "<p>Launch Amazon EC2 instances in an AWS Elastic Beanstalk environment and configure the environment as the deployment target in AWS CodePipeline.</p>", "<p>Implement an Amazon SQS queue to decouple the pipeline components.</p>", "<p>Provision all resources using AWS CloudFormation.</p>"]}, "correct_response": ["a", "d", "f"], "section": "AWS Developer Tools", "question_plain": "A company needs is deploying a new application in AWS and requires a CI/CD pipeline to automate process. The company requires that the entire CI/CD pipeline can be re-provisioned in different AWS accounts or Regions within minutes.The pipeline must support continuous integration, continuous delivery, and automatic rollback upon deployment failure. A DevOps engineer has already created an AWS CodeCommit repository to store the source code.Which combination of actions should the DevOps engineer take to meet these requirements? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 57188366, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer builds an artifact locally and then uploads it to an Amazon S3 bucket. The application has a local cache that must be cleared as part of the deployment. The engineer executes a command to do this, retrieves the artifact from Amazon S3, and unzips the artifact to complete the deployment.</p><p>The engineer wants to migrate to an automated CI/CD solution and incorporate checks to stop and roll back the deployment in the event of a failure. This requires tracking the progression of the deployment.</p><p>Which combination of actions will accomplish this? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The engineer wants to build an automated CI/CD pipeline. Therefore, the best solution is to use a code repository such as CodeCommit for committing the code. Once committed a CodePipeline will automatically pick up the changes and initiate CodeBuild which will build the artifacts and upload the S3.</p><p>After the build artifact has been uploaded CodeDeploy can then be used to deploy the application. The AppSpec file is used by CodeDeploy during deployments. The engineer should add the script to clear the cache to the BeforeInstall lifecycle hook, so it is executed before the install occurs.</p><p><strong>CORRECT: </strong>\"Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3\" is incorrect.</p><p>A better solution is to use CodePipeline which is designed for automating CI/CD pipelines and CodeBuild for building the artifact.</p><p><strong>INCORRECT:</strong> \"Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again\" is incorrect.</p><p>User data only runs when the instance is first started so is not useful for running any commands after that time.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all of the EC2 instances\" is incorrect.</p><p>Systems Manager is not suitable for deploying application updates and CodeDeploy should be used instead.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Check the code into a code repository. On each pull into master use Amazon CloudWatch Events to trigger an AWS Lambda function that builds the artifact and uploads it to Amazon S3.</p>", "<p>Write a custom script that clears the cache and specify the script in the BeforeInstall lifecycle hook in the AppSpec file.</p>", "<p>Add user data to the Amazon EC2 instances that contains script to clear the cache. Once deployed, test the application. If it is not successful, deploy it again.</p>", "<p>Set up an AWS CodePipeline pipeline to deploy the application. Check the code into a code repository as a source for the pipeline.</p>", "<p>Use AWS CodeBuild to build the artifact and upload it to Amazon S3. Use AWS CodeDeploy to deploy the artifact to the Amazon EC2 instances.</p>", "<p>Use AWS Systems Manager to download the artifact from Amazon S3 and deploy it to all the EC2 instances.</p>"]}, "correct_response": ["b", "d", "e"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer builds an artifact locally and then uploads it to an Amazon S3 bucket. The application has a local cache that must be cleared as part of the deployment. The engineer executes a command to do this, retrieves the artifact from Amazon S3, and unzips the artifact to complete the deployment.The engineer wants to migrate to an automated CI/CD solution and incorporate checks to stop and roll back the deployment in the event of a failure. This requires tracking the progression of the deployment.Which combination of actions will accomplish this? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 57188368, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. When bringing new EC2 instances online, the application must be tested before traffic can be directed to the instances.</p><p>Which Auto Scaling process should a DevOps engineer suspend to allow testing to be performed without removing the instances from the Auto Scaling group?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Auto Scaling processes can be suspended and then resumed. You might want to do this, for example, so that you can investigate a configuration issue that is causing the process to fail, or to prevent Amazon EC2 Auto Scaling from marking instances unhealthy and replacing them while you are making changes to your Auto Scaling group.</p><p>In this scenario the engineer wants to test the instances before directing traffic to them whilst keeping them in the auto scaling group. The best process to suspend in this case is AddToLoadBalancer.</p><p>This will prevent the instances from being added to the load balancer so no traffic will be directed to them.</p><p><strong>CORRECT: </strong>\"Suspend the process AddToLoadBalancer\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the process Health Check\" is incorrect.</p><p>This will stop all health checks from running. This may not have the desired effect as instances will not be marked as healthy.</p><p><strong>INCORRECT:</strong> \"Suspend the process Replace Unhealthy\" is incorrect.</p><p>This is useful if you want to ensure that unhealthy instances are not terminated and replaced.</p><p><strong>INCORRECT:</strong> \"Suspend the process AZ Rebalance\" is incorrect.</p><p>This would be used to suspend rebalancing of instances across availability zones.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Suspend the process Health Check.</p>", "<p>Suspend the process AddToLoadBalancer.</p>", "<p>Suspend the process Replace Unhealthy.</p>", "<p>Suspend the process AZ Rebalance.</p>"]}, "correct_response": ["b"], "section": "AWS Compute", "question_plain": "An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. When bringing new EC2 instances online, the application must be tested before traffic can be directed to the instances.Which Auto Scaling process should a DevOps engineer suspend to allow testing to be performed without removing the instances from the Auto Scaling group?", "related_lectures": []}, {"_class": "assessment", "id": 57188370, "assessment_type": "multi-select", "prompt": {"question": "<p>To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.</p><p>Which configuration actions should be taken to implement this? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Systems Manager Session Manager enables secure remote access to EC2 instances without the need to open ports for SSH or create bastion hosts. You can connect to instances through Session Manager privately by establishing a VPC endpoint in your VPC. This ensures that all connectivity takes place using private addresses.</p><p>The EC2 instances must have the Systems Manager agent running and they must have permissions to be able to communicate with the Systems Manager service. An instance profile can be easily attached to provide these permissions to instances.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-12_23-59-28-dac1a7fafdbe3507d5ca4b1fe07ed213.jpg\"><p><strong>CORRECT: </strong>\"Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create a VPC endpoint for Systems Manager in the Region where the instances are running\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range\" is incorrect.</p><p>Port 22 is used by the SSH protocol and is not required by Systems Manager Session Manager.</p><p><strong>INCORRECT:</strong> \"Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables\" is incorrect.</p><p>You cannot connect to EC2 instances via a NAT gateway. These gateways are used for outbound (internet) connectivity from the EC2 instances in private subnets.</p><p><strong>INCORRECT:</strong> \"Establish private Session Manager connectivity using the instance IDs of EC2 instances\" is incorrect.</p><p>You cannot connect privately to instances simply by connecting via the instance ID. You must establish a VPC endpoint and attach the necessary permissions to instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>", "answers": ["<p>Allow inbound access to TCP port 22 in all associated EC2 security groups from the VPC CIDR range.</p>", "<p>Attach an IAM instance profile to the EC2 instances that provides the necessary permissions for Systems Manager.</p>", "<p>Create a VPC endpoint for Systems Manager in the Region where the instances are running.</p>", "<p>Attach an Elastic IP to a NAT gateway in a public subnet and specify a route to the NAT gateway in the private subnet route tables.</p>", "<p>Establish private Session Manager connectivity using the instance IDs of EC2 instances.</p>"]}, "correct_response": ["b", "c"], "section": "AWS Management & Governance", "question_plain": "To increase security, a company plans to use AWS Systems Manager Session Manager to managed Amazon EC2 instances rather than SSH. The connectivity to Session Manager should also use a private network connection.Which configuration actions should be taken to implement this? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57188372, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application is deployed on Amazon EC2 instances in an Auto Scaling group. The application includes a process that sometimes fails causing the application to return an error. Restarting the process resolves the issue. The DevOps team are investigating the issue. While the investigation is ongoing an engineer needs a method of identifying the issue and quickly remediating it without affecting the capacity of the Auto Scaling group.</p><p>Which approach meets this requirement in the fastest possible time?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The Amazon CloudWatch agent procstat plugin continuously watches specified processes and reports their metrics to Amazon CloudWatch. After the data is in Amazon CloudWatch, you can associate alarms to trigger actions like notifying teams or remediations like restarting the processes, resizing the instances, and so on. In this case AWS Systems Manager Run Command can then be used to restart any processes that are failed.</p><p>This solution will resolve the issue in the fastest time as metrics can be reported on a 1-minute timeframe and restarting the process is much quicker than restarting the instance or terminating it and launching a replacement. It also does not affect the capacity of the Auto Scaling group.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances and use the procstat plugin to monitor the application process and send metrics to CloudWatch. Use Systems Manager Run Command to restart any processes that are failed\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run a regular cron job to execute commands and check if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy\" is incorrect.</p><p>This will cause the instance to be terminated and a new instance will be launched. This could be slower than creating an automated process to restart the affected process. This also affects the capacity of the Auto Scaling group while the new instances are launched and brought into service.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to run an AWS Lambda function that checks if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy\" is incorrect.</p><p>This is like the previous answer and has the same issues though it is potentially better as it uses AWS services to automate the process.</p><p><strong>INCORRECT:</strong> \"Use Amazon EventBridge to run an event every 10 minutes. Configure the event to run an AWS Lambda function that takes instances out of service one-by-one, restarts them, and then places them back in service\" is incorrect.</p><p>This may be the slowest of the options presented in terms of identification of the issue and it would be better to restart the process rather than restarting the instances.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/detecting-remediating-process-issues-on-ec2-instances-using-amazon-cloudwatch-aws-systems-manager/\">https://aws.amazon.com/blogs/mt/detecting-remediating-process-issues-on-ec2-instances-using-amazon-cloudwatch-aws-systems-manager/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>", "answers": ["<p>Run a regular cron job to execute commands and check if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy.</p>", "<p>Use Amazon EventBridge to run an AWS Lambda function that checks if the process is running. If the process returns an error, run the AWS CLI set-instance-health command to set the health state of the specified instance to Unhealthy.</p>", "<p>Install the Amazon CloudWatch agent on the EC2 instances and use the procstat plugin to monitor the application process and send metrics to CloudWatch. Use Systems Manager Run Command to restart any processes that are failed.</p>", "<p>Use Amazon EventBridge to run an event every 10 minutes. Configure the event to run an AWS Lambda function that takes instances out of service one-by-one, restarts them, and then places them back in service.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "An application is deployed on Amazon EC2 instances in an Auto Scaling group. The application includes a process that sometimes fails causing the application to return an error. Restarting the process resolves the issue. The DevOps team are investigating the issue. While the investigation is ongoing an engineer needs a method of identifying the issue and quickly remediating it without affecting the capacity of the Auto Scaling group.Which approach meets this requirement in the fastest possible time?", "related_lectures": []}, {"_class": "assessment", "id": 57188374, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is using AWS CodeBuild to build an application into a Docker image. The buildspec file is used to run the application build. The engineer needs to push the Docker image to an Amazon ECR repository only upon the successful completion of each build.</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The post_build phase is an optional sequence. It represents the commands, if any, that CodeBuild runs after the build. For example, you might use Maven to package the build artifacts into a JAR or WAR file, or you might push a Docker image into Amazon ECR. Then you might send a build notification through Amazon SNS.</p><p>Here is an example of a buildspec file with a post_build phase that pushes a Docker image to Amazon ECR:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-01-33-359af91dc55ebec29bd1a3cf7c7306b8.jpg\"><p><strong>CORRECT: </strong>\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the finally block to push the Docker image\" is incorrect.</p><p>Commands specified in a final block are run after commands in the commands block. The commands in a final block are run even if a command in the commands block fails. This would not be ideal as this would push the image to ECR even if commands in previous sequences failed.</p><p><strong>INCORRECT:</strong> \"Add an install phase to the buildspec file that uses the commands block to push the Docker image\" is incorrect.</p><p>These are commands that are run during installation. The develop would want to push the image only after all installations have succeeded. Therefore, the post_build phase should be used.</p><p><strong>INCORRECT:</strong> \"Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR\" is incorrect.</p><p>The artifacts sequence is not required if you are building and pushing a Docker image to Amazon ECR, or you are running unit tests on your source code, but not building it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Add a post_build phase to the buildspec file that uses the commands block to push the Docker image.</p>", "<p>Add a post_build phase to the buildspec file that uses the finally block to push the Docker image.</p>", "<p>Add an install phase to the buildspec file that uses the commands block to push the Docker image.</p>", "<p>Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer is using AWS CodeBuild to build an application into a Docker image. The buildspec file is used to run the application build. The engineer needs to push the Docker image to an Amazon ECR repository only upon the successful completion of each build.", "related_lectures": []}, {"_class": "assessment", "id": 57188376, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.</p><p>The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.</p><p>To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.</p><p>Which solution should the DevOps engineer recommend to achieve these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS DMS the data can be continuously replicated with high availability and the multiple databases can be consolidated into a petabyte-scale data warehouse.</p><p>The Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance. During a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-02-48-34cbd839e896555130df72dfa20efc35.jpg\"><p><strong>CORRECT: </strong>\u201cUse AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>AWS Glue is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" is incorrect.</p><p>Amazon Kinesis Data Streams is a fully managed, serverless data streaming service that stores and ingests various streaming data in real time at any scale. It is not suitable for migrating data between databases.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-migration-services/\">https://digitalcloud.training/aws-migration-services/</a></p>", "answers": ["<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift.</p>", "<p>Use Amazon EMR to replicate the data from the databases into Amazon Redshift.</p>", "<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift.</p>", "<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift.</p>"]}, "correct_response": ["c"], "section": "AWS Database", "question_plain": "A retail company has hired a DevOps engineer to provide consultancy services. The company runs Oracle and PostgreSQL services on Amazon RDS for storing large quantities of data generated by manufacturing equipment.The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team.To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift.Which solution should the DevOps engineer recommend to achieve these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188378, "assessment_type": "multi-select", "prompt": {"question": "<p>A retail company uses a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. Whenever traffic spikes occur, there have been inconsistent errors when the application auto scales. The following error message was generated:</p><p><em>\u201cInstance failed to complete user's Lifecycle Action: Lifecycle Action with token&lt;token-Id&gt; was abandoned: Heartbeat Timeout\u201d.</em></p><p>Which actions should a DevOps engineer take to collect logs for all affected instances and store them for later analysis? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The lifecycle-action-token is provided by Auto Scaling in the message sent as part of processing the lifecycle hook. You need to get the token from the original message.</p><p>The error message reported usually indicates one of the following:</p><ul><li><p>The maximum number of concurrent AWS CodeDeploy deployments associated with an AWS account was reached.</p></li><li><p>The Auto Scaling group tried to launch too many EC2 instances too quickly. The API calls to RecordLifecycleActionHeartbeat or CompleteLifecycleAction for each new instance were throttled.</p></li><li><p>An application in CodeDeploy was deleted before its associated deployment groups were updated or deleted.</p></li></ul><p>Therefore, the engineer can update the deployment group is limits have been reached and create a solution for extracting the application logs for later analysis. This solution can use Amazon EventBridge, Lambda and SSM Run Command with S3 as the destination.</p><p>Amazon Athena allows for running SQL queries against data in an Amazon S3 bucket. The engineer can then perform analysis to identify there are any application issues that must be fixed.</p><p><strong>CORRECT: \"</strong>Update the deployment group as the AWS CodeDeploy limits have been reached<strong>\" is a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>CORRECT: \"</strong>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena<strong>\" is also a correct answer (as explained above.)</strong></p><p><strong>INCORRECT: \"</strong>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination<strong>\" is incorrect.</strong></p><p><strong>The access logs will not provide the necessary information to troubleshoot and analyze the issues that are occurring. Access logs record information about the requests from clients.</strong></p><p><strong>INCORRECT: \"</strong>Analyze the logs by loading them into an Amazon EMR cluster<strong>\" is incorrect.</strong></p><p><strong>This won\u2019t be needed as this is not a map reduce use case and data can be analyzed by EMR in Amazon S3.</strong></p><p><strong>INCORRECT: \"</strong>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application<strong>\" is incorrect.</strong></p><p><strong>There is no evidence that health checks are misconfigured from the errors that were generated. Auto scaling must be using the health checks as it is managing to auto scale and bring instances into service.</strong></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-reboot</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Update the deployment group as the AWS CodeDeploy limits have been reached.</p>", "<p>Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create an Amazon EventBridge rule and invoke a Lambda function that uses SSM Run Command to extract the application logs and store them in S3.</p>", "<p>Enable Access Logs at the Target Group level and configure an Amazon S3 bucket as the destination.</p>", "<p>Analyze the logs directly in the Amazon S3 bucket using Amazon Athena.</p>", "<p>Analyze the logs by loading them into an Amazon EMR cluster.</p>", "<p>Update the health checks on the Auto Scaling group to use the correct port and protocol for the application.</p>"]}, "correct_response": ["a", "b", "d"], "section": "AWS Compute", "question_plain": "A retail company uses a Spring boot web application running in an Auto Scaling group and behind an Application Load Balancer. Whenever traffic spikes occur, there have been inconsistent errors when the application auto scales. The following error message was generated:\u201cInstance failed to complete user's Lifecycle Action: Lifecycle Action with token&lt;token-Id&gt; was abandoned: Heartbeat Timeout\u201d.Which actions should a DevOps engineer take to collect logs for all affected instances and store them for later analysis? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 57188380, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances and calls an AWS Lambda function to process certain operations. A DevOps engineer needs to update the Lambda code without needing to modify the application on EC2. The updates should be initially tested with a subset of the traffic and rollback must be possible if issues occur. The solution must be operationally efficient.</p><p>Which actions should the DevOps engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can create one or more aliases for your Lambda function. A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN). You can add multiple versions of a function to an alias and configure a weighting to direct percentages of traffic to one version or another.</p><p>This enables a blue/green deployment in which new code updates can be tested with a subset of the traffic. It is easy to then move to the new function or roll back to the original function by simply updating the alias configuration.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-06-30-d1d011076d05223238260eb865eff3ba.jpg\"><p><strong>CORRECT: </strong>\"Create an alias that points to the current version of the function. Publish an additional version with the new code, add the version to the alias, and configure a weight that directs 10% of traffic to the new version. Modify the application on EC2 to use the alias endpoint address\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an additional function with the new code. Create Amazon Route 53 weighted records. Configure the records to direct 90% of the traffic to the original function and 10% to the new function. Modify the application on EC2 to use the DNS record configured in Route 53\" is incorrect.</p><p>Though this is indeed possible, it would be better to use the built-in features of AWS Lambda rather than external services. It is also much less operationally efficient to use multiple functions rather than versioning.</p><p><strong>INCORRECT:</strong> \"Create an alias that points to the current version of the function. Create a second function with the updated code and create an alias for that function that uses the same alias name. Configure the provisioned concurrency for the second function so it only serves 10% of requests. Modify the application on EC2 to use the alias endpoint address\" is incorrect.</p><p>This is not how aliases work; they are created within a function and point to separate versions of that function. Even if you create aliases with the same name on different functions they will still have a different ARN as the function name is included. It is also not possible to use provisioned concurrency to control weightings of traffic in this manner.</p><p><strong>INCORRECT:</strong> \"Create multiple Lambda layers for different versions of the code. Add the updated code to a new layer using a .zip archive. Create an alias that points to the layer containing the original code. Add the new layer to the alias and configure a weight that directs 10% of traffic to the new layer. Modify the application on EC2 to use the alias endpoint address\" is incorrect.</p><p>A Lambda <em>layer</em> is a .zip file archive that can contain additional code or other content. A layer can contain libraries, a custom runtime, data, or configuration files. In this case, versions should be used rather than layers for updating the code. Aliases also point to versions, not layers.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>", "answers": ["<p>Create an alias that points to the current version of the function. Publish an additional version with the new code, add the version to the alias, and configure a weight that directs 10% of traffic to the new version. Modify the application on EC2 to use the alias endpoint address.</p>", "<p>Create an additional function with the new code. Create Amazon Route 53 weighted records. Configure the records to direct 90% of the traffic to the original function and 10% to the new function. Modify the application on EC2 to use the DNS record configured in Route 53.</p>", "<p>Create an alias that points to the current version of the function. Create a second function with the updated code and create an alias for that function that uses the same alias name. Configure the provisioned concurrency for the second function so it only serves 10% of requests. Modify the application on EC2 to use the alias endpoint address.</p>", "<p>Create multiple Lambda layers for different versions of the code. Add the updated code to a new layer using a .zip archive. Create an alias that points to the layer containing the original code. Add the new layer to the alias and configure a weight that directs 10% of traffic to the new layer. Modify the application on EC2 to use the alias endpoint address.</p>"]}, "correct_response": ["a"], "section": "AWS Compute", "question_plain": "An application runs on Amazon EC2 instances and calls an AWS Lambda function to process certain operations. A DevOps engineer needs to update the Lambda code without needing to modify the application on EC2. The updates should be initially tested with a subset of the traffic and rollback must be possible if issues occur. The solution must be operationally efficient.Which actions should the DevOps engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 57188382, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A custom application generates events and produces data that must be processed for each event. An event-driven solution is required to process the events and save the output to a serverless key/value store.</p><p>Which actions should a DevOps engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To create an event-driven architecture for this requirement the engineer can configure the application to submit the event data to an SNS topic. The Lambda function can be subscribed to the topic and will process the data and then save the results to Amazon DynamoDB which is a key/value database.</p><p><strong>CORRECT: </strong>\"Configure the application to submit the event data to an SNS topic. Subscribe an AWS Lambda function to the topic and configure the function to process the data and save the output to an Amazon DynamoDB table\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an Amazon S3 bucket. Create an Amazon EventBridge rule that reacts to state changes in S3 and triggers Amazon Athena to process the data and save the output to an Amazon DynamoDB table\" is incorrect.</p><p>An event notification rule can be created on S3 rather than using EventBridge. However, you cannot use Athena to process data as it is an analytics service, not a compute service.</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an Amazon Kinesis Data Firehose delivery stream with an Amazon S3 destination. Configure an event notification for an AWS Lambda function to process the data and save the output to an Amazon RDS database\" is incorrect.</p><p>Amazon RDS is not a serverless key/value store. It is a relational database that uses Amazon EC2 instances so it is not suitable for this requirement.</p><p><strong>INCORRECT:</strong> \"Configure the application to submit the event data to an SQS queue. Configure a trigger for an AWS Lambda function and configure the function to process the data and save the output to an Amazon ElastiCache cluster\" is incorrect.</p><p>Amazon ElastiCache is a key/value store, but it is not a serverless database; it uses Amazon EC2 instances, so it is not suitable for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html\">https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>", "answers": ["<p>Configure the application to submit the event data to an Amazon S3 bucket. Create an Amazon EventBridge rule that reacts to state changes in S3 and triggers Amazon Athena to process the data and save the output to an Amazon DynamoDB table.</p>", "<p>Configure the application to submit the event data to an SQS queue. Configure a trigger for an AWS Lambda function and configure the function to process the data and save the output to an Amazon ElastiCache cluster.</p>", "<p>Configure the application to submit the event data to an Amazon Kinesis Data Firehose delivery stream with an Amazon S3 destination. Configure an event notification for an AWS Lambda function to process the data and save the output to an Amazon RDS database.</p>", "<p>Configure the application to submit the event data to an SNS topic. Subscribe an AWS Lambda function to the topic and configure the function to process the data and save the output to an Amazon DynamoDB table.</p>"]}, "correct_response": ["d"], "section": "AWS Application Integration", "question_plain": "A custom application generates events and produces data that must be processed for each event. An event-driven solution is required to process the events and save the output to a serverless key/value store.Which actions should a DevOps engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 57188384, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial company has applications hosted in multiple AWS accounts which are managed by AWS organizations. A security audit was conducted to address any potential security breaches and implement best practices. The security audit report recommended that all security related logging and security findings are collect in a centralized security account.</p><p>How can this be achieved?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>GuardDuty analyzes and processes VPC flow log, AWS CloudTrail event log, and DNS log data sources. You don\u2019t need to manually manage these data sources because the data is automatically leveraged and analyzed when you activate GuardDuty.</p><p>For example, GuardDuty consumes VPC Flow Log events directly from the VPC Flow Logs feature through an independent and duplicative stream of flow logs. As a result, you don\u2019t incur any operational burden on existing workloads.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-10-01-c38c34b5c636f2d9f839cf769fa2e58c.jpg\">", "answers": ["<p>Use Amazon GuardDuty in each organization to detect the attacks on EC2 instances. Specify an organization as the GuardDuty administrator. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>", "<p>Use Amazon Macie in each organization to detect the attacks on EC2 instances. Specify an organization as the Macie administrator. Create a CloudWatch rule in the Macie administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>", "<p>Use Amazon GuardDuty in each account to detect the attacks on EC2 instances. Create a CloudWatch rule in the GuardDuty administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Streams to send findings to a designated S3 bucket.</p>", "<p>Use Amazon Inspector in each organization to detect the attacks on EC2 instances. Specify an organization as the Inspector administrator. Create a CloudWatch rule in the Inspector administrator account to detect these findings. Create an automation chain from the CloudWatch rule to trigger Kinesis Data Firehose to send findings to a designated S3 bucket.</p>"]}, "correct_response": ["a"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A financial company has applications hosted in multiple AWS accounts which are managed by AWS organizations. A security audit was conducted to address any potential security breaches and implement best practices. The security audit report recommended that all security related logging and security findings are collect in a centralized security account.How can this be achieved?", "related_lectures": []}, {"_class": "assessment", "id": 57188386, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps manager requires a disaster recovery (DR) solution for a workload deployed on Amazon EC2 instances in an Amazon VPC within the us-east-1 Region. The DR solution should enable data replication to a staging area subnet in another AWS Region. The DR solution minimize operational overhead and enable fast failover and failback.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Elastic Disaster Recovery (AWS DRS) minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery.</p><p>You can increase IT resilience when you use AWS Elastic Disaster Recovery to replicate on-premises or cloud-based applications running on supported operating systems. Use the AWS Management Console to configure replication and launch settings, monitor data replication, and launch instances for drills or recovery.</p><p>Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication.</p><p><strong>CORRECT: </strong>\"Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems\" is incorrect.</p><p>AWS RAM cannot be used to share VPCs across Regions. Also, DataSync is not suitable for synchronizing the application data on Amazon EC2 instances.</p><p><strong>INCORRECT:</strong> \"Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery\" is incorrect.</p><p>AWS Resilience Hub provides a central place to define, validate, and track the resilience of applications on AWS. It cannot be used to configure DR protection or enable automated recovery.</p><p><strong>INCORRECT:</strong> \"Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery\" is incorrect.</p><p>You can use AWS Backup to enable cross-Region backups of EC2 instances. However, AWS Fault Injection Simulator is not used for automated recovery, it is used for running fault injection experiments to improve an application\u2019s performance, observability, and resiliency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html\">https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html</a></p>", "answers": ["<p>Use AWS Elastic Disaster Recovery to replicate data to a staging area subnet in another Region using the replication agent installed on the EC2 instance.</p>", "<p>Use AWS Resource Access Manager (RAM) to share the VPC across Regions and configure AWS DataSync to synchronize data between source and target systems.</p>", "<p>Create a resource group for the EC2 instances and configure DR protection using AWS Resilience Hub. Configure RPO/RTO settings and enable automated recovery.</p>", "<p>Use AWS Backup to enable cross-Region snapshots for the EC2 instances. Use AWS Fault Injection Simulator to enable automated recovery.</p>"]}, "correct_response": ["a"], "section": "AWS Compute", "question_plain": "A DevOps manager requires a disaster recovery (DR) solution for a workload deployed on Amazon EC2 instances in an Amazon VPC within the us-east-1 Region. The DR solution should enable data replication to a staging area subnet in another AWS Region. The DR solution minimize operational overhead and enable fast failover and failback.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188388, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company requires an extremely high performance in-memory database for an application. The database used should store data durably across multiple availability zones. The application requires that the database support strong consistency and can scale seamlessly to many terabytes in size.</p><p>Which database should the company use for this application?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon MemoryDB for Redis is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance. It is purpose-built for modern applications with microservices architectures.</p><p>Amazon MemoryDB for Redis vs ElastiCache:</p><p>\u2022 Use ElastiCache for caching DB queries.</p><p>\u2022 Use MemoryDB for a full DB solution combining DB and cache.</p><p>\u2022 MemoryDB offers higher performance with lower latency .</p><p>\u2022 MemoryDB offers strong consistency for primary nodes and eventual consistency for replica nodes.</p><p>\u2022 With ElastiCache there can be some inconsistency and latency depending on the engine and caching strategy.</p><p>For this scenario the requirement is for a full DB solution, not a caching solution, so MemoryDB for Redis is the best choice.</p><p><strong>CORRECT: </strong>\"Use Amazon MemoryDB for Redis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache for Redis\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use Amazon ElastiCache for Memcached\" is incorrect as explained above.</p><p><strong>INCORRECT:</strong> \"Use Amazon Managed Grafana\" is incorrect. This service is not a database service, it is used for data visualization of operational metrics, logs, and traces.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/memorydb/features/\">https://aws.amazon.com/memorydb/features/</a></p>", "answers": ["<p>Use Amazon ElastiCache for Redis.</p>", "<p>Use Amazon ElastiCache for Memcached.</p>", "<p>Use Amazon MemoryDB for Redis.</p>", "<p>Use Amazon Managed Grafana.</p>"]}, "correct_response": ["c"], "section": "AWS Database", "question_plain": "A company requires an extremely high performance in-memory database for an application. The database used should store data durably across multiple availability zones. The application requires that the database support strong consistency and can scale seamlessly to many terabytes in size.Which database should the company use for this application?", "related_lectures": []}, {"_class": "assessment", "id": 57188390, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An organization is running containerized applications across Amazon EKS, Amazon ECS, and on-premises clusters. Due to some recent issues that caused outages, a solution is required to track container performance and system health, detect errors. The solution should enable collection and aggregation of time-series metrics from all container services for monitoring and analytics.</p><p>Which combination of services can the organization use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale. The service is integrated with Amazon Elastic Kubernetes Service (EKS), Amazon Elastic Container Service (ECS), and AWS Distro for OpenTelemetry.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-05_23-21-58-4a441982fe750d346208169ad7754e86.jpg\"><p>The company can use Amazon Managed Grafana, a fully managed AWS service that makes it easy to use Grafana to monitor operational data with interactive data visualizations in a single console across multiple data sources, without needing to deploy, manage, and operate Grafana servers.</p><p><strong>CORRECT: </strong>\"Use Amazon Managed Service for Prometheus for collection of metrics and use Amazon Managed Grafana for visualization and analytics\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the unified Amazon CloudWatch agent to collect the metrics, Amazon Athena to run SQL queries, and AWS Glue for visualization\" is incorrect.</p><p>The CloudWatch agent is used with Amazon EC2 instances and cannot be used with containers running on all these platforms. The Container Insights tool which is part of CloudWatch can be used for containers running in the AWS Cloud.</p><p><strong>INCORRECT:</strong> \"Use the AWS Systems Manager agent to collect the metrics and use Amazon Managed Service for Prometheus for visualization and analytics\" is incorrect.</p><p>Systems Manager cannot collect the time-series metrics from containerized applications running on the specified platforms.</p><p><strong>INCORRECT:</strong> \"Use AWS AppConfig to collect application metrics from the containers and use Amazon OpenSearch Service for visualization and analytics\" is incorrect.</p><p>AWS AppConfig is a capability of Systems Manager that is used to create, manage, and quickly deploy application configurations. It is not used for collecting metrics.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/prometheus/features/\">https://aws.amazon.com/prometheus/features/</a></p>", "answers": ["<p>Use the unified Amazon CloudWatch agent to collect the metrics, Amazon Athena to run SQL queries, and AWS Glue for visualization.</p>", "<p>Use Amazon Managed Service for Prometheus for collection of metrics and use Amazon Managed Grafana for visualization and analytics.</p>", "<p>Use the AWS Systems Manager agent to collect the metrics and use Amazon Managed Service for Prometheus for visualization and analytics.</p>", "<p>Use AWS AppConfig to collect application metrics from the containers and use Amazon OpenSearch Service for visualization and analytics.</p>"]}, "correct_response": ["b"], "section": "AWS Management & Governance", "question_plain": "An organization is running containerized applications across Amazon EKS, Amazon ECS, and on-premises clusters. Due to some recent issues that caused outages, a solution is required to track container performance and system health, detect errors. The solution should enable collection and aggregation of time-series metrics from all container services for monitoring and analytics.Which combination of services can the organization use?", "related_lectures": []}, {"_class": "assessment", "id": 57188392, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.</p><p>What is the MOST operationally efficient solution that meets this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Cognito can invoke a trigger after signing in a user such as triggering an AWS Lambda function. Cognito will pass certain parameters to the Lambda function and the Lambda function can then trigger Amazon SES to send an email notification.</p><p>These are the parameters that Amazon Cognito passes to this Lambda function along with the event information in the common parameters.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-05_23-23-34-5d88ec6b26463c78dd328dc8e674ea16.jpg\"><p><strong>CORRECT: </strong>\"Create an AWS Lambda function that uses Amazon SES to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that uses Amazon SES to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received\" is incorrect.</p><p>The API is not suitable and not necessary for this purpose when Cognito can directly trigger Lambda which has the logic to send the necessary notification.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that uses Amazon SES to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status\" is incorrect.</p><p>As above, this is unnecessary as Cognito can trigger the Lambda function directly.</p><p><strong>INCORRECT:</strong> \"Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user\" is incorrect.</p><p>As above, this is unnecessary as Cognito can trigger the Lambda function directly.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html#cognito-user-pools-lambda-trigger-syntax-post-auth\">https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html#cognito-user-pools-lambda-trigger-syntax-post-auth</a></p>", "answers": ["<p>Create an AWS Lambda function that uses Amazon SES to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received.</p>", "<p>Create an AWS Lambda function that uses Amazon SES to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function.</p>", "<p>Create an AWS Lambda function that uses Amazon SES to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status.</p>", "<p>Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user.</p>"]}, "correct_response": ["b"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.What is the MOST operationally efficient solution that meets this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 57188394, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.</p><p>What should a DevOps engineer do to meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Secrets Manager is ideal for this scenario as it can be used to securely store the secrets. Automatic rotation can be enabled for several AWS databases and can be configured through custom Lambda functions for other databases.</p><p><strong>CORRECT: </strong>\"Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID\" is incorrect.</p><p>CloudHSM is used for storing encryption keys, not connection credentials.</p><p><strong>INCORRECT:</strong> \"Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution\" is incorrect.</p><p>This is not a secure method of storing the credentials and Secrets Manager is more secure.</p><p><strong>INCORRECT:</strong> \"Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID\" is incorrect.</p><p>KMS is used for storing encryption keys, not connection credentials.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/secrets-manager/\">https://aws.amazon.com/secrets-manager/</a></p>", "answers": ["<p>Store the credentials in AWS CloudHSM. Associate the Lambda function with a role that can retrieve the credentials from CloudHSM using the key ID.</p>", "<p>Store the credentials in AWS Secrets Manager. Associate the Lambda function with a role that can retrieve the credentials from Secrets Manager given using the secret ID.</p>", "<p>Move the database credentials to an environment variable associated with the Lambda function. Retrieve the credentials from the environment variable upon execution.</p>", "<p>Store the credentials in AWS Key Management Service (AWS KMS). Associate the Lambda function with a role that can retrieve the credentials from AWS KMS using the key ID.</p>"]}, "correct_response": ["b"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A DevOps engineer is developing an application that calls AWS Lambda functions. The functions must connect to a database and credentials must not be stored in source code. The credentials for connection to the database must be regularly rotated to meet security policy requirements.What should a DevOps engineer do to meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188346, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The information security policy of a company has been updated and now requires that all Amazon EBS volumes must be encrypted. Any volumes that are not encrypted should be marked as non-compliant. The company uses AWS Organizations to manage multiple accounts. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is applied.</p><p>Which solution will accomplish this MOST efficiently?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Config allows you to manage AWS Config rules across all AWS accounts within an organization. You can:</p><ul><li><p>Centrally create, update, and delete AWS Config rules across all accounts in your organization.</p></li><li><p>Deploy a common set of AWS Config rules across all accounts and specify accounts where AWS Config rules should not be created.</p></li><li><p>Use the APIs from the master account in AWS Organizations to enforce governance by ensuring that the underlying AWS Config rules are not modifiable by your organization\u2019s member accounts.</p></li></ul><p>The DevOps engineer should create an organization level rule and then setup an SCP that prevents any modifications from happening that would stop the rule from running.</p><p>The engineer can use the AWS Config \u201cec2-ebs-encryption-by-default\u201d rule. This rule checks that Amazon Elastic Block Store (EBS) encryption is enabled by default. The rule is NON_COMPLIANT if the encryption is not enabled.</p><p><strong>CORRECT: </strong>\"Create an AWS Config rule at the AWS organization level to check whether EBS encryption is enabled. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable the AWS Config ec2-ebs-encryption-by-default rule to check whether EBS encryption is enabled. Deploy the rule in the management account of the Organization\" is incorrect.</p><p>Deploying the rule in the management account will not suffice as the company has multiple accounts in an AWS Organizations configuration. The rule must be deployed across all accounts.</p><p><strong>INCORRECT:</strong> \"Run a scheduled AWS Lambda function in each account that checks the encryption status of EBS volumes in the account. Publish the report to a centralized Amazon S3 bucket. Use Amazon Athena to analyze the data\" is incorrect.</p><p>While this may provide the required data, this is not the most efficient solution. Using Config is preferable as it will have less overhead and is designed specifically for compliance purposes and is a superior solution.</p><p><strong>INCORRECT:</strong> \"Apply an SCP in Organizations that uses conditional expressions to prevent the launch of Amazon EC2 instances that do not have encrypted EBS volumes. Apply the SCP to all AWS accounts\" is incorrect.</p><p>SCPs don't affect users or roles in the management account and condition elements may not affect users logged in with root user credentials. AWS Config will be a better solution for this requirement.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html\">https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/ec2-ebs-encryption-by-default.html\">https://docs.aws.amazon.com/config/latest/developerguide/ec2-ebs-encryption-by-default.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-config/\">https://digitalcloud.training/aws-config/</a></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>", "answers": ["<p>Enable the AWS Config ec2-ebs-encryption-by-default rule to check whether EBS encryption is enabled. Deploy the rule in the management account of the Organization.</p>", "<p>Run a scheduled AWS Lambda function in each account that checks the encryption status of EBS volumes in the account. Publish the report to a centralized Amazon S3 bucket. Use Amazon Athena to analyze the data.</p>", "<p>Create an AWS Config rule at the AWS organization level to check whether EBS encryption is enabled. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.</p>", "<p>Apply an SCP in Organizations that uses conditional expressions to prevent the launch of Amazon EC2 instances that do not have encrypted EBS volumes. Apply the SCP to all AWS accounts.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "The information security policy of a company has been updated and now requires that all Amazon EBS volumes must be encrypted. Any volumes that are not encrypted should be marked as non-compliant. The company uses AWS Organizations to manage multiple accounts. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is applied.Which solution will accomplish this MOST efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 57188348, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.</p><p>Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The existing Jenkins implementation runs on an Amazon EC2 instance, and this requires more operational management compared to using AWS CodeBuild which is a managed service. The DevOps engineer should replace Jenkins with AWS CodeBuild which is a fully managed build service.</p><p>CodeBuild compiles source code, runs unit tests, and produces artifacts that are ready to deploy. Encryption for build artifacts such as a cache, logs, exported raw test report data files, and build results, is enabled by default, and uses AWS managed keys (AWS KMS).</p><p><strong>CORRECT: </strong>\"Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes\" is incorrect.</p><p>This solution may be secure, but it does require more operational management compared to using AWS CodeBuild and is therefore not the best option.</p><p><strong>INCORRECT:</strong> \"Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group\" is incorrect.</p><p>Moving Jenkins to an Auto Scaling group does not reduce the operational management. This is also an option that requires more overhead to manage compared to using a managed service such as AWS CodeBuild.</p><p><strong>INCORRECT:</strong> \"Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM)\" is incorrect.</p><p>Artifacts cannot be encrypted using ACM as that is a service that issues SSL/TLS certificates which are used for encryption in-transit rather than encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Configure AWS Systems Manager to patch the Jenkins EC2 instances and set encryption for all Amazon EBS volumes.</p>", "<p>Replace the Jenkins instance running on Amazon EC2 with AWS CodeBuild and configure artifact encryption.</p>", "<p>Store build artifacts on Amazon S3 with default encryption enabled and move Jenkins to an Auto Scaling group.</p>", "<p>Add a build action using AWS CodePipeline and encrypt the artifacts using AWS Certification Manager (ACM).</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "A company manages a continuous integration and continuous delivery (CI/CD) pipeline that includes a Jenkins implementation that runs on Amazon EC2. The security team has requested that all build artifacts are encrypted as they contain company sensitive data.Which changes should a DevOps engineer make to meet these requirements whilst reducing operational overhead for ongoing management?", "related_lectures": []}, {"_class": "assessment", "id": 57188350, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.</p><p>What is the MOST efficient and cost-effective solution?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most efficient and cost-effective solution is to use the CloudWatch agent to collect log files from both AWS resources and on-premises servers and save that data to a centralized Amazon S3 bucket. S3 event notifications can be used to trigger an AWS Lambda function that analyzes the data looking for anomalies.</p><p>Amazon Athena is ideal for running ad-hoc SQL queries on data stored in S3. This can be used by the company when they have specific queries they need to run against the data.</p><p><strong>CORRECT: </strong>\"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center\" is incorrect.</p><p>The best solution is not to store the data on-premises, it should be in the AWS Cloud. You would then be able to use the ELK stack to analyze the data.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket\" is incorrect.</p><p>You cannot use S3 APIs to export log files from on-premises servers. You also cannot use KDA to analyze data in S3, it is used for analyzing data in other Kinesis services.</p><p><strong>INCORRECT:</strong> \"Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account\" is incorrect.</p><p>As above, you cannot use the S3 API with on-premises resources.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://aws.amazon.com/solutions/implementations/centralized-logging/\">https://aws.amazon.com/solutions/implementations/centralized-logging/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to export on-premises logs and store the logs in an S3 bucket in a central account. Use Amazon Kinesis Data Analytics to query the data in the S3 bucket.</p>", "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Use the Amazon S3 API to export log files and store them on-premises. Use an Amazon Elasticsearch Logstash Kibana stack to analyze logs in the on-premises data center.</p>", "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Use the Amazon S3 API to import on-premises logs. Store all logs in S3 buckets in individual accounts. Use Amazon Athena to run SQL queries on the logs in each account.</p>", "<p>Collect system logs and application logs using the Amazon CloudWatch Logs agent. Install the CloudWatch Logs agent on the on-premises servers. Store all logs in an S3 bucket in a central account. Set up an Amazon S3 trigger and an AWS Lambda function to analyze incoming logs and automatically identify anomalies. Use Amazon Athena to run ad hoc queries on the logs in the central account.</p>"]}, "correct_response": ["d"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company has several AWS accounts and an on-premises data center. Several microservices applications run across the accounts and data center. The distributed architecture results in challenges with investigating application issues as the logs are saved in a variety of locations. A DevOps engineer must configure a solution that centralizes and aggregates the logs for analytics.What is the MOST efficient and cost-effective solution?", "related_lectures": []}]}
5794166
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 57188396, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is concerned about the security of their Amazon EC2 instances. They require an automated solution for identifying security vulnerabilities on the instances and notifying the security team. They also require an audit trail of all login activities on the EC2 instances.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. This is the best service to use for automatic detection of security vulnerabilities on the EC2 instances.</p><p>The unified CloudWatch agent enables you to collect internal system-level metrics from Amazon EC2 instances across operating systems. The metrics can include in-guest metrics, in addition to the metrics for EC2 instances.</p><p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. The system logs that are collected will include information on all login activities on the EC2 instances.</p><p><strong>CORRECT: </strong>\"Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket\" is incorrect.</p><p>Systems Manager Patch Manager can install patches to resolve vulnerabilities but does not provide automated detection of vulnerabilities. Instead, it scans to see if specific patches are installed or not. The KCL is used with Kinesis Data Streams for processing streaming data and does not collect system logs from EC2 instances.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console\" is incorrect.</p><p>As above, Systems Manager is not suitable for this task and does not capture auditing information by processing system logs.</p><p><strong>INCORRECT:</strong> \"Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console\" is incorrect.</p><p>GuardDuty is a threat detection service that monitors for malicious activity. It does not detect vulnerabilities on EC2 instances. The X-Ray service is used for gathering trace data for troubleshooting and understanding application performance.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/inspector/features/\">https://aws.amazon.com/inspector/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-inspector/\">https://digitalcloud.training/amazon-inspector/</a></p>", "answers": ["<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Kinesis Client Library (KCL) to capture system logs and save them to an Amazon S3 bucket.</p>", "<p>Use AWS Systems Manager to automatically detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture an audit trail using system logs and view login activity in the AWS CloudTrail console.</p>", "<p>Use AWS GuardDuty to detect vulnerabilities on the EC2 instances. Configure the AWS X-Ray daemon to gather trace data and add metrics to Amazon CloudWatch. View the audit trail of login activities in the CloudWatch console.</p>", "<p>Use Amazon Inspector to automatically detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and upload them to Amazon CloudWatch Logs.</p>"]}, "correct_response": ["d"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A company is concerned about the security of their Amazon EC2 instances. They require an automated solution for identifying security vulnerabilities on the instances and notifying the security team. They also require an audit trail of all login activities on the EC2 instances.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188398, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.</p><p>Which deployment configuration will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The benefits of using AWS SAM to create the serverless application include that it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM can perform the following actions:</p><ul><li><p>Deploys new versions of the Lambda function, and automatically creates aliases that point to the new version.</p></li><li><p>Gradually shifts customer traffic to the new version until you\u2019re satisfied that it's working as expected, or you roll back the update.</p></li><li><p>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly, and your application operates as expected.</p></li><li><p>Rolls back the deployment if CloudWatch alarms are triggered.</p></li></ul><p>The DevOps engineer can choose the Deployment Preference Type. The following options are available:</p><ul><li><p><strong>Canary:</strong> Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p></li><li><p><strong>Linear:</strong> Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment.</p></li><li><p><strong>All-at-once:</strong> All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p></li></ul><p>The best option to minimize the impact of failed deployments is to use the canary deployment type. This will ensure that only a small amount of traffic reaches the new Lambda function in the first shift and if any issues occur the deployment can be stopped.</p><p><strong>CORRECT: </strong>\"Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch\" is incorrect.</p><p>The all-at-once deployment preference type would shift all traffic across to the new functions which would increase the impact of failed deployments. Also, CloudFormation is not a continuous deployment tool and is not a suitable substitute for CodePipeline and CodeDeploy.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version\" is incorrect.</p><p>As above, CloudFormation is not designed for continuous delivery and should not be used in this scenario. Creating a CodePipeline that leverages CodeDeploy and a source control service such as CodeCommit would be a better solution.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version\" is incorrect.</p><p>This option requires more operational overhead and testing on behalf of the DevOps team. The better solution is an automated pipeline that shifts some traffic across to the new functions so actual end users are attempting to use the application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Use an AWS SAM template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent30Minutes deployment type.</p>", "<p>Use AWS CloudFormation to deploy the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the AllAtOnce deployment type. Monitor error rates using Amazon CloudWatch.</p>", "<p>Use AWS CloudFormation to publish a new version on every stack update and use the Routing Config property of the AWS::Lambda::Alias resource to shift traffic to the new version.</p>", "<p>Use AWS CloudFormation to publish a new version on each stack update and configure an AWS CodePipeline approval action for a DevOps engineer to test and approve the new version.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A company is deploying a new serverless application that uses AWS Lambda functions. A DevOps engineer must create a continuous deployment pipeline for the application. The deployment preferences must be configured to minimize the impact of failed deployments.Which deployment configuration will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188400, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer must implement a serverless service that uses Amazon API Gateway, AWS Lambda, and DynamoDB. The serverless service must be deployed in multiple Regions and ensure fast response times for users within each geography.</p><p>Which solution will meet the requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The best available solution is to deploy APIs into multiple Regions and configure each API with a local Lambda function as an integration. Users can be directed to the closest API by configuring Amazon Route 53 with latency-based routing rules. The data should be stored in a Global Table, so it is synchronized across Regions. This solution ensures a single data set using the multi-master Global Tables technology and low latency for users.</p><p><strong>CORRECT: </strong>\"Create API Gateway APIs in each Region and configure Amazon Route 53 with latency-based routing rules and health checks. Configure the APIs with a Lambda proxy integration to a function in the same Region. Retrieve and update the data in a DynamoDB global table\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create API Gateway APIs in each Region and configure Amazon Route 53 with health checks for each API. Configure the APIs with a Lambda proxy integration to a function in the same Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function\" is incorrect.</p><p>The issue with this question is there are multiple records in Route 53 for each API but no way to direct users to the best one. A latency-based routing policy would be the best solution for this requirement.</p><p><strong>INCORRECT:</strong> \"Create API Gateway APIs in two Regions and configure Amazon Route 53 with a failover routing policy and health checks. Configure the APIs with a Lambda proxy integration to a function in the same Region. Retrieve and update the data in a DynamoDB global table\" is incorrect.</p><p>Failover routing works when you have an active and passive deployment. In this case we want an active-active deployment with the serverless service available to users in multiple Regions.</p><p><strong>INCORRECT:</strong> \"Create API Gateway APIs in each Region and configure Amazon Route 53 with latency-based routing rules and health checks. Configure the APIs with a Lambda proxy integration to a central function in one AWS account. Configure the Lambda function to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function\" is incorrect.</p><p>You cannot configure the APIs with a central Lambda function. You must choose a Lambda function in the same Region as the API.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>", "answers": ["<p>Create API Gateway APIs in each Region and configure Amazon Route 53 with health checks for each API. Configure the APIs with a Lambda proxy integration to a function in the same Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.</p>", "<p>Create API Gateway APIs in each Region and configure Amazon Route 53 with latency-based routing rules and health checks. Configure the APIs with a Lambda proxy integration to a function in the same Region. Retrieve and update the data in a DynamoDB global table.</p>", "<p>Create API Gateway APIs in two Regions and configure Amazon Route 53 with a failover routing policy and health checks. Configure the APIs with a Lambda proxy integration to a function in the same Region. Retrieve and update the data in a DynamoDB global table.</p>", "<p>Create API Gateway APIs in each Region and configure Amazon Route 53 with latency-based routing rules and health checks. Configure the APIs with a Lambda proxy integration to a central function in one AWS account. Configure the Lambda function to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.</p>"]}, "correct_response": ["b"], "section": "AWS Networking & Content Delivery", "question_plain": "A DevOps engineer must implement a serverless service that uses Amazon API Gateway, AWS Lambda, and DynamoDB. The serverless service must be deployed in multiple Regions and ensure fast response times for users within each geography.Which solution will meet the requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188402, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is deploying a series of updates to a web application that runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The updates are being deployed using a blue/green strategy with immutable instances.</p><p>An issue has been occurring where users are being logged of the application during deployments. The DevOps engineer needs to ensure that users remain logged in when scaling events occur and application updates are deployed.</p><p>How can these requirements be met with the LOWEST latency?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The issue here is that instances are being terminated and replaced and users who were bound to those instances and then being asked to reauthenticate when the load balancer directs them to another instance. This is because the information associated with their authenticated session was stored on the instance that has been terminated.</p><p>The best way to ensure that users are not asked to reauthenticate in this situation is to store the data in a session store. Amazon ElastiCache is ideal for this use case as it stores key/value pairs and offers extremely low latency.</p><p>The diagram below shows how you can use either DynamoDB or ElastiCache as a shared store for session state data:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-26-17-6d066d4bf6cf5e2b25e2c66fc70b4464.jpg\"><p><strong>CORRECT: </strong>\"Configure the application to store authenticated session information in an Amazon ElastiCache cluster\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the application to store authenticated session information in an Amazon S3 bucket\" is incorrect.</p><p>Amazon S3 could be used in this situation but the latency would likely be higher compared to using Amazon ElastiCache.</p><p><strong>INCORRECT:</strong> \"Enable sticky sessions on the target group and store authenticated session information on the instance\u2019s attached block volumes\" is incorrect.</p><p>Storing the data in the attached block volume is a bad idea as this would be deleted when the instance is terminated.</p><p><strong>INCORRECT:</strong> \"Enable session affinity on the load balancer and store authenticated session information on the instance\u2019s attached block volumes\" is incorrect.</p><p>Stick sessions (also known as session affinity) is enabled at the target group, not at the load balancer level. Also, the session state data should not be stored on a block volume.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\">https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-elasticache/\">https://digitalcloud.training/amazon-elasticache/</a></p>", "answers": ["<p>Enable session affinity on the load balancer and store authenticated session information on the instance\u2019s attached block volumes.</p>", "<p>Configure the application to store authenticated session information in an Amazon ElastiCache cluster.</p>", "<p>Enable sticky sessions on the target group and store authenticated session information on the instance\u2019s attached block volumes.</p>", "<p>Configure the application to store authenticated session information in an Amazon S3 bucket.</p>"]}, "correct_response": ["b"], "section": "AWS Database", "question_plain": "A DevOps engineer is deploying a series of updates to a web application that runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The updates are being deployed using a blue/green strategy with immutable instances.An issue has been occurring where users are being logged of the application during deployments. The DevOps engineer needs to ensure that users remain logged in when scaling events occur and application updates are deployed.How can these requirements be met with the LOWEST latency?", "related_lectures": []}, {"_class": "assessment", "id": 57188404, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company plans to migrate a Python application to AWS. A DevOps engineer must deploy the application in a configuration that supports blue/green deployments and rollback.</p><p>Which solution will meet these requirements with the LEAST operational complexity?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>With an Elastic Beanstalk blue/green update you can deploy the new application versions to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly. If any issues occur, you can simply swap the CNAMEs back again and traffic will be redirected back to your original set of instances. This is the solution that meets the requirements with the least operational complexity.</p><p><strong>CORRECT: </strong>\"Use AWS Elastic Beanstalk to host the application. Deploy new versions to separate environments and use Elastic Beanstalk to swap CNAMEs to redirect traffic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk to host the application. Configure an Application Load Balancer and use a traffic splitting deployment to implement a blue/green update\" is incorrect,</p><p>Traffic splitting is a feature that implements deployments using a canary strategy in which a portion of traffic is directed to a new set of instances running the latest code and then after a period, if successful, the remaining traffic is redirected.</p><p><strong>INCORRECT:</strong> \"Use Amazon LightSail to host the application. Store application code as zip files in an Amazon S3 bucket and use LightSail to deploy updates and manage the deployment\" is incorrect.</p><p>LightSail is a simple solution for hosted servers in the cloud and offers limited features. It cannot help you to achieve the requirements of this question.</p><p><strong>INCORRECT:</strong> \"Use CodeCommit to store the application code. Use AWS CodeDeploy to deploy the application to Amazon ECS tasks and configure Amazon Route 53 failover routing\" is incorrect.</p><p>Route 53 failover routing is not ideal for blue/green deployment scenarios as you must cause the failover by taking the source instances out of action (or causing them to fail health checks). It is also not straightforward to roll back.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>", "answers": ["<p>Use AWS Elastic Beanstalk to host the application. Configure an Application Load Balancer and use a traffic splitting deployment to implement a blue/green update.</p>", "<p>Use Amazon LightSail to host the application. Store application code as zip files in an Amazon S3 bucket and use LightSail to deploy updates and manage the deployment.</p>", "<p>Use CodeCommit to store the application code. Use AWS CodeDeploy to deploy the application to Amazon ECS tasks and configure Amazon Route 53 failover routing.</p>", "<p>Use AWS Elastic Beanstalk to host the application. Deploy new versions to separate environments and use Elastic Beanstalk to swap CNAMEs to redirect traffic.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "A company plans to migrate a Python application to AWS. A DevOps engineer must deploy the application in a configuration that supports blue/green deployments and rollback.Which solution will meet these requirements with the LEAST operational complexity?", "related_lectures": []}, {"_class": "assessment", "id": 57188406, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.</p><p>How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The solution is to enable synchronization of the updates in the mapping.yaml file into the AWS Service Catalog portfolio. This is achieved through an AWS Lambda function that scans the source file and checks for any updates. The Lambda function is called via the CodePipeline using an action. Note that this is an AWS CodePipeline orchestration, the Lambda action is not called via CodeBuild. This can be seen in the diagram below.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-29-08-f02d561f1751c786165860bab8598864.jpg\"><p><strong>CORRECT: </strong>\"Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The Lambda action is called via an AWS CodePipeline orchestration, not an CodeBuild action.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>INCORRECT:</strong> \"Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog\" is incorrect.</p><p>The solution should use an AWS Lambda function to make the updates, and this must be called via the pipeline.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/\">https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Use the AWS Service Catalog deploy action in AWS CodePipeline to push new versions of products into the AWS Service Catalog.</p>", "<p>Use the AWS Service Catalog deploy action in AWS CodeBuild to verify and push new versions of products into the AWS Service Catalog.</p>", "<p>Use an AWS Lambda action in CodePipeline to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>", "<p>Use an AWS Lambda action in AWS CodeBuild to run an AWS Lambda function to verify and push new versions of products into the AWS Service Catalog.</p>"]}, "correct_response": ["c"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer is deploying an AWS Service Catalog portfolio using AWS CodePipeline. A mapping.yaml file is used to define the portfolio and its associated permissions and products and is committed to an AWS CodeCommit repository which is defined in the pipeline.How can the engineer automate the creation of new portfolios and products when the mapping.yaml file is updated?", "related_lectures": []}, {"_class": "assessment", "id": 57188408, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application is hosted on Amazon EC2 instances behind an Application Load Balancer with an Amazon API Gateway REST API as the front end. Users should experience minimal disruptions during any deployment of a new version of the application. It must also be possible to quickly roll back if there is an issue.</p><p>Which solution will meet these requirements with MINIMAL changes to the application?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A parallel environment will be required behind the ALB that is launched with the updated application code. The API configuration would be updated to point to a different listener than the production traffic. Then, a canary release deployment can be used to direct a subset of traffic to the new environment with the latest application code.</p><p>In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a pre-configured ratio. Typically, the canary release receives a small percentage of API traffic, and the production release takes up the rest. The updated API features are only visible to API traffic through the canary.</p><p>By keeping canary traffic small and the selection random, most users are not adversely affected at any time by potential bugs in the new version, and no single user is adversely affected all the time.</p><p><strong>CORRECT: </strong>\"Deploy updates into a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small percentage of API traffic to the new environment\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy updates into a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group\" is incorrect.</p><p>This would direct all traffic to the new application version which would affect all users. Also, the ALB would not know to route traffic to the new target group unless the original was taken out of service.</p><p><strong>INCORRECT:</strong> \"Deploy updates into a separate environment parallel to the existing one. Update the Amazon Route 53 alias records to point to the new environment\" is incorrect.</p><p>This would affect all users as all traffic would be directed to the new application version.</p><p><strong>INCORRECT:</strong> \"Deploy updates into a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group using a weighted distribution\" is incorrect.</p><p>There is no mechanism in API gateway for directing traffic using a weighted distribution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-api-gateway/\">https://digitalcloud.training/amazon-api-gateway/</a></p>", "answers": ["<p>Deploy updates into a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small percentage of API traffic to the new environment.</p>", "<p>Deploy updates into a separate environment parallel to the existing one. Update the Amazon Route 53 alias records to point to the new environment.</p>", "<p>Deploy updates into a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group using a weighted distribution.</p>", "<p>Deploy updates into a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group.</p>"]}, "correct_response": ["a"], "section": "AWS Networking & Content Delivery", "question_plain": "An application is hosted on Amazon EC2 instances behind an Application Load Balancer with an Amazon API Gateway REST API as the front end. Users should experience minimal disruptions during any deployment of a new version of the application. It must also be possible to quickly roll back if there is an issue.Which solution will meet these requirements with MINIMAL changes to the application?", "related_lectures": []}, {"_class": "assessment", "id": 57188410, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A startup company is launching a web application on AWS that uses Amazon EC2 instances behind an Application Load Balancer. The EC2 instances will store data in an Amazon RDS database and an Amazon DynamoDB table. The DevOps team require separate environments for development, testing, and production.</p><p>What is the MOST secure method of obtaining password credentials?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most secure solution is to use a combination of an AWS IAM Instance Profile with a policy attached providing permissions to Amazon DynamoDB and AWS Secrets Manager for retrieving SecretString parameters for the Amazon RDS DB.</p><p>Instance profiles are far more secure compared to access keys are they leverage the AWS STS service to obtain temporary security credentials. No security credentials are stored on the EC2 instances when using this method.</p><p>For Amazon RDS you may need to use database connection credentials for authentication depending on your configuration. If this is the case you can securely store these credentials as SecretString (encrypted) parameters in AWS Secrets Manager. Your application can then issue API calls to Secrets Manager to securely retrieve the encrypted parameters when authentication is needed.</p><p><strong>CORRECT: </strong>\"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata\" is incorrect.</p><p>Database passwords should not be stored in system metadata as this would be very insecure.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter\" is incorrect.</p><p>Access keys are less secure compared to using instance profiles as with access keys the actual access key ID and secret access key are stored in plaintext on the EC2 instance file system.</p><p><strong>INCORRECT:</strong> \"Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter\" is incorrect.</p><p>As above, access keys should not be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html\">https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_CreateSecret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-iam/\">https://digitalcloud.training/aws-iam/</a></p>", "answers": ["<p>Configure access keys to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.</p>", "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Retrieve the database credentials from AWS Secrets Manager.</p>", "<p>Configure access keys to access AWS services. Retrieve the database credentials from an AWS Secrets Manager SecretString parameter.</p>", "<p>Launch the EC2 instances with an IAM instance profile with permissions to access AWS services. Store the database passwords in an encrypted config file in system metadata.</p>"]}, "correct_response": ["b"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A startup company is launching a web application on AWS that uses Amazon EC2 instances behind an Application Load Balancer. The EC2 instances will store data in an Amazon RDS database and an Amazon DynamoDB table. The DevOps team require separate environments for development, testing, and production.What is the MOST secure method of obtaining password credentials?", "related_lectures": []}, {"_class": "assessment", "id": 57188412, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is using AWS CodePipeline to automate the release lifecycle of an application. AWS CodeDeploy used to deploy an application to Amazon ECS using the blue/green deployment model. The company wants to run test scripts to validate the green version of the application before shifting traffic. The scripts will complete in less than 5 minutes. If errors are discovered by the scripts, the application must be rolled back.</p><p>Which strategy will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use the 'hooks' section of the CodeDeploy AppSpec file to specify a Lambda function that CodeDeploy can call to validate an Amazon ECS deployment.</p><p>You can use the same function or a different one for the BeforeInstall, AfterInstall, AfterAllowTestTraffic, BeforeAllowTraffic, and AfterAllowTraffic deployment lifecyle events. Following completion of the validation tests, the Lambda AfterAllowTraffic function calls back CodeDeploy and delivers a result of Succeeded or Failed.</p><p><strong>CORRECT: </strong>\"Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to trigger rollback\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment\" is incorrect.</p><p>The CLI command is not required, either the Lambda function can use AfterAllowTraffic to call back and deliver a success/failure message or if it does not report back within one hour the deployment is assumed to have failed.</p><p><strong>INCORRECT:</strong> \"Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create an execution environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment\" is incorrect.</p><p>CodeBuild is not required for running the test scripts which can be added using hooks instead.</p><p><strong>INCORRECT:</strong> \"Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment\" is incorrect.</p><p>There is no need to use an extra stage to run an AWS Lambda function, this can be achieved using hooks in the AppSpec file.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-section-structure-ecs-sample-function\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-section-structure-ecs-sample-function</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create an execution environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.</p>", "<p>Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.</p>", "<p>Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to trigger rollback.</p>", "<p>Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.</p>"]}, "correct_response": ["c"], "section": "AWS Developer Tools", "question_plain": "A company is using AWS CodePipeline to automate the release lifecycle of an application. AWS CodeDeploy used to deploy an application to Amazon ECS using the blue/green deployment model. The company wants to run test scripts to validate the green version of the application before shifting traffic. The scripts will complete in less than 5 minutes. If errors are discovered by the scripts, the application must be rolled back.Which strategy will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188414, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has deployed a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. The web service uses Amazon Route 53 records for DNS requests for example.com. The records are configured with health checks that assess the availability of the web service.</p><p>A second environment has been deployed into eu-west-1. The company requires traffic to be routed to the environment that provides the lowest latency for user requests. In the event of a regional outage, traffic should be directed to the alternate Region.</p><p>Which configuration will achieve these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>There are two key requirements that inform the design. Firstly, the solution must route based on latency. Secondly, failover across Regions must be automatic. This is a more complex DNS routing configuration. To meet both requirements the company will need to use a combination of latency-based routing records and failover routing records.</p><p>The solution is to create subdomains for each Region that can be used for the failover records and pointing the secondary to the alternate Region. Then, on top of those records the solution includes a latency-based routing record for example.com.</p><p>With this solution example.com will resolve to the subdomain that represents the lowest latency from the user request location. If the environment in that Region is not available (has failed health checks) then the request will be failed over to the alternate Region.</p><p><strong>CORRECT: </strong>\"Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com\" is incorrect.</p><p>The solution should use failover routing and latency routing, not weighted routing and geolocation routing.</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target\" is incorrect.</p><p>This solution gets the failover and latency records the wrong way around. Failover routing should be used for the subdomain and latency routing for the apex domain (example.com).</p><p><strong>INCORRECT:</strong> \"Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com\" is incorrect.</p><p>Multivalue routing is a form of DNS load balancing and will simply route records across all registered and available targets. This does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>", "answers": ["<p>Create a subdomain named us.example.com with failover routing. Configure the US ALB as primary and the EU ALB as secondary. Create another subdomain named eu.example.com with failover routing. Configure the EU ALB as primary and the US ALB as secondary. Create latency-based routing records for example.com that are aliased to us.example.com and eu.example.com.</p>", "<p>Create a subdomain named us.example.com with weighted routing. Configure the US ALB with weight 2 and the EU ALB with weight 1. Create another subdomain named eu.example.com with weighted routing. Configure the EU ALB with weight 2 and the US ALB with weight 1. Create geolocation routing records for example.com with North America aliased to us.example.com and Europe aliased to eu.example.com.</p>", "<p>Create a subdomain named us.example.com with latency-based routing. Configure the US ALB as the first target and the EU ALB as the second target. Create another subdomain named eu.example.com with latency-based routing. Configure the EU ALB as the first target and the US ALB as the second target. Create failover routing records for example.com aliased to us.example.com as the first target and eu.example.com as the second target.</p>", "<p>Create a subdomain named us.example.com with multivalue answer routing. Configure the US ALB first and the EU ALB second. Create another subdomain named eu.example.com with multivalue answer routing. Configure the EU ALB first and the US ALB second. Create failover routing records for example.com that are aliased to us.example.com and eu.example.com.</p>"]}, "correct_response": ["a"], "section": "AWS Networking & Content Delivery", "question_plain": "A company has deployed a web service that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company has deployed the application in us-east-1. The web service uses Amazon Route 53 records for DNS requests for example.com. The records are configured with health checks that assess the availability of the web service.A second environment has been deployed into eu-west-1. The company requires traffic to be routed to the environment that provides the lowest latency for user requests. In the event of a regional outage, traffic should be directed to the alternate Region.Which configuration will achieve these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 57188416, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps team is building a pipeline in AWS CodePipeline that will build, stage, test, and then deploy an application on Amazon EC2. The team will add a manual approval stage between the test stage and the deployment stage. The development team uses a custom chat tool that offers a webhook interface for sending notifications.</p><p>The DevOps team require status updates for pipeline activity and approval requests to be posted to the chat tool. How can this be achieved?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules that include an event pattern and event target. Each type of execution state change event in CodePipeline emits notifications with specific message content. In this case the team should filter for the \u201cCodePipeline Pipeline Execution State Change\u201d events and route to an SNS Topic as a target. The Lambda function can then be subscribed to the topic.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation\" is incorrect.</p><p>CloudWatch Logs subscription filters can be used to publish to Kinesis Data Streams, Kinesis Data Firehose, or AWS Lambda. You cannot publish directly to SNS. Also, the log will not contain execution state change events for CodePipeline.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL\" is incorrect.</p><p>You cannot use API events to look for this specific event and then send directly from CloudTrail to the chat webhook URL.</p><p><strong>INCORRECT:</strong> \"Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic\" is incorrect.</p><p>AWS Config is used for configuration compliance and cannot check for events that relate to pipeline execution state changes in AWS CodePipeline.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>", "<p>Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change events. Publish subscription events to an Amazon SNS topic and subscribe the chat webhook URL to the SNS topic and complete the subscription validation.</p>", "<p>Create an AWS Lambda function that is invoked by AWS CloudTrail API events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details directly to the chat webhook URL.</p>", "<p>Create an AWS Config rule that checks for CodePipeline Pipeline Execution State Change events. Publish the events to an Amazon SNS topic. Create an AWS Lambda function that sends event details to the chat webhook URL and subscribe the function to the SNS topic.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A DevOps team is building a pipeline in AWS CodePipeline that will build, stage, test, and then deploy an application on Amazon EC2. The team will add a manual approval stage between the test stage and the deployment stage. The development team uses a custom chat tool that offers a webhook interface for sending notifications.The DevOps team require status updates for pipeline activity and approval requests to be posted to the chat tool. How can this be achieved?", "related_lectures": []}, {"_class": "assessment", "id": 57188418, "assessment_type": "multi-select", "prompt": {"question": "<p>An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.</p><p>Which actions should the engineer take? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You cannot enable encryption for an existing EBS volume. You must enable encryption of the volume at creation time. There are a couple of ways to enable encryption of data stored on an unencrypted volume:</p><p>1) Create a snapshot of the volume. The snapshot will also be unencrypted, but you can then copy it and enable encryption. Then, you can create an encrypted volume from the snapshot and attach it to the instance.</p><p>2) Create and mount a new, encrypted EBS volume. The engineer would then need to move data onto the volume.</p><p>In both cases the engineer will need to update the application to use the new volume.</p><p><strong>CORRECT: </strong>\"Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume\" is incorrect.</p><p>This has not resulted in any change to the EBS volume, it is still unencrypted.</p><p><strong>INCORRECT:</strong> \"Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data\" is incorrect.</p><p>You cannot enable encryption for existing volumes through any AWS tools.</p><p><strong>INCORRECT:</strong> \"Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume\" is incorrect.</p><p>SSL/TLS certificates are used for enabling encryption in-transit, not encryption at-rest.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ebs/\">https://digitalcloud.training/amazon-ebs/</a></p>", "answers": ["<p>Unmount the EBS volume, take a snapshot and encrypt the snapshot. Re-mount the EBS volume.</p>", "<p>Use AWS Data Lifecycle Manager to automatically enable encryption for the EBS volume and encrypt the existing data.</p>", "<p>Copy an unencrypted snapshot of the volume and encrypt the new snapshot. Volumes restored from this encrypted snapshot will also be encrypted.</p>", "<p>Create and mount a new, encrypted EBS volume. Move the data to the new volume. Delete the old EBS volume.</p>", "<p>Upload a self-signed SSL/TLS certificate to the EC2 instance. Use a secure session to encrypt all data transferred to the EBS volume.</p>"]}, "correct_response": ["c", "d"], "section": "AWS Storage", "question_plain": "An application running on an Amazon EC2 instance stores sensitive data on an attached Amazon EBS volume. The volume is not encrypted. A DevOps engineer must enable encryption at rest for the data.Which actions should the engineer take? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57188420, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer created an S3 event notification to trigger an AWS Lambda function when objects are uploaded to the bucket. The solution was initially working but has since stopped working. The engineer noticed that the function is no longer being invoked.</p><p>What are two possible causes for this error? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>If the function is not being invoked then either the configuration that should initiate an invocation has been removed or the permissions the function requires have been removed. The Lambda function must have permissions defined in a resource-based policy that specify that the bucket can invoke the function.</p><p><strong>CORRECT: </strong>\"The event notification has been deleted from the S3 bucket\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The resource-based policy has been removed from the function\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The S3 bucket does not allow public access to objects\" is incorrect.</p><p>There is no need to allow public access to objects as this is not a requirement for an event notification to succeed.</p><p><strong>INCORRECT:</strong> \"The IAM role the function is assuming does not have permissions\" is incorrect.</p><p>The function does not need to assume a role. With an event notification, S3 directly invokes Lambda, and the function uses permissions assigned through a resource-based policy.</p><p><strong>INCORRECT:</strong> \"Default encryption has been enabled on the S3 bucket\" is incorrect.</p><p>Encryption status does not affect this solution at all.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-invoke-error-s3-bucket-permission/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-invoke-error-s3-bucket-permission/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-lambda/\">https://digitalcloud.training/aws-lambda/</a></p>", "answers": ["<p>The event notification has been deleted from the S3 bucket.</p>", "<p>The resource-based policy has been removed from the function.</p>", "<p>The S3 bucket does not allow public access to objects.</p>", "<p>The IAM role the function is assuming does not have permissions.</p>", "<p>Default encryption has been enabled on the S3 bucket.</p>"]}, "correct_response": ["a", "b"], "section": "AWS Compute", "question_plain": "A DevOps engineer created an S3 event notification to trigger an AWS Lambda function when objects are uploaded to the bucket. The solution was initially working but has since stopped working. The engineer noticed that the function is no longer being invoked.What are two possible causes for this error? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57188422, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps team manages an application that consists of four separate AWS Lambda functions. A DevOps Engineer on the team has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule that executes the pipeline after a change is made to the application source code. During testing, the engineer noticed that the pipeline takes a long time to complete.</p><p><br></p><p>What should the DevOps Engineer do to improve the speed of the pipeline?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The best way to speed up the pipeline will be to run the builds in parallel. This can be achieved through the pipeline configuration by specifying the runOrder to be the same for the build of each function within the action structure.</p><p>To specify parallel actions, you use the same integer for each action you want to run in parallel.</p><p><strong>CORRECT: </strong>\"Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput\" is incorrect.</p><p>Connecting to a VPC does not help and using dedicated instances is not the best way to improve the speed of the pipeline. Without specifying other changes the builds will still run sequentially.</p><p><strong>INCORRECT:</strong> \"Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs\" is incorrect.</p><p>This may offer some improvement in speed but not as much as running the builds in parallel.</p><p><strong>INCORRECT:</strong> \"Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain\" is incorrect.</p><p>CodeBuild can be configured to run builds in batches but a build list or build matrix should be used for running the builds in parallel. The build graph deployment runs the builds sequentially with dependencies mapped out.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Modify the CodeBuild projects within the pipeline to use a compute type with more available vCPUs.</p>", "<p>Configure parallel executions of the Lambda functions by specifying the same runOrder in the CodePipeline configuration for the stage.</p>", "<p>Modify CodeBuild execute the builds in batches using a build graph deployment and specify the dependency chain.</p>", "<p>Configure the CodeBuild projects to run within a VPC and use dedicated instances to increase the build throughput.</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "A DevOps team manages an application that consists of four separate AWS Lambda functions. A DevOps Engineer on the team has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule that executes the pipeline after a change is made to the application source code. During testing, the engineer noticed that the pipeline takes a long time to complete.What should the DevOps Engineer do to improve the speed of the pipeline?", "related_lectures": []}, {"_class": "assessment", "id": 57188424, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs several business-critical applications on Amazon EC2 instances in an Amazon VPC. The company requires the applications to be available 24/7 and there should be no outages except for pre-arranged updates. The DevOps team requires an automated notification mechanism that lets them know if the state of any of the instance\u2019s changes.</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To receive email notifications when your instance changes state, create an Amazon SNS topic and then create an EventBridge rule for the EC2 Instance State-change Notification event. This will trigger a notification by SNS whenever the state of an EC2 instance changes.</p><p><strong>CORRECT: </strong>\"Create an Amazon EventBridge rule with an event pattern configured with the \u2018EC2 Instance State-change Notification\u2019 event type and send an Amazon SNS notification\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a scheduled AWS Lambda function that checks the running state of the Amazon EC2 instances and updates the AWS Personal Health Dashboard\" is incorrect.</p><p>The Personal Health Dashboard shows issues that may affect your resources. AWS updates the dashboard; it is not something you can do yourself.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance\" is incorrect.</p><p>This would recover an instance if there were an issue with the underlying hardware that requires AWS to fix. However, the requirement here is simply to notify the team if the instance changes state.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance\" is incorrect.</p><p>As above, the team requires a notification if the state of the instance changes, they do not ask for automated recovery or restarting of instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instance-state-changes.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>", "answers": ["<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance.</p>", "<p>Create an Amazon EventBridge rule with an event pattern configured with the \u2018EC2 Instance State-change Notification\u2019 event type and send an Amazon SNS notification.</p>", "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance.</p>", "<p>Create a scheduled AWS Lambda function that checks the running state of the Amazon EC2 instances and updates the AWS Personal Health Dashboard.</p>"]}, "correct_response": ["b"], "section": "AWS Application Integration", "question_plain": "A company runs several business-critical applications on Amazon EC2 instances in an Amazon VPC. The company requires the applications to be available 24/7 and there should be no outages except for pre-arranged updates. The DevOps team requires an automated notification mechanism that lets them know if the state of any of the instance\u2019s changes.", "related_lectures": []}, {"_class": "assessment", "id": 57188426, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.</p><p>Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that can be set up, you can match events and route them to one or more target functions or streams. Custom application-level events can be generated and published to CloudWatch Events. Scheduled events can be set up that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.</p><p><strong>CORRECT: </strong>\"Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event Rules cannot have S3 buckets as a target. It is more efficient to directly invoke the Lambda function from the CloudWatch Event rule than use S3 as a temporary store and then trigger a function execution.</p><p><strong>INCORRECT:</strong> \"Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected\" is incorrect.</p><p>You cannot create a CodeCommit hook on an EC2 instance, so this is not possible</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API\" is incorrect.</p><p>CloudWatch Event rules on a schedule would introduce lag and would be inefficient. So, this option is ruled out.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/\">https://aws.amazon.com/blogs/devops/building-end-to-end-aws-devsecops-ci-cd-pipeline-with-open-source-sca-sast-and-dast-tools/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Create an Amazon CloudWatch Event rule on the CodeCommit repository that reacts to pushes. Choose an S3 bucket as a target so the code will be automatically zipped into S3. Create an S3 event notification rule to trigger an AWS Lambda function that will retrieve the zipped code from S3 and send it to the 3rd party API.</p>", "<p>Create a CodeCommit hook on an EC2 instance that streams changes from CodeCommit into the local filesystem. A cron job on the EC2 instance will zip the code and send it to the 3rd party API upon changes being detected.</p>", "<p>Create a CloudWatch Event rule on the CodeCommit repository that reacts to code pushes. Choose an AWS Lambda function as a target that will request the code from CodeCommit, zip it, and send it to the 3rd party API.</p>", "<p>Create an Amazon CloudWatch Event rule on a schedule of 5 minutes that triggers an AWS Lambda function that checks for new commits in the CodeCommit repository. If new commits are detected, the function will download and zip the code and then send it to the 3rd party API.</p>"]}, "correct_response": ["c"], "section": "AWS Developer Tools", "question_plain": "The DevOps team at an e-commerce company introduced multiple stages of security to the code release process. As an additional measure, they want to add additional SAST &amp; DAST tools into an automated pipeline. These tools should be invoked for every code push in an AWS CodeCommit repository. The code must be sent via an external API.Which actions should a DevOps engineer take to achieve these requirements MOST efficiently?", "related_lectures": []}, {"_class": "assessment", "id": 57188428, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company\u2019s shopping website is hosted in an Auto Scaling group (ASG) of Amazon EC2 instances. There\u2019s a sale coming up and the company anticipate huge traffic on the website. Currently, there\u2019s a dynamic target tracking policy which scales up the instances gradually. However, the EC2 instances are taking a long time to spin up and the load balancer is sending traffic to unhealthy instances.</p><p>How can the issue be resolved with minimal operational overhead and cost?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Add a lifecycle hook in EC2 instance and to scale up quickly, utilize a warm pool.</p><p>A lifecycle hook can be added on scale out/ scale in events and get signal back from EC2 instances. This is to make the load balancer aware when the instances are ready to serve traffic.</p><p>A warm pool gives the ability to decrease latency for the applications that have exceptionally long boot times, for example, because instances need to write massive amounts of data to disk. With warm pools, Auto Scaling groups don\u2019t need to be over provisioned to manage latency and improve application performance.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to the Auto Scaling group, and to scale up quickly, utilize a warm pool\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Change the dynamic scaling policy to use a manual scaling policy and increase the pool size\" is incorrect.</p><p>This will increase the effort and cost of the solution.</p><p><strong>INCORRECT:</strong> \"Put the EC2 instances in standby state to debug the instance spin up time and add lifecycle hooks\" is incorrect.</p><p>Standby mode is useful when there\u2019s a specific issue within the instance that needs to be debugged and is not a regular practice.</p><p><strong>INCORRECT:</strong> \"Configure the desired capacity to a large number of EC2 instances before the event so the application can handle the additional load\" is incorrect.</p><p>This will increase the overall cost and will result in underutilization of instances.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/\">https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Add a lifecycle hook to the Auto Scaling group, and to scale up quickly, utilize a warm pool.</p>", "<p>Change the dynamic scaling policy to use a manual scaling policy and increase the pool size.</p>", "<p>Put the EC2 instances in standby state to debug the instance spin up time and add lifecycle hooks.</p>", "<p>Configure the desired capacity to a large number of EC2 instances before the event so the application can handle the additional load.</p>"]}, "correct_response": ["a"], "section": "AWS Compute", "question_plain": "A company\u2019s shopping website is hosted in an Auto Scaling group (ASG) of Amazon EC2 instances. There\u2019s a sale coming up and the company anticipate huge traffic on the website. Currently, there\u2019s a dynamic target tracking policy which scales up the instances gradually. However, the EC2 instances are taking a long time to spin up and the load balancer is sending traffic to unhealthy instances.How can the issue be resolved with minimal operational overhead and cost?", "related_lectures": []}, {"_class": "assessment", "id": 57188430, "assessment_type": "multi-select", "prompt": {"question": "<p>Several applications will be deployed using AWS CloudFormation. The applications must be deployed into multiple accounts the company manages. Multiple administrator accounts must be granted permissions to create and manage the CloudFormation stacks and operational overhead should be minimized.</p><p>Which actions meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf.</p><p>In this case it is better to use AWS Organizations and service-managed permissions as this will reduce operational overhead and meet the requirements. Administrators can be granted permissions to CloudFormation to create and update stacks and the stacks will be deployed using service accounts.</p><p><strong>CORRECT: </strong>\"Create an AWS Organization with all features enabled and add all the accounts to the organization\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Enable trusted access with AWS Organizations and create CloudFormation stack sets using the management account\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with consolidated billing enabled and all the accounts to the organization\" is incorrect.</p><p>All features must be enabled to that trusted access can be used to integrate CloudFormation.</p><p><strong>INCORRECT:</strong> \"Create CloudFormation stacks in each account using cross-account IAM roles with permissions in each account\" is incorrect.</p><p>This requires more operational effort. Better to use AWS Organizations with trusted access and service-managed permissions rather than self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Enable trusted access with AWS Organizations and create CloudFormation stack sets using self-managed permissions\" is incorrect.</p><p>When using AWS Organizations and trusted access, service-managed permissions are used rather than self-managed permissions. This is the main advantage of enabling trusted access.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-enable-trusted-access.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgs-enable-trusted-access.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>Create an AWS Organization with consolidated billing enabled and all the accounts to the organization.</p>", "<p>Create an AWS Organization with all features enabled and add all the accounts to the organization.</p>", "<p>Enable trusted access with AWS Organizations and create CloudFormation stack sets using the management account.</p>", "<p>Create CloudFormation stacks in each account using cross-account IAM roles with permissions in each account.</p>", "<p>Enable trusted access with AWS Organizations and create CloudFormation stack sets using self-managed permissions.</p>"]}, "correct_response": ["b", "c"], "section": "AWS Management & Governance", "question_plain": "Several applications will be deployed using AWS CloudFormation. The applications must be deployed into multiple accounts the company manages. Multiple administrator accounts must be granted permissions to create and manage the CloudFormation stacks and operational overhead should be minimized.Which actions meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57188432, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.</p><p>They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.</p><p>Which of the below architectures best suits their needs?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Kinesis Data Firehose is used to reliably load streaming data into data lakes, data stores, and analytics tools like Amazon Redshift. Process the incoming data from Firehose with Kinesis Data Analytics to provide real-time dashboarding of website activity.</p><p><strong>CORRECT: </strong>\"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>To load data into S3, Kinesis Firehose is a more suitable tool.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>As per the option above, Kinesis Data Firehose is a better service for this use case.</p><p><strong>INCORRECT:</strong> \"Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis\" is incorrect.</p><p>Athena is more suited to data analysis within S3 buckets and cannot be used for loading data into S3.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/\">https://aws.amazon.com/solutions/implementations/real-time-web-analytics-with-kinesis/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>", "answers": ["<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Streams to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>", "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Analytics to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>", "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Athena to Amazon S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to the Amazon RedShift warehouse for analysis.</p>", "<p>Build the website to run in stateless EC2 instances which auto scale with traffic and migrate the databases into Amazon RDS. Push ad/referrer data using Amazon Kinesis Data Firehose to S3 where it can be consumed by the internal billing system to determine referral fees. Additionally create another Kinesis delivery stream to push the data to Amazon Redshift warehouse for analysis.</p>"]}, "correct_response": ["d"], "section": "AWS Analytics", "question_plain": "An automotive organization is planning to migrate their website into AWS across multiple accounts. The current infrastructure uses an on-premises Microsoft IIS web server and Microsoft SQL server for the data persistence layer.They want to be able to scale their infrastructure based on demand. Along with the current website, they also want to collect user interest data from ad clicks that occur on the website. Amazon RedShift has been chosen for the consumption and aggregation of data.Which of the below architectures best suits their needs?", "related_lectures": []}, {"_class": "assessment", "id": 57188434, "assessment_type": "multi-select", "prompt": {"question": "<p>A media company is developing a new application which will be supported by mobile devices, tablets, and desktops. Each platform is customized for a different user experience and various viewing modes based on the resources being requested by users. This is enabled by path-based routing behind the scenes, using instances deployed on Amazon EC2. An auto scaling group has been configured to ensure EC2 instances are highly scalable.</p><p>Which of the following combinations will ensure high performance and minimal cost? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p><strong>Amazon CloudFront with Lambda@Edge</strong> - Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). By combining Lambda@Edge with other AWS services, developers can build dynamic, powerful web applications at the edge that automatically scale up and down\u2014with zero origin infrastructure and administrative effort required for automatic scaling, backups, or data center redundancy.</p><p>By using Lambda@Edge to dynamically route requests to different origins based on different viewer characteristics, you can balance the load on your origins, while improving the performance for your users. For example, you can route requests to origins within a home region, based on a viewer's location.</p><p>With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_00-53-55-d101810f6e21e0583638ef6be55f9c9c.jpg\"><p><strong>Application Load Balancer</strong> - An Application Load Balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. ALB offers support for Path conditions. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL.</p><p><strong>CORRECT: </strong>\"Amazon CloudFront with Lambda@Edge and utilize an application load balancer behind auto scaling group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> Utilize a Network Load Balancer behind auto scaling group\" is incorrect.</p><p>The Application Load Balancer works at the Application Layer (Layer 7 of the OSI model, Request level). The Network Load Balancer works at the Transport layer (Layer 4 of the OSI model). The NLB just forwards requests whereas ALB examines the contents of the HTTP request header to determine where to route the request. So, the ALB can perform content-based routing while NLB cannot.</p><p><strong>INCORRECT</strong>: \"Utilize a static website hosted behind a S3 bucket\" is incorrect.</p><p>It is mentioned in the question that the application is hosted on an Amazon EC2 instances and that a solution is needed for performance and cost-effectiveness. Hence, using Amazon S3 as a static website is not suitable answer.</p><p><strong>INCORRECT</strong>: \"Utilize Amazon Route 53 with traffic flow policies\" is incorrect.</p><p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect. Amazon Route 53 is fully compliant with IPv6 as well.</p><p>Traffic Flow policy allows an Amazon Web Services customer to define how end-user traffic is routed to application endpoints through a visual interface. Route 53 can route between domains and subdomains but is not useful for path-based traffic flows.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/lambda/edge/\">https://aws.amazon.com/lambda/edge/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/create-subdomain-route-53/\">https://aws.amazon.com/premiumsupport/knowledge-center/create-subdomain-route-53/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>", "answers": ["<p>Utilize a Network Load Balancer behind auto scaling group.</p>", "<p>Utilize Amazon Route 53 with traffic flow policies.</p>", "<p>Utilize a static website hosted behind an Amazon S3 bucket.</p>", "<p>Amazon CloudFront with Lambda@Edge.</p>", "<p>Utilize an Application Load Balancer behind auto scaling group.</p>"]}, "correct_response": ["d", "e"], "section": "AWS Networking & Content Delivery", "question_plain": "A media company is developing a new application which will be supported by mobile devices, tablets, and desktops. Each platform is customized for a different user experience and various viewing modes based on the resources being requested by users. This is enabled by path-based routing behind the scenes, using instances deployed on Amazon EC2. An auto scaling group has been configured to ensure EC2 instances are highly scalable.Which of the following combinations will ensure high performance and minimal cost? (Select TWO.)", "related_lectures": []}]}
5794170
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 69900202, "assessment_type": "multi-select", "prompt": {"question": "<p>A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.</p><p>Which combination of actions will meet these requirements? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>In a hybrid environment the most secure deployment includes using Systems Manager Hybrid activations for on-premises servers and an IAM role for the EC2 instances. The hybrid activations feature in systems manager is a more secure option than using access keys as it uses an IAM service role.</p><p>To ensure that systems are not affected during business hours the DevOps engineer needs to simply select a patching window during acceptable times and Systems Manager Patch Manager will only apply updates during that timeframe.</p><p><strong>CORRECT: </strong>\"Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager\" is incorrect.</p><p>This is a less secure method than using Hybrid Activations in Systems Manager and would also be harder to manage.</p><p><strong>INCORRECT:</strong> \"Create an AWS Systems Manager Automation document that installs that latest patches every hour\" is incorrect.</p><p>Automation documents are used for running tasks on managed instances using Systems Manager but are not used for patch management.</p><p><strong>INCORRECT:</strong> \"Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours\" is incorrect.</p><p>CloudWatch Events responds to state changes in resources but cannot be used to schedule patch installation using AWS Systems Manager Patch Manager.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>", "answers": ["<p>Create IAM access keys for the on-premises servers to provide permission to AWS Systems Manager.</p>", "<p>Add the on-premises servers into AWS Systems Manager using Systems Manager Hybrid Activations.</p>", "<p>Attach an IAM role to the EC2 instances, granting AWS Systems Manager permission to manage the instances.</p>", "<p>Use AWS Systems Manager Maintenance Windows to schedule a patch window outside of business hours.</p>", "<p>Create an AWS Systems Manager Automation document that installs that latest patches every hour.</p>", "<p>Use Amazon CloudWatch Events scheduled events to schedule a patch window outside of business hours.</p>"]}, "correct_response": ["b", "c", "d"], "section": "AWS Management & Governance", "question_plain": "A company manages both Amazon EC2 instances and on-premises servers running Linux and Windows. A DevOps engineer needs to manage patching across these environments. All patching must take place outside of business hours.Which combination of actions will meet these requirements? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69900204, "assessment_type": "multi-select", "prompt": {"question": "<p>A company leverages AWS CloudFormation to build their infrastructure on AWS. The security department is concerned about sensitive data being exposed. A DevOps engineer has been asked to implement steps to prevent the exposure of sensitive parameters such as passwords during infrastructure deployment operations.</p><p>Which combination of steps should be taken to improve security when deploying infrastructure using AWS CloudFormation? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The key issue here is that some parameters such as passwords may be exposed when deploying infrastructure using AWS CloudFormation. This data could be exposed through the console, command line tools, or API.</p><p>There are a couple of ways the company can deal with this issue. Firstly, AWS Secrets Manager can be used to store secure encrypted values. These values can be securely used in CloudFormation by referencing them as dynamic references.</p><p>Another option is to use the NoEcho parameter property to mask sensitive data. If you set the NoEcho attribute to true, CloudFormation returns the parameter value masked as asterisks (*****) for any calls that describe the stack or stack events.</p><p><strong>CORRECT: </strong>\"Use AWS Secrets Manager to store sensitive data as secret values and use dynamic references in AWS CloudFormation\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use the CloudFormation NoEcho parameter property to mask any sensitive parameter values\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager Parameter Store to store sensitive data as secure strings and reference the strings using tags in the CloudFormation template\" is incorrect.</p><p>You cannot securely share secure strings in SSM Parameter Store by using tags in the CloudFormation template. Tags are used for assigning optional metadata.</p><p><strong>INCORRECT:</strong> \"Use AWS Secrets Manager to encrypt the CloudFormation template at rest using an AWS KMS key\" is incorrect.</p><p>Secrets Manager cannot be used to encrypt the template and the main issue is not the encryption of the template but preventing exposure of sensitive data during deployment operations.</p><p><strong>INCORRECT:</strong> \"Use an Amazon S3 bucket with default encryption enabled to store the AWS CloudFormation template\" is incorrect.</p><p>This ensures the encryption of the template but does not protect sensitive data from being exposed during deployment operations.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-secret.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-secret.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>Use AWS Secrets Manager to store sensitive data as secret values and use dynamic references in AWS CloudFormation.</p>", "<p>Use AWS Systems Manager Parameter Store to store sensitive data as secure strings and reference the strings using tags in the CloudFormation template.</p>", "<p>Use AWS Secrets Manager to encrypt the CloudFormation template at rest using an AWS KMS key.</p>", "<p>Use the CloudFormation NoEcho parameter property to mask any sensitive parameter values.</p>", "<p>Use an Amazon S3 bucket with default encryption enabled to store the AWS CloudFormation template.</p>"]}, "correct_response": ["a", "d"], "section": "AWS Management & Governance", "question_plain": "A company leverages AWS CloudFormation to build their infrastructure on AWS. The security department is concerned about sensitive data being exposed. A DevOps engineer has been asked to implement steps to prevent the exposure of sensitive parameters such as passwords during infrastructure deployment operations.Which combination of steps should be taken to improve security when deploying infrastructure using AWS CloudFormation? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900206, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.</p><p>How can the company meet these requirements with the LEAST amount of effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The simplest solution for this request is to enable server access logging on the bucket and then analyze the data that is logged using Amazon Athena. Server access logging provides detailed records for the requests that are made to a bucket. This includes the information requested by the company.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p><strong>CORRECT: </strong>\"Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns\" is incorrect $</p><p><strong>INCORRECT:</strong> \"Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket\" is incorrect $</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html#log-record-fields</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/\">https://aws.amazon.com/premiumsupport/knowledge-center/analyze-logs-athena/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-athena/\">https://digitalcloud.training/amazon-athena/</a></p>", "answers": ["<p>Enable S3 server access logging and import the access logs into an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>", "<p>Enable event notifications on the bucket to trigger an AWS Lambda function for all object access events. Configure the function to write access request information to an Amazon RedShift database. Run SQL queries to analyze the access patterns.</p>", "<p>Enable S3 server access logging and use Amazon Athena to create an external table for the log files. Use Athena to run SQL queries and analyze the access patterns.</p>", "<p>Enable logging to Amazon CloudWatch Logs for the S3 bucket. Create a subscription to the log stream and invoke an AWS Lambda function that analyzes the access patterns and saves results to another S3b bucket.</p>"]}, "correct_response": ["c"], "section": "AWS Analytics", "question_plain": "A company runs a serverless application that includes video sharing functionality for logged in users. The video service has become popular, and the company need to know which videos are getting the most interest. A DevOps engineer has been asked to identify the access patterns of the videos. The video files are stored in an Amazon S3 bucket. The information that must be gathered includes the number of users who access specific video files per day and the number of access requests for these files.How can the company meet these requirements with the LEAST amount of effort?", "related_lectures": []}, {"_class": "assessment", "id": 69900208, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A new application is being deployed on AWS using Amazon EC2 instances. The operations team must monitor the instance\u2019s application logs and API activity and need a solution for querying both sets of logs.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The unified CloudWatch agent enables you to collect system logs, application logs, and metrics from your Amazon EC2 instances. The log files can be stored in Amazon CloudWatch Logs.</p><p>AWS CloudTrail logs API activity from your account and can also be configured to stream the log data to an Amazon CloudWatch Logs log group.</p><p>CloudWatch Logs Insights can then be used to query both sets of logs. This service is used to interactively search and analyze your log data in Amazon CloudWatch Logs.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs\" is incorrect.</p><p>CloudWatch Logs Insights can be used to query log data in CloudWatch Logs log groups but not data that is stored directly in an Amazon S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs\" is incorrect.</p><p>The CloudWatch agent can be configured to send the log files to an Amazon CloudWatch Logs log group but cannot send data into an S3 bucket.</p><p><strong>INCORRECT:</strong> \"Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue\" is incorrect.</p><p>The SQS queue is not a destination you can configure in the CloudWatch Logs agent and it not a suitable storage location as it is meant for temporary storage of data that is awaiting processing.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and use AWS CloudTrail to log API activity to Amazon S3. Use CloudWatch Logs Insights to query both sets of logs.</p>", "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon S3 bucket and configure AWS CloudTrail to log API activity to the same S3 bucket. Use Amazon Athena to query both sets of logs.</p>", "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to an Amazon SQS queue and configure AWS CloudTrail to log API activity to the same SQS queue. Create an AWS Lambda function to query messages in the queue.</p>", "<p>Install the Amazon CloudWatch agent on the instances to send the application logs to Amazon CloudWatch Logs and configure AWS CloudTrail to log API activity to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.</p>"]}, "correct_response": ["d"], "section": "AWS Management & Governance", "question_plain": "A new application is being deployed on AWS using Amazon EC2 instances. The operations team must monitor the instance\u2019s application logs and API activity and need a solution for querying both sets of logs.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900210, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.</p><p>What is the most likely cause of this issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Elastic Load Balancing creates a load balancer node for each Availability Zone you enable for the load balancer. The most likely issue here is that the ALB has not been updated with the new AZ so it has not created a node in that AZ and will not distribute traffic there. The DevOps engineer simply needs to update the ALB to include the new AZ and it will then register the targets in the target group and start distributing traffic to them.</p><p><strong>CORRECT: </strong>\"The new AZ has not been enabled for the ALB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Cross-zone load balancing has not been enabled for the ALB\" is incorrect.</p><p>Cross-zone load balancing aims to balance the load between instances evenly across AZs. But you still need to enable the AZ for the ALB.</p><p><strong>INCORRECT:</strong> \"The AMI has not been updated to include the new AZ\" is incorrect.</p><p>AMIs do not have configuration settings to control which AZ they can be launched into.</p><p><strong>INCORRECT:</strong> \"There are no subnets associated with the new AZ\" is incorrect.</p><p>This cannot be the case as the ASG has already been updated with the new AZ and instances have already been launched there.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Cross-zone load balancing has not been enabled for the ALB.</p>", "<p>The new AZ has not been enabled for the ALB.</p>", "<p>The AMI has not been updated to include the new AZ.</p>", "<p>There are no subnets associated with the new AZ.</p>"]}, "correct_response": ["b"], "section": "AWS Networking & Content Delivery", "question_plain": "An application runs on Amazon EC2 instances in an Auto Scaling group behind an application Load Balancer (ALB). The applications runs across two Availability Zones (AZs). Recently the application has become very popular and to increase reliability of the application a DevOps engineer has configured the Auto Scaling group to launch instances across three AZs. However, instances launched in the newly added AZ are not receiving any traffic.What is the most likely cause of this issue?", "related_lectures": []}, {"_class": "assessment", "id": 69900212, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.</p><p>What should the DevOps engineer do to solve this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The configuration requires that the Lambda IAM execution role that is specified in the CloudWatch Logs configuration must have the trust relationship configured to allow lambda.amazonaws.com to assume the role. The AWSLambdaVPCAccessExecutionRole policy must also be added to the function to allow access to the VPC-based OpenSearch cluster.</p><p><strong>CORRECT: </strong>\"Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role\" is incorrect.</p><p>The trust relationship is defined in the role attached to the CloudWatch Logs configuration.</p><p><strong>INCORRECT:</strong> \"Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch\" is incorrect.</p><p>This policy should be attached to the Lambda IAM execution role, not the OpenSearch cluster.</p><p><strong>INCORRECT:</strong> \"Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch\" is incorrect.</p><p>The subscription filter is used instead of using an export which is a better and more automated solution.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/\">https://aws.amazon.com/premiumsupport/knowledge-center/opensearch-stream-data-cloudwatch/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-opensearch/\">https://digitalcloud.training/amazon-opensearch/</a></p>", "answers": ["<p>Add lambda.amazonaws.com in the trust relationship of the AWS Lambda function IAM execution role.</p>", "<p>Create an IAM role that has the AWSLambdaVPCAccessExecutionRole policy. Attach the role to Amazon OpenSearch.</p>", "<p>Add lambda.amazonaws.com in the trust relationship of the CloudWatch Logs Lambda IAM execution role.</p>", "<p>Create an export task in Amazon CloudWatch. Integrate the export task into Amazon OpenSearch.</p>"]}, "correct_response": ["c"], "section": "AWS Analytics", "question_plain": "A DevOps engineer needs to stream data from Amazon CloudWatch Logs to a VPC-based Amazon OpenSearch Service cluster. The DevOps engineer creates an OpenSearch subscription filter for the OpenSearch cluster. However, the DevOps engineer notices that the logs are not visible in Amazon OpenSearch.What should the DevOps engineer do to solve this problem?", "related_lectures": []}, {"_class": "assessment", "id": 69900214, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is deploying a new application with a MySQL-compatible Amazon Aurora database cluster. The Multi-AZ database will have a cross-Region read replica deployed for disaster recovery. The DevOps engineer wants to automate promotion of the replica in the event of a failure of the primary database.</p><p>Which deployment option should the DevOps engineer choose?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Systems Manager Parameter Store is an ideal service to use for storing database connection strings. The application can be configured to recheck for updated connection strings if the application experiences a failure.</p><p>Amazon EventBridge can be used to trigger an action based on state changes in the Aurora database. For instance, you can use the \u201cRDS DB Instance Event\u201d event pattern to detect state changes in the Aurora database.</p><p>When the database is in a failed state a Lambda function can be used to promote the replica and update the database connection endpoint in Systems Manager Parameter Store. This will fully automate the promotion of the replica and the application will automatically pick up the new endpoint.</p><p><strong>CORRECT: </strong>\"Save the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Configure the application to retrieve the updated endpoint from the parameter store if the application fails\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon EventBridge event that detects the database failure and modifies the application's AWS CloudFormation template to promote the replica. Create an AWS Lambda function to apply the updated template to update the stack and point the application to the newly promoted instance\" is incorrect.</p><p>EventBridge cannot update CloudFormation templates. You would need to use an AWS Lambda function to update the template, but the answer states the EventBridge would do this work.</p><p><strong>INCORRECT:</strong> \"Save the Aurora endpoint in AWS Secrets Manager. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and run an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Secrets Manager. Configure the application to retrieve the updated endpoint from the parameter store if the application fails\" is incorrect.</p><p>There is no such thing as publishing RDS failure notifications into AWS CloudTrail, so this solution is not possible.</p><p><strong>INCORRECT:</strong> \"Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to detect database failures and then run an AWS Lambda function. Configure the Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance\" is incorrect.</p><p>As with the previous answer you cannot use CloudTrail to detect database failures as it is a service that is used for recording the API calls made within an account. It is not used for performance monitoring or recording the changes in state of resources (use CloudWatch/EventBridge instead).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>", "answers": ["<p>Save the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Configure the application to retrieve the updated endpoint from the parameter store if the application fails.</p>", "<p>Create an Amazon EventBridge event that detects the database failure and modifies the application's AWS CloudFormation template to promote the replica. Create an AWS Lambda function to apply the updated template to update the stack and point the application to the newly promoted instance.</p>", "<p>Save the Aurora endpoint in AWS Secrets Manager. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and run an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Secrets Manager. Configure the application to retrieve the updated endpoint from the parameter store if the application fails.</p>", "<p>Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to detect database failures and then run an AWS Lambda function. Configure the Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.</p>"]}, "correct_response": ["a"], "section": "AWS Database", "question_plain": "A DevOps engineer is deploying a new application with a MySQL-compatible Amazon Aurora database cluster. The Multi-AZ database will have a cross-Region read replica deployed for disaster recovery. The DevOps engineer wants to automate promotion of the replica in the event of a failure of the primary database.Which deployment option should the DevOps engineer choose?", "related_lectures": []}, {"_class": "assessment", "id": 69900216, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps team is assisting with the deployment of a web application's infrastructure using AWS CloudFormation. The database management team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. The software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to maintain. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A nested stack is a stack that you create within another stack by using the AWS::CloudFormation::Stack resource. With nested stacks, you deploy and manage all resources from a single stack. You can use outputs from one stack in the nested stack group as inputs to another stack in the group. This differs from exporting values.</p><p>If you want to isolate information sharing to within a nested stack group, AWS suggests that you use nested stacks. To share information with other stacks (not just within the group of nested stacks), you should export values instead.</p><p>Change sets for nested stacks affect the entire stack hierarchy which does not meet the requirements as each team requires resource-level change-set reviews. Therefore, in this scenario it is better to export values rather than use a nested stack.</p><p><strong>CORRECT: </strong>\"Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>As described above, to meet the requirements an export of values is preferable to using nested stacks in this scenario.</p><p><strong>INCORRECT:</strong> \"Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks\" is incorrect.</p><p>Stack sets are used for extending the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions.</p><p><strong>INCORRECT:</strong> \"Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack\" is incorrect.</p><p>Values should be exported as per the correct answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html#output-vs-nested</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.</p>", "<p>Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.</p>", "<p>Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.</p>", "<p>Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</p>"]}, "correct_response": ["a"], "section": "AWS Management & Governance", "question_plain": "A DevOps team is assisting with the deployment of a web application's infrastructure using AWS CloudFormation. The database management team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. The software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to maintain. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900218, "assessment_type": "multi-select", "prompt": {"question": "<p>A fast-growing US based company has just opened an office in Frankfurt, Germany. The users in the new office have reported high latency for the company\u2019s CRM application. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and stores data in an Amazon DynamoDB table. The CRM application runs in the us-east-1 Region.</p><p>A DevOps engineer must minimize application response times and improve availability for users in both Regions.</p><p>Which combination of actions should be taken to address the latency issues? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>To reduce latency the application must be deployed in a Region closest to the users, in this case eu-central-1. As ALBs and Auto Scaling groups are regional services they must be deployed in this new Region. Amazon Route 53 can be used to create records with latency-based routing to direct users to the Region that provides the lowest latency. This will typically be the eu-central-1 Region for users in Frankfurt.</p><p>The database must be replicated to ensure the CRM data is consistent. A multi-master database is required so that changes in either Region will be synchronized. DynamoDB tables is ideal for this requirement and can be easily enabled at any time by creating a replication configuration on the existing table.</p><p><strong>CORRECT: </strong>\"Create a new ALB and Auto Scaling group in the eu-central-1 Region and configure the new ALB to direct traffic to the new Auto Scaling group\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use DynamoDB Global Tables to create a replica of the existing DynamoDB table in the eu-central-1 Region\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a new DynamoDB table in the eu-central-1 Region with cross-Region replication enabled\" is incorrect.</p><p>DynamoDB Global Tables should be used and can be enabled by creating a replication configuration on the existing table.</p><p><strong>INCORRECT:</strong> \"Create a new Auto Scaling group in the eu-central-1 Region and configure the existing ALB to direct traffic to the new Auto Scaling group\" is incorrect.</p><p>You cannot direct traffic to an ASG that is in another Region as both ELBs and ASGs are regional services.</p><p><strong>INCORRECT:</strong> \"Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB\" is incorrect.</p><p>Latency-based routing should be used rather than failover routing as the objective is to direct users to the application endpoint that has the lowest latency.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables.tutorial.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>", "answers": ["<p>Create a new DynamoDB table in the eu-central-1 Region with cross-Region replication enabled.</p>", "<p>Create a new Auto Scaling group in the eu-central-1 Region and configure the existing ALB to direct traffic to the new Auto Scaling group.</p>", "<p>Create a new ALB and Auto Scaling group in the eu-central-1 Region and configure the new ALB to direct traffic to the new Auto Scaling group.</p>", "<p>Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.</p>", "<p>Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.</p>", "<p>Use DynamoDB Global Tables to create a replica of the existing DynamoDB table in the eu-central-1 Region.</p>"]}, "correct_response": ["c", "d", "f"], "section": "AWS Database", "question_plain": "A fast-growing US based company has just opened an office in Frankfurt, Germany. The users in the new office have reported high latency for the company\u2019s CRM application. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and stores data in an Amazon DynamoDB table. The CRM application runs in the us-east-1 Region.A DevOps engineer must minimize application response times and improve availability for users in both Regions.Which combination of actions should be taken to address the latency issues? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69900220, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Elastic Beanstalk offers automatic rollback options for deployment updates. This coupled with auto scaling and the ALB meets the requirements for a scalable compute and web tier. The RDS database provides a managed solution for the MySQL database. The RDS MySQL database should be created outside of the Elastic Beanstalk environment as it may be used by other applications. If it is created within the Elastic Beanstalk environment it could be automatically deleted if the environment is deleted.</p><p><strong>CORRECT: </strong>\"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack\" is incorrect.</p><p>As explained above the RDS database should be created outside of the Elastic Beanstalk environment.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>ECS does not offer automatic rollback so Elastic Beanstalk is a better solution to meet the requirements.</p><p><strong>INCORRECT:</strong> \"Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier\" is incorrect.</p><p>Automating the creation of snapshots is not a suitable solution for rollback. Elastic Beanstalk offers several deployment options which offer automatic rollback.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>", "answers": ["<p>Deploy the application on Amazon ECS. Configure Elastic Load Balancing and Auto Scaling. Create an ECS service and specify the desired task count. Use an Amazon RDS MySQL instance for the database tier.</p>", "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create an Amazon RDS MySQL instance inside the Elastic Beanstalk stack.</p>", "<p>Deploy the application on Amazon EC2. Configure Elastic Load Balancing and Auto Scaling. Schedule an AWS Lambda function to take regular snapshots of attached EBS volumes. Use an Amazon RDS MySQL instance for the database tier.</p>", "<p>Deploy the application using AWS Elastic Beanstalk. Configure the environment type for Elastic Load Balancing and Auto Scaling. Create the Amazon RDS MySQL instance outside the Elastic Beanstalk stack.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "A DevOps Engineer needs a scalable Node.js application in AWS with a MySQL database. There should be no downtime during deployments and if issues occur rollback to a previous version must be easy to implement. The database may also be used by other applications.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900222, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.</p><p>Which solution will meet these requirements with the least operational overhead?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. The agent includes the following components:</p><p>\u00b7 A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</p><p>\u00b7 A script (daemon) that initiates the process to push data to CloudWatch Logs.</p><p>\u00b7 A cron job that ensures that the daemon is always running.</p><p>You can create metric filters to match terms in your log events and convert log data into metrics. When a metric filter matches a term, it increments the metric's count. For example, you can create a metric filter that counts the number of times the word <strong><em>ERROR</em></strong> occurs in your log events.</p><p>In this case the metric filter can search for user login data and then if this information is found it can send an SNS notification to the security team.</p><p><strong>CORRECT: </strong>\"Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>The Systems Manager agent will not gather this information from EC2 instances. The CloudWatch Logs agent must be installed.</p><p><strong>INCORRECT:</strong> \"Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>CloudTrail will only report on API activity, and this does not include login data from an Amazon EC2 instance.</p><p><strong>INCORRECT:</strong> \"Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS\" is incorrect.</p><p>This is possible though it is not the best solution as it requires the script to be rerun on a regular basis and requires more operational overhead to create and maintain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Install the AWS Systems Manager agent on each EC2 instance. Subscribe to Amazon CloudWatch Events notifications. Trigger an AWS Lambda function to check if a message contains user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>", "<p>Configure an AWS CloudTrail trail and send events to Amazon CloudWatch Logs. Subscribe CloudWatch Logs to an AWS Lambda function. Configure the Lambda function to check if the logs contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>", "<p>Use AWS Systems Manager to automate the execution of a script on each Amazon EC2 instance that pushes all logs to Amazon S3. Set up an S3 event to trigger an AWS Lambda function that checks if files in S3 contain user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>", "<p>Install the Amazon CloudWatch agent on the EC2 instances. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user login data. If this information is found, send a notification to the security team using Amazon SNS.</p>"]}, "correct_response": ["d"], "section": "AWS Management & Governance", "question_plain": "A financial services company requires that DevOps engineers should not log directly into Amazon EC2 instances that process highly sensitive data except in exceptional circumstances. The security team requires a notification within 15 minutes if a DevOps engineer does log in to an instance.Which solution will meet these requirements with the least operational overhead?", "related_lectures": []}, {"_class": "assessment", "id": 69900224, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer deployed an application to AWS which makes use of Amazon EC2 Auto Scaling to launch new instances. The engineer needs to modify the instance type used for all new instances that are launched through automatic scaling. The Auto Scaling group is configured to use a launch template.</p><p>Which of the following actions should be taken to achieve this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When you make a change to an existing launch template it creates a new version of the template. It is then easy to modify the Auto Scaling group to use the new version. As you can see in the diagram below you can modify the launch template used or the version of the launch template used.</p><p>In this case the engineer can simply save a new version of the template and then update the version of template used by the Auto Scaling group. All subsequent scaling events will launch using the new version of the template.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-06-22-87f49932ae89a3d9a078c0a894e0c1f5.jpg\"><p><strong>CORRECT: </strong>\"Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group\" is incorrect.</p><p>The Overrides structure is used to define a different launch template for specific instance types. The best solution for this scenario is to simply update the launch template.</p><p><strong>INCORRECT:</strong> \"Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events\" is incorrect.</p><p>There is no need to move the solution to Elastic Beanstalk as the launch template can simply be updated.</p><p><strong>INCORRECT:</strong> \"Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group\" is incorrect.</p><p>This is not a good solution as it is highly manual. The solution should be automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/examples-launch-templates-aws-cli.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Create an AWS Elastic Beanstalk environment to deploy the new instance type for all scaling events.</p>", "<p>Use the Overrides structure to define a new launch template for individual instance types using the existing Auto Scaling group.</p>", "<p>Launch new EC2 instances with the new instance type using the AWS CLI and attach them to the Auto Scaling group.</p>", "<p>Modify the existing launch template to create a new version that uses the new instance type and modify the Auto Scaling group to use the new template version.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "A DevOps engineer deployed an application to AWS which makes use of Amazon EC2 Auto Scaling to launch new instances. The engineer needs to modify the instance type used for all new instances that are launched through automatic scaling. The Auto Scaling group is configured to use a launch template.Which of the following actions should be taken to achieve this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69900226, "assessment_type": "multi-select", "prompt": {"question": "<p>A company uses GitHub to store the code for their software development. The company\u2019s DevOps team want to automate the build process of an application. When the GitHub repository is updated, the code should be compiled, tested, and then pushed to an Amazon S3 bucket.</p><p>Which combination of steps would address these requirements? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>AWS CodeBuild can be used for this project to perform the compilation of code, testing, and pushing the package to S3. AWS CodeBuild supports webhooks when the source repository is GitHub. This means that for a CodeBuild build project that has its source code stored in a GitHub repository, webhooks can be used to rebuild the source code every time a code change is pushed to the repository.</p><p>The buildspec.yml file is used to define the build commands and sequence to run during the build process. This file is placed in the source code.</p><p><strong>CORRECT: </strong>\"Create an AWS CodeBuild project with GitHub as the source repository\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Add a buildspec.yml file to the source code with build instructions\" is also a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform\" is incorrect.</p><p>CodeDeploy is used for deploying the code. In this case the code needs to be compiled and tested. CodeBuild is used for this purpose. There is no mention in the question about how the code will be deployed to servers.</p><p><strong>INCORRECT:</strong> \"Create an AWS OpsWorks deployment with the install dependencies command\" is incorrect.</p><p>OpsWorks is not needed in this scenario as this is a pure compile and build requirement which is a good use case for CodeBuild.</p><p><strong>INCORRECT:</strong> \"Launch an Amazon EC2 instance to perform the build\" is incorrect.</p><p>There is no need to launch an instance and CodeBuild can automatically do this on the platform of your choice.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/sample-github-pull-request.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Add a buildspec.yml file to the source code with build instructions.</p>", "<p>Configure a GitHub webhook to trigger a build every time a code change is pushed to the repository.</p>", "<p>Create an AWS CodeDeploy application with the Amazon EC2/On-Premises compute platform.</p>", "<p>Create an AWS OpsWorks deployment with the install dependencies command.</p>", "<p>Launch an Amazon EC2 instance to perform the build.</p>", "<p>Create an AWS CodeBuild project with GitHub as the source repository.</p>"]}, "correct_response": ["a", "b", "f"], "section": "AWS Developer Tools", "question_plain": "A company uses GitHub to store the code for their software development. The company\u2019s DevOps team want to automate the build process of an application. When the GitHub repository is updated, the code should be compiled, tested, and then pushed to an Amazon S3 bucket.Which combination of steps would address these requirements? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 69900228, "assessment_type": "multi-select", "prompt": {"question": "<p>A company is deploying a new application that uses Amazon EC2 instances for the web tier and a MySQL database for the database tier. An Application Load Balancer (ALB) will be used in front of the web tier. The company requires an RPO of 2 hours and an RTO of 10 minutes for the solution.</p><p>Which combination of deployment strategies will meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To meet the RTO and RPO requirements the best solution for the database tier is to use Amazon Aurora global database. This solution provides replication from the Aurora storage layer across Regions. The reader endpoint in the secondary endpoint can be promoted in the event of a DR scenario to be the main database which takes on read/write responsibilities.</p><p>For the web tier this can be placed behind ALBs in each Region. Amazon Route 53 failover-based routing policies with health checks should be created. These records will point to the ALBs in each Region and if the health checks fail in the primary Region, automatic failover to the secondary Region will occur.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-10-42-d7b7624f7e1003a9bd3098d979fb2910.jpg\"></p><p><strong>CORRECT: </strong>\"Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes\" is incorrect.</p><p>The better solution is to use Aurora Global Database as this will simplify the failover process which can be instantiated through a few API calls. Also, this replication uses Aurora replication with low latency and the Aurora reader endpoint can also be utilized in the second Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions\" is incorrect.</p><p>It is not possible to create multi-master clusters across Regions, they work within a Region only.</p><p><strong>INCORRECT:</strong> \"Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records\" is incorrect.</p><p>Failover routing policy records should be created, not latency routing records. With latency records users closer to the secondary Region will be directed there but the DB layer is not in read/write mode except in a DR scenario.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>", "answers": ["<p>Create two Amazon Aurora clusters spread across two Regions. Use AWS Database Migration Service (AWS DMS) to synchronize changes.</p>", "<p>Create an Amazon Aurora global database in two Regions for the database tier. In the event of a failure, promote the secondary Region to take on read/write responsibilities.</p>", "<p>Create an Amazon Aurora multi-master cluster across multiple Regions for the database tier. Configure the database in an active-active mode across the Regions.</p>", "<p>Deploy the application in two Regions and create Amazon Route 53 failover-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>", "<p>Deploy the application in two Regions and create Amazon Route 53 latency-based routing records pointing to the ALB in each Regions. Enable health checks for the records.</p>"]}, "correct_response": ["b", "d"], "section": "AWS Database", "question_plain": "A company is deploying a new application that uses Amazon EC2 instances for the web tier and a MySQL database for the database tier. An Application Load Balancer (ALB) will be used in front of the web tier. The company requires an RPO of 2 hours and an RTO of 10 minutes for the solution.Which combination of deployment strategies will meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900230, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.</p><p>Which actions should a DevOps engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Elastic Load Balancing provides access logs that capture detailed information about the TLS requests sent to your Network Load Balancer. You can use these access logs to analyze traffic patterns and troubleshoot issues. The logs are sent to an Amazon S3 bucket you configure as the logging destination. This bucket can be encrypted using one of the available server-side encryption options.</p><p>When you enable access logging, you must specify an S3 bucket for the access logs. The policy must grant permission to the AWS service account \u2018delivery.logs.amazonaws.com\u2019. In this case, the DevOps team also require permissions to access the bucket, and this can be granted through an IAM policy attached to the team members, most likely via an IAM group.</p><p><strong>CORRECT: </strong>\"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account\" is incorrect.</p><p>This will not allow read access for the DevOps team as the only permission is write access.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>INCORRECT:</strong> \"Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account\" is incorrect.</p><p>This solution does not grant any permissions to the DevOps team to access the data in the logging location.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-access-logs.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019. Apply an IAM permissions policy to the DevOps team that grants read access.</p>", "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the IAM group containing the DevOps team user accounts and the AWS service account.</p>", "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-S3 encryption on the S3 bucket and configure the bucket policy to allow write access for the principal \u2018delivery.logs.amazonaws.com\u2019.</p>", "<p>Enable access logs on the NLB and configure an Amazon S3 bucket as the destination. Enable SSE-KMS encryption on the S3 bucket and configure the bucket policy to allow write access for the AWS service account.</p>"]}, "correct_response": ["a"], "section": "AWS Networking & Content Delivery", "question_plain": "An application sits behind a Network Load Balancer (NLB) that is configured with a TLS listener. The DevOps team must analyze traffic patterns and require information about the connections made by clients. The data that is captured must be stored securely with encryption at rest and should only be accessible to the DevOps team members.Which actions should a DevOps engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 69900232, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial organization is using a multi-account strategy with AWS Organizations. The AWS accounts are organized into different organizational units. Amazon CloudWatch Logs is used for logging within each account. The company needs to ship all the logs to a single centralized account for archiving purposes. The solution must be secure, and centralized, and the logs must be stored cost-effectively.</p><p>How can a DevOps Engineer meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Subscriptions can be used to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format.</p><p>For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on the AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore, we must subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.</p><p><strong>CORRECT: </strong>\"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon EFS which is not a supported destination.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS\" is incorrect.</p><p>This solution is possible, but Amazon EFS is not the most cost-efficient storage solution for archiving purposes. Amazon S3 is more suitable.</p><p><strong>INCORRECT:</strong> \"Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift\" is incorrect.</p><p>The issue with this option is that the target for Kinesis Firehose is set as Amazon Redshift which is not suitable for archiving data and would be very expensive. RedShift is used for analytics purposes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-kinesis/\">https://digitalcloud.training/amazon-kinesis/</a></p>", "answers": ["<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3.</p>", "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon EFS.</p>", "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a subscription for the log stream that triggers and AWS Lambda function that copies the log data to Amazon EFS.</p>", "<p>Create a log destination in the centralized account and create a log subscription on that destination. Create a Kinesis Streams and subscribe it to the destination. Create a Kinesis Firehose delivery stream and subscribe it to the Kinesis Stream. The target of the Kinesis Firehose should be Amazon Redshift.</p>"]}, "correct_response": ["a"], "section": "AWS Analytics", "question_plain": "A financial organization is using a multi-account strategy with AWS Organizations. The AWS accounts are organized into different organizational units. Amazon CloudWatch Logs is used for logging within each account. The company needs to ship all the logs to a single centralized account for archiving purposes. The solution must be secure, and centralized, and the logs must be stored cost-effectively.How can a DevOps Engineer meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900234, "assessment_type": "multi-select", "prompt": {"question": "<p>An online retail chain processes thousands of orders each day from 100 countries and its website is localized in 15 languages. The company\u2019s website faces continual security threats and challenges in the form of HTTP flood attacks, distributed denial of service (DDoS) attacks, and rogue robots that flood its website with traffic. Most of these attacks originate from certain countries.</p><p>The company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running behind an Application Load Balancer (ALB) with AWS WAF.</p><p>How can a DevOps engineer protect workloads from DDoS attacks whilst allowing the development team to be productive? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.</p><p>You can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p><p><strong>CORRECT: </strong>\"Use an AWS WAF IP set statement that specifies the IP addresses that needs to be allowed\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Use an AWS WAF geo match statement listing the countries that need to be blocked\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use an ALB geo match statement listing the countries that needs to be blocked\" is incorrect.</p><p>An ALB cannot block or allow traffic based on geographic match conditions or IP based conditions.</p><p><strong>INCORRECT:</strong> \"Use an ALB IP set statement that specifies the IP addresses that needs to be allow through\" is incorrect.</p><p>Geo match restriction works with WAF and not ALB. The correct way to add geo restriction is to bind the condition as an WAF rule.</p><p><strong>INCORRECT:</strong> \"Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances\" is incorrect.</p><p>Creating a Network Access Control List (NACL) would be tedious and won\u2019t entirely cover the requirements here. AWS WAF is designed for protecting against the types of attacks mentioned.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html\">https://docs.aws.amazon.com/waf/latest/developerguide/classic-web-acl-ip-conditions.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/\">https://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-waf-shield/\">https://digitalcloud.training/aws-waf-shield/</a></p>", "answers": ["<p>Use an AWS WAF IP set statement that specifies the IP addresses that needs to be allowed.</p>", "<p>Use an ALB geo match statement listing the countries that needs to be blocked.</p>", "<p>Use an ALB IP set statement that specifies the IP addresses that needs to be allow through.</p>", "<p>Use an AWS WAF geo match statement listing the countries that need to be blocked.</p>", "<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances.</p>"]}, "correct_response": ["a", "d"], "section": "AWS Security, Identity, & Compliance", "question_plain": "An online retail chain processes thousands of orders each day from 100 countries and its website is localized in 15 languages. The company\u2019s website faces continual security threats and challenges in the form of HTTP flood attacks, distributed denial of service (DDoS) attacks, and rogue robots that flood its website with traffic. Most of these attacks originate from certain countries.The company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running behind an Application Load Balancer (ALB) with AWS WAF.How can a DevOps engineer protect workloads from DDoS attacks whilst allowing the development team to be productive? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900236, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.</p><p>Which actions should be taken to deploy the application across these accounts?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A <em>stack set</em> lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that template requires.</p><p>You can create a stack set using the AWS Management Console or using AWS CloudFormation commands in the AWS CLI. You can create a stack set with either self-managed or service-managed permissions.</p><p>With self-managed permissions, you can deploy stack instances to specific AWS accounts in specific Regions. To do this, you must first create the necessary IAM roles to establish a trusted relationship between the account you're administering the stack set from and the account you're deploying stack instances to.</p><p>With service-managed permissions, you can deploy stack instances to accounts managed by AWS Organizations in specific Regions. With this model, you don't need to create the necessary IAM roles; StackSets creates the IAM roles on your behalf</p><p>In this case the best solution is to use self-managed permissions as the partner companies have only granted the ability to create IAM roles. They would certainly not want their entire account to be controlled by the service provider through AWS Organizations just so they can deploy a single application.</p><p><strong>CORRECT: </strong>\"Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>You cannot have a role in one account that has permissions in another account. The roles must be created in each AWS account and self-managed permissions must be used with AWS CloudFormation stack sets.</p><p><strong>INCORRECT:</strong> \"Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application\" is incorrect.</p><p>The partner companies would not want the service provider to control their accounts with AWS Organizations just for a single application. They have granted the ability to create roles which is enough for the service provider to deploy the application using stack sets with self-managed permissions.</p><p><strong>INCORRECT:</strong> \"Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template\" is incorrect.</p><p>The partner companies have granted the ability to create roles, not user accounts. Also, this would be inefficient.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs-self-managed.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>Add the CloudFormation template to a central shared Amazon S3 bucket. Create administrative user accounts in each partner account. Log in to each partner account and create a stack set to deploy the application from the shared template.</p>", "<p>Create an AWS Organization with all features enabled and add the partner accounts to the organization. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>", "<p>Create an IAM role in the service provider account that grants permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with service-managed permissions to deploy the application.</p>", "<p>Create IAM roles in each partner account that grant administrators in the service provider\u2019s account permissions to perform stack set operations in the partner accounts. Use CloudFormation stack sets with self-managed permissions to deploy the application.</p>"]}, "correct_response": ["d"], "section": "AWS Management & Governance", "question_plain": "A service provider has created business relationships with several companies. The service provider plans to deploy an application to multiple AWS accounts managed by these partner companies using AWS CloudFormation. Each partner company has granted the permissions to create IAM roles with permissions for the deployment in their respective accounts. The organization must minimize operational overhead and stack management.Which actions should be taken to deploy the application across these accounts?", "related_lectures": []}, {"_class": "assessment", "id": 69900238, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.</p><p>It is mandated that additional checks must be included before code is promoted into production.</p><p>What additional steps could be taken to ensure code quality?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An approval action can be added to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop, so someone can manually approve or reject the action. This is the simplest and most effective option presented.</p><p><strong>CORRECT: </strong>\"Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Pause the deployment stage to perform a final validation on the deployment\" is incorrect.</p><p>Pausing can help in the above scenario but could lead to additional delay based on user input, adding a manual approval stage is a better solution.</p><p><strong>INCORRECT:</strong> \"Add test stages before and after the deploy action\" is incorrect.</p><p>This could work but would need more configuration and effort as compared to adding a stage.</p><p><strong>INCORRECT:</strong> \"Have the build stage trigger a Lambda function that can perform a verification before deployment\" is incorrect.</p><p>Since the verification could change, Lambda would need customization depending on scenario and would be a time-consuming option.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>", "answers": ["<p>Pause the deployment stage to perform a final validation on the deployment.</p>", "<p>Add test stages before and after the deploy action.</p>", "<p>Add a manual approval to a stage before the deploy action in the code pipeline deploying code to PROD.</p>", "<p>Have the build stage trigger an AWS Lambda function that can perform a verification before deployment.</p>"]}, "correct_response": ["c"], "section": "AWS Developer Tools", "question_plain": "As part of single click deployment strategy, a financial organization is utilizing AWS CodeCommit, CodeBuild and CodeDeploy in an automated pipeline using AWS CodePipeline. It was discovered that in some instances, code was deployed which led to vulnerabilities in production systems.It is mandated that additional checks must be included before code is promoted into production.What additional steps could be taken to ensure code quality?", "related_lectures": []}, {"_class": "assessment", "id": 69900240, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A global multi-player gaming application that was deployed in Europe now needs to be extended globally. Availability of the application must be extremely high, and latency should be low. The application is hosted on Amazon EC2 instances. It is anticipated that the traffic will be unevenly distributed with a few locations having much more traffic than others.</p><p>Which of the below routing strategies would be best fit considering above parameters?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Geo-proximity Routing - Lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources.</p><p>You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.</p><p>The catch in the question is a few regions have heavier traffic load than the others so a bias needs to be configured and geoproximity routing is a better fit for this solution.</p><p><strong>CORRECT: </strong>\"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers\" is incorrect.</p><p>If the discrepancy between traffic volume was unknown, this would also been a correct option.</p><p><strong>INCORRECT:</strong> \"Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions\" is incorrect.</p><p>Latency-based routing is based on latency measurements performed over a period, and the measurements reflect changes in network connectivity and routing.</p><p><strong>INCORRECT:</strong> \"Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally\" is incorrect.</p><p>It is unlikely that a CDN can be used to cache a dynamic gaming application. A load balanced deployment of auto scaling EC2 instances is needed to run the application and ensure availability and performance.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-route-53/\">https://digitalcloud.training/amazon-route-53/</a></p>", "answers": ["<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geoproximity-based routing record. Point the record to each of your load balancers.</p>", "<p>Deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions. Create a Route 53 geolocation-based routing record. Point the record to each of your load balancers.</p>", "<p>Deploy Amazon CloudFront in front of the instances to cache requests and deliver content from Edge Locations globally.</p>", "<p>Utilize Route 53 latency-based routing and deploy the EC2 instances in an Auto Scaling group behind an Application Load Balancer in two different Regions.</p>"]}, "correct_response": ["a"], "section": "AWS Networking & Content Delivery", "question_plain": "A global multi-player gaming application that was deployed in Europe now needs to be extended globally. Availability of the application must be extremely high, and latency should be low. The application is hosted on Amazon EC2 instances. It is anticipated that the traffic will be unevenly distributed with a few locations having much more traffic than others.Which of the below routing strategies would be best fit considering above parameters?", "related_lectures": []}]}
5794174
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 69900378, "assessment_type": "multi-select", "prompt": {"question": "<p>To improve security, a company plans to use AWS Systems Manager Session Manager to manage EC2 instances instead of using key pairs. The company also requires that access to Session Manager goes across private networks only.</p><p>Which combinations of actions will accomplish this? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Session Manager is a fully managed AWS Systems Manager capability. With Session Manager, you can manage Amazon EC2 instances, edge devices, and on-premises servers and virtual machines (VMs).</p><p>Session Manager provides secure and auditable node management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. You do not need to open SSH ports in security groups and can enable private access using VPC endpoints.</p><p>To manage your instances using Session Manager you must install the Systems Manager agent on them and provide the necessary permissions for management. Permissions should be assigned through IAM instance profiles and policies.</p><p><strong>CORRECT: </strong>\"Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR\" is incorrect.</p><p>With Systems Manager Session Manager you do not need to open SSH ports.</p><p><strong>INCORRECT:</strong> \"Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager\" is incorrect.</p><p>The private access between Session Manager and EC2 instances happens within AWS via VPC endpoints. A VPN cannot be used.</p><p><strong>INCORRECT:</strong> \"Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions\" is incorrect.</p><p>This is a less secure method of providing the permissions needed by the EC2 instances. The better option is to use IAM instance profiles and policies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>", "answers": ["<p>Update all EC2 instance security groups to allow SSH port TCP 22 inbound from the VPC CIDR.</p>", "<p>Deploy an AWS Site to Site VPN in the relevant AWS Region for private access to Systems Manager.</p>", "<p>Attach an IAM policy providing the required Systems Manager permissions to an existing IAM instance profile.</p>", "<p>Run the \u2018aws configure\u2019 command on all EC2 instances to add access keys that provide the required Systems Manager permissions.</p>", "<p>Create VPC endpoints for Systems Manager in the relevant AWS Region to provide private access.</p>"]}, "correct_response": ["c", "e"], "section": "AWS Management & Governance", "question_plain": "To improve security, a company plans to use AWS Systems Manager Session Manager to manage EC2 instances instead of using key pairs. The company also requires that access to Session Manager goes across private networks only.Which combinations of actions will accomplish this? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900380, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company runs an application across thousands of EBS-backed Amazon EC2 instances. The company needs to ensure availability of the application and requires that instances are restarted when an EC2 instance retirement event is scheduled.</p><p>How can this a DevOps engineer automate this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An EC2 instance is scheduled for retirement when AWS detects an irreparable failure in the infrastructure that's hosting your instance. You are required to stop and then start the instance at your preferred time before the instance retirement date. Stopping and starting the instance moves the instance to another healthy host.</p><p>The best way to automate this process is to create a rule in Amazon EventBridge that looks for AWS Health events. The specific event is:</p><p>\u201cAWS_EC2_INSTANCE_RETIREMENT_SCHEDULED\u201d</p><p>When this event occurs EventBridge can trigger an AWS Systems Manager automation document that stops and starts the EC2 instances.</p><p><strong>CORRECT: </strong>\"Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances\" is incorrect.</p><p>Status checks do not inform us that an instance retirement event is scheduled, they let us know if there are issues that are affecting the instances or hosts.</p><p><strong>INCORRECT:</strong> \"Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours\" is incorrect.</p><p>Auto Recovery will recover an instance automatically, but this is not related to retirement events. You also cannot configure a time schedule in the alarm action.</p><p><strong>INCORRECT:</strong> \"Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances\" is incorrect.</p><p>This would restart all instances that are shutdown, so the scope is too broad. We specifically want to target only the instances that are affected by retirement events.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-retirement/</a></p><p><a href=\"https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html\">https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Create an Amazon CloudWatch alarm for EC2 status checks. Configure the alarm to trigger an Amazon SNS notification to the operations team and have them stop and start affected instances.</p>", "<p>Create a rule in Amazon EventBridge with AWS Health as the source and look for instance retirement scheduled events. Run an AWS Systems Manager automation document that stops and starts affected instances.</p>", "<p>Enable EC2 Auto Recovery on all instances. Configure an Amazon CloudWatch alarm with the alarm action set to Recover. Specify a time for recovery that is outside of business hours.</p>", "<p>Create a rule in Amazon EventBridge with Amazon EC2 as the source and look for EC2 instance state-change notifications that indicate the instance is shutting down. Run an AWS Systems Manager automation document that starts the affected instances.</p>"]}, "correct_response": ["b"], "section": "AWS Application Integration", "question_plain": "A company runs an application across thousands of EBS-backed Amazon EC2 instances. The company needs to ensure availability of the application and requires that instances are restarted when an EC2 instance retirement event is scheduled.How can this a DevOps engineer automate this task?", "related_lectures": []}, {"_class": "assessment", "id": 69900382, "assessment_type": "multi-select", "prompt": {"question": "<p>An application runs on Amazon ECS instances in an Auto Scaling group behind an Application Load Balancer. An issue has occurred where instances are failing to respond to requests and are failing HTTP target group health checks.</p><p>A DevOps engineer has reviewed the configuration and noticed that the application has failed on some EC2 instances and error messages relating to memory usage have been logged in the system logs of affected instances.</p><p>The application may have a memory leak and the DevOps engineer needs to take steps to improve the resiliency of the application. Monitoring and notifications should be enabled for when issues occur.</p><p>Which combination of actions will meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default, Amazon EC2 Auto Scaling ignores the results of the Elastic Load Balancing health checks. However, you can enable these health checks for your Auto Scaling group. After you do this, when Elastic Load Balancing reports a registered instance as unhealthy, Amazon EC2 Auto Scaling marks the instance as unhealthy on its next periodic health check and replaces it. This will resolve the issue of having EC2 instances running that are out of memory and on which the application has failed.</p><p>Amazon CloudWatch does not collect memory utilization metrics by default. To get notifications the DevOps engineer can install the Amazon CloudWatch agent on the EC2 instances. The agent includes metrics such as \u2018mem_active\u2019, \u2018mem_available\u2019, and \u2018mem_free\u2019. This information can be used in a CloudWatch alarm to trigger notifications via SNS if an alarm threshold is exceeded.</p><p><strong>CORRECT: </strong>\"Configure the Auto Scaling group configuration to replace the EC2 instances when they fail the load balancer's health checks\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Configure the Amazon CloudWatch agent on the EC2 instances. Create an alarm based on memory utilization metrics and send a message to an Amazon SNS topic when the alarm is triggered\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an Amazon CloudWatch alarm that automatically recovers instances based on EC2 status checks\" is incorrect.</p><p>EC2 instance status checks do not check the application, they check the underlying instance, software, and network connectivity. In this case, the underlying instance is healthy, but the application has failed, and this will not be detected through status checks.</p><p><strong>INCORRECT:</strong> \"Configure the target group health checks to use TCP rather than HTTP and set the port to the port the application is listening on\" is incorrect.</p><p>The application runs on port 80 and it is a web service. Changing the health check from HTTP to TCP but with the same port will not make any difference. Also, with an ALB you can only use HTTP or HTTPS for health checks, so the configuration is not even possible.</p><p><strong>INCORRECT:</strong> \"Configure an alarm in Amazon CloudWatch that monitors memory utilization and sends a message to an Amazon SNS topic when memory utilization is high\" is incorrect.</p><p>CloudWatch does collect memory utilization metrics from EC2 instances by default. You must use the CloudWatch agent.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Configure the Auto Scaling group configuration to replace the EC2 instances when they fail the load balancer's health checks.</p>", "<p>Configure an Amazon CloudWatch alarm that automatically recovers instances based on EC2 status checks.</p>", "<p>Configure the target group health checks to use TCP rather than HTTP and set the port to the port the application is listening on.</p>", "<p>Configure an alarm in Amazon CloudWatch that monitors memory utilization and sends a message to an Amazon SNS topic when memory utilization is high.</p>", "<p>Configure the Amazon CloudWatch agent on the EC2 instances. Create an alarm based on memory utilization metrics and send a message to an Amazon SNS topic when the alarm is triggered.</p>"]}, "correct_response": ["a", "e"], "section": "AWS Compute", "question_plain": "An application runs on Amazon ECS instances in an Auto Scaling group behind an Application Load Balancer. An issue has occurred where instances are failing to respond to requests and are failing HTTP target group health checks.A DevOps engineer has reviewed the configuration and noticed that the application has failed on some EC2 instances and error messages relating to memory usage have been logged in the system logs of affected instances.The application may have a memory leak and the DevOps engineer needs to take steps to improve the resiliency of the application. Monitoring and notifications should be enabled for when issues occur.Which combination of actions will meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900384, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying a serverless application that includes an Amazon API Gateway REST API and an AWS Lambda function. A DevOps engineer must come up with a strategy for updating the application that enables new features to be tested on a small number of users before rolling out the update to all users.</p><p>Which deployment strategy will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>CloudFormation can be used to deploy both the AWS Lambda function and API Gateway REST API. The CloudFormation stack can then be updated to point to new function code when the version needs to be updated. For the REST API the update can be configured to use a canary release strategy to test the updates on a small number of users before full rollout. Once fully tested, the new version of the function code can be configured to take all traffic.</p><p><strong>CORRECT: </strong>\"Use AWS CloudFormation to deploy the serverless services and use Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AWS CDK to deploy the serverless services. When code needs to be changed, update the AWS CloudFormation stack, and deploy the new version of the APIs and Lambda functions. Use an Amazon Route 53 failover routing policy for the canary release strategy\" is incorrect.</p><p>The CDK is used when you need to define cloud application resources using programming languages such as TypeScript, Python, Java etc. and then deploy through CloudFormation. The CDK isn\u2019t needed here, and the Route 53 failover routing policy cannot be used for a canary strategy as it involves one target receiving all traffic until it fails a health check at which time all traffic is failed to the failover target.</p><p><strong>INCORRECT:</strong> \"Use AWS Elastic Beanstalk to deploy the serverless services. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic using a canary release strategy\" is incorrect.</p><p>You cannot use Elastic Beanstalk to deploy these serverless services.</p><p><strong>INCORRECT:</strong> \"Use AWS SAM to deploy the serverless services and use a Lambda alias. When code needs to be changed, update the CloudFormation stack with the new Lambda code and API version. Use an Amazon Route 53 latency routing policy for the canary release strategy\" is incorrect.</p><p>The serverless services can be deployed via SAM but should then be updated via the \u201csam deploy\u201d command. The update of the stack will also update everything in place and the latency routing would not achieve a canary release strategy as it simply routes traffic based on latency (and we only have one target anyway).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-apigateway-deployment-deploymentcanarysettings.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-apigateway-deployment-deploymentcanarysettings.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>", "answers": ["<p>Use the AWS CDK to deploy the serverless services. When code needs to be changed, update the AWS CloudFormation stack, and deploy the new version of the APIs and Lambda functions. Use an Amazon Route 53 failover routing policy for the canary release strategy.</p>", "<p>Use AWS CloudFormation to deploy the serverless services and use Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.</p>", "<p>Use AWS Elastic Beanstalk to deploy the serverless services. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic using a canary release strategy.</p>", "<p>Use AWS SAM to deploy the serverless services and use a Lambda alias. When code needs to be changed, update the CloudFormation stack with the new Lambda code and API version. Use an Amazon Route 53 latency routing policy for the canary release strategy.</p>"]}, "correct_response": ["b"], "section": "AWS Networking & Content Delivery", "question_plain": "A company is deploying a serverless application that includes an Amazon API Gateway REST API and an AWS Lambda function. A DevOps engineer must come up with a strategy for updating the application that enables new features to be tested on a small number of users before rolling out the update to all users.Which deployment strategy will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900386, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A development team use a staging deployment of an application to test updates. The application includes an Amazon RDS database instances and Amazon EC2 instances. The resources only need to run when testing deployments are run using AWS CodePipeline. The testing usually runs for just a few hours a couple of times each week. A DevOps engineer wants cost-effective automating the instantiation and shutdown of the resources without changing the architecture of the application</p><p>Which solution best meets the requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can monitor CodePipeline events in EventBridge and then trigger another service to run using the event data. In this case, the DevOps engineer can create an event pattern that triggers the Systems Manager automation document to run when a CodePipeline execution has been initiated.</p><p><strong>CORRECT: </strong>\"Configure CodePipeline to subscribe to an event in Amazon EventBridge that triggers an AWS Systems Manager automation document that starts and stops the EC2 and RDS instances before and after deployment tests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Convert the RDS database to an Amazon Aurora Serverless database and create an Application Load Balancer for EC2. Use an AWS Lambda function to start and stop the EC2 and RDS instances before and after tests\" is incorrect.</p><p>This requires more of an architectural change to the application and there is no automated solution for triggering the execution of the Lambda function.</p><p><strong>INCORRECT:</strong> \"Put the EC2 instances into an Auto Scaling group. Use Application Auto Scaling to configure a scheduled scaling event that runs at the start of the deployment tests\" is incorrect.</p><p>There is no solution here for the RDS database, this answer only provides a partial solution for EC2 instances.</p><p><strong>INCORRECT:</strong> \"Replace the EC2 instances with EC2 Spot Instances and the RDS database with an RDS Reserved Instance. Use AWS CLI commands to start and stop EC2 and RDS instances before and after tests\" is incorrect.</p><p>Changing the pricing structure does not automate starting and stopping the resources. Also, a reserved instance is not a good pricing option for this solution as it means you have paid regardless of whether the resource is running. The AWS CLI commands are not automated in this answer.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/\">https://aws.amazon.com/blogs/mt/systems-manager-automation-documents-manage-instances-cut-costs-off-hours/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Configure CodePipeline to subscribe to an event in Amazon EventBridge that triggers an AWS Systems Manager automation document that starts and stops the EC2 and RDS instances before and after deployment tests.</p>", "<p>Convert the RDS database to an Amazon Aurora Serverless database and create an Application Load Balancer for EC2. Use an AWS Lambda function to start and stop the EC2 and RDS instances before and after tests.</p>", "<p>Put the EC2 instances into an Auto Scaling group. Use Application Auto Scaling to configure a scheduled scaling event that runs at the start of the deployment tests.</p>", "<p>Replace the EC2 instances with EC2 Spot Instances and the RDS database with an RDS Reserved Instance. Use AWS CLI commands to start and stop EC2 and RDS instances before and after tests.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A development team use a staging deployment of an application to test updates. The application includes an Amazon RDS database instances and Amazon EC2 instances. The resources only need to run when testing deployments are run using AWS CodePipeline. The testing usually runs for just a few hours a couple of times each week. A DevOps engineer wants cost-effective automating the instantiation and shutdown of the resources without changing the architecture of the applicationWhich solution best meets the requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900388, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application is being deployed using an AWS CodePipeline pipeline. The pipeline includes an AWS CodeBuild stage which downloads source code from AWS CodeCommit, pulls data from an S3 bucket, and builds and tests the application before deployment.</p><p>A DevOps engineer has discovered that the S3 data is not being successfully downloaded due to a permissions issue.</p><p>How can the permissions be assigned to CodeBuild in the MOST secure manner?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The most likely issue is that the service role used by AWS CodeBuild does not have the correct permissions to download the data securely from the Amazon S3 bucket. CodeBuild uses the service role for all operations that are performed on your behalf. Therefore, the role must have the permissions needed during the build stage.</p><p>In this case, simply adding the correct permissions statements to the policy attached to the service role should resolve the permission issue. The data can then be downloaded from S3 using the AWS CLI by specifying commands in the buildspec document.</p><p><strong>CORRECT: </strong>\"Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data\" is incorrect.</p><p>This is an insecure method of using credentials and should be avoided. It would also not provide the permissions needed by CodeBuild as the service gets those permissions from the service role.</p><p><strong>INCORRECT:</strong> \"Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data\" is incorrect.</p><p>The condition key referenced is used in policies to restrict access to specific HTTP referers. This is not useful here as it does not provide any permissions to CodeBuild.</p><p><strong>INCORRECT:</strong> \"Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data\" is incorrect.</p><p>You cannot configure different authentication options on S3 as it is a managed service. You can only limit who can access the bucket and objects and under what conditions.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role\">https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html#setting-up-service-role</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Use an aws:Referer condition key in the CodeBuild project settings. Update the buildspec to use the AWS CLI to download the data.</p>", "<p>Modify the service role for the CodeBuild project to include permissions for S3. Use the AWS CLI to download the data.</p>", "<p>Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the buildspec to use cURL to pass the token and download the data.</p>", "<p>Configure an IAM access key and a secret access key in the application code and use the AWS CLI to download the data.</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "An application is being deployed using an AWS CodePipeline pipeline. The pipeline includes an AWS CodeBuild stage which downloads source code from AWS CodeCommit, pulls data from an S3 bucket, and builds and tests the application before deployment.A DevOps engineer has discovered that the S3 data is not being successfully downloaded due to a permissions issue.How can the permissions be assigned to CodeBuild in the MOST secure manner?", "related_lectures": []}, {"_class": "assessment", "id": 69900390, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The instances often come online before they are ready which leads to errors being experienced. The health check configuration provides a 60-second grace period and considers instances healthy after two 200 response codes from /index.php. This page can respond intermittently during the deployment process.</p><p>A DevOps engineer has been tasked with troubleshooting the errors. The instances should come online as soon as possible but not before they are ready.</p><p>Which strategy can be used to address this issue?</p><p>Which strategy would address this issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The correct solution includes the creation of a health-check.php file at the end of the deployment when the application should be running consistently. The health check path is reconfigured to point to this file. This means that as soon as the application deployment is complete the health checks should start to return success (200) status codes and the instances will be marked as healthy.</p><p><strong>CORRECT: </strong>\"Modify the deployment script to create a /health-check.php file at the end of the deployment and modify the health check path to point to that file\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Modify the deployment script to create a /health-check.php file at the beginning of the deployment and modify the health check path to point to that file\" is incorrect.</p><p>This solution creates the health-check.php file too early in the process. The application may not be ready at this point so if the file is present the 200 response codes will be returned, and the instance will be marked as healthy despite the application not being ready to process requests. The question indicates that errors have been experienced due to the application being marked as healthy too early.</p><p><strong>INCORRECT:</strong> \"Increase the instance grace period from 60 seconds to 300 seconds, and the consecutive health check requirement from 2 to 3\" is incorrect.</p><p>The grace period helps Amazon EC2 Auto Scaling distinguish unhealthy instances from newly launched instances that are not yet ready to serve traffic. This grace period can prevent Amazon EC2 Auto Scaling from marking InService instances as unhealthy and terminating them before they have time to finish initializing. The issue experienced is not related to instances being terminated too early, it is related to instances coming online (marked as healthy) too early.</p><p>Changing the health check requirement from 2 to 3 may help delay bringing the application online but depends on the interval specified.</p><p><strong>INCORRECT:</strong> \"Increase the instance grace period from 60 seconds to 180 seconds and change the response code requirement from 200 to 202\" is incorrect.</p><p>As above, the changing the health check requirement from 2 to 3 may help delay bringing the application online but depends on the interval specified. Changing the response code requirement will not make any positive difference here.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Increase the instance grace period from 60 seconds to 300 seconds, and the consecutive health check requirement from 2 to 3.</p>", "<p>Modify the deployment script to create a /health-check.php file at the beginning of the deployment and modify the health check path to point to that file.</p>", "<p>Increase the instance grace period from 60 seconds to 180 seconds and change the response code requirement from 200 to 202.</p>", "<p>Modify the deployment script to create a /health-check.php file at the end of the deployment and modify the health check path to point to that file.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The instances often come online before they are ready which leads to errors being experienced. The health check configuration provides a 60-second grace period and considers instances healthy after two 200 response codes from /index.php. This page can respond intermittently during the deployment process.A DevOps engineer has been tasked with troubleshooting the errors. The instances should come online as soon as possible but not before they are ready.Which strategy can be used to address this issue?Which strategy would address this issue?", "related_lectures": []}, {"_class": "assessment", "id": 69900392, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application uses an Elastic Load Balancer (ELB) in front of an Auto Scaling group of Amazon EC2 instances. A recent update to the application has resulted in longer times to run and complete bootstrap scripts. The instances often become healthy before they are ready to accept traffic resulting in errors. A DevOps engineer must prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic.</p><p>Which solution meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling groups. These hooks let you create solutions that are aware of events in the Auto Scaling instance lifecycle, and then perform a custom action on instances when the corresponding lifecycle event occurs.</p><p>A popular use of lifecycle hooks is to control when instances are registered with Elastic Load Balancing. By adding a launch lifecycle hook to your Auto Scaling group, you can ensure that your bootstrap scripts have completed successfully and the applications on the instances are ready to accept traffic before they are registered to the load balancer at the end of the lifecycle hook.</p><p>The following illustration shows the transitions between Auto Scaling instance states:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-33-11-47dc5f5dc473e042857e287968db6fd8.jpg\"><p><strong>CORRECT: </strong>\"Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5\" is incorrect.</p><p>The question specifically states that the solution should prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic. This solution does not meet the requirement.</p><p><strong>INCORRECT:</strong> \"Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic\" is incorrect.</p><p>The HealthCheckGracePeriod parameter for the Auto Scaling group helps Amazon EC2 Auto Scaling distinguish unhealthy instances from newly launched instances that are not yet ready to serve traffic. This grace period can prevent Amazon EC2 Auto Scaling from marking InService instances as unhealthy and terminating them before they have time to finish initializing. This does not affect registration with the load balancer.</p><p><strong>INCORRECT:</strong> \"Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete\" is incorrect.</p><p>A better solution would be to suspend registration with the load balancer but that was not offered. Suspending health check processes does not meet the requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Create an AWS Lambda function that uses the Auto Scaling API to suspend the health check processes until the bootstrap scripts are complete.</p>", "<p>Increase the health check grace period from 300 seconds to 600 seconds to ensure the instances are not marked as healthy before they are ready to accept traffic.</p>", "<p>Use an Auto Scaling lifecycle hook to verify that the bootstrap scripts have completed before registering the instances with the ELB.</p>", "<p>Increase the health check timeout from the default value to 120 seconds and configure the healthy threshold count to 5.</p>"]}, "correct_response": ["c"], "section": "AWS Compute", "question_plain": "An application uses an Elastic Load Balancer (ELB) in front of an Auto Scaling group of Amazon EC2 instances. A recent update to the application has resulted in longer times to run and complete bootstrap scripts. The instances often become healthy before they are ready to accept traffic resulting in errors. A DevOps engineer must prevent the instances from being registered with Elastic Load Balancing until the instances are ready to accept traffic.Which solution meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900394, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps manager has been asked to optimize the costs associated with Amazon EBS volumes. There are many unattached EBS volumes which should be deleted if not used for 14 days. The manager asked a DevOps engineer to create an automation solution that deletes volumes that have been unattached for 14 days or more.</p><p>Which solution will accomplish this?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>CloudWatch Events can be configured with a rule to run an AWS Lambda function on a schedule. The function can be written to find unattached volumes and tag them with the current date. It should avoid tagging volumes that are already tagged and it should delete volumes which have dates older than 14 days. This meets the requirements of the question.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume\" is incorrect.</p><p>Trusted Advisor checks for underutilized volumes but the criteria is that a volume is unattached or had less than 1 IOPS per day for the past 7 days.</p><p><strong>INCORRECT:</strong> \"Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes\" is incorrect.</p><p>The function should run daily to delete those that have reached 14 days since being attached. This answer indicates that a specific rule is created for each instance of a non-compliant volume which is inefficient.</p><p><strong>INCORRECT:</strong> \"Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *\" is incorrect.</p><p>DLM assists with automating the lifecycle of snapshots and AMIs. In this case the DevOps team need to automate deletion of unattached EBS volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Use the AWS Config ec2-volume-inuse-check managed rule to mark unattached volumes are non-compliant. Create a new Amazon CloudWatch Events rule scheduled to invoke an AWS Lambda function in 14 days to delete the non-compliant volumes.</p>", "<p>Use Amazon EC2 and Amazon Data Lifecycle Manager to configure a volume lifecycle policy. Set the interval period for unattached EBS volumes to 14 days and set the retention rule to delete. Set the policy target volumes as *.</p>", "<p>Create an Amazon CloudWatch Events rule to invoke an AWS Lambda function daily. Configure the function to find unattached EBS volumes and tag them with the current date and delete unattached volumes that have tags with dates that are more than 14 days old.</p>", "<p>Use AWS Trusted Advisor to detect EBS volumes that have been detached for more than 14 days. Invoke an AWS Lambda function that creates a snapshot and then deletes the EBS volume.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "A DevOps manager has been asked to optimize the costs associated with Amazon EBS volumes. There are many unattached EBS volumes which should be deleted if not used for 14 days. The manager asked a DevOps engineer to create an automation solution that deletes volumes that have been unattached for 14 days or more.Which solution will accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 69900396, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps Engineer manages a containerized rules engine application running inside an Amazon ECS container which pulls configuration files from AWS Systems Manager Parameter Store every time the container spins up. The configuration files are in a JSON format and are around 3 KB in size.</p><p>As new clients are onboarding, the engineer must load many new configuration files. The configuration files may increase significantly in number with increasing customer load. The engineer must load the configuration files on the fly after changes are done. The engineer must also ensure a history can be maintained for configuration changes.</p><p>What is the best solution for onboarding the new configuration files with the LEAST effort?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>SSM Parameter Store is an ideal choice for externalizing the configuration, but it has a limit of 4kb in standard tier and 8kb in advanced tier.</p><p>In the scenario where size might exceed 8kb, this configuration needs to externalize via an external store for which S3 is an ideal candidate.</p><p><strong>CORRECT: </strong>\u201cStore configuration files in a versioning enabled Amazon S3 bucket and load the configuration settings using an Amazon EventBridge trigger and AWS Lambda function that is triggered by a cron job\u201d is the correct answer (as explained above).</p><p><strong>INCORRECT:</strong> \"Put the configuration files as source code in the form of a properties file and refresh the binding via container restart \" is a possible option but efforts will be increased hence this is incorrect.</p><p><strong>INCORRECT:</strong> \"Use AWS Secret Manager instead of AWS Systems Manager Parameter Store \" is incorrect since secret manager also impose 512-character length restriction on secrets stored.</p><p><strong>INCORRECT:</strong> \"Save hierarchical configuration in a parameter store and load this data on application start up\" is incorrect since due to size restriction this will fail\" is incorrect since due to size restriction this will fail.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-advanced-parameters.html\">Managing parameter tiers - AWS Systems Manager (amazon.com)</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-systems-manager/\">https://digitalcloud.training/aws-systems-manager/</a></p>", "answers": ["<p>Put the configuration files as source code in the form of a properties file and refresh the binding via container restart.</p>", "<p>Save hierarchical configuration settings in AWS Systems Manager Parameter Store and load this data on application start up.</p>", "<p>Store configuration files in a versioning enabled Amazon S3 bucket and load the configuration settings using an Amazon EventBridge trigger and AWS Lambda function that is triggered by a cron job.</p>", "<p>Use AWS Secret Manager instead of AWS Systems Manager Parameter Store.</p>"]}, "correct_response": ["c"], "section": "AWS Management & Governance", "question_plain": "A DevOps Engineer manages a containerized rules engine application running inside an Amazon ECS container which pulls configuration files from AWS Systems Manager Parameter Store every time the container spins up. The configuration files are in a JSON format and are around 3 KB in size.As new clients are onboarding, the engineer must load many new configuration files. The configuration files may increase significantly in number with increasing customer load. The engineer must load the configuration files on the fly after changes are done. The engineer must also ensure a history can be maintained for configuration changes.What is the best solution for onboarding the new configuration files with the LEAST effort?", "related_lectures": []}, {"_class": "assessment", "id": 69900398, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses a tagging strategy to allocate usage costs for AWS resources. An application runs on Amazon EC2 instances in an Auto scaling group. The Amazon EBS volumes that are attached to instances are being created without the correct cost center tags. A DevOps engineer must correct the configuration to ensure the EBS volumes are tagged appropriately.</p><p>What is the MOST efficient solution that meets this requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can tag new or existing Auto Scaling groups. You can also propagate tags from an Auto Scaling group to the EC2 instances that it launches.</p><p>Tags are not propagated to Amazon EBS volumes. To add tags to Amazon EBS volumes, specify the tags in a launch template.</p><p><strong>CORRECT: </strong>\"Update the Auto Scaling group launch template to include the cost center tags for EBS volumes\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true\" is incorrect.</p><p>As noted above, you cannot propagate tags from an Auto Scaling group to EBS volumes.</p><p><strong>INCORRECT:</strong> \"Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags\" is incorrect.</p><p>AWS Config can be used to report on compliance but cannot stop volume creation.</p><p><strong>INCORRECT:</strong> \"Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes\" is incorrect.</p><p>Tag Editor is not an efficient solution as it would involve manual work. The correct solution is fully automated.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-tagging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Update the Auto Scaling group launch template to include the cost center tags for EBS volumes.</p>", "<p>Update the Auto Scaling group to include the cost center tags. Set the PropagateAtLaunch property to true.</p>", "<p>Use AWS Config to enforce tagging at EBS volume creation time and deny creation of any volumes that do not have the appropriate cost center tags.</p>", "<p>Use Tag Editor to scan the account for EBS volumes that are missing the tags and then add the cost center tags to the volumes.</p>"]}, "correct_response": ["a"], "section": "AWS Management & Governance", "question_plain": "A company uses a tagging strategy to allocate usage costs for AWS resources. An application runs on Amazon EC2 instances in an Auto scaling group. The Amazon EBS volumes that are attached to instances are being created without the correct cost center tags. A DevOps engineer must correct the configuration to ensure the EBS volumes are tagged appropriately.What is the MOST efficient solution that meets this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 69900400, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.</p><p>The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.</p><p>What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The DevOps engineer can add a lifecycle hook to the AWS Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state. In this state, the engineer can access instances before they're terminated, and then troubleshoot why they were marked as unhealthy.</p><p>By default, an instance remains in the Terminating:Wait state for 3600 seconds (1 hour). To increase this time, you can use the heartbeat-timeout parameter in the put-lifecycle-hook API call. The maximum time that you can keep an instance in the Terminating:Wait state is 48 hours or 100 times the heartbeat timeout, whichever is smaller.</p><p><strong>CORRECT: </strong>\"Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Suspend the AZRebalance auto scaling process to prevent instances from being terminated\" is incorrect.</p><p>This would not stop instances that are unhealthy from being terminated. This simply prevents Auto Scaling rebalancing the instances across AZs.</p><p><strong>INCORRECT:</strong> \"Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image\" is incorrect.</p><p>It may not be possible to take a snapshot from an instance that is being terminated. Also, the state of the instance would not be captured, only the contents of the EBS volume.</p><p><strong>INCORRECT:</strong> \"Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated\" is incorrect.</p><p>This will only prevent the Auto Scaling group from being deleted, it does not have any effect on how instances are terminated.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-delay-termination/</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ec2-auto-scaling/\">https://digitalcloud.training/amazon-ec2-auto-scaling/</a></p>", "answers": ["<p>Take a snapshot of the attached EBS volumes. Create an image from the snapshot and launch an instance from the image.</p>", "<p>Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state.</p>", "<p>Suspend the AZRebalance auto scaling process to prevent instances from being terminated.</p>", "<p>Edit the Auto Scaling group to enable termination protection as this will protect unhealthy instances from being terminated.</p>"]}, "correct_response": ["b"], "section": "AWS Compute", "question_plain": "An application that was updated is returning HTTP 502 Bad Gateway errors to users. The application runs on Amazon EC2 instances in an Auto Scaling group that spans multiple Availability Zones.The DevOps engineer wants to analyze the issue, but Auto Scaling is terminating the instances shortly after launch as the health check status is changing to unhealthy.What steps can the DevOps engineer take to gain access to one of the instances for troubleshooting?", "related_lectures": []}, {"_class": "assessment", "id": 69900402, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The security team at a company requires a solution to identify activities that indicate that Amazon EC2 instances have been compromised. The solution should notify them by email if issues are discovered.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, EC2 workloads, container applications, and data stored in Amazon Simple Storage Service (S3)</p><p>GuardDuty provides threat detection services that cover several types of activity. One of these is instance compromise as per the following description of the scope of this service:</p><p><em>Activity indicating an instance compromise, such as cryptocurrency mining, backdoor command, and control (C&amp;C) activity, malware using domain generation algorithms (DGA), outbound denial of service activity, unusually high network traffic volume, unusual network protocols, outbound instance communication with a known malicious IP, temporary Amazon EC2 credentials used by an external IP address, and data exfiltration using DNS.</em></p><p>The GuardDuty findings are automatically logged to Amazon CloudWatch Events. You can then use EventBridge to create a rule that triggers an action when certain events that match the filter pattern are logged. In this case the trigger will be Amazon SNS as the security team require an email notification.</p><p><strong>CORRECT: </strong>\"Configure AWS GuardDuty to identify activities that indicate the EC2 instances have been compromised. Create an Amazon EventBridge rule with an event source set to \u2018aws.guardduty\u2019. Send a notification using an Amazon SNS topic when the specified events are logged\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure Amazon Inspector to identify activities that indicate the EC2 instances have been compromised. Configure Inspector to send notifications directly via an Amazon SNS topic when there are changes in the state of findings\" is incorrect.</p><p>Inspector does not identify instance compromise activities, use GuardDuty for this use case.</p><p><strong>INCORRECT:</strong> \"Install the AWS Systems Manager agent on the EC2 instances and attach an instance profile with the necessary permissions. Configure Systems Manager to alert via an Amazon SNS topic if malicious activities are detected on the EC2 instances\" is incorrect.</p><p>Systems Manager does not identify instance compromise activities, use GuardDuty for this use case.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail trail and log API events that indicate account compromise. Create an alarm in Amazon CloudWatch based on a custom metric filter for the API events. Send a notification via an Amazon SNS topic\" is incorrect.</p><p>The question asks for identifying activities relating to instance compromise rather than account compromise. Either way, GuardDuty is the best tool for this job as it will discover possible compromise by looking at CloudTrail API events as well as other data sources.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/features/\">https://aws.amazon.com/guardduty/features/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-guardduty/\">https://digitalcloud.training/aws-guardduty/</a></p>", "answers": ["<p>Configure AWS GuardDuty to identify activities that indicate the EC2 instances have been compromised. Create an Amazon EventBridge rule with an event source set to \u2018aws.guardduty\u2019. Send a notification using an Amazon SNS topic when the specified events are logged.</p>", "<p>Configure Amazon Inspector to identify activities that indicate the EC2 instances have been compromised. Configure Inspector to send notifications directly via an Amazon SNS topic when there are changes in the state of findings.</p>", "<p>Install the AWS Systems Manager agent on the EC2 instances and attach an instance profile with the necessary permissions. Configure Systems Manager to alert via an Amazon SNS topic if malicious activities are detected on the EC2 instances.</p>", "<p>Create an AWS CloudTrail trail and log API events that indicate account compromise. Create an alarm in Amazon CloudWatch based on a custom metric filter for the API events. Send a notification via an Amazon SNS topic.</p>"]}, "correct_response": ["a"], "section": "AWS Security, Identity, & Compliance", "question_plain": "The security team at a company requires a solution to identify activities that indicate that Amazon EC2 instances have been compromised. The solution should notify them by email if issues are discovered.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900404, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer must implement a serverless service that runs on multiple AWS Lambda functions and uses Amazon DynamoDB as the data store. The functions require a front end that supports unencrypted HTTP and allows routing to the functions based on the path in the URL.</p><p>Which solution will meet the requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>It is not possible to use Amazon API Gateway for this solution as it only supports encrypted endpoints. This solution requires unencrypted HTTP. The best solution is to use an ALB as the front end and configure the Lambda functions as targets. An HTTP listener can be configured, and rules can be created to map requests to targets based on the path values in the request.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-39-10-dd12ec7535c85f563ca42e623666299c.jpg\"><p><strong>CORRECT: </strong>\"Deploy an Application Load Balancer (ALB) as the front end. Create an HTTP listener and configure the Lambda functions as targets in separate target groups. Create path-based routing rules that forward requests to targets based on path values in the request\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Deploy a Network Load Balancer (NLB) as the front end. Create an HTTP listener and configure the Lambda functions as targets in separate target groups. Create IP-based routing rules that forward requests to targets based on path values in the request\" is incorrect.</p><p>You cannot use an NLB for routing to targets based on path values in the requests. IP-based routing does not assist here and is also supported only on the ALB.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon API Gateway REST API as the front end with an HTTP endpoint. Create resources to represent each URL path and use the ANY method. Use Lambda proxy integrations for each resource\" is incorrect.</p><p>As explained above, you cannot deploy unencrypted HTTP endpoints with API Gateway, so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Deploy an Amazon API Gateway HTTP API as the front end with an HTTP endpoint. Create resources to represent each URL path and use the ANY method. Use Lambda non-proxy integrations for each resource\" is incorrect.</p><p>As explained above, you cannot deploy unencrypted HTTP endpoints with API Gateway, so this solution does not work.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Deploy an Application Load Balancer (ALB) as the front end. Create an HTTP listener and configure the Lambda functions as targets in separate target groups. Create path-based routing rules that forward requests to targets based on path values in the request.</p>", "<p>Deploy an Amazon API Gateway REST API as the front end with an HTTP endpoint. Create resources to represent each URL path and use the ANY method. Use Lambda proxy integrations for each resource.</p>", "<p>Deploy a Network Load Balancer (NLB) as the front end. Create an HTTP listener and configure the Lambda functions as targets in separate target groups. Create IP-based routing rules that forward requests to targets based on path values in the request.</p>", "<p>Deploy an Amazon API Gateway HTTP API as the front end with an HTTP endpoint. Create resources to represent each URL path and use the ANY method. Use Lambda non-proxy integrations for each resource.</p>"]}, "correct_response": ["a"], "section": "AWS Compute", "question_plain": "A DevOps engineer must implement a serverless service that runs on multiple AWS Lambda functions and uses Amazon DynamoDB as the data store. The functions require a front end that supports unencrypted HTTP and allows routing to the functions based on the path in the URL.Which solution will meet the requirements?", "related_lectures": []}, {"_class": "assessment", "id": 69900406, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer updated the AWS CloudFormation template for an application. The stack update failed and CloudFormation attempted to roll back the stack to its previous state. The roll back process failed and generated a UPDATE_ROLLBACK_FAILED error message.</p><p>What are the most likely causes for this issue? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>This error indicates that a dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). A dependent resource has likely been changed or cannot be modified. There are several possible causes of this issue which include that a dependent resource has been changed outside of the CloudFormation stack or the resource cannot be modified as the user or role that performed the update has insufficient permissions.</p><p><strong>CORRECT: </strong>\"The user or role that was used to perform the stack update had insufficient permissions\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Changes to resources were made outside of CloudFormation and the template was not updated\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"A change set was not created and executed prior to deploying the updated template to the stack\" is incorrect.</p><p>Change sets are useful but not mandatory. There is no reason that the stack update would fail to roll back because a change set had not been used.</p><p><strong>INCORRECT:</strong> \"An interface VPC endpoint was not operational and CloudFormation could not update resources in the VPC\" is incorrect.</p><p>CloudFormation uses APIs rather than the network, so it is not reliant on interface endpoints to be able to modify resources in a VPC.</p><p><strong>INCORRECT:</strong> \"Amazon EC2 instances included in the stack were updated recently using the \u2018yum update\u2019 command\" is incorrect.</p><p>This command updates the operating system with the latest patches. There is no reason that running this update would have any bearing on CloudFormation\u2019s ability to roll back a failed update.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>The user or role that was used to perform the stack update had insufficient permissions.</p>", "<p>A change set was not created and executed prior to deploying the updated template to the stack.</p>", "<p>An interface VPC endpoint was not operational and CloudFormation could not update resources in the VPC.</p>", "<p>Amazon EC2 instances included in the stack were updated recently using the \u2018yum update\u2019 command.</p>", "<p>Changes to resources were made outside of CloudFormation and the template was not updated.</p>"]}, "correct_response": ["a", "e"], "section": "AWS Management & Governance", "question_plain": "A DevOps engineer updated the AWS CloudFormation template for an application. The stack update failed and CloudFormation attempted to roll back the stack to its previous state. The roll back process failed and generated a UPDATE_ROLLBACK_FAILED error message.What are the most likely causes for this issue? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900408, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A gaming startup company is finishing its migration to AWS and realizes that many DevOps engineers have permissions to delete Amazon DynamoDB tables.</p><p>A solution is required to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.</p><p>Which actions should be taken to achieve this requirement most cost-effectively?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>To create a rule that triggers on an action by an AWS service that does not emit events, you can base the rule on API calls made by that service. The API calls are recorded by AWS CloudTrail. You must create an AWS CloudTrail trail. For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.</p><p><strong>CORRECT: </strong>\"Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS\" is incorrect.</p><p>This would be less cost-effective compared to using AWS CloudTrail and Amazon EventBridge.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification\" is incorrect.</p><p>Event filters are used with CloudWatch, not with CloudTrail.</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS\" is incorrect.</p><p>API actions are tracked by AWS CloudTrail but not by Amazon CloudWatch.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-CloudTrail-Rule.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Enable DynamoDB Streams and configure an AWS Lambda function to process events from the stream. Send alerts using Amazon SNS.</p>", "<p>Create an AWS CloudTrail trail. Create an Amazon EventBridge rule to track an AWS API call via CloudTrail and use Amazon SNS as a target.</p>", "<p>Create an AWS CloudTrail event filter and use an AWS Lambda function to send an Amazon SNS notification.</p>", "<p>Create an Amazon CloudWatch event filter that monitors for DeleteTable API actions and sends an alert via Amazon SNS.</p>"]}, "correct_response": ["b"], "section": "AWS Security, Identity, & Compliance", "question_plain": "A gaming startup company is finishing its migration to AWS and realizes that many DevOps engineers have permissions to delete Amazon DynamoDB tables.A solution is required to receive near real-time notifications when the API call DeleteTable is invoked in DynamoDB.Which actions should be taken to achieve this requirement most cost-effectively?", "related_lectures": []}, {"_class": "assessment", "id": 69900410, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A critical application runs on Amazon EC2 instances in an Auto Scaling group. A script runs on the instances every 10 seconds to check application availability. A DevOps engineer must use this information returned by the script to monitor the application and trigger an alarm if there is an issue. The data should be collected every 1-minute and the solution must be cost-effective.</p><p>Which action should the engineer take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can create custom metrics to send to Amazon CloudWatch. With custom metrics you can choose standard or high resolution and you can aggregate multiple data points and publish data as a statistic set to reduce cost and increase efficiency.</p><p>Each metric is one of the following:</p><ul><li><p>Standard resolution, with data having a one-minute granularity.</p></li><li><p>High resolution, with data at a granularity of one second.</p></li></ul><p>Metrics produced by AWS services are standard resolution by default. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p><p>You can aggregate your data before you publish to CloudWatch. When you have multiple data points per minute, aggregating data minimizes the number of calls to <strong>put-metric-data</strong>.</p><p>Therefore, the engineer can use statistic sets to aggregate and publish the data every one minute. This is the most cost-effective solution that meets the requirements.</p><p><strong>CORRECT: </strong>\"Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute\" is incorrect.</p><p>You cannot use high resolution with a default CloudWatch metric and a default CloudWatch metric would not be available for the application availability data.</p><p><strong>INCORRECT:</strong> \"Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds\" is incorrect.</p><p>This would be less efficient and more costly as the put-metric-data API action would be run every 10 seconds. Fewer API calls means lower cost so aggregating into a statistic set is better and publishing every 1-minute.</p><p><strong>INCORRECT:</strong> \"Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute\" is incorrect.</p><p>There would not be a default metric available that uses the data returned from the application availability script. A dimension is used for organizing and clarifying what the metric data is and what it stores.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Use a custom Amazon CloudWatch metric and configure a statistic set that aggregates data points and publishes the data every 1-minute.</p>", "<p>Use a default CloudWatch metric with a high resolution, aggregate multiple data points, and publish the data every 1-minute.</p>", "<p>Use a custom Amazon CloudWatch metric with a high resolution and publish the data every 10 seconds.</p>", "<p>Use a default CloudWatch metric with a standard resolution, use a dimension to publish data sets every 1-minute.</p>"]}, "correct_response": ["a"], "section": "AWS Management & Governance", "question_plain": "A critical application runs on Amazon EC2 instances in an Auto Scaling group. A script runs on the instances every 10 seconds to check application availability. A DevOps engineer must use this information returned by the script to monitor the application and trigger an alarm if there is an issue. The data should be collected every 1-minute and the solution must be cost-effective.Which action should the engineer take?", "related_lectures": []}, {"_class": "assessment", "id": 69900412, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer needs a managed environment for running a Node.js application. The infrastructure should support load balancing and auto scaling. The application will require a managed relational database, and data should be stored persistently and protected from accidental deletion. The solution should minimize ongoing operational effort.</p><p>Which actions should the engineer take to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS Elastic Beanstalk is a service that provides managed infrastructure onto which developers and DevOps engineers can simply add their code. Node.js is supported along with many other popular programming languages. Elastic Beanstalk supports load balancing and auto scaling for the underlying infrastructure.</p><p>The question calls for a relational database that is managed. This requirement can be satisfied by deploying an Amazon RDS database. To ensure the database is protected from accidental deletion it should be created independently of the Elastic Beanstalk environment. The engineer may also want to enable deletion protection and automatic backups.</p><p><strong>CORRECT: </strong>\"Create an AWS Elastic Beanstalk environment with load balancing and auto scaling enabled\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an independent Amazon RDS database in an Amazon VPC with automatic backups and deletion protection enabled\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an auto scaling group of Amazon EC2 instances managed by AWS Systems Manager\" is incorrect.</p><p>This does not provide the managed infrastructure platform the question requires. Elastic Beanstalk is a better solution as it takes care of the management of the underlying infrastructure. Systems Manager can help with management of EC2, but you are still responsible.</p><p><strong>INCORRECT:</strong> \"Create multiple AWS Lambda functions and associated Amazon Route 53 multivalue records\" is incorrect.</p><p>This is not a good solution for running highly available and managed code. Lambda scales concurrently and therefore using Route 53 to load balance via DNS resolution is not necessary.</p><p><strong>INCORRECT:</strong> \"Create an Amazon DynamoDB table in an Amazon VPC with automatic backups and deletion protection enabled\" is incorrect.</p><p>DynamoDB is a NoSQL database, and the question requires that a relational database is deployed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/security.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/security.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>", "answers": ["<p>Create an auto scaling group of Amazon EC2 instances managed by AWS Systems Manager.</p>", "<p>Create multiple AWS Lambda functions and associated Amazon Route 53 multivalue records.</p>", "<p>Create an AWS Elastic Beanstalk environment with load balancing and auto scaling enabled.</p>", "<p>Create an independent Amazon RDS database in an Amazon VPC with automatic backups and deletion protection enabled.</p>", "<p>Create an Amazon DynamoDB table in an Amazon VPC with automatic backups and deletion protection enabled.</p>"]}, "correct_response": ["c", "d"], "section": "AWS Compute", "question_plain": "A DevOps engineer needs a managed environment for running a Node.js application. The infrastructure should support load balancing and auto scaling. The application will require a managed relational database, and data should be stored persistently and protected from accidental deletion. The solution should minimize ongoing operational effort.Which actions should the engineer take to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 69900414, "assessment_type": "multiple-choice", "prompt": {"question": "<p>When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.</p><p>The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.</p><p>The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.</p><p>What is the most efficient way for a DevOps engineer to resolve the issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><p>\u00b7 <strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p><p>\u00b7 <strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-44-54-3b27da721fdb31cd0114ba6bbff8f1d5.jpg\"><p><strong>CORRECT: </strong>\"Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond\" is incorrect.</p><p>You can use this deployment lifecycle event to run tasks on instances after they are deregistered from a load balancer.</p><p><strong>INCORRECT:</strong> \"Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed\" is incorrect.</p><p>Since the error resolves after some time, the issue will most likely be resolved by ensuring the application is not brought online until it is ready.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook to validate that the deployment was completed successfully\" is incorrect.</p><p>This is used to verify the deployment was completed successfully, this will only detect deployment status and will not help in this scenario.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>", "answers": ["<p>Add a BeforeAllowTraffic hook to the AppSpec file which tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.</p>", "<p>Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.</p>", "<p>Use DownloadBundle event hook in which CodeDeploy agent copies the application revision files to a temporary location which can be analyzed.</p>", "<p>Use the ValidateService hook to validate that the deployment was completed successfully.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "When deploying a newly developed application on AWS, a DevOps team notices an intermittent error when attempting to make a connection to the application.The application has a two-tier architecture with an AWS Lambda function backed by an Amazon API gateway and a NoSQL database as the data store.The DevOps team noticed that sometime after deployment the error stops occurring. This application is deployed by AWS CodeDeploy and the Lambda function is deployed as the last step of pipeline.What is the most efficient way for a DevOps engineer to resolve the issue?", "related_lectures": []}, {"_class": "assessment", "id": 69900416, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An online sales application is being migrated to AWS with the application layer hosted on Amazon EC2 instances and the database layer on a PostgreSQL database. It is mandated that the application must have minimal downtime as it receives traffic 24/7 and any downtime may reduce business revenue. The application must also be fault tolerant including the data layer.</p><p>Concerns have been raised around performance of the database layer during sales events and other peak periods. The application must also be continually scanned for vulnerabilities.</p><p>Which option will meet the above requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The above question clearly mandates three requirements:</p><p>1. Performance- Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group</p><p>2. Database performance- Amazon Aurora will perform better than PostgreSQL since it provides multi-master configuration and can be scaled better than RDS on a global scale.</p><p>3. Vulnerability assessment- Amazon Inspector is the right fit for the scanning. The difference between Amazon Inspector and Amazon GuardDuty is that the former \"checks what happens when you actually get an attack\" and the latter \"analyzes the actual logs to check if a threat exists\". The purpose of Amazon Inspector is to test whether you are addressing common security risks in the target AWS.</p><p>Database categorization and selection parameters:</p><p>\u00b7 If your scaling needs are for standard/ general purpose applications, RDS is the better option. You can auto-scale the database to max capacity with just a few clicks on the AWS console.</p><p>\u00b7 You also have the option of Aurora Serverless that can scale up or scale down well, you have to be aware of several <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations\">restrictions that apply in the Serverless mode</a>.</p><p>\u00b7 If you must handle a very high volume of read/write requests, DynamoDB is a better choice. It scales seamlessly with no impact on performance. You can run these database servers in on-demand or provisioned capacity mode.</p><p>If you have heavy write workloads and require more than five read replicas, Aurora is a better choice. Since Aurora uses shared storage for writer and readers, there is minimal replica lag. RDS allows only up to five replicas and the replication process is slower than Aurora.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_08-46-18-8cd4e56a381e9f1f934f96761a29cae7.jpg\"><p><strong>CORRECT: </strong>\"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments\" is incorrect.</p><p>Amazon Aurora is a better fit for this use case as described above.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>INCORRECT:</strong> \"Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments\" is incorrect.</p><p>Amazon Inspector should be used for performing the security assessments.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p><p>https://aws.amazon.com/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/</p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-aurora/\">https://digitalcloud.training/amazon-aurora/</a></p>", "answers": ["<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use RDS PostgreSQL for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>", "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon Macie to perform automatic security assessments.</p>", "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration. Use Amazon GuardDuty to perform automatic security assessments.</p>", "<p>Create an Auto Scaling group of EC2 instances in a multi-AZ configuration. Deploy an Application Load Balancer to serve traffic to the Auto Scaling group. For the database, use Amazon Aurora for improved throughput in a multi-master configuration for high availability. Use Amazon Inspector to perform automatic security assessments.</p>"]}, "correct_response": ["d"], "section": "AWS Database", "question_plain": "An online sales application is being migrated to AWS with the application layer hosted on Amazon EC2 instances and the database layer on a PostgreSQL database. It is mandated that the application must have minimal downtime as it receives traffic 24/7 and any downtime may reduce business revenue. The application must also be fault tolerant including the data layer.Concerns have been raised around performance of the database layer during sales events and other peak periods. The application must also be continually scanned for vulnerabilities.Which option will meet the above requirements?", "related_lectures": []}]}
5794178
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70265808, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is developing a continuous delivery pipeline using AWS CodePipeline. The application is deployed across multiple ALBs each of which has a dedicated Auto Scaling group. A DevOps engineer must configure the pipeline so that a simultaneous deployment is triggered to all ALBs and Auto Scaling groups when code is committed to an AWS CodeCommit repository.</p><p>Which deployment option meets these requirements with the LEAST amount of configuration?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times or in parallel.</p><p>In this case the configuration can deploy to each ALB and Auto Scaling group pair in parallel within a single AWS CodePipeline pipeline. This is the most efficient solution and requires the least configuration.</p><p>Deployment options include using a blue/green deployment strategy or an in-place deployment.</p><p><strong>CORRECT: </strong>\"Use a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and separate deployment groups for each pair of ALB and Auto Scaling group\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use a single AWS CodePipeline pipeline that deploys the application in parallel using separate AWS CodeDeploy applications and deployment groups for each pair of ALB and Auto Scaling group\" is incorrect.</p><p>A separate deployment group can be specified to deploy the updates in parallel. These deployment groups can be created within a single CodeDeploy application. You cannot select multiple CodeDeploy applications within the pipeline.</p><p><strong>INCORRECT:</strong> \"Use a separate AWS CodePipeline pipeline for each pair of ALB and Auto Scaling group that deploys the application using a single AWS CodeDeploy application and deployment group for those resources\" is incorrect.</p><p>This is a more complex configuration that requires more configuration and management so is not the preferred option.</p><p><strong>INCORRECT:</strong> \"Use a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group that includes all ALBs and Auto Scaling groups\" is incorrect.</p><p>You cannot include all ALBs and Auto Scaling groups in a single deployment group within CodeDeploy. Each ALB and Auto Scaling group pair should be added to a separate deployment group within the single CodeDeploy application. The pipeline can then be configured to run the deployments in parallel.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Use a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and separate deployment groups for each pair of ALB and Auto Scaling group.</p>", "<p>Use a single AWS CodePipeline pipeline that deploys the application in parallel using separate AWS CodeDeploy applications and deployment groups for each pair of ALB and Auto Scaling group.</p>", "<p>Use a separate AWS CodePipeline pipeline for each pair of ALB and Auto Scaling group that deploys the application using a single AWS CodeDeploy application and deployment group for those resources.</p>", "<p>Use a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group that includes all ALBs and Auto Scaling groups.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A company is developing a continuous delivery pipeline using AWS CodePipeline. The application is deployed across multiple ALBs each of which has a dedicated Auto Scaling group. A DevOps engineer must configure the pipeline so that a simultaneous deployment is triggered to all ALBs and Auto Scaling groups when code is committed to an AWS CodeCommit repository.Which deployment option meets these requirements with the LEAST amount of configuration?", "related_lectures": []}, {"_class": "assessment", "id": 70265810, "assessment_type": "multi-select", "prompt": {"question": "<p>A company manages several AWS accounts in an AWS Organizations organization. There are several applications running in each account. Many resources have not been tagged properly and the finance team cannot fully determine the costs that should be attributed to each application.</p><p>A DevOps engineer has been asked to remediate this issue and ensure it does not happen again.</p><p>Which combination of actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>You can use service control policies (SCPs) to require that tags are specified when creating resources. If the appropriate tags are not specified the resource creation fails. This will prevent the issue of resources not being properly tagged from occurring again in the future.</p><p>To resolve the issue now, the DevOps engineer can use Tag Editor to scan resources looking for those that are missing the appropriate tags. The resources can then be tagged.</p><p><strong>CORRECT: </strong>\"Create and attach an SCP that requires tags to be specified when creating resources\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Scan all accounts with Tag Editor. Assign the required tag to each resource\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Activate the user-defined cost allocation tags in each AWS account\" is incorrect.</p><p>We can assume that this has already been done. The question does not state that we need to enable cost allocation tagging. The context infers that this has already been done and the issue is simply that tagging has not been performed properly on some resources.</p><p><strong>INCORRECT:</strong> \"Define each application in AWS Budgets. Assign the required tag to each resource\" is incorrect.</p><p>You cannot define application in AWS Budgets or add tags to resources.</p><p><strong>INCORRECT:</strong> \"Use the budget report to find untagged resources. Assign tags to the resources\" is incorrect.</p><p>The Tag Editor should be used to find untagged resources and then add tags to them.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html\">https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-organizations/\">https://digitalcloud.training/aws-organizations/</a></p>", "answers": ["<p>Activate the user-defined cost allocation tags in each AWS account.</p>", "<p>Create and attach an SCP that requires tags to be specified when creating resources.</p>", "<p>Define each application in AWS Budgets. Assign the required tag to each resource.</p>", "<p>Scan all accounts with Tag Editor. Assign the required tag to each resource.</p>", "<p>Use the budget report to find untagged resources. Assign tags to the resources.</p>"]}, "correct_response": ["b", "d"], "section": "AWS Management & Governance", "question_plain": "A company manages several AWS accounts in an AWS Organizations organization. There are several applications running in each account. Many resources have not been tagged properly and the finance team cannot fully determine the costs that should be attributed to each application.A DevOps engineer has been asked to remediate this issue and ensure it does not happen again.Which combination of actions should the DevOps engineer take to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265812, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer needs to implement an automated deployment process for an application running on AWS. The implementation should minimize deployment costs whilst ensuring that at least half of the instances are available at any time to service user requests. If the application fails on some instances they should be automatically replaced.</p><p>Which deployment strategy will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>With a CodeDeploy \u2018CodeDeployDefault.HalfAtAtime\u2019 deployment strategy, half of the instances will be updated at a time. The overall deployment succeeds if the application revision is deployed to at least half of the instances. Otherwise, the deployment fails.</p><p>Configuring an ELB health check and updating the ASG to use this health check ensures that the ALB can take the instances that fail out of service. If you do not enable ELB health checks on the ASG it will not know when the target group health checks have failed.</p><p><strong>CORRECT: </strong>\"Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with the CodeDeployDefault.HalfAtAtime deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use a rolling deployment strategy and configure a batch size of 50%\" is incorrect.</p><p>There are no health checks configured for the target group with this answer so if the application fails the ASG may not replace the instances. This is a key requirement that has been missed with this configuration.</p><p><strong>INCORRECT:</strong> \"Create an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer\" is incorrect.</p><p>Auto healing with OpsWorks only checks the instance health, it does not check the application health so this will not detect if the application has failed.</p><p><strong>INCORRECT:</strong> \"Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with a blue/green deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB\" is incorrect.</p><p>This is a more expensive option as a full additional deployment will be running alongside the existing deployment (doubling the number of instances) during the rollout.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Create an AWS Elastic Beanstalk environment and configure it to use Auto Scaling and an Elastic Load Balancer. Use a rolling deployment strategy and configure a batch size of 50%.</p>", "<p>Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with the CodeDeployDefault.HalfAtAtime deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB.</p>", "<p>Create an AWS OpsWorks stack. Configure the application layer to use rolling deployments as a deployment strategy. Add an Elastic Load Balancing layer. Enable auto healing on the application layer.</p>", "<p>Implement Auto Scaling with an Elastic Load Balancer. Use AWS CodeDeploy with a blue/green deployment strategy. Enable an ELB health check and set the Auto Scaling health check to ELB.</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer needs to implement an automated deployment process for an application running on AWS. The implementation should minimize deployment costs whilst ensuring that at least half of the instances are available at any time to service user requests. If the application fails on some instances they should be automatically replaced.Which deployment strategy will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265814, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.</p><p>How can a DevOps engineer ensure that the files become visible?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>In file gateway mode the users and applications in the branch office can access the storage using either NFS or SMB protocols. The new objects in the bucket may sometimes not be visible. The RefreshCache operation refreshes the cached inventory of objects for the specified file share. This operation finds objects in the Amazon S3 bucket that were added, removed, or replaced since the gateway last listed the bucket's contents and cached the results.</p><p><strong>CORRECT: </strong>\"Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded\" is incorrect.</p><p>Volume Gateway mode uses block storage targets (iSCSI) rather than file storage targets (NFS/SMB) and would therefore be a major change to the architecture. Event notifications cannot be used to refresh the cache in a storage gateway.</p><p><strong>INCORRECT:</strong> \"Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway\" is incorrect. S3 replication cannot be used to replicate to storage gateway.</p><p><strong>INCORRECT:</strong> \"Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration\" is incorrect. Transfer acceleration is used to improve upload speeds and does not assist at all in this situation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html\">https://docs.aws.amazon.com/storagegateway/latest/APIReference/API_RefreshCache.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-storage-gateway/\">https://digitalcloud.training/aws-storage-gateway/</a></p>", "answers": ["<p>Configure the batch process to upload files to the S3 bucket using Amazon S3 transfer acceleration.</p>", "<p>Configure the Storage Gateway to run in cached Volume Gateway mode and use S3 event notifications to update the storage gateway when objects are uploaded.</p>", "<p>Use S3 same-Region replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</p>", "<p>Configure an Amazon EventBridge event to run on a schedule and trigger an AWS Lambda functions that executes the RefreshCache command.</p>"]}, "correct_response": ["d"], "section": "AWS Storage", "question_plain": "A company is using AWS Storage Gateway for a branch office location. The gateway is configured in file gateway mode in front of an Amazon S3 bucket that contains files that must be processed by workers in the branch office. Each night a batch process uploads many files to the S3 bucket. Users have reported that the new files are not visible in the morning though they do exist in the S3 bucket.How can a DevOps engineer ensure that the files become visible?", "related_lectures": []}, {"_class": "assessment", "id": 70265816, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer has created a CI/CD pipeline in which AWS CodeDeploy is used to deploy an AWS Lambda function as the last stage of the pipeline. For proper execution, the Lambda function relies on an Amazon API Gateway API to have been fully deployed and ready to accept requests.</p><p>The DevOps engineer needs to ensure that the API is ready to accept requests before traffic is shifted to the deployed Lambda function version.</p><p>How can this requirement be met?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Here are descriptions of the hooks available for use in your AppSpec file.</p><ul><li><p><strong>BeforeAllowTraffic</strong> \u2013 Use to run tasks before traffic is shifted to the deployed Lambda function version.</p></li><li><p><strong>AfterAllowTraffic</strong> \u2013 Use to run tasks after all traffic is shifted to the deployed Lambda function version.</p></li></ul><p>In a serverless Lambda function version deployment, event hooks run in the following order: BeforeAllowTraffic &gt; AllowTraffic &gt; AfterAllowTraffic.</p><p>Use the 'hooks' section to specify a Lambda function that CodeDeploy can call to validate a Lambda deployment. You can use the same function or a different one for the BeforeAllowTraffic and AfterAllowTraffic deployment lifecyle events. Following completion of the validation tests, the Lambda validation function calls back CodeDeploy and delivers a result of Succeeded or Failed.</p><p><strong>CORRECT: </strong>\"Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests\" is incorrect.</p><p>This hook is run after traffic is shifted to the Lambda function, so it doesn\u2019t offer any value here.</p><p><strong>INCORRECT:</strong> \"Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function\" is incorrect.</p><p>The ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully. At that point traffic has already been shifted.</p><p><strong>INCORRECT:</strong> \"Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API\u2019s production stage endpoint is reachable\" is incorrect.</p><p>CodeDeploy is being used as the last stage of the pipeline so any build commands would have already been completed at this point.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda\">https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Use the BeforeAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>", "<p>Use the AfterInstall hook in the buildspec file to call AWS CodeBuild and run test commands to check that the API\u2019s production stage endpoint is reachable.</p>", "<p>Use the AfterAllowTraffic hook in the AppSpec file to call a validation function that checks that the API is ready to accept requests.</p>", "<p>Use the ValidateService hook in the AppSpec file to check that the deployment was completed successfully before shifting traffic to the deployed Lambda function.</p>"]}, "correct_response": ["a"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer has created a CI/CD pipeline in which AWS CodeDeploy is used to deploy an AWS Lambda function as the last stage of the pipeline. For proper execution, the Lambda function relies on an Amazon API Gateway API to have been fully deployed and ready to accept requests.The DevOps engineer needs to ensure that the API is ready to accept requests before traffic is shifted to the deployed Lambda function version.How can this requirement be met?", "related_lectures": []}, {"_class": "assessment", "id": 70265818, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company is deploying an application in four AWS Regions across North America, Europe and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.</p><p>Which solution will meet these requirements with the LOWEST latency of reads and writes?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p>This is the only workable solution in the list that provides both reads and writes in each Region that are replicated across the other Regions. This is also the best solution as it provides low latency reads and writes.</p><p><strong>CORRECT: </strong>\"Create a table in Amazon DynamoDB and enable global tables in each of the four Regions\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions\" is incorrect.</p><p>This solutions does not provide local writes in each Region as the Read Replicas cannot be written to. Therefore, this solution only offers low latency reads in each Region.</p><p><strong>INCORRECT:</strong> \"Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions\" is incorrect.</p><p>Replication groups in ElastiCache are used within a Region and not across Regions so this solution does not work.</p><p><strong>INCORRECT:</strong> \"Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions\" is incorrect.</p><p>This solution provides a database in each Region, there is no mechanism for replication. You can create replica instances in different Regions but that would only provide low latency reads (as with RDS), and not low latency writes.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-dynamodb/\">https://digitalcloud.training/amazon-dynamodb/</a></p>", "answers": ["<p>Create a table in Amazon DynamoDB and enable global tables in each of the four Regions.</p>", "<p>Create a database in Amazon DocumentDB (with MongoDB compatibility) in each of the four Regions.</p>", "<p>Create an Amazon ElastiCache for Redis cluster and configure replication groups in each of the four Regions.</p>", "<p>Create an Amazon RDS database in one Region and create Read Replicas in each of the three other Regions.</p>"]}, "correct_response": ["a"], "section": "AWS Database", "question_plain": "A company is deploying an application in four AWS Regions across North America, Europe and Asia. The application will be used by millions of users. The application must allow users to submit data through the application layer in each Region and have it saved in a low-latency database layer. The company also must ensure that the data can be read through the application layer in each Region.Which solution will meet these requirements with the LOWEST latency of reads and writes?", "related_lectures": []}, {"_class": "assessment", "id": 70265820, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer is troubleshooting an issue with an AWS CodeDeploy deployment to a deployment group containing Amazon EC2 instances. The engineer noticed that all events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.</p><p>Which of the following as possible reasons for this failure? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>There are several reasons why the deployment may show a Skipped status for all events. These include the networking configuration of the instances not being correctly setup to include a path to a NAT gateway or internet gateway. This is required for internet access and to connect to the CodeDeploy public endpoints.</p><p>Another possible cause of this issue is that the EC2 instances do not have an instance profile attached that uses an IAM policy to assign the required permissions for the application deployment.</p><p><strong>CORRECT: </strong>\"The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"The target EC2 instances do not have an instance profile attached that provides the required permissions\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy\" is incorrect.</p><p>The deployment uses a service role rather than the IAM user permissions.</p><p><strong>INCORRECT:</strong> The target EC2 instances were not properly registered with the CodeDeploy endpoint\" is incorrect.</p><p>An agent must be installed, and the instances must have the required permissions for CodeDeploy. However, they do not need to be registered.</p><p><strong>INCORRECT:</strong> \"The EC2 instances were deployed using instance store volumes rather than EBS volumes\" is incorrect.</p><p>There is no requirement to use EBS volumes over instance store volumes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events\">https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>The EC2 instances cannot reach the internet or CodeDeploy public endpoints using a NAT gateway or internet gateway.</p>", "<p>The IAM user who initiated the deployment does not have the required permissions to interact with CodeDeploy.</p>", "<p>The target EC2 instances were not properly registered with the CodeDeploy endpoint.</p>", "<p>The target EC2 instances do not have an instance profile attached that provides the required permissions.</p>", "<p>The EC2 instances were deployed using instance store volumes rather than EBS volumes.</p>"]}, "correct_response": ["a", "d"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer is troubleshooting an issue with an AWS CodeDeploy deployment to a deployment group containing Amazon EC2 instances. The engineer noticed that all events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.Which of the following as possible reasons for this failure? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265822, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is creating a pipeline in AWS CodePipeline for automation of a testing process. The engineer wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-02-13_09-03-34-adc59425bdf24abb6630e1ead4f37af0.jpg\"><p>Which type of events will match this event pattern?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as AWS Lambda and Amazon Simple Notification Service.</p><p>Events are composed of rules. A rule is configured by choosing the following:</p><ul><li><p><strong>Event Pattern.</strong> Each rule is expressed as an event pattern with the source and type of events to monitor, and event targets. To monitor events, you create a rule with the service you are monitoring as the event source, such as CodePipeline. For example, you can create a rule with an event pattern that that uses CodePipeline as an event source to trigger the rule when there are changes in the state of a pipeline, stage, or action.</p></li><li><p><strong>Targets.</strong> The new rule receives a selected service as the event target. You might want to set up a target service to send notifications, capture state information, take corrective action, initiate events, or take other actions. When you add your target, you must also grant permissions to Amazon CloudWatch Events to allow it to invoke the selected target service.</p></li></ul><p>For approval actions, the FAILED state means the action was either rejected by the reviewer or failed due to an incorrect action configuration.</p><p><strong>CORRECT: </strong>\"All rejected or failed approval actions across all the pipelines\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Failed stage execution events for the executed stage\" is incorrect.</p><p>The specified pattern applies to a CodePipeline Action Execution State Change rather than a CodePipeline Stage Execution State Change.</p><p><strong>INCORRECT:</strong> \"Failed deploy and build actions across all the pipelines\" is incorrect.</p><p>The specified pattern applies to rejected or failed approval actions only.</p><p><strong>INCORRECT:</strong> \"All abandoned or cancelled approval actions across all pipelines\" is incorrect.</p><p>The specified pattern applies to rejected or failed approval actions only.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-developer-tools/\">https://digitalcloud.training/aws-developer-tools/</a></p>", "answers": ["<p>Failed deploy and build actions across all the pipelines.</p>", "<p>All rejected or failed approval actions across all the pipelines.</p>", "<p>All abandoned or cancelled approval actions across all pipelines.</p>", "<p>Failed stage execution events for the executed stage.</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "A DevOps engineer is creating a pipeline in AWS CodePipeline for automation of a testing process. The engineer wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:Which type of events will match this event pattern?", "related_lectures": []}, {"_class": "assessment", "id": 70265824, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps team is developing a PHP web application which will be deployed on Amazon EC2 instances. The application has been designed to use a MySQL database. The team requires a deployment model in which they can reliably build, test, and deploy new updates daily, without downtime or degraded performance. The application must also be able to scale to meet an unpredictable number of concurrent users.</p><p>Which action will allow the team to quickly meet these objectives?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application might become unavailable to users for a short period of time. To avoid this, perform a blue/green deployment. To do this, deploy the new version to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly.</p><p>This solution that meets the requirements of this question by ensuring that the team can build, test, and deploy new updates daily, without downtime or degraded performance. The auto scaling support for the environments will automatically scale the infrastructure to ensure that the application can meet changing demands.</p><p><strong>CORRECT: </strong>\"Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks\" is incorrect.</p><p>Using the CLI to upload code is not a best practice. It is better to build and test with a service such as AWS CodeBuild. The main problem with this solution is there is no method of deploying updates in a blue/green strategy and to be able to rollback changes when necessary.</p><p><strong>INCORRECT:</strong> \"Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions\" is incorrect.</p><p>CloudFormation stacks are best used for infrastructure deployments and updates and not for application/code level changes.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure\" is incorrect.</p><p>DynamoDB is not a suitable solution when the question states that the application has been designed to work with MySQL which is an SQL type of database. A code rewrite would be required to support a NoSQL database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-beanstalk/\">https://digitalcloud.training/aws-elastic-beanstalk/</a></p>", "answers": ["<p>Create two Amazon ECS services: one for test and one for production. Upload the web application code to the test ECS tasks using the AWS CLI. Test the application and if the tests pass, upload the code to the production ECS tasks.</p>", "<p>Create an AWS CloudFormation template which deploys an Auto Scaling group of EC2 instances behind an Application Load Balancer. Use AWS CodeBuild to build and test the PHP application. Install the application and the MySQL database on each EC2 instance. Update the stack to deploy new application versions.</p>", "<p>Use AWS OpsWorks to create a stack for the application with a DynamoDB database layer, an Application Load Balancing layer, and an Amazon EC2 instance layer. Use Chef recipes to build and deploy the application. Use custom health checks to run unit tests on each instance with rollback on failure.</p>", "<p>Create two auto scaling AWS Elastic Beanstalk environments: one for test and one for production. Build the application using AWS CodeBuild and use Amazon RDS to store data. When new versions of the applications have passed all tests, use the Elastic Beanstalk 'swap cname' to promote the test environment to production.</p>"]}, "correct_response": ["d"], "section": "AWS Compute", "question_plain": "A DevOps team is developing a PHP web application which will be deployed on Amazon EC2 instances. The application has been designed to use a MySQL database. The team requires a deployment model in which they can reliably build, test, and deploy new updates daily, without downtime or degraded performance. The application must also be able to scale to meet an unpredictable number of concurrent users.Which action will allow the team to quickly meet these objectives?", "related_lectures": []}, {"_class": "assessment", "id": 70265826, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps Engineer manages an application running across accounts which is deployed via AWS CloudFormation.</p><p>The application stack has an Amazon ECS Fargate cluster which spins up multiple tasks for the application layer and utilizes an Amazon ElastiCache Redis cache to store frequently accessed data.</p><p>The accounts are labelled \u201csandbox\u201d and \u201cstaging\u201d. While the stack spins up fine, the application is unable to connect to Redis with the below error logged in Amazon CloudWatch Logs.</p><p>\u201cStopped reason ResourceInitializationError: unable to pull secrets or registry auth: pull command failed :: signal: killed\u201c.</p><p>What is the possible fix for the above error? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS Fargate clusters with the 1.4 version upgrade provides ENI at the task level. Below is AWS documentation for the same:</p><p>Fargate tasks run on a fleet of virtual machines that AWS manages on behalf of the customer. These VMs are connected to AWS owned VPCs via so called \u201cFargate ENIs\u201d. When a user launches a task on Fargate, the task is assigned an ENI and this ENI is connected to the customer owned VPC</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-05-26-622074b8a2ac8ab2c36144cac8b52c3e.jpg\"><p><strong>CORRECT: </strong>\"Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet\" is a correct answer (as explained above).</p><p><strong>CORRECT: </strong>\"Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint\" is also a correct answer.</p><p><strong>INCORRECT:</strong> \"Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them\" is incorrect since your tasks can sit in a private subnet as well.</p><p><strong>INCORRECT:</strong> \"Ensure that the IAM role provides the required permissions\" is incorrect since question is more around connectivity and stack worked fine in one of the environments.</p><p><strong>INCORRECT:</strong> \"Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/containers/aws-fargate-launches-platform-version-1-4/\">AWS Fargate launches platform version 1.4.0 | Containers (amazon.com)</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p>", "answers": ["<p>Ensure inbound connectivity is allowed in the application security group on port 6379 from the Redis cluster subnet.</p>", "<p>Ensure an ENI is configured for the ECS tasks in the AWS Fargate cluster and there is an api.ecr endpoint.</p>", "<p>Ensure the ECS tasks are launched in a public subnet and public IP addresses are assigned to them.</p>", "<p>Ensure that the IAM role provides the required permissions.</p>", "<p>Ensure that the application security group has a rule that accepts connections from 0.0.0.0/0.</p>"]}, "correct_response": ["a", "b"], "section": "AWS Compute", "question_plain": "A DevOps Engineer manages an application running across accounts which is deployed via AWS CloudFormation.The application stack has an Amazon ECS Fargate cluster which spins up multiple tasks for the application layer and utilizes an Amazon ElastiCache Redis cache to store frequently accessed data.The accounts are labelled \u201csandbox\u201d and \u201cstaging\u201d. While the stack spins up fine, the application is unable to connect to Redis with the below error logged in Amazon CloudWatch Logs.\u201cStopped reason ResourceInitializationError: unable to pull secrets or registry auth: pull command failed :: signal: killed\u201c.What is the possible fix for the above error? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265828, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps Engineer must deploy a three-tier web application on AWS. The application will run on Amazon EC2 instances and will use an Amazon RDS database tier. The engineer must select a deployment model the reduces operational overhead as much as possible.</p><p>Which solution best meets these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS OpsWorks Stacks provides a simple and flexible way to create and manage stacks and applications. Using layers, the various components of the three-tier application can be deployed, updated, and managed.</p><p>OpsWorks can handle deployment of all resources including application code. Lifecycle events can be used to manage updates to application code and changes to infrastructure.</p><p>The diagram below depicts an OpsWorks stack that includes three layers representing each tier of the application:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-07-05-2cc66e24169197dc611cfcb4efc3ae51.jpg\"><p><strong>CORRECT: </strong>\"Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application and database resources. Deploy application updates using OpsWorks lifecycle events\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to create an Application Load Balancer and an Auto Scaling group. Use AWS OpsWorks to create the application and database resources. Deploy application updates with OpsWorks using lifecycle events\" is incorrect.</p><p>It would be better to use OpsWorks for all components of the application and infrastructure deployment and management as it will reduce operational overhead.</p><p><strong>INCORRECT:</strong> \"Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application resources. Use AWS CloudFormation to create the database resources. Deploy application updates using CloudFormation rolling updates\" is incorrect.</p><p>As with the previous answer, it is better to use OpsWorks to manage the stack. Also, CloudFormation is well suited to deploying infrastructure but is not as good at deploying application code.</p><p><strong>INCORRECT:</strong> \"Use AWS CloudFormation to create an Application Load Balancer, an Auto Scaling group and database resources. Deploy application updates using CloudFormation rolling updates\" is incorrect.</p><p>CloudFormation is an infrastructure as code tool that deploys AWS infrastructure extremely well. However, it is not the best tool to use for deploying application updates. OpsWorks should be used in this scenario as it offers a better solution for reducing overall operational overhead whilst fully meeting the requirements of the solution.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/welcome_classic.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/welcome_classic.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-opsworks/\">https://digitalcloud.training/aws-opsworks/</a></p>", "answers": ["<p>Use AWS CloudFormation to create an Application Load Balancer and an Auto Scaling group. Use AWS OpsWorks to create the application and database resources. Deploy application updates with OpsWorks using lifecycle events.</p>", "<p>Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application and database resources. Deploy application updates using OpsWorks lifecycle events.</p>", "<p>Use AWS OpsWorks to create an Application Load Balancer, an Auto Scaling group, and application resources. Use AWS CloudFormation to create the database resources. Deploy application updates using CloudFormation rolling updates.</p>", "<p>Use AWS CloudFormation to create an Application Load Balancer, an Auto Scaling group and database resources. Deploy application updates using CloudFormation rolling updates.</p>"]}, "correct_response": ["b"], "section": "AWS Management & Governance", "question_plain": "A DevOps Engineer must deploy a three-tier web application on AWS. The application will run on Amazon EC2 instances and will use an Amazon RDS database tier. The engineer must select a deployment model the reduces operational overhead as much as possible.Which solution best meets these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265830, "assessment_type": "multi-select", "prompt": {"question": "<p>A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.</p><p>Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>By default health checks are configured to use the HTTP protocol and port 80. For an ALB the traffic protocol must be HTTP or HTTPS, but the port can be customized. The most likely cause of the issue is that the web service is running on the instances and the default protocol/port is used for health checks on the ALB and ASG. This will result in instances becoming \u201chealthy\u201d despite the actual application service not functioning correctly.</p><p>The engineer should check the ASG health check configuration and the target group health check configuration. If the default values are used then the correct custom port number should be entered instead. The path may also be updated if a specific web page should be checked.</p><p>The image below shows how you can override the default port number and path in a target group health check:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-08-15-0585684e22874b6c255bd441d63a099e.jpg\"><p><strong>CORRECT: </strong>\"Modify the Target Group health check configuration to check the application process on the custom port and path\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Modify the Auto Scaling group health check configuration to check the application process on the custom port and path\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port\" is incorrect.</p><p>You cannot use a TCP listener with an ALB. It must be HTTP or HTTPS though a custom port number can certainly be used.</p><p><strong>INCORRECT:</strong> \"Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address\" is incorrect.</p><p>The issue is not related to the IP addresses traffic is being directed to on the instances; it is related to the port number and path the health checks are configured to check.</p><p><strong>INCORRECT:</strong> \"Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances\" is incorrect.</p><p>Path-based routing rules are used to route traffic to different target groups based on the path in the HTTP request. In this case there is only one target group, so a path-based routing rule is useless. Instead, the ALB must direct traffic to a custom port number (configured in the listener) and validate the application is healthy be running health checks against the appropriate port and path.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><strong>Save time with our exam-specific cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Inspect the listener configuration on the ALB and check it is configured with the TCP protocol and the custom port.</p>", "<p>Modify the Target Group configuration to specify targets by IP rather than instance ID to allow routing to any private IP address.</p>", "<p>Modify the Target Group health check configuration to check the application process on the custom port and path.</p>", "<p>Modify the Auto Scaling group health check configuration to check the application process on the custom port and path.</p>", "<p>Create a path-based routing rule to direct traffic to the custom port and path on the EC2 instances.</p>"]}, "correct_response": ["c", "d"], "section": "AWS Compute", "question_plain": "A web application runs on a custom port. The application has been deployed in an Auto Scaling group with an Application Load Balancer (ALB). After launching instances the Auto Scaling and Target Group health checks are returning a healthy status. However, users report that the application is not accessible.Which steps should a DevOps engineer take to troubleshoot the issue? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265832, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A legacy application runs on a single Amazon EC2 instance and is backed with Amazon EBS storage. The company requires the ability to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.</p><p>Which solution will meet these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair.</p><p>The alarm uses the StatusCheckFailed_System metric and can be configured to initiate a recover action. When this occurs you will be notified by the Amazon SNS topic you configured when creating the alarm.</p><p>During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p><p>Examples of problems that cause system status checks to fail include:</p><ul><li><p>Loss of network connectivity</p></li><li><p>Loss of system power</p></li><li><p>Software issues on the physical host</p></li><li><p>Hardware issues on the physical host that impact network reachability</p></li></ul><p><strong>CORRECT: </strong>\"Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance\" is incorrect.</p><p>This metric indicates issues that involve your involvement to repair. The StatusCheckFailed_System metric should be used instead to identify issues with the underlying hardware or network.</p><p><strong>INCORRECT:</strong> \"Create an Amazon EC2 Auto Scaling group and configure the minimum, maximum, and desired capacity to 1\" is incorrect.</p><p>Auto Scaling groups use health checks to identify if the instance is functioning. The StatusCheckFailed_System metric is more useful in this scenario as it can identify several issues that may affect the instance including hardware issues on the physical host that impact network reachability (an instance health check won\u2019t do this).</p><p><strong>INCORRECT:</strong> \"Create a spread placement group configured across distinct underlying hardware. Enable automatic recovery\" is incorrect.</p><p>Spread placement groups are used when you have more than one instance and want to spread the instances across distinct underlying hardware. In this case there is only one instance, so it is of no benefit.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingRecoverActions</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudwatch/\">https://digitalcloud.training/amazon-cloudwatch/</a></p>", "answers": ["<p>Create an Amazon EC2 Auto Scaling group and configure the minimum, maximum, and desired capacity to 1.</p>", "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric and select the EC2 action to recover the instance.</p>", "<p>Create a spread placement group configured across distinct underlying hardware. Enable automatic recovery.</p>", "<p>Create an Amazon CloudWatch alarm for the StatusCheckFailed_Instance metric and select the EC2 action to reboot the instance.</p>"]}, "correct_response": ["b"], "section": "AWS Management & Governance", "question_plain": "A legacy application runs on a single Amazon EC2 instance and is backed with Amazon EBS storage. The company requires the ability to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.Which solution will meet these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 70265834, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.</p><p>Which action should the engineer take to achieve this outcome?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can configure a dead-letter queue (DLQ) by specifying a redrive policy on the SQS queue. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate unconsumed messages to determine why their processing doesn't succeed.</p><p><strong>CORRECT: </strong>\"Configure a redrive policy to move the messages to a dead-letter queue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable FIFO support for the queue to preserve ordering of the messages\" is incorrect.</p><p>You cannot enable FIFO on a standard queue, and it does not help with this issue anyway.</p><p><strong>INCORRECT:</strong> \"Enable long-polling by increasing WaitTimeSeconds parameter\" is incorrect.</p><p>Long polling just helps with efficiency of API calls as it waits for messages to appear in the queue rather than returning an immediate response. This does not assist with isolating messages for analysis.</p><p><strong>INCORRECT:</strong> \"Configure a delay queue by increasing the DelaySeconds parameter\" is incorrect.</p><p>A delay queue simply delays visibility of the messages for a specified time. This does not assist with this issue.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-application-integration-services/\">https://digitalcloud.training/aws-application-integration-services/</a></p>", "answers": ["<p>Enable FIFO support for the queue to preserve ordering of the messages.</p>", "<p>Configure a redrive policy to move the messages to a dead-letter queue.</p>", "<p>Enable long-polling by increasing WaitTimeSeconds parameter.</p>", "<p>Configure a delay queue by increasing the DelaySeconds parameter.</p>"]}, "correct_response": ["b"], "section": "AWS Application Integration", "question_plain": "A DevOps engineer is troubleshooting problems with an application which uses AWS Lambda to process messages in an Amazon SQS standard queue. The function sometimes fails to process the messages in the queue. The engineer needs to analyze the events to determine the cause of the issue and update the function code.Which action should the engineer take to achieve this outcome?", "related_lectures": []}, {"_class": "assessment", "id": 70265836, "assessment_type": "multi-select", "prompt": {"question": "<p>A DevOps engineer is migrating an application running on Docker containers from an on-premises data center to AWS. The application must run with minimal management overhead. Encrypted communications between the data center and AWS must be implemented for secure connectivity with on-premises resources.</p><p>Which actions should the DevOps engineer take to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The requirements are for low management overhead of the container-based application on AWS and encryption for traffic between AWS and the on-premises network. To minimize management overhead of the application the DevOps engineer should deploy the container-based application on Amazon ECS with the Fargate launch type. This is the serverless solution for Amazon ECS and will be the easiest to manage.</p><p>To encrypt communications between the on-premises data center and the VPC, the engineer should deploy an AWS Managed VPN which will used IPSec to encrypt the traffic between these networks.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-16-13-2d804006a971e4e66b25f15f06b9dd58.jpg\"><p><strong>CORRECT: </strong>\"Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC\" is incorrect.</p><p>The EC2 launch type has a higher management overhead as you must maintain the EC2 instances that run the container agent.</p><p><strong>INCORRECT:</strong> \"Implement a VPC endpoint and update security groups to enable access to Amazon ECS\" is incorrect.</p><p>With the Fargate launch type there is no need to add a VPC endpoint to enable connectivity for the ECS cluster.</p><p><strong>INCORRECT:</strong> \"Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener\" is incorrect.</p><p>You cannot create an HTTPS listener with an NLB and there is no need to use a load balancer to meet these requirements.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\">https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/</a></p>", "answers": ["<p>Migrate the container-based workload to Amazon ECS with the EC2 launch type in a custom VPC.</p>", "<p>Migrate the container-based workload to Amazon ECS with the Fargate launch type in a custom VPC.</p>", "<p>Implement a VPC endpoint and update security groups to enable access to Amazon ECS.</p>", "<p>Implement an AWS Managed VPN to encrypt traffic between the on-premises data center and the VPC.</p>", "<p>Implement a Network Load Balancer with IP-based targets and configure an HTTPS listener.</p>"]}, "correct_response": ["b", "d"], "section": "AWS Networking & Content Delivery", "question_plain": "A DevOps engineer is migrating an application running on Docker containers from an on-premises data center to AWS. The application must run with minimal management overhead. Encrypted communications between the data center and AWS must be implemented for secure connectivity with on-premises resources.Which actions should the DevOps engineer take to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265838, "assessment_type": "multi-select", "prompt": {"question": "<p>A global health care company uses AWS CloudFormation templates to deploy applications across various environments. The template includes Amazon EC2 instances and an Amazon RDS database. During the testing phase before the application went live, the Amazon RDS instance type was changed and caused the instance to be re-created, resulting in the loss of test data. Also, when the CloudFormation stacks are deleted, it is mandatory to keep a snapshot of the EBS volumes for backup and compliance purposes.</p><p>How can this be achieved? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To control how AWS CloudFormation handles the EBS volume when the stack is deleted, set a deletion policy for the volume. It could be deleted, retained, or stored as a snapshot.</p><p>CloudFormation keeps the resource without deleting the resource or its contents when the resource is replaced. This policy could be added to any resource type. Resources that are retained continue to exist and continue to incur applicable charges until the resources are deleted.</p><p><strong>CORRECT: </strong>\"Use DeletionPolicy=Snapshot to persist the EBS volumes attached to Amazon EC2 instances\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"In the AWS CloudFormation template, set the DeletionPolicy of the AWS::RDS::DBInstance DeletionPolicy property to \u201cRetain.\u201d\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Enable termination protection for the Amazon EC2 instances and the Amazon RDS database\" is incorrect.</p><p>With AWS CloudFormation stacks the template should be updated to configure retention of resources and persistence of snapshots. This will achieve the required outcome.</p><p><strong>INCORRECT:</strong> \"Use an AWS CloudFormation stack policy to deny updates to the instance. Only allow UpdateStack permission to IAM principals that are denied SetStackPolicy\" is incorrect.</p><p>Using permissions will complicate the stack creation and update process and may introduce issues. It is better to use the techniques in the correct answers instead.</p><p><strong>INCORRECT:</strong> \"In the AWS CloudFormation template, set the AWS::RDS::DBInstance DBlnstanceClass property to be read-only\" is incorrect.</p><p>The solution should not prevent the instance type from being changed as that may be a legitimate requirement. It should instead ensure that the database is not lost, and the snapshots are not deleted.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-cloudformation/\">https://digitalcloud.training/aws-cloudformation/</a></p>", "answers": ["<p>Use DeletionPolicy=Snapshot to persist the EBS volumes attached to Amazon EC2 instances.</p>", "<p>Enable termination protection for the Amazon EC2 instances and the Amazon RDS database.</p>", "<p>Use an AWS CloudFormation stack policy to deny updates to the instance. Only allow UpdateStack permission to IAM principals that are denied SetStackPolicy.</p>", "<p>In the AWS CloudFormation template, set the DeletionPolicy of the AWS::RDS::DBInstance DeletionPolicy property to \u201cRetain.\u201d</p>", "<p>In the AWS CloudFormation template, set the AWS::RDS::DBInstance DBlnstanceClass property to be read-only.</p>"]}, "correct_response": ["a", "d"], "section": "AWS Management & Governance", "question_plain": "A global health care company uses AWS CloudFormation templates to deploy applications across various environments. The template includes Amazon EC2 instances and an Amazon RDS database. During the testing phase before the application went live, the Amazon RDS instance type was changed and caused the instance to be re-created, resulting in the loss of test data. Also, when the CloudFormation stacks are deleted, it is mandatory to keep a snapshot of the EBS volumes for backup and compliance purposes.How can this be achieved? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265840, "assessment_type": "multi-select", "prompt": {"question": "<p>An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application is used by users around the world who access the application using a custom DNS domain name. The application must support encryption in transit, be protected from DDoS attacks and web exploits, should be optimized for performance.</p><p>Which actions should a DevOps engineer take to meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>To improve performance for global users the solution should use Amazon CloudFront. The distribution should specify the ALB as the origin and use a custom domain name and SSL/TLS certificate. This will enable caching of content in Edge Locations around the world and CloudFront offers DDoS protection.</p><p>To protect against web exploits AWS WAF should be used. A Web ACL must be created with an action and rule specified to deal with threats. The web ACL can be specified in the CloudFront distribution.</p><p><strong>CORRECT: </strong>\"Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate\" is incorrect.</p><p>You can configure an ELB as an origin for the distribution, but you cannot specify an ASG.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect\" is incorrect.</p><p>The web ACL should be attached to the CloudFront distribution in this case as it sits in front of the ALB. It is always better to protect as close to the edge as possible.</p><p><strong>INCORRECT:</strong> \"Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect\" is incorrect.</p><p>The web ACL must be attached to the CloudFront distribution, not the ASG.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html\">https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-cloudfront/\">https://digitalcloud.training/amazon-cloudfront/</a></p>", "answers": ["<p>Create an Amazon CloudFront distribution with the Auto Scaling group as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>", "<p>Create an Amazon CloudFront distribution with the ALB as an origin. Configure the custom domain name and attach an SSL/TLS certificate.</p>", "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the CloudFront distribution that WAF should inspect.</p>", "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the ALB that WAF should inspect.</p>", "<p>Create an AWS WAF web ACL, configure a default action and rule, and specify the Auto Scaling group that WAF should inspect.</p>"]}, "correct_response": ["b", "c"], "section": "AWS Networking & Content Delivery", "question_plain": "An application runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application is used by users around the world who access the application using a custom DNS domain name. The application must support encryption in transit, be protected from DDoS attacks and web exploits, should be optimized for performance.Which actions should a DevOps engineer take to meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265842, "assessment_type": "multi-select", "prompt": {"question": "<p>A company is migrating an important production application to the AWS Cloud. The application includes a containerized web tier and a MySQL database. The application must be deployed with high availability and fault tolerance and must minimize cost.</p><p>How should a DevOps engineer refactor the application? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS Fargate is a managed and serverless service for running Docker containers. It is part of the Amazon ECS family of AWS services and supports multiple availability zones for all deployments. To attach a load balancer you must create a service and you can define the desired count of ECS tasks to run.</p><p>The database tier of the application can be deployed on a highly available Amazon RDS database that has a multi-AZ replica. This provides fault tolerance across multiple AZs.</p><p><strong>CORRECT: </strong>\"Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer\" is a correct answer (as explained above.)</p><p><strong>CORRECT: </strong>\"Migrate the MySQL database to an Amazon RDS Multi-AZ deployment\" is also a correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer\" is incorrect.</p><p>EC2 is not the most cost-effective platform for this solution as the application is already containerized and therefore should run on Amazon ECS for best performance and cost efficiency.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ\" is incorrect.</p><p>A Read replica in another AZ can be used as an asynchronous backup target but the promotion time can be quite long in a failure scenario. A multi-AZ instance allows fast failover.</p><p><strong>INCORRECT:</strong> \"Migrate the MySQL database to an Amazon DynamoDB with Global Tables\" is incorrect.</p><p>DynamoDB is a NoSQL database, so it is not suitable for migrating MySQL which is a relational database.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/amazon-ecs-and-eks/\">https://digitalcloud.training/amazon-ecs-and-eks/</a></p><p><a href=\"https://digitalcloud.training/amazon-rds/\">https://digitalcloud.training/amazon-rds/</a></p>", "answers": ["<p>Migrate the web tier to an AWS Fargate cluster and configure a service and attach an Application Load Balancer.</p>", "<p>Migrate the web tier to an Auto Scaling group of EC2 instances across multiple AZs and use an Application Load Balancer.</p>", "<p>Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ.</p>", "<p>Migrate the MySQL database to an Amazon RDS Multi-AZ deployment.</p>", "<p>Migrate the MySQL database to an Amazon DynamoDB with Global Tables.</p>"]}, "correct_response": ["a", "d"], "section": "AWS Compute", "question_plain": "A company is migrating an important production application to the AWS Cloud. The application includes a containerized web tier and a MySQL database. The application must be deployed with high availability and fault tolerance and must minimize cost.How should a DevOps engineer refactor the application? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265844, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial organization is already utilizing a CI/CD pipeline to deploy applications in production. A DevOps team has automated the entire upgrade flow for a database upgrade process.</p><p>During deployment, they triggered a CI/CD pipeline with no human intervention, and the upgrade progressed smoothly. Then, 20 minutes in, the pipeline became stuck, and the update halted. It was discovered that a planned outage in an AWS region caused the pipeline to fail.</p><p>What solution can a DevOps engineer implement to prevent this from happening again without causing unnecessary delay or cost?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>In AWS CodePipeline, you can create a new stage with a single action to asynchronously invoke a Lambda function. The function will call AWS Health <a href=\"https://docs.aws.amazon.com/health/latest/APIReference/API_DescribeEvents.html\">DescribeEvents API</a> to retrieve the list of active health incidents. Then, the function will complete the event analysis and decide whether it may impact the running deployment.</p><p>Finally, the function will call back CodePipeline with the evaluation results through either <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobSuccessResult.html\">PutJobSuccessResult</a> or <a href=\"https://docs.aws.amazon.com/codepipeline/latest/APIReference/API_PutJobFailureResult.html\">PutJobFailureResult</a> API operations.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-02-13_09-20-46-825daf1581271e9aa301dbc5540619c9.jpg\"><p><strong>CORRECT: </strong>\"Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process\" is incorrect.</p><p>The issue with this option is that in case AWS health event is still there, the deployment might fail again.</p><p><strong>INCORRECT:</strong> \"Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions\" is incorrect.</p><p>This is a manual step which will work but can unnecessarily delay the upgrade in case of business critical/ compliance upgrades.</p><p><strong>INCORRECT:</strong> \"Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances\" is incorrect.</p><p>This is a high-cost option and above explained scenario is a better suited one.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/\">https://aws.amazon.com/blogs/devops/build-health-aware-ci-cd-pipelines/</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/\">https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-tools/</a></p>", "answers": ["<p>Schedule all database upgrades to avoid any planned outages from AWS in the relevant AWS Regions.</p>", "<p>Embed AWS Health API insights into the CI/CD pipelines using AWS Lambda functions to automatically stop deployments when an AWS Health event is reported in the Region.</p>", "<p>Utilize an additional instance of database and apply upgrades on passive database. When the upgrade is complete, route traffic to the passive database and perform a switch between database instances.</p>", "<p>Create an AWS CodePipeline pipeline stage to retrigger the pipeline and rerun the CI/CD process.</p>"]}, "correct_response": ["b"], "section": "AWS Developer Tools", "question_plain": "A financial organization is already utilizing a CI/CD pipeline to deploy applications in production. A DevOps team has automated the entire upgrade flow for a database upgrade process.During deployment, they triggered a CI/CD pipeline with no human intervention, and the upgrade progressed smoothly. Then, 20 minutes in, the pipeline became stuck, and the update halted. It was discovered that a planned outage in an AWS region caused the pipeline to fail.What solution can a DevOps engineer implement to prevent this from happening again without causing unnecessary delay or cost?", "related_lectures": []}, {"_class": "assessment", "id": 70265846, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An application was recently migrated to AWS. While the initial version worked, a new version is displaying an error relating to the database layer.</p><p>A DevOps engineer checked Amazon CloudWatch Logs and determined that the error was due to a database connection issue. However, no new database deployment has been performed and there were no changes to the database. Typically the application fetches the database credentials from AWS Secrets Manager.</p><p>What is the most likely cause of the issue?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Secrets manager is the preferred option to store database credentials rather than the application code base since Secrets Manager gives many options to manage, rotate and retrieve secrets out of the box. This is more secure as this information is not embedded in the application code.</p><p><strong>CORRECT: </strong>\"When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager CMK for the account. The application and secret are in different AWS accounts though, and no cross-account access has been granted\" is incorrect.</p><p>As the account is the same and it is a subsequent deployment post initial version, this is unlikely to be the correct explanation.</p><p><strong>INCORRECT:</strong> \"The secretsmanager:DescribeSecret permission is not enabled for the client-side application component and so it is unable to retrieve the database password\" is incorrect.</p><p>Since permissions were not changed, this is not a correct explanation of the issue.</p><p><strong>INCORRECT:</strong> \"The GetSecretValue API call in the application didn't include the version of the secret to return\" is incorrect.</p><p>This is also a possible failure when there are multiple versions to a secret and you are fetching the incorrect version of secret but in this scenario, this is not the cause of the issue.</p><p><strong>References:</strong></p><p><a href=\"https://awscloudsecvirtualevent.com/workshops/module4/rds/\">https://awscloudsecvirtualevent.com/workshops/module4/rds/</a></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/secretsmanager/latest/userguide/cfn-example_reference-secret.html\">https://docs.amazonaws.cn/en_us/secretsmanager/latest/userguide/cfn-example_reference-secret.html</a></p><p><strong>Save time with our AWS cheat sheets:</strong></p><p><a href=\"https://digitalcloud.training/aws-secrets-manager/\">https://digitalcloud.training/aws-secrets-manager/</a></p>", "answers": ["<p>The DB credentials and connection details in Secrets Manager have been encrypted using the default Secrets Manager KMS key for the account. The application and secret are in different AWS accounts though, and no cross-account access has been granted.</p>", "<p>The secretsmanager:DescribeSecret permission is not enabled for the client-side application component and so it is unable to retrieve the database password.</p>", "<p>When secret rotation is configured in Secrets Manager, it causes the secret to rotate once as soon as you store the secret. This can lead to a situation where the old credentials are not usable anymore after the initial rotation. It is possible that the team forgot to update the application to retrieve the secret from Secrets Manager.</p>", "<p>The GetSecretValue API call in the application didn't include the version of the secret to return.</p>"]}, "correct_response": ["c"], "section": "AWS Security, Identity, & Compliance", "question_plain": "An application was recently migrated to AWS. While the initial version worked, a new version is displaying an error relating to the database layer.A DevOps engineer checked Amazon CloudWatch Logs and determined that the error was due to a database connection issue. However, no new database deployment has been performed and there were no changes to the database. Typically the application fetches the database credentials from AWS Secrets Manager.What is the most likely cause of the issue?", "related_lectures": []}]}
