4755096
~~~
{"count": 10, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 56402490, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>0.50</p>", "<p>0.67</p>", "<p>0.56</p>", "<p>0.47</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>0.56</strong></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_11-46-41-f21f36b928807beaae9e665b1fc3985e.jpg\"></p><p>The actual output of a multi-class classification algorithm is a set of prediction <em>scores</em>. The scores indicate the model's certainty that the given observation belongs to each of the classes. Unlike for binary classification problems, you do not need to choose a score cut-off to make predictions. The predicted answer is the class (for example, label) with the highest predicted score.</p><p>Amazon ML provides a <em>confusion matrix</em> as a way to visualize the accuracy of multiclass classification predictive models. The confusion matrix illustrates in a table the number or percentage of correct and incorrect predictions for each class by comparing an observation's predicted class and its true class.</p><p>An example of confusion matrix is show in the example below:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_11-48-24-0a78b2bad522d6db86ae81451b8ae24f.jpg\"></p><p>Precision for Platinum = (True Positives / (True Positives + False Positives))</p><p>= (30/(30+ (20+10) )) = 30/60 = 0.50</p><p>Precision for Gold = (True Positives / (True Positives + False Positives))</p><p>= (60/(60+ (50+10) )) = 60/120 = 0.50</p><p>Precision for Silver = (True Positives / (True Positives + False Positives))</p><p>= (80/(80+ (20+20) )) = 80/120 = 0.67</p><p>Overall Precision = Average of the precision for Platinum, Gold and Silver</p><p>= (0.50+0.50+0.67)/3 = 0.56</p><p><br></p><p>Incorrect options:</p><p><strong>0.50</strong></p><p><strong>0.67</strong></p><p><strong>0.47</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html</a></p>", "relatedLectureIds": "", "question": "<p>The data science team at a financial services company has created a multi-class classification model to segment the company\u2019s customers into three tiers - Platinum, Gold and Silver. The confusion matrix for the underlying model was reported as follows:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-26_11-48-49-45b8fa46a861b927d9c4bf81892e7e93.png\"></p><p>What is the overall precision for this multi-class classification model?</p>"}, "correct_response": ["c"], "section": "Modeling", "question_plain": "The data science team at a financial services company has created a multi-class classification model to segment the company\u2019s customers into three tiers - Platinum, Gold and Silver. The confusion matrix for the underlying model was reported as follows:What is the overall precision for this multi-class classification model?", "related_lectures": []}, {"_class": "assessment", "id": 56402492, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Model A</p>", "<p>Model B</p>", "<p>Both Model A and Model B are equally cost effective, as accuracy is same</p>", "<p>None of the Model A and Model B are cost effective. Company needs to try something different.</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Model A</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_12-19-17-c3b438158e9684939e54d86c40c807d6.jpg\"><p>The actual output of many binary classification algorithms is a prediction score. The score indicates the system\u2019s certainty that the given observation belongs to the positive class. To make the decision about whether the observation should be classified as positive or negative, as a consumer of this score, you will interpret the score by picking a classification threshold (cut-off) and compare the score against it. Any observations with scores higher than the threshold are then predicted as the positive class and scores lower than the threshold are predicted as the negative class.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_11-53-52-0d27f9e5d6b015558448aff676d13bda.jpg\"><p>The predictions now fall into four groups based on the actual known answer and the predicted answer: correct positive predictions (true positives), correct negative predictions (true negatives), incorrect positive predictions (false positives) and incorrect negative predictions (false negatives).</p><p>Depending on your business problem, you might be more interested in a model that performs well for a specific subset of these metrics. For example, two business applications might have very different requirements for their ML models:</p><p>One application might need to be extremely sure about the positive predictions actually being positive (high precision) and be able to afford to misclassify some positive examples as negative (moderate recall).</p><p>Another application might need to correctly predict as many positive examples as possible (high recall) and will accept some negative examples being misclassified as positive (moderate precision).</p><p>In Amazon ML, observations get a predicted score in the range [0,1]. The score threshold to make the decision of classifying examples as 0 or 1 is set by default to be 0.5. Amazon ML allows you to review the implications of choosing different score thresholds and allows you to pick an appropriate threshold that matches your business needs.</p><p>For the given use-case, the classification model predicts if a customer is likely to churn. This implies that a False Negative is very costly for the company because the model predicted that the customer will not churn, however, in reality the customer did churn. So the ideal model would focus on reducing the False Negatives. Thus Model A is the right choice.</p><p>Incorrect options:</p><p><strong>Model B</strong></p><p>As discussed above, Model B&nbsp;is not the right choice as it has more False Negatives.</p><p><strong>Both Model A and Model B are equally cost effective, as accuracy is same </strong>- Accuracy is not the right criteria to assess the model, as we need to reduce the False Negatives in order to reduce the costs to the company.</p><p><strong>None of the Model A and Model B are cost effective. Company needs to try something different </strong>- This option has been added as a distractor. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html</a></p>", "relatedLectureIds": "", "question": "<p>The marketing team at an Enterprise SaaS company has determined that the cost of customer churn is much greater than the cost of customer retention for its existing customer base. To address this issue, the team worked on a classification model to predict if a customer is likely to churn and boiled it down to two model variants. Model A had 92% accuracy with 40 False Negatives (FN) and 100 False Positives (FP) whereas model B also had 92% accuracy with 100 FN and 40 FP. </p><p>Which of the two models is more cost effective for the company?</p>"}, "correct_response": ["a"], "section": "Modeling", "question_plain": "The marketing team at an Enterprise SaaS company has determined that the cost of customer churn is much greater than the cost of customer retention for its existing customer base. To address this issue, the team worked on a classification model to predict if a customer is likely to churn and boiled it down to two model variants. Model A had 92% accuracy with 40 False Negatives (FN) and 100 False Positives (FP) whereas model B also had 92% accuracy with 100 FN and 40 FP. Which of the two models is more cost effective for the company?", "related_lectures": []}, {"_class": "assessment", "id": 56402494, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Factorization Machines</p>", "<p>BlazingText Word2Vec mode</p>", "<p>Object2Vec</p>", "<p>XGBoost</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Object2Vec</strong></p><p>Object2Vec can be used to find semantically similar objects such as questions. BlazingText Word2Vec can only find semantically similar words. Factorization Machines and XGBoost are not fit for this use-case. </p><p>Object2Vec : The Amazon SageMaker Object2Vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space. You can use the learned embeddings to efficiently compute nearest neighbors of objects and to visualize natural clusters of related objects in low-dimensional space, for example. You can also use the embeddings as features of the corresponding objects in downstream supervised tasks, such as classification or regression.</p><p>A good reference read for Object2Vec:</p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/\">https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/</a></p><p>Incorrect options:</p><p><strong>Factorization Machines - </strong> The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p><br></p><p><strong>BlazingText Word2Vec mode</strong> - The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.</p><p>The Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a word embedding. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words.</p><p><br></p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p>", "relatedLectureIds": "", "question": "<p>The data science team at a leading Questions and Answers website wants to improve the user experience and therefore would like to identify duplicate questions based on similarity of the text found in a given question. </p><p>As an ML Specialist, which SageMaker algorithm would you recommend to help solve this problem?</p>"}, "correct_response": ["c"], "section": "Modeling", "question_plain": "The data science team at a leading Questions and Answers website wants to improve the user experience and therefore would like to identify duplicate questions based on similarity of the text found in a given question. As an ML Specialist, which SageMaker algorithm would you recommend to help solve this problem?", "related_lectures": []}, {"_class": "assessment", "id": 56402496, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As part of a clinical study, you have processed millions of medical records with hundreds of features and reduced the feature dimensions to just two using a model based on Principal Component Analysis (PCA). The following graph illustrates the distribution from the PCA model output in the form of two distinct classes shown in red and blue dots.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2020-10-01_11-18-16-0a72d83f499e60d1344eb4dfd912775a.png\"></p><p>Which algorithm can you use to accurately classify the above output from the PCA model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Support Vector Machine</strong></p><p>Principal Component Analysis (PCA) Algorithm\u2014reduces the dimensionality (number of features) within a dataset by projecting data points onto the first few principal components. The objective is to retain as much information or variation as possible. For the given use-case, the features have been reduced to just two dimensions using a model based on Principal Component Analysis (PCA).</p><p>Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVM&nbsp;can solve linear and non-linear problems and work well for many practical problems. SVM creates a line or a hyperplane which separates the data into classes.</p><p>SVM is used for text classification tasks such as category assignment, detecting spam and sentiment analysis. It is also commonly used for image recognition challenges, performing particularly well in aspect-based recognition and color-based classification.</p><p>Since SVM can classify for non-linear boundary, so that\u2019s the right option.</p><p>Incorrect options:</p><p><strong>Linear Regression</strong> - The output of a regression ML model is a numeric value for the model's prediction of the target. For example, if you are predicting housing prices, the prediction of the model could be a value such as $ 254,013. This is a classification problem, so Linear Regression can be ruled out.</p><p><strong>K-means </strong>- It is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity.</p><p>Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time. As K-means is used for clustering, so this option is also incorrect for the given use-case.</p><p><strong>Logistic Regression </strong>- This is a classification algorithm that is used where the response variable is categorical. As the decision boundary is non-linear, Logistic Regression is not the right fit for the given use-case.&nbsp; </p><p>References:</p><p><a href=\"https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496\">https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496</a></p><p><a href=\"https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\">https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc</a></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/learning-algorithm.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/learning-algorithm.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/train-faster-more-flexible-models-with-amazon-sagemaker-linear-learner/\">https://aws.amazon.com/blogs/machine-learning/train-faster-more-flexible-models-with-amazon-sagemaker-linear-learner/</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/\">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/</a></p>", "answers": ["<p>Linear Regression</p>", "<p>Support Vector Machine</p>", "<p>Logistic Regression</p>", "<p>K-means</p>"]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "As part of a clinical study, you have processed millions of medical records with hundreds of features and reduced the feature dimensions to just two using a model based on Principal Component Analysis (PCA). The following graph illustrates the distribution from the PCA model output in the form of two distinct classes shown in red and blue dots.Which algorithm can you use to accurately classify the above output from the PCA model?", "related_lectures": []}, {"_class": "assessment", "id": 56402498, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Amazon SageMaker ensures that machine learning (ML) model artifacts and other system artifacts are encrypted in transit, so no special measures are required</p>", "<p>Use Amazon Virtual Private Cloud interface endpoints powered by AWS PrivateLink for private connectivity between the customer's VPC and the request router to access hosted model endpoints</p>", "<p>Use SSH for private connectivity between the customer's VPC and the request router to access hosted model endpoints</p>", "<p>Use AWS SSE-KMS for private connectivity between the customer's VPC and the request router to access hosted model endpoints</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use Amazon Virtual Private Cloud interface endpoints powered by AWS PrivateLink for private connectivity between the customer's VPC and the request router to access hosted model endpoints</strong></p><p>You can connect directly to the SageMaker API or to the SageMaker Runtime through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the internet. When you use a VPC interface endpoint, communication between your VPC and the SageMaker API or Runtime is conducted entirely and securely within the AWS network.</p><p>The SageMaker API and Runtime support Amazon Virtual Private Cloud (Amazon VPC) interface endpoints that are powered by AWS PrivateLink. Each VPC endpoint is represented by one or more Elastic Network Interfaces with private IP addresses in your VPC subnets.</p><p>The VPC interface endpoint connects your VPC directly to the SageMaker API or Runtime without an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. The instances in your VPC don't need public IP addresses to communicate with the SageMaker API or Runtime.</p><p>You can create an interface endpoint to connect to SageMaker or to SageMaker Runtime with either the AWS console or AWS Command Line Interface (AWS CLI) commands.</p><p>Incorrect options:</p><p><strong>Use SSH for private connectivity between the customer's VPC and the request router to access hosted model endpoints</strong> - This option has been added as a distractor as SSH is used for encrypted data communications between two computers connecting over an open network, such as the internet. By itself SSH cannot facilitate API&nbsp;calls that bypass the public internet.</p><p><strong>Use AWS SSE-KMS for private connectivity between the customer's VPC and the request router to access hosted model endpoints</strong> - SSE-KMS&nbsp;is used for the Amazon S3 service. When you create an object in S3, you can specify the use of server-side encryption with AWS Key Management Service (AWS KMS) customer master keys (CMKs) to encrypt your data. This is true when you are either uploading a new object or copying an existing object. This encryption is known as SSE-KMS. This option has been added as a distractor, as SSE-KMS cannot facilitate API&nbsp;calls that bypass the public internet.</p><p><strong>Amazon SageMaker ensures that machine learning (ML) model artifacts and other system artifacts are encrypted in transit, so no special measures are required</strong> - This option does not address the requirement to bypass the public internet, so this option is incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inter-network-privacy.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inter-network-privacy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-kms-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-kms-encryption.html</a></p>", "relatedLectureIds": "", "question": "<p>You want to secure the API calls made to your published Amazon SageMaker model endpoints from your customer VPC. By default, these API calls traverse the public network to the request router. </p><p>What measures would you take to address this issue so that the API&nbsp;calls do not use the public internet?</p>"}, "correct_response": ["b"], "section": "ML Implementation and Operations", "question_plain": "You want to secure the API calls made to your published Amazon SageMaker model endpoints from your customer VPC. By default, these API calls traverse the public network to the request router. What measures would you take to address this issue so that the API&nbsp;calls do not use the public internet?", "related_lectures": []}, {"_class": "assessment", "id": 56402500, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A ride-hailing company needs to ingest and store certain attributes of real-time automobile health data which is in JSON format. The company does not want to manage the underlying infrastructure and it wants the data to be available for visualization on a near real time basis. </p><p>As an ML specialist, what is your recommendation so that the solution requires the least development time and infrastructure management?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Ingest the data using Kinesis Firehose that uses a Lambda function to write the selected attributes from the input data stream into an S3 location. Further pipe this processed data into QuickSight for visualizations</strong></p><p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. Kinesis Data Firehose manages all underlying infrastructure, storage, networking, and configuration needed to capture and load your data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, or Splunk. You do not have to worry about provisioning, deployment, ongoing maintenance of the hardware, software, or write any other application to manage this process.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_17-12-18-457057887d2b4b8d77a1a1d1101724e4.png\"></p><p>Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud. QuickSight lets you easily create and publish interactive BI dashboards that include Machine Learning-powered insights. QuickSight dashboards can be accessed from any device, and seamlessly embedded into your applications, portals, and websites.</p><p>This is the correct option as it can be used to process the streaming JSON data via Kinesis Firehose that uses a Lambda to write the selected attributes as JSON data into an S3 location. You should note that Firehose offers built-in integration with intermediary lambda functions to handle any transformations. This transformed data is then consumed in QuickSight for visualizations.</p><p>Incorrect options:</p><p><strong>Ingest the data using Kinesis Data Streams and pipe the data to a Glue Streaming Job to select specific attributes from the source data and write the output in another S3 location. Use this processed data in QuickSight for visualizations</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration, so you can start analyzing your data and putting it to use in minutes instead of months.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_17-14-50-7dc454da011ff632dd633d9002625028.png\"></p><p>Glue ETL can be used for batch ETL use cases as well as processing real time data. You can create streaming extract, transform, and load (ETL) jobs that run continuously, consume data from streaming sources like Amazon Kinesis Data Streams, Apache Kafka, and Amazon Managed Streaming for Apache Kafka (Amazon MSK). The jobs cleanse and transform the data, and then load the results into Amazon S3 data lakes or JDBC data stores. </p><p>This option involves custom code development for the Glue script to handle the streaming data. Also you need to set up both Kinesis Data Streams and Glue Streaming for this option, so it turns out to be more complex then just directly using Kinesis Firehose. So this option is not the best fit for the&nbsp; given use case.</p><p><strong>Ingest the data using a Spark Streaming application running on an EMR cluster. The output data with selected attributes is written in JSON format on S3. Further pipe this data as source into QuickSight for visualizations</strong> - </p><p>Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads.</p><p>EMR cluster is not an option as the company does not want to manage the underlying infrastructure. So this option is incorrect.</p><p><strong>Ingest the data into S3 using Kinesis Firehose. Launch a Glue ETL Job every 15 minutes to write S3 data into RDS. Use RDS Connector to visualize this data in QuickSight</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration, so you can start analyzing your data and putting it to use in minutes instead of months.</p><p>There is no need to write the data in RDS&nbsp;via Glue ETL, as QuickSight can directly read from S3. In addition, Glue ETL&nbsp;Job will introduce a delay of 15 minutes, which is unacceptable for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#streaming-etl-intro\">https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#streaming-etl-intro</a></p>", "answers": ["<p>Ingest the data using Kinesis Firehose that uses a Lambda function to write the selected attributes from the input data stream into an S3 location. Further pipe this processed data into QuickSight for visualizations</p>", "<p>Ingest the data using Kinesis Data Streams and pipe the data to a Glue Streaming Job to select specific attributes from the source data and write the output in another S3 location. Use this processed data in QuickSight for visualizations</p>", "<p>Ingest the data using a Spark Streaming application running on an EMR cluster. The output data with selected attributes is written in JSON format on S3. Further pipe this data as source into QuickSight for visualizations</p>", "<p>Ingest the data into S3 using Kinesis Firehose. Launch a Glue ETL Job every 15 minutes to write S3 data into RDS. Use RDS Connector to visualize this data in QuickSight</p>"]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "A ride-hailing company needs to ingest and store certain attributes of real-time automobile health data which is in JSON format. The company does not want to manage the underlying infrastructure and it wants the data to be available for visualization on a near real time basis. As an ML specialist, what is your recommendation so that the solution requires the least development time and infrastructure management?", "related_lectures": []}, {"_class": "assessment", "id": 56402502, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The ML solutions team at a leading ecommerce company wants to build a real time fraud detection system. As an ML Specialist, what is the recommended course of action to build such a system with the least number of components that also ensures ease of maintenance?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Ingest the clickstream data into Kinesis Data Streams, which is then written into Kinesis Data Analytics for real time fraud detection using the Random Cut Forest Algorithm. A Lambda function sends an email alert for every fraud detected by the algorithm. The processed stream data is finally sent to Kinesis Data Firehose for subsequent storage in S3</strong></p><p>Amazon Kinesis Data Analytics is the easiest way to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating streaming applications with other AWS services. You can quickly build SQL queries and sophisticated Apache Flink applications in a supported language such as Java or Scala using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_17-22-54-21f761e7d9dd3fd2e07c139ca9121b35.png\"></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_17-29-02-d4abbda8865a2bab40b30226888f61a4.png\"></p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-11_17-26-16-1dfcc015e714c184d16c1ba543f46a8e.png\"></p><p>Using the combination of Kinesis Data Streams followed by Kinesis Data Analytics (running RCF algorithm) and then using Kinesis Data Firehose is the correct solution.</p><p>Incorrect options:</p><p><strong>Ingest the clickstream data into Kinesis Data Analytics for real time Fraud Detection and the data is finally sent to Kinesis Data Firehose for subsequent storage in S3</strong> - Kinesis Data Analytics cannot directly ingest incoming stream data so this option is incorrect. You can use Kinesis Data Streams to ingest the incoming stream data. </p><p><strong>Ingest the clickstream data into Kinesis Data Streams, which is then written into Kinesis Data Analytics for real time fraud detection and the processed stream data is finally directly written into EFS</strong> - Kinesis Data Analytics cannot directly write data into EFS, so this option is incorrect.</p><p><strong>Ingest the clickstream data into a Spark Streaming application running on EMR cluster to detect fraud records. The application then writes the data into S3</strong> - Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Using an EMR cluster would imply managing the underlying infrastructure so it\u2019s ruled out.</p><p>Using an EMR cluster would imply managing the underlying infrastructure so this option is ruled out. </p><p>References:</p><p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p>", "answers": ["<p>Ingest the clickstream data into Kinesis Data Streams, which is then written into Kinesis Data Analytics for real time fraud detection using the Random Cut Forest Algorithm. A Lambda function sends an email alert for every fraud detected by the algorithm. The processed stream data is finally sent to Kinesis Data Firehose for subsequent storage in S3</p>", "<p>Ingest the clickstream data into Kinesis Data Analytics for real time Fraud Detection and the data is finally sent to Kinesis Data Firehose for subsequent storage in S3</p>", "<p>Ingest the clickstream data into Kinesis Data Streams, which is then written into Kinesis Data Analytics for real time fraud detection and the processed stream data is finally directly written into EFS</p>", "<p>Ingest the clickstream data into a Spark Streaming application running on EMR cluster to detect fraud records. The application then writes the data into S3</p>"]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "The ML solutions team at a leading ecommerce company wants to build a real time fraud detection system. As an ML Specialist, what is the recommended course of action to build such a system with the least number of components that also ensures ease of maintenance?", "related_lectures": []}, {"_class": "assessment", "id": 56402504, "assessment_type": "multi-select", "prompt": {"question": "<p>An insurance company is building a binary classification model to predict insurance claims. The training data contains 1800 instances of the positive class (customers who did claim insurance) and 100 instances of the negative class (customers who did not claim insurance). The final model has 85% accuracy, but poor precision. </p><p>How can you improve the model performance? (Select three)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Over-sample from the negative class</strong></p><p><strong>Collect more training data for the negative class</strong></p><p><strong>Create more samples using algorithms such as SMOTE</strong></p><p>In case of a binary classification model with strongly unbalanced classes, we need to over-sample from the minority class, collect more training data for the minority class and create more samples using algorithms such as SMOTE which effectively uses a k-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. Here are a few good references :</p><p><a href=\"http://www.svds.com/learning-imbalanced-classes/\">http://www.svds.com/learning-imbalanced-classes/</a></p><p><a href=\"https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes\">https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes</a></p><p>Incorrect options:</p><p><strong>Over-sample from the positive class</strong></p><p><strong>Collect more training data for the positive class</strong></p><p>Over-sampling from the positive class or collecting more training data for the positive class would further worsen the model. </p>", "answers": ["<p>Over-sample from the negative class</p>", "<p>Collect more training data for the negative class</p>", "<p>Collect more training data for the positive class</p>", "<p>Create more samples using algorithms such as SMOTE</p>", "<p>Over-sample from the positive class</p>"]}, "correct_response": ["a", "b", "d"], "section": "Exploratory Data Analysis", "question_plain": "An insurance company is building a binary classification model to predict insurance claims. The training data contains 1800 instances of the positive class (customers who did claim insurance) and 100 instances of the negative class (customers who did not claim insurance). The final model has 85% accuracy, but poor precision. How can you improve the model performance? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 56402506, "assessment_type": "multiple-choice", "prompt": {"question": "<p>As an ML Specialist, you observe that one of the features used in a SageMaker Linear Learner model had 30% missing data. You also believe that this specific feature was somehow related to a few other features in the data-set. </p><p>Which technique would you use to address the missing data?</p>", "relatedLectureIds": "", "answers": ["<p>Drop the missing feature as 30% missing data would severely distort the overall training data</p>", "<p>Use multiple imputations approach via a supervised learning technique that uses other features to figure out the imputed value</p>", "<p>Replace the missing values with mean</p>", "<p>Replace the missing values with median</p>"], "explanation": "<p>Correct option:</p><p><strong>Use multiple imputations approach via a supervised learning technique that uses other features to figure out the imputed value</strong></p><p>For the given use-case, you need to prepare the dataset for a machine learning problem and you need to fix missing values. You can fix missing values by applying machine learning to that dataset itself! In multiple imputations approach, you need to generate missing values from the dataset many times. The individual datasets are then pooled together into the final imputed dataset, with the values chosen to replace the missing data being drawn from the combined results in some way.The multiple imputations approach breaks imputation process into three steps: imputation (multiple times), analysis (staging how the results should be combined), and pooling (integrating the results into the final imputed matrix). There are a variety of multiple imputation algorithms and implementations available. The most popular algorithm is called MICE.</p><p>Here is a great reference for a deep-dive on the multiple imputations approach:</p><p><a href=\"https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation\">https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation</a></p><p>Incorrect options:</p><p><strong>Drop the missing feature as 30% missing data would severely distort the overall training data</strong></p><p>Dropping the feature is not recommended as we can impute the missing values for the feature via a supervised learning technique that uses other features to figure out the imputed value. </p><p><strong>Replace the missing values with mean</strong></p><p><strong>Replace the missing values with median</strong></p><p>Replacing with mean or median can introduce bias into the model as these are naive methods, therefore both these options are incorrect.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "As an ML Specialist, you observe that one of the features used in a SageMaker Linear Learner model had 30% missing data. You also believe that this specific feature was somehow related to a few other features in the data-set. Which technique would you use to address the missing data?", "related_lectures": []}, {"_class": "assessment", "id": 56402508, "assessment_type": "multiple-choice", "prompt": {"question": "<p>What technique would you use in SageMaker to train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Incremental Training</strong></p><p>Over time, you might find that a model generates inferences that are not as good as they were in the past. With incremental training, you can use the artifacts from an existing model and use an expanded dataset to train a new model. Incremental training saves both time and resources.</p><p>You can use incremental training to:</p><p>Train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance.</p><p>Use the model artifacts or a portion of the model artifacts from a popular publicly available model in a training job. You don't need to train a new model from scratch.</p><p>Resume a training job that was stopped.</p><p>Train several variants of a model, either with different hyperparameter settings or using different datasets.</p><p>You can read more on this reference link -</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html</a></p><p>Incorrect options:</p><p><strong>Batch Training</strong> - Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options:</p><p>batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent</p><p>mini-batch mode: where the batch size is greater than one but less than the total dataset size. Usually, a number that can be divided into the total dataset size.</p><p>stochastic mode: where the batch size is equal to one. Therefore the gradient and the neural network parameters are updated after each sample.</p><p>There is no such thing as \"batch training\" and this option has been added as a distractor.</p><p><strong>Beta Testing</strong> -&nbsp; Beta Testing is one of the Acceptance Testing types used in traditional software engineering, which adds value to the product as the end-user (intended real user) validates the product for functionality, usability, reliability, and compatibility.</p><p>This option has been added as a distractor.</p><p><strong>Transfer Learning</strong> - This is a technique used in image classification algorithms. The image classification algorithm takes an image as input and classifies it into one of the output categories. Image classification in Amazon SageMaker can be run in two modes: full training and transfer learning. In full training mode, the network is initialized with random weights and trained on user data from scratch. In transfer learning mode, the network is initialized with pre-trained weights and just the top fully connected layer is initialized with random weights. Then, the whole network is fine-tuned with new data. In this mode, training can be achieved even with a smaller dataset. This is because the network is already trained and therefore can be used in cases without sufficient training data.</p><p>Transfer Learning is a general machine learning technique that is not relevant to the SageMaker specific use-case described in the question. </p>", "answers": ["<p>Transfer Learning</p>", "<p>Incremental Training</p>", "<p>Batch Training</p>", "<p>Beta Testing</p>"]}, "correct_response": ["b"], "section": "Modeling", "question_plain": "What technique would you use in SageMaker to train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance?", "related_lectures": []}]}
4755118
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 52820606, "assessment_type": "multi-select", "prompt": {"question": "<p>An e-learning company runs a website where the users may sign up for free but must pay to access pro features. The company wants to develop an automated system that can forecast if a new user will upgrade to a pro subscription within a year. The company has put together a labeled collection of dataset from two million users. The training dataset contains 2,000 positive samples (users who paid within a year) and 1,998,000 negative samples (users who never upgraded to pro). Each data sample contains 100 attributes about the user such as their age, device, profession, etc.</p><p>The analytics team at the company developed a random forest model on this dataset, which converged to above 99 percent accuracy on the training set. However, the prediction accuracy on a test dataset was below par.</p><p>Which of the following solutions would you recommend to the analytics team for addressing this use case? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Create additional positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data</strong></p><p>The actual output of many binary classification algorithms is a prediction score. The score indicates the system\u2019s certainty that the given observation belongs to the positive class. To make the decision about whether the observation should be classified as positive or negative, you can interpret the score by picking a classification threshold (cut-off) and compare the score against it. Any observations with scores higher than the threshold are then predicted as the positive class and scores lower than the threshold are predicted as the negative class.</p><p><img src=\"https://lh3.googleusercontent.com/kRUQ2iaARHuzs0yonSdAtiYgWf_IAOY8KhNm-NXXtRprdP6h3pWimKvHJlTrr7qune96cosytsFwD1Rk_tVEJ0ej2EuQIjHcki5kx3rO-bjt3zkE-hzfTfaMzhdnw6oDr_6tciAo\"></p><p>The predictions now fall into four groups based on the actual known answer and the predicted answer: correct positive predictions (true positives), correct negative predictions (true negatives), incorrect positive predictions (false positives) and incorrect negative predictions (false negatives).</p><p>You should note that the model has 99% accuracy on the training data because it's biased to predict all outcomes to be negative. Since the training data is imbalanced towards negative classes, the training accuracy turns out to be high.</p><p>However, when it comes to test data, there is no such imbalance and as a result, the model has a dismal performance. To fix this issue, we need to provide additional positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data and then train the model.</p><p><strong>Change the cost function so that false negatives have a higher impact on the cost value than false positives</strong></p><p>A learning algorithm consists of a cost function and an optimization technique. The cost is the penalty that is incurred when the estimate of the target provided by the ML model does not equal the target exactly. A cost function quantifies this penalty as a single value. An optimization technique seeks to minimize the loss.</p><p>For the given use case, since the training data has a very low proportion of positive samples, we must penalize the model for predicting a negative outcome when the actual outcome is positive (that is, penalize false negatives). This can be done by changing the cost function so that false negatives have a higher impact on the cost value than false positives.</p><p>Incorrect options:</p><p><strong>Create additional negative samples by duplicating the negative samples and adding a small amount of noise to the duplicated data</strong> - As the training data is already skewed towards negative samples, so there is no need to create additional negative samples. As mentioned above, we need to create additional positive samples to address the imbalance in the data.</p><p><strong>Change the cost function so that true negatives have a higher impact on the cost value than true positives</strong> - This option has been added as a distractor as neither true negatives nor true positives should have a bearing on the cost value.</p><p><strong>Change the cost function so that false positives have a higher impact on the cost value than false negatives</strong> - As mentioned above, we should change the cost function so that false negatives have a higher impact on the cost value than false positives. So this option is incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html</a></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/learning-algorithm.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/learning-algorithm.html</a></p>", "answers": ["<p>Create additional positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data</p>", "<p>Change the cost function so that false negatives have a higher impact on the cost value than false positives</p>", "<p>Create additional negative samples by duplicating the negative samples and adding a small amount of noise to the duplicated data</p>", "<p>Change the cost function so that true negatives have a higher impact on the cost value than true positives</p>", "<p>Change the cost function so that false positives have a higher impact on the cost value than false negatives</p>"]}, "correct_response": ["a", "b"], "section": "Modeling", "question_plain": "An e-learning company runs a website where the users may sign up for free but must pay to access pro features. The company wants to develop an automated system that can forecast if a new user will upgrade to a pro subscription within a year. The company has put together a labeled collection of dataset from two million users. The training dataset contains 2,000 positive samples (users who paid within a year) and 1,998,000 negative samples (users who never upgraded to pro). Each data sample contains 100 attributes about the user such as their age, device, profession, etc.The analytics team at the company developed a random forest model on this dataset, which converged to above 99 percent accuracy on the training set. However, the prediction accuracy on a test dataset was below par.Which of the following solutions would you recommend to the analytics team for addressing this use case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820510, "assessment_type": "multi-select", "prompt": {"question": "<p>Which of the following SageMaker unsupervised learning algorithms can be used for Fraud Detection? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Random Cut Forest</p>", "<p>Object2Vec</p>", "<p>IP Insights</p>", "<p>Factorization Machines</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Random Cut Forest</strong></p><p>Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the \"regular\" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the \"regular\" data can often be described with a simple model.</p><p>With each data point, RCF associates an anomaly score. Low score values indicate that the data point is considered \"normal.\" High values indicate the presence of an anomaly in the data. The definitions of \"low\" and \"high\" depend on the application but common practice suggests that scores beyond three standard deviations from the mean score are considered anomalous.</p><p><strong>IP Insights</strong></p><p>Amazon SageMaker IP Insights is an unsupervised learning algorithm that learns the usage patterns for IPv4 addresses. It is designed to capture associations between IPv4 addresses and various entities, such as user IDs or account numbers. You can use it to identify a user attempting to log into a web service from an anomalous IP address, for example. Or you can use it to identify an account that is attempting to create computing resources from an unusual IP address. Trained IP Insight models can be hosted at an endpoint for making real-time predictions or used for processing batch transforms.</p><p>SageMaker IP insights ingests historical data as (entity, IPv4 Address) pairs and learns the IP usage patterns of each entity. When queried with an (entity, IPv4 Address) event, a SageMaker IP Insights model returns a score that infers how anomalous the pattern of the event is. For example, when a user attempts to log in from an IP address, if the IP Insights score is high enough, a web login server might decide to trigger a multi-factor authentication system. In more advanced solutions, you can feed the IP Insights score into another machine learning model.</p><p>Therefore, both Random Cut Forest and IP Insights can be used for Fraud Detection. </p><p>Incorrect options:</p><p><strong>Object2Vec</strong> - The Amazon SageMaker Object2Vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space. You can use the learned embeddings to efficiently compute nearest neighbors of objects and to visualize natural clusters of related objects in low-dimensional space, for example. You can also use the embeddings as features of the corresponding objects in downstream supervised tasks, such as classification or regression. </p><p>Object2Vec cannot be used for Fraud Detection.</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p>Factorization Machines is a supervised learning algorithm and it cannot be used for Fraud Detection.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_04-34-17-28712ebfc08c44511ec7a0a159b8a221.jpg\"><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html</a></p>"}, "correct_response": ["a", "c"], "section": "Modeling", "question_plain": "Which of the following SageMaker unsupervised learning algorithms can be used for Fraud Detection? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820608, "assessment_type": "multi-select", "prompt": {"question": "<p>A social media company is experiencing poor accuracy while training on SageMaker's default built-in image classification algorithm. The Deep Learning team at the company wants to use an Inception neural network architecture rather than a ResNet one.</p><p>Which of the following options represent the most effective way to do this? (Select two)</p><p><br></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. It uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available.</p><p>If you need a different combination of the pre-trained network and the image data on which it has been trained, Amazon SageMaker also supports transfer learning for image classification through the built-in image classification algorithm.</p><p>In Transfer Learning, when you build a new model to classify your original dataset, you reuse the feature extraction part and re-train the classification part with your dataset. Since you don\u2019t have to train the feature extraction part (which is the most complex part of the model), you can train the model with less computational resources and training time.</p><p>For the given use case, you can take an inception v3 network pre-trained on an ImageNet dataset and bundle your own container and import it into Amazon Elastic Container Registry (Amazon ECR). You can also customize the container provided by Amazon SageMaker with our own transfer learning code in the TensorFlow framework. Then you can import this container into Amazon ECR, and use it for model training and inferencing.</p><p><strong>Exam Alert:</strong></p><p><em>If you plan to use GPU devices, make sure that your containers are nvidia-docker compatible. Only the CUDA toolkit should be included on containers. Don't bundle NVIDIA drivers with the docker image.</em></p><p><br></p><p><img src=\"https://lh5.googleusercontent.com/7LOufQ_9LePpP5ZzQ8wyT9k806EUOQSNW1dm8ran3ZrMGPGPvOz_WlXlpZzNQ-oirfInkNp1Mn3Z_Bwn_gVGjFsfx1R-BJvvaFatVic4wUqrT7l3sC6rG1Bm0Uwz4Chqi7SZKQNo\"></p><p>Incorrect options:</p><p><strong>Discard the built-in SageMaker image classification algorithm and use the open source Inception Convolution Neural Network Model on an EC2 instance</strong> - This option has been added as a distractor. There is no need to discard the SageMaker image classification algorithm for the given use case.</p><p><strong>Open a ticket with AWS Support to configure the SageMaker image classification algorithm to use Inception</strong> - This option has been added as a distractor. There is no need to open a ticket with AWS Support to configure the image classification algorithm to use Inception.</p><p><strong>Import the inception network into a SageMaker notebook instance</strong> - Amazon SageMaker notebook instance is a machine learning (ML) compute instance running the Jupyter Notebook App. SageMaker manages creating the instance and related resources. Use Jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to SageMaker hosting, and test or validate your models.</p><p>Importing the inception network into a SageMaker notebook instance will not help, as the requirement is to support the Inception neural network architecture for the SageMaker image classification algorithm which can be done via containers.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/transfer-learning-for-custom-labels-using-a-tensorflow-container-and-bring-your-own-algorithm-in-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/transfer-learning-for-custom-labels-using-a-tensorflow-container-and-bring-your-own-algorithm-in-amazon-sagemaker/</a></p><p><br></p>", "answers": ["<p>Discard the built-in SageMaker image classification algorithm and use the open source Inception Convolution Neural Network Model on an EC2 instance</p>", "<p>Open a ticket with AWS Support to change the default image classification algorithm to Inception</p>", "<p>Import the inception network into a SageMaker notebook instance</p>", "<p>Bundle your own Docker container with TensorFlow Estimator loaded with an Inception network, import it into Amazon Elastic Container Registry (Amazon ECR) and use this for model training</p>", "<p>Customize the container provided by Amazon SageMaker with your own transfer learning code in TensorFlow framework so that the TensorFlow Estimator is loaded with an Inception network, import it into Amazon Elastic Container Registry (Amazon ECR) and use this for model training</p>"]}, "correct_response": ["d", "e"], "section": "ML Implementation and Operations", "question_plain": "A social media company is experiencing poor accuracy while training on SageMaker's default built-in image classification algorithm. The Deep Learning team at the company wants to use an Inception neural network architecture rather than a ResNet one.Which of the following options represent the most effective way to do this? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820512, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An FMCG company has 33 shampoo and 37 conditioner variants in its product portfolio. Senior executives are planning to launch a hybrid product with features from its shampoo and conditioner portfolio. </p><p>Given the lack of reference historical data for this hybrid product, which AWS SageMaker algorithm can help the executives in predicting the product sales over the next financial year?</p>", "relatedLectureIds": "", "answers": ["<p>XGBoost</p>", "<p>Linear Learner</p>", "<p>DeepAR</p>", "<p>Factorization Machines</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>DeepAR</strong></p><p>The Amazon SageMaker DeepAR forecasting algorithm is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series. They then use that model to extrapolate the time series into the future.</p><p>In many applications, however, you have many similar time series across a set of cross-sectional units. For example, you might have time series groupings for demand for different products, server loads, and requests for webpages. For this type of application, you can benefit from training a single model jointly over all of the time series. DeepAR takes this approach. When your dataset contains hundreds of related time series, DeepAR outperforms the standard ARIMA and ETS methods. <em>You can also use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on.</em></p><p>Based on historical data for a behavior, you can predict future behavior using DeepAR algorithm. For example, you can predict sales on a new product based on previous sales data. SageMaker DeepAR algorithm specializes in forecasting new product performance. </p><p>Incorrect options:</p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems. XGBoost cannot be used to forecast new product sales.</p><p><strong>Linear Learner</strong> - Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1. For multiclass classification problems, the labels must be from 0 to num_classes - 1. For regression problems, y is a real number. The algorithm learns a linear function, or, for classification problems, a linear threshold function, and maps a vector x to an approximation of the label y. Linear Learner cannot be used to forecast new product sales.</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. Factorization Machines cannot be used to forecast new product sales.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_05-02-29-3b86b4e906bac312a2de34c4b1a57c4a.jpg\"><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html</a></p>"}, "correct_response": ["c"], "section": "Modeling", "question_plain": "An FMCG company has 33 shampoo and 37 conditioner variants in its product portfolio. Senior executives are planning to launch a hybrid product with features from its shampoo and conditioner portfolio. Given the lack of reference historical data for this hybrid product, which AWS SageMaker algorithm can help the executives in predicting the product sales over the next financial year?", "related_lectures": []}, {"_class": "assessment", "id": 52820514, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following represents a correct statement regarding SageMaker Inference Pipelines?</p>", "relatedLectureIds": "", "answers": ["<p>Inference Pipelines can be used to make either real-time predictions or to process batch transforms</p>", "<p>Inference Pipelines can only be used to make real-time predictions but not to process batch transforms</p>", "<p>Inference Pipelines can only be used to process batch transforms but not to make real-time predictions</p>", "<p>Inference Pipelines can neither be used to process batch transforms nor to make real-time predictions</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Inference Pipelines can be used to make either real-time predictions or to process batch transforms</strong></p><p>An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.</p><p>Inference Pipeline can be considered as an Amazon SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.</p><p>You can use trained models in an inference pipeline to make <em>real-time predictions </em>directly without performing external preprocessing. When you configure the pipeline, you can choose to use the built-in feature transformers already available in Amazon SageMaker. Or, you can implement your own transformation logic using just a few lines of scikit-learn or Spark code.</p><p>To get inferences on an entire dataset you run a <em>batch transform</em> on a trained model. To run inferences on a full dataset, you can use the same inference pipeline model created and deployed to an endpoint for real-time processing in a batch transform job. To run a batch transform job in a pipeline, you download the input data from Amazon S3 and send it in one or more HTTP requests to the inference pipeline model.</p><p>Incorrect options:</p><p><strong>Inference Pipelines can only be used to make real-time predictions but not to process batch transforms</strong></p><p><strong>Inference Pipelines can only be used to process batch transforms but not to make real-time predictions</strong></p><p><strong>Inference Pipelines can neither be used to process batch transforms nor to make real-time predictions</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</a></p>"}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "Which of the following represents a correct statement regarding SageMaker Inference Pipelines?", "related_lectures": []}, {"_class": "assessment", "id": 52820516, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Silicon Valley startup has introduced a new email service that would completely eradicate spam from the inbox. The data scientists at the startup have prepared the following confusion matrix for the underlying model. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-26_14-50-47-32aa5239f0be562e50831096e2bb24ca.png\"></p><p>What is the F1 score for the underlying model?</p>", "relatedLectureIds": "", "answers": ["<p>0.73</p>", "<p>0.84</p>", "<p>0.93</p>", "<p>0.63</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>0.84</strong></p><p>Summary for Precision and Recall:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_05-22-07-138c74112897d35563ad62f2777e5879.jpg\"><p>Summary for F1-Score:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_05-22-30-4b8bf38b1405733a74f529259f27ca7c.jpg\"><p>Precision (P) = (True Positives / (True Positives + False Positives))</p><p>= (8000/(8000+2000)) = 0.8</p><p>Recall (R) = (True Positives / (True Positives + False Negatives))</p><p>= (8000/(8000+1000)) = 0.89</p><p>F1 score = (2 * P * R) / (P+R) = (2 * 0.8 * 0.89) / (0.8+0.89) = 0.84</p><p>Incorrect options:</p><p><strong>0.73</strong></p><p><strong>0.93</strong></p><p><strong>0.63</strong></p><p>These three options are incorrect per the calculations shown above.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html</a></p><p><a href=\"https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\">https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "A Silicon Valley startup has introduced a new email service that would completely eradicate spam from the inbox. The data scientists at the startup have prepared the following confusion matrix for the underlying model. What is the F1 score for the underlying model?", "related_lectures": []}, {"_class": "assessment", "id": 52820518, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A bio-technology company has invented a new drug testing procedure that can identify substance abuse from blood samples within a minute. The law stipulates that anyone found indulging in substance abuse faces a steep fine along with a prison term. </p><p>As an AWS&nbsp;Certified ML&nbsp;Specialist, can you identify the metric that the data scientists at the company need to focus on, so that they can analyze the results of the trials for the underlying model (The model\u2019s predicted value of 1 implies that the individual was predicted to have consumed drugs)?</p>", "relatedLectureIds": "", "answers": ["<p>Accuracy</p>", "<p>Recall</p>", "<p>F1-score</p>", "<p>Specificity</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Specificity</strong></p><p>Specificity = (True Negatives / (True Negatives + False Positives))</p><p>If the model has a high specificity, it implies that all false positives (think of it as false alarms) have been weeded out. In other words, the <em>specificity</em> of a test refers to how well the test identifies those who have not indulged in substance abuse. </p><p>Here is an excellent reference article for a deep-dive on specificity:</p><p><a href=\"https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/\">https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/</a></p><p>Incorrect options:</p><p><strong>Accuracy</strong> - Accuracy measures the percentage of correct predictions.</p><p><strong>Recall</strong> - Recall shows the percentage of actual positives among the total number of relevant instances (actual positives). In other words, how many positive items are selected?</p><p><strong>F1-score</strong> - F1 score is a binary classification metric that considers both binary metrics precision and recall. It is the harmonic mean between precision and recall. The range is 0 to 1. A larger value indicates better predictive accuracy.</p>"}, "correct_response": ["d"], "section": "Modeling", "question_plain": "A bio-technology company has invented a new drug testing procedure that can identify substance abuse from blood samples within a minute. The law stipulates that anyone found indulging in substance abuse faces a steep fine along with a prison term. As an AWS&nbsp;Certified ML&nbsp;Specialist, can you identify the metric that the data scientists at the company need to focus on, so that they can analyze the results of the trials for the underlying model (The model\u2019s predicted value of 1 implies that the individual was predicted to have consumed drugs)?", "related_lectures": []}, {"_class": "assessment", "id": 52820520, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A financial services company has the goal of reducing fraud transactions by 10% over the next financial year. In order to achieve this goal, which of the following is the most relevant model evaluation metric that the data scientists at the company need to focus on (the model\u2019s predicted value of 1 implies that the transaction is predicted to be fraud)?</p>", "relatedLectureIds": "", "answers": ["<p>Precision-Recall&nbsp;Area-Under-Curve (PR AUC)</p>", "<p>F1-score</p>", "<p>Recall</p>", "<p>Precision</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Precision-Recall&nbsp;Area-Under-Curve (PR AUC)</strong></p><p>This is an example where the dataset is imbalanced with fewer instances of positive class because of a fewer number of actual fraud records in the dataset. In such scenarios where we care more about the positive class, using PR AUC is a better choice, which is more sensitive to the improvements for the positive class. </p><p>PR AUC is a curve that combines precision (PPV) and Recall (TPR) in a single visualization. For every threshold, you calculate PPV and TPR and plot it. The higher on y-axis your curve is the better your model performance.</p><p>Please review these excellent resources for a deep-dive into PR AUC:</p><p><a href=\"https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\">https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc</a></p><p><a href=\"https://machinelearningmastery.com/imbalanced-classification-with-the-fraudulent-credit-card-transactions-dataset/\">https://machinelearningmastery.com/imbalanced-classification-with-the-fraudulent-credit-card-transactions-dataset/</a></p><p>Incorrect options:</p><p><strong>F1-score</strong> - F1 score is a binary classification metric that considers both binary metrics precision and recall. It is the harmonic mean between precision and recall. The range is 0 to 1. A larger value indicates better predictive accuracy.</p><p><strong>Recall</strong> - Recall shows the percentage of actual positives among the total number of relevant instances (actual positives). In other words, how many positive items are selected?</p><p><strong>Precision</strong> - Precision shows the percentage of actual positive instances (as opposed to false positives) among those instances that have been retrieved (those predicted to be positive). In other words, how many selected items are positive?</p>"}, "correct_response": ["a"], "section": "Modeling", "question_plain": "A financial services company has the goal of reducing fraud transactions by 10% over the next financial year. In order to achieve this goal, which of the following is the most relevant model evaluation metric that the data scientists at the company need to focus on (the model\u2019s predicted value of 1 implies that the transaction is predicted to be fraud)?", "related_lectures": []}, {"_class": "assessment", "id": 52820522, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data science team at an Email Service Provider has determined that the long term cost of marking a bona fide email as spam is much greater than the cost of marking a spam email as bona fide. To address this issue, the team worked on a classification model to predict if an email is spam and boiled it down to two model variants. Model A had 95% accuracy with 40 False Negatives (FN) and 100 False Positives (FP) whereas model B also had 95% accuracy with 100 FN and 40 FP. </p><p>Which of the two models is more cost effective for the company?</p>", "relatedLectureIds": "", "answers": ["<p>Model A</p>", "<p>Model B</p>", "<p>Both Model A and Model B are equally cost effective, as accuracy is same</p>", "<p>None of the Model A and Model B are cost effective. Company needs to try something different</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Model B</strong></p><p>Summary for Identifying True Positives, True Negatives, False Positives and False Negatives:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_05-55-18-326883489c398acc678d0ef73b142747.jpg\"><p>For the given use-case, the classification model predicts if an email is spam. This implies that a False Positive is very costly for the company because in the case of a False Positive, the model predicted that the email is spam however in reality the email turned out to be bona fide. So the ideal model would focus on reducing the False Positives. Thus Model B is the right choice.</p><p>Incorrect options:</p><p><strong>Model A</strong> - Since Model A&nbsp;has more False Positives compared to Model B, so it is not cost effective compared to Model B.</p><p><strong>Both Model A and Model B are equally cost effective, as accuracy is same</strong> - As explained earlier, even though accuracy is same, but Model A&nbsp;has more False Positives compared to Model B, so it is not cost effective compared to Model B.</p><p><strong>None of the Model A and Model B are cost effective. Company needs to try something different</strong> - This option has been as a distractor.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "The data science team at an Email Service Provider has determined that the long term cost of marking a bona fide email as spam is much greater than the cost of marking a spam email as bona fide. To address this issue, the team worked on a classification model to predict if an email is spam and boiled it down to two model variants. Model A had 95% accuracy with 40 False Negatives (FN) and 100 False Positives (FP) whereas model B also had 95% accuracy with 100 FN and 40 FP. Which of the two models is more cost effective for the company?", "related_lectures": []}, {"_class": "assessment", "id": 52820524, "assessment_type": "multi-select", "prompt": {"question": "<p>After training a SageMaker Linear Learner model over a training dataset, the data science team observed that it achieved high accuracy on the training data, but had low accuracy on the test data. </p><p>As an AWS&nbsp;ML specialist, which of the following techniques are likely to help resolve this problem? (Select three)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Use more training data</strong></p><p><strong>Add regularization to the model</strong></p><p><strong>Use less features in the model</strong></p><p>Your model is <em>underfitting</em> the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is <em>overfitting</em> your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.</p><p>As the model has high accuracy on the training data but low accuracy on the test data, it suggests that the model is overfitting. When a model is overfitting then adding more training data, adding regularization or using less features can help in addressing the underlying problem.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_06-03-10-64739495cc2622854339bf5fe72d2226.jpg\"></p><p><br></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/q_and_a/2022-02-22_13-39-35-ed5664e9ba0fa594b48a6346435ff9c8.png\"></p><p><br></p><p>Incorrect options:</p><p><strong>Use more features in the model</strong></p><p><strong>Remove regularization from the model</strong></p><p><strong>Use less training data</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p><p><a href=\"https://zhu45.org/posts/2017/Jul/21/andrew-ngs-ml-week-06-11/#overfitting-vs-underfitting\">https://zhu45.org/posts/2017/Jul/21/andrew-ngs-ml-week-06-11/#overfitting-vs-underfitting</a></p>", "answers": ["<p>Use more features in the model</p>", "<p>Use more training data</p>", "<p>Add regularization to the model</p>", "<p>Remove regularization from the model</p>", "<p>Use less training data</p>", "<p>Use less features in the model</p>"]}, "correct_response": ["b", "c", "f"], "section": "Modeling", "question_plain": "After training a SageMaker Linear Learner model over a training dataset, the data science team observed that it achieved high accuracy on the training data, but had low accuracy on the test data. As an AWS&nbsp;ML specialist, which of the following techniques are likely to help resolve this problem? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820526, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An online retail company specializing in fashion wear wants to automate the various categories of fashion wear in their catalog. They have about 50,000 images in their product catalog but none of them have the right labels for the associated categories. </p><p>As an ML Specialist, which of the following solutions will you recommend to build the training data with the correct labels?</p>", "relatedLectureIds": "", "answers": ["<p>Use SageMaker Image Classification to create the category labels for the training images</p>", "<p>Use SageMaker Semantic Segmentation to create the category labels for the training images</p>", "<p>Use SageMaker Ground Truth to create the category labels for the training images</p>", "<p>Use Amazon Rekognition to create the category labels for the training images</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use SageMaker Ground Truth to create the category labels for the training images</strong></p><p>Amazon SageMaker Ground Truth is a fully managed data labeling service that makes it easy to build highly accurate training datasets for machine learning. Amazon SageMaker Ground Truth makes it easy for you to efficiently and accurately label the datasets required for training machine learning systems. SageMaker Ground Truth can automatically label a portion of the dataset based on the labels done manually by human labelers.</p><p>So, Ground Truth is the correct service for this use-case.</p><p>Incorrect options:</p><p><strong>Use SageMaker Image Classification to create the category labels for the training images</strong> - The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. It uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available. Image Classification cannot be used to create labels for training data. </p><p><strong>Use SageMaker Semantic Segmentation to create the category labels for the training images</strong> - The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing. Semantic Segmentation cannot be used to create labels for training data. </p><p><strong>Use Amazon Rekognition to create the category labels for the training images - </strong>Amazon Rekognition is a service that makes it easy to add powerful visual analysis to your applications. Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them. Rekognition cannot be used to create labels for training data. </p><p>Reference:</p><p><a href=\"https://aws.amazon.com/sagemaker/groundtruth/\">https://aws.amazon.com/sagemaker/groundtruth/</a></p>"}, "correct_response": ["c"], "section": "Modeling", "question_plain": "An online retail company specializing in fashion wear wants to automate the various categories of fashion wear in their catalog. They have about 50,000 images in their product catalog but none of them have the right labels for the associated categories. As an ML Specialist, which of the following solutions will you recommend to build the training data with the correct labels?", "related_lectures": []}, {"_class": "assessment", "id": 52820528, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A social media company has a huge repository of unindexed images, text, audio, and video files. The company wants to index these files in order to enable the development team to quickly identify relevant information. The company wishes to use machine learning to expedite the work of its development team as it has minimal experience with machine learning.</p><p>Which of the following represents the FASTEST solution for indexing the files?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Leverage Amazon Rekognition, Amazon Comprehend, and Amazon Transcribe to tag data into distinct categories/classes</strong></p><p>Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p>Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them.</p><p>Thus, Rekognition can be used for analyzing the images/videos and then tag data into distinct categories/classes</p><p><img src=\"https://lh4.googleusercontent.com/DSP_p4La78GnrFQqQf11O4SE9f4y2vBIDEBxNuSyfoiIObLaDa-bqew5X4sWnuNphHG0HAbfNJ2HgBcLRv2uJdsH0w9V3K-EW9jTZWtC1rMYYVNEkq_3UInT5sBvmq9gNCuVvjiC\"></p><p>Amazon Transcribe makes it easy for developers to add speech to text capabilities to their applications. Amazon Transcribe uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately. Amazon Transcribe can be used to transcribe customer service calls, automate subtitling, and generate metadata for media assets to create a fully searchable archive. You can use Amazon Transcribe Medical to add medical speech to text capabilities to clinical documentation applications.</p><p>Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. The service can identify critical elements in data, including references to language, people, and places, and the text files can be categorized by relevant topics. In real time, you can automatically and accurately detect customer sentiment in your content. This accelerates more informed, real-time decision making to improve customer experiences.</p><p><img src=\"https://lh6.googleusercontent.com/vIECdqt3x2FDjKl_X8G5JwDIIEi3R_g1fz0hGFHRQKa8juwG-41KnlO8N7T56BGEvGXWGEghIi2vJdv5bmhGd2ptKhNSu1cF3AD5PyRogAHXf3ZIZiWaMybtFRJ7_ghst37qXI_1\"></p><p>For the given use-case, the audio can be transcribed and then fed into the Comprehend service for sentiment analysis and then tag this data into distinct categories/classes. The text files can be directly analyzed via Comprehend for tagging data into distinct categories/classes.</p><p>Incorrect options:</p><p><strong>Set up Mechanical Turk Human Intelligence Tasks to label all footage</strong> - Amazon Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. MTurk tasks are human led, so these would be slow paced, therefore this option is not the right fit for the given use case.</p><p><img src=\"https://lh4.googleusercontent.com/R-U5pXsP9zQRSacAh00hWxCXA8ScJt_eKdDOz56ak2DmExTIiy6o_rgDJO7-1nwfiekWpGxsKRT9nCwEcXD5bGjthTGBZeZOPfpJzO5LDp2qhBnQFKpUCZkju6aq56AJFupHSzOQ\"></p><p><strong>Leverage Amazon Transcribe to convert speech to text. Use the Amazon SageMaker Neural Topic Model (NTM), Image Classification and Object Detection algorithms to tag data into distinct categories/classes</strong> - Amazon SageMaker algorithms require custom code development, testing and deployment. Therefore this option does not represent the fastest solution for the given use case.</p><p><strong>Create your own custom models for audio transcription and topic modeling, and use object detection to tag data into distinct categories/classes</strong> - Developing custom models for audio transcription, topic modeling and object detection do not represent the fastest solution for the given use case.</p><p>References:</p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><a href=\"https://www.mturk.com/\">https://www.mturk.com/</a></p><p><a href=\"https://aws.amazon.com/transcribe/\">https://aws.amazon.com/transcribe/</a></p><p><a href=\"https://aws.amazon.com/comprehend/\">https://aws.amazon.com/comprehend/</a></p><p><br></p>", "answers": ["<p>Set up Mechanical Turk Human Intelligence Tasks to label all footage</p>", "<p>Leverage Amazon Transcribe to convert speech to text. Use the Amazon SageMaker Neural Topic Model (NTM), Image Classification and Object Detection algorithms to tag data into distinct categories/classes</p>", "<p>Create your own custom models for audio transcription and topic modeling, and use object detection to tag data into distinct categories/classes</p>", "<p>Leverage Amazon Rekognition, Amazon Comprehend, and Amazon Transcribe to tag data into distinct categories/classes</p>"]}, "correct_response": ["d"], "section": "Data Engineering", "question_plain": "A social media company has a huge repository of unindexed images, text, audio, and video files. The company wants to index these files in order to enable the development team to quickly identify relevant information. The company wishes to use machine learning to expedite the work of its development team as it has minimal experience with machine learning.Which of the following represents the FASTEST solution for indexing the files?", "related_lectures": []}, {"_class": "assessment", "id": 52820530, "assessment_type": "multi-select", "prompt": {"question": "<p>After training a SageMaker XGBoost based model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data. </p><p>As an AWS Certified ML Specialist, which of the following techniques would you recommend to help resolve this problem? (Select two):</p>", "relatedLectureIds": "", "answers": ["<p>Use more features in the model</p>", "<p>Use more training data</p>", "<p>Add regularization to the model</p>", "<p>Remove regularization from the model</p>", "<p>Use less training data</p>", "<p>Use less features in the model</p>"], "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Use more features in the model</strong></p><p><strong>Remove regularization from the model</strong></p><p>Your model is <em>underfitting</em> the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is <em>overfitting</em> your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.</p><p>For the given use case, as the model has low accuracy on the training data as well as low accuracy on the test data, it suggests that the model has a bias, or in other words, the model is underfitting. When a model is underfitting, then adding more features to the model or removing regularization can help in addressing the underlying problem. In case of an underfitting model, adding more training data may or may not help.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_06-35-59-bb7754a2fdbac75f595a3e894cb0de5d.jpg\"><p>Incorrect options:</p><p><strong>Use more training data</strong></p><p><strong>Add regularization to the model</strong></p><p><strong>Use less training data</strong></p><p><strong>Use less features in the model</strong></p><p>These four options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p>"}, "correct_response": ["a", "d"], "section": "Modeling", "question_plain": "After training a SageMaker XGBoost based model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data. As an AWS Certified ML Specialist, which of the following techniques would you recommend to help resolve this problem? (Select two):", "related_lectures": []}, {"_class": "assessment", "id": 52820532, "assessment_type": "multi-select", "prompt": {"question": "<p>The ML team at a research lab has trained a Deep Neural Network using a huge training dataset. After a series of training runs, the team observes that the training error is much lower than the test error. </p><p>Which of the following steps would you recommend to address this issue? (Select three)</p>", "relatedLectureIds": "", "answers": ["<p>Use early stopping while training</p>", "<p>Remove parameter regularization</p>", "<p>Do not use early stopping while training</p>", "<p>Use dropout</p>", "<p>Add parameter regularization</p>"], "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Use early stopping while training</strong></p><p>Early Stopping is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space. Thus it helps prevent overfitting.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_06-48-24-589e77406a554e17e41d5711d6ebdef6.jpg\"><p><strong>Use dropout</strong></p><p>Dropout is a method in Deep Learning where randomly selected neurons are dropped during training. They are \u201cdropped-out\u201d arbitrarily. A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data. Dropout is an approach to regularization in neural networks which helps reducing interdependent learning amongst the neurons.</p><p><strong>Add parameter regularization</strong></p><p>Regularization refers to a set of different techniques that lower the complexity of a neural network model during training, and thus prevent the overfitting. L1 regularization forces the weight parameters to become zero. L2 regularization forces the weight parameters towards zero (but never exactly zero)</p><p>Highly recommend to review the following resource for a deep-dive on preventing overfitting for Deep Learning based solutions: </p><p><a href=\"https://www.jeremyjordan.me/deep-neural-networks-preventing-overfitting/\">https://www.jeremyjordan.me/deep-neural-networks-preventing-overfitting/</a></p><p>Incorrect options:</p><p><strong>Remove parameter regularization</strong></p><p><strong>Do not use early stopping while training</strong></p><p>These two options contradict the explanation provided above, so these options are incorrect.</p>"}, "correct_response": ["a", "d", "e"], "section": "Modeling", "question_plain": "The ML team at a research lab has trained a Deep Neural Network using a huge training dataset. After a series of training runs, the team observes that the training error is much lower than the test error. Which of the following steps would you recommend to address this issue? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820534, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer is trying to develop a linear regression model and the following represents the residual plot (residuals on the y axis and the independent variable on the x axis) for the model:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/quiz_question/2022-03-25_07-13-43-66dcd9099224a618ac7099351632ba4b.png\"></p><p>Given the above residual plot, what would you attribute as the MOST LIKELY reason behind the model's failure?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Linear regression is not the right choice for the underlying model and the residuals do not have constant variance</strong></p><p>Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).</p><p>Residual is the error between a predicted value and the observed actual value.</p><p>The most important assumption of a linear regression model is that the residuals are independent and normally distributed. This implies that if we capture all of the predictive information, all that is left behind (residuals) should be completely random &amp; unpredictable. Hence, we want our residuals to follow a normal distribution.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2022-03-26_02-49-10-abf3a5a2c1d3515264fbab8e45770f4a.jpg\"><p>For the given use case, we can see that the&nbsp; residuals do not have constant variance. If the residuals do not form a zero-centered bell shape, there is some structure in the model\u2019s prediction error. Adding more variables to the model might help the model capture the pattern that is not captured by the current model. Therefore we can say that the linear regression is not the right choice for the underlying model.</p><p>Incorrect options:</p><p><strong>Linear regression is the right choice for the underlying model and the residuals have constant variance</strong></p><p><strong>Linear regression is not the right choice for the underlying model and the residuals have constant variance</strong></p><p>We can see from the plot that the residuals do not have constant variance, so both these options are incorrect.</p><p><strong>Linear regression is the right choice for the underlying model but the residuals do not have constant variance </strong>- As explained earlier in the explanation, linear regression is not the right choice for the underlying model, so this option is incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html</a></p><p><a href=\"https://www.originlab.com/doc/origin-help/residual-plot-analysis\">https://www.originlab.com/doc/origin-help/residual-plot-analysis</a></p><p><br></p>", "answers": ["<p>Linear regression is not the right choice for the underlying model and the residuals do not have constant variance</p>", "<p>Linear regression is the right choice for the underlying model but the residuals do not have constant variance</p>", "<p>Linear regression is the right choice for the underlying model and the residuals have constant variance</p>", "<p>Linear regression is not the right choice for the underlying model and the residuals have constant variance</p>"]}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "A machine learning engineer is trying to develop a linear regression model and the following represents the residual plot (residuals on the y axis and the independent variable on the x axis) for the model:Given the above residual plot, what would you attribute as the MOST LIKELY reason behind the model's failure?", "related_lectures": []}, {"_class": "assessment", "id": 52820536, "assessment_type": "multi-select", "prompt": {"question": "<p>A healthcare company runs machine learning (ML) models on sensitive health data for its customers and the company is concerned about data security. The company is looking out for a solution to protect the data at rest as well as in-transit while running the Machine Learning specific processes on this data. The company uses Amazon SageMaker as its primary data science environment.</p><p>As an AWS&nbsp;Certified Machine Learning Specialist, which of the following options would you combine to address the given use case? (Select three.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Connect to the SageMaker API or to the SageMaker Runtime through an interface endpoint in your Virtual Private Cloud (VPC) </strong></p><p>A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, such as SageMaker, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC.</p><p>An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet. It serves as an entry point for traffic destined to a service that is owned by AWS or owned by an AWS customer or partner. </p><p>A gateway endpoint is a gateway that is a target for a route in your route table used for traffic destined to either Amazon S3 or DynamoDB.</p><p>You can connect to the SageMaker API or to the SageMaker Runtime through an interface endpoint in your VPC instead of connecting over the internet. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2022-03-26_11-09-53-1e741cf7d3b496845758e5ddbdbc0d04.jpg\"></p><p><strong>Enable network isolation for training jobs and models</strong></p><p>If you use an Amazon VPC by specifying a value for the VpcConfig parameter when you call CreateTrainingJob, CreateHyperParameterTuningJob, or CreateModel, you can protect your data and resources by managing security groups and restricting internet access from your VPC. However, this comes at the cost of additional network configuration, and has the risk of configuring your network incorrectly. If you do not want SageMaker to provide external network access to your training or inference containers, you can enable network isolation.</p><p><strong>Use AWS Key Management Service (AWS KMS) to manage encryption keys for encrypting data at rest and use TLS&nbsp;for encrypting data in transit</strong></p><p>You can use AWS KMS to create and control the cryptographic keys used to protect your data. . AWS KMS integrates with services such as SageMaker to let the customers control the lifecycle of and permissions on the keys used to encrypt data on the customer\u2019s behalf. Because access to encrypt or decrypt the data within the service is independently controlled by AWS KMS policies under the customer\u2019s control, customers can isolate control over access to the data, from access to the keys. You can use TLS&nbsp;for encrypting data in transit.</p><p>Incorrect options:</p><p><strong>Use Permissions boundary to restrict access to SageMaker</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p><p>This option is a distractor as the use case is about protecting data at-rest and in-transit while using SageMaker and not about restricting access to the SageMaker service itself.</p><p><strong>Leverage IAM&nbsp;policies to deny root access on the SageMaker notebook instances - </strong></p><p>The credentials of the account owner allow full access to all resources in the account. You cannot use IAM policies to explicitly deny the root user access to resources. You can only use an AWS Organizations service control policy (SCP) to limit the permissions of the root user. Therefore this option is incorrect.</p><p><strong>Connect to the SageMaker API or to the SageMaker Runtime through a gateway endpoint in your Virtual Private Cloud (VPC)</strong> - You can connect to the SageMaker API or to the SageMaker Runtime through an interface endpoint in your VPC and not through a gateway endpoint. </p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-protection.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-protection.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/interface-vpc-endpoint.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/logical-separation/encrypting-data-at-rest-and--in-transit.html\">https://docs.aws.amazon.com/whitepapers/latest/logical-separation/encrypting-data-at-rest-and--in-transit.html</a></p><p><a href=\"https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html\">https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html</a></p>", "answers": ["<p>Connect to the SageMaker API or to the SageMaker Runtime through an interface endpoint in your Virtual Private Cloud (VPC) </p>", "<p>Use Permissions boundary to restrict access to SageMaker</p>", "<p>Enable network isolation for training jobs and models</p>", "<p>Leverage IAM&nbsp;policies to deny root access on the SageMaker notebook instances</p>", "<p>Connect to the SageMaker API or to the SageMaker Runtime through a gateway endpoint in your Virtual Private Cloud (VPC) </p>", "<p>Use AWS Key Management Service (AWS KMS) to manage encryption keys for encrypting data at rest and use TLS&nbsp;for encrypting data in transit. </p>"]}, "correct_response": ["a", "c", "f"], "section": "ML Implementation and Operations", "question_plain": "A healthcare company runs machine learning (ML) models on sensitive health data for its customers and the company is concerned about data security. The company is looking out for a solution to protect the data at rest as well as in-transit while running the Machine Learning specific processes on this data. The company uses Amazon SageMaker as its primary data science environment.As an AWS&nbsp;Certified Machine Learning Specialist, which of the following options would you combine to address the given use case? (Select three.)", "related_lectures": []}, {"_class": "assessment", "id": 52820538, "assessment_type": "multi-select", "prompt": {"question": "<p>A research group at a university is leveraging census data to ascertain healthcare and social program requirements by province and city to develop targeted healthcare improvement initiatives for population segments. Each respondent has provided answers to around 300 questions on the census form.</p><p>Which combination of algorithms would you use to develop the necessary insights? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Principal Component Analysis (PCA)</strong></p><p>Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2022-03-27_12-06-14-242e8edc707937eaa98e9c3d58a4125d.png\"><p>via - <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p><p>You can think of PCA as taking many features and combining similar or redundant features together to form a new, smaller feature set. This allows you to reduce the number of variables being considered from 300 variables in the census form to a much smaller feature set that can be used for downstream segment analysis using k-means.</p><p><strong>k-means</strong></p><p>K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity.</p><p>Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time.</p><p>To build a solution for the given use case, you will consume the output from the PCA&nbsp;stage and use the resultant smaller feature set to find similar population segments (groupings)&nbsp;and use that as the basis to develop the necessary insights for creating the targeted healthcare improvement initiatives.</p><p>Incorrect options:</p><p><strong>Latent Dirichlet Allocation (LDA)</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus.</p><p>LDA is used for topic modeling, so it is not the right fit for the given use case.</p><p><strong>Factorization Machines (FM)</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category.</p><p>Factorization Machines cannot be used for finding similar population segments (groupings).</p><p><strong>Random Cut Forest (RCF) </strong>-<strong> </strong>Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. RCF&nbsp;cannot be used for the given use case.</p><p>References: </p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p><p><br></p>", "answers": ["<p>Latent Dirichlet Allocation (LDA)</p>", "<p>Factorization Machines (FM) </p>", "<p>Principal Component Analysis (PCA)</p>", "<p>Random Cut Forest (RCF)</p>", "<p>k-means</p>"]}, "correct_response": ["c", "e"], "section": "Modeling", "question_plain": "A research group at a university is leveraging census data to ascertain healthcare and social program requirements by province and city to develop targeted healthcare improvement initiatives for population segments. Each respondent has provided answers to around 300 questions on the census form.Which combination of algorithms would you use to develop the necessary insights? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820540, "assessment_type": "multi-select", "prompt": {"question": "<p>Which of the following options represent the mandatory parameters that must be provided while submitting Amazon SageMaker training tasks that use one of the built-in algorithms? (Select three)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Provide the Amazon Resource Name (ARN) of an IAM role that SageMaker can assume to perform tasks on your behalf</strong></p><p>You must provide the Amazon Resource Name (ARN) that SageMaker assumes to perform tasks on your behalf during model training. You must grant this role the necessary permissions so that SageMaker can successfully complete model training. You can use the mandatory RoleArn parameter to configure this option.</p><p><strong>Specify the resources, ML compute instances, and ML storage volumes to deploy for model training</strong></p><p>You need to specify the resources, ML compute instances, and ML storage volumes to deploy for model training. In distributed training, you specify more than one instance. ML storage volumes store model artifacts and incremental states. Training algorithms might also use ML storage volumes for scratch space. If you want SageMaker to use the ML storage volume to store the training data, choose File as the TrainingInputMode in the algorithm specification. For distributed training algorithms, specify an instance count greater than 1. You can use the mandatory ResourceConfig parameter to configure this option.</p><p><strong>Specify the output path on an Amazon S3 bucket where the trained model will persist</strong></p><p>Specifies the path to the S3 location where you want to store model artifacts. SageMaker creates subfolders for the artifacts. You can use the mandatory OutputDataConfig parameter to configure this option.</p><p>Training a model with SageMaker:</p><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2022-03-29_06-22-43-c41ed23bb1065a9de5f462a4efb1b66f.jpg\"><p><br></p><p><br></p><p>Incorrect options:</p><p><strong>Identify the training dataset and the Amazon S3, EFS, or FSx location where it is stored</strong></p><p><strong>Specify the validation channel identifying the location of validation data on an Amazon S3 bucket</strong></p><p>Algorithms can accept input data from one or more channels. For example, an algorithm might have two channels of input data, training_data and validation_data. The configuration for each channel provides the S3, EFS, or FSx location where the input data is stored. It also provides information about the stored data: the MIME type, compression method, and whether the data is wrapped in RecordIO format. SageMaker uses the InputDataConfig parameter to Identify the training dataset and the Amazon S3, EFS, or FSx location where it is stored. This parameter is not mandatory, so both these options are incorrect.</p><p><strong>Specify the hyperparameters in a JSON array as documented for the algorithm</strong> - Hyperparameters are the algorithm-specific parameters that influence the quality of the model. You set hyperparameters before you start the learning process. SageMaker uses the HyperParameters parameter for this option and it is not mandatory. </p><p><br></p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html\">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-algo-train.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-algo-train.html</a></p><p><br></p>", "answers": ["<p>Provide the Amazon Resource Name (ARN) of an IAM role that SageMaker can assume to perform tasks on your behalf</p>", "<p>Identify the training dataset and the Amazon S3, EFS, or FSx location where it is stored</p>", "<p>Specify the resources, ML compute instances, and ML storage volumes to deploy for model training</p>", "<p>Specify the output path on an Amazon S3 bucket where the trained model will persist</p>", "<p>Specify the validation channel identifying the location of validation data on an Amazon S3 bucket</p>", "<p>Specify the hyperparameters in a JSON array as documented for the algorithm</p>"]}, "correct_response": ["a", "c", "d"], "section": "ML Implementation and Operations", "question_plain": "Which of the following options represent the mandatory parameters that must be provided while submitting Amazon SageMaker training tasks that use one of the built-in algorithms? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820542, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has one year of unlabeled data on demographics and sales for existing customers. The digital marketing executives at the company want to identify potential customers on social media. As the festive season is coming up, the solution needs to be built in the shortest possible time. </p><p>What is the best course of action to meet this objective?</p>", "relatedLectureIds": "", "answers": ["<p>Use K-means to identify groups of customers and then find similar customers on social media</p>", "<p>Use Linear Regression to predict the best matching customers from their social media profiles</p>", "<p>Use XGBoost to predict the best matching customers from their social media profiles</p>", "<p>Use Recommendation Systems to build the existing customer profiles and then predict the best matching customers from their social media profiles</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use K-means to identify groups of customers and then find similar customers on social media</strong></p><p>K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity.</p><p>Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time.</p><p>For the given use-case, you can use K-means to identify groups of customers and then find similar customers on social media. K-means, being an unsupervised algorithm, is the right choice, since there is no labeled data available.</p><p>Incorrect options:</p><p><strong>Use Linear Regression to predict the best matching customers from their social media profiles</strong> - Linear Regression is a machine learning algorithm based on <em>supervised learning.</em> It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. </p><p><strong>Use XGBoost to predict the best matching customers from their social media profiles</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a <em>supervised learning</em> algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p>Given that the solution needs to be built in the shortest possible time, as there is no labeled data (for both customers and non-customers) to be used for supervised learning techniques such as Linear Regression and XGBoost, these two are ruled out. </p><p><strong>Use Recommendation Systems to build the existing customer profiles and then predict the best matching customers from their social media profiles</strong></p><p>Recommendation Systems also need significant time investment for either collaborative-filtering or content-based approaches. </p><p>Content-based filtering is one of the simplest systems, but sometimes is still useful. It is based on known user preferences provided explicitly or implicitly, and data about item features (such as categories to which items belong).</p><p>Collaborative filtering is based on (<em>user, item, rating</em>) tuples. So, unlike content-based filtering, it leverages other users\u2019 experiences. The main concept behind collaborative filtering is that users with similar tastes (based on observed user-item interactions) are more likely to have similar interactions with items they haven\u2019t seen before.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p>"}, "correct_response": ["a"], "section": "Modeling", "question_plain": "A company has one year of unlabeled data on demographics and sales for existing customers. The digital marketing executives at the company want to identify potential customers on social media. As the festive season is coming up, the solution needs to be built in the shortest possible time. What is the best course of action to meet this objective?", "related_lectures": []}, {"_class": "assessment", "id": 52820610, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Machine Learning engineer is running an Amazon SageMaker endpoint on a P3 instance and using the built-in SageMaker object detection algorithm to make real-time object detections in a custom application. Upon analysis of the model's resource consumption, it is noticed that the model is only using a portion of the GPU.</p><p>As an AWS Certified Machine Learning Specialist, which architectural improvements would you recommend to maximize the use of provided resources?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Redeploy the model on an M4 instance. Attach Amazon Elastic Inference to the instance</strong></p><p>Since the use case states that the model is only using a portion of the GPU, so you can deploy the model on an M4 instance which will cost much less than a P3 instance and allow for optimal utilization of the instance.</p><p>Furthermore, you can leverage Amazon Elastic Inference which allows you to attach low-cost GPU-powered acceleration to Amazon EC2 and Sagemaker instances or Amazon ECS tasks, to reduce the cost of running deep learning inference by up to 75%</p><p>Incorrect options:</p><p><strong>Redeploy the model as a batch transform job on an M4 instance</strong> - The use case is related to making real-time object detections in a custom application, so using batch transform jobs is ruled out.</p><p>You can use Batch Transform to preprocess entire datasets quickly or to get inferences from a trained model for large datasets when you don't need a persistent endpoint for real-time predictions.</p><p><strong>Redeploy the model on an P3 family instance optimized for distributed machine learning such as P3dn</strong> - Since the use case states that the model is only using a portion of the GPU, so it does not make sense to transition to an instance optimized for distributed machine learning such as P3dn, as it would worsen the original issue of the sub-optimal usage of resources.</p><p><strong>Deploy the model onto an Amazon Elastic Container Service (Amazon ECS) cluster using a P3 instance </strong>- This option has been added as a distractor as you still end up using a P3 instances which will not address the original issue of the sub-optimal usage of resources.</p><p><br></p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html</a></p><p><a href=\"https://aws.amazon.com/sagemaker/pricing/\">https://aws.amazon.com/sagemaker/pricing/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/10/introducing-amazon-sagemaker-mlp3dn24xlarge-instances/\">https://aws.amazon.com/about-aws/whats-new/2019/10/introducing-amazon-sagemaker-mlp3dn24xlarge-instances/</a></p>", "answers": ["<p>Redeploy the model as a batch transform job on an M4 instance</p>", "<p>Redeploy the model on an P3 family instance optimized for distributed machine learning such as P3dn</p>", "<p>Redeploy the model on an M4 instance. Attach Amazon Elastic Inference to the instance</p>", "<p>Deploy the model onto an Amazon Elastic Container Service (Amazon ECS) cluster using a P3 instance</p>"]}, "correct_response": ["c"], "section": "ML Implementation and Operations", "question_plain": "A Machine Learning engineer is running an Amazon SageMaker endpoint on a P3 instance and using the built-in SageMaker object detection algorithm to make real-time object detections in a custom application. Upon analysis of the model's resource consumption, it is noticed that the model is only using a portion of the GPU.As an AWS Certified Machine Learning Specialist, which architectural improvements would you recommend to maximize the use of provided resources?", "related_lectures": []}, {"_class": "assessment", "id": 52820544, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following represents the correct definitions for the terms used in the Latent Dirichlet Allocation algorithm?</p>", "relatedLectureIds": "", "answers": ["<p>Observations are referred to as vocabulary. The feature set is referred to as documents. A feature is referred to as a topic. And the resulting categories are referred to as words</p>", "<p>Observations are referred to as documents. The feature set is referred to as vocabulary. A feature is referred to as a word. And the resulting categories are referred to as topics</p>", "<p>Observations are referred to as topics. The feature set is referred to as word. A feature is referred to as a vocabulary. And the resulting categories are referred to as documents</p>", "<p>Observations are referred to as words. The feature set is referred to as vocabulary. A feature is referred to as a document. And the resulting categories are referred to as topics</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Observations are referred to as documents. The feature set is referred to as vocabulary. A feature is referred to as a word. And the resulting categories are referred to as topics</strong></p><p>The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_13-34-48-4facc42250108e1650c15c81cc6cc05a.jpg\"></p><p>Incorrect options:</p><p><strong>Observations are referred to as vocabulary. The feature set is referred to as documents. A feature is referred to as a topic. And the resulting categories are referred to as words</strong></p><p><strong>Observations are referred to as topics. The feature set is referred to as word. A feature is referred to as a vocabulary. And the resulting categories are referred to as documents</strong></p><p><strong>Observations are referred to as words. The feature set is referred to as vocabulary. A feature is referred to as a document. And the resulting categories are referred to as topics</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "Which of the following represents the correct definitions for the terms used in the Latent Dirichlet Allocation algorithm?", "related_lectures": []}, {"_class": "assessment", "id": 52820546, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data science team at an analytics company is working on a dataset with a large number of observations and features. They want to use the SageMaker Principal Component Analysis (PCA) Algorithm to reduce the dimensionality within the dataset. </p><p>Which mode should be used for using PCA on this dataset?</p>", "relatedLectureIds": "", "answers": ["<p>Use PCA in randomized mode</p>", "<p>Use PCA in regular mode</p>", "<p>Use PCA in either regular or randomized mode</p>", "<p>PCA is not the right selection for this use-case</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use PCA in randomized mode</strong></p><p>For datasets with a large number of observations and features, you should use PCA in randomized mode as this mode uses an approximation algorithm.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_13-38-51-e6c543f47d66a9a6792f077a54bf3635.jpg\"><p>Incorrect options:</p><p><strong>PCA is not the right selection for this use-case</strong> - This option has been added as a distractor.</p><p><strong>Use PCA in regular mode</strong></p><p><strong>Use PCA in either regular or randomized mode</strong></p><p>Both these options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p>"}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "The data science team at an analytics company is working on a dataset with a large number of observations and features. They want to use the SageMaker Principal Component Analysis (PCA) Algorithm to reduce the dimensionality within the dataset. Which mode should be used for using PCA on this dataset?", "related_lectures": []}, {"_class": "assessment", "id": 52820548, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following SageMaker algorithms can be used both as a built-in-algorithm as well as a framework to run your own customized scripts?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>XGBoost</strong></p><p>The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p>With SageMaker, you can use XGBoost as a built-in algorithm or framework. By using XGBoost as a framework, you have more flexibility and access to more advanced scenarios, such as k-fold cross-validation, because you can customize your own training scripts.</p><p>Use XGBoost as a framework to run your customized training scripts that can incorporate additional data processing into your training jobs. In the following code example, you can find how SageMaker Python SDK provides the XGBoost API as a framework in the same way it provides other framework APIs, such as TensorFlow, MXNet, and PyTorch.</p><p>Incorrect options:</p><p><strong>Linear Learner</strong> - Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1. For multiclass classification problems, the labels must be from 0 to num_classes - 1. For regression problems, y is a real number. The algorithm learns a linear function, or, for classification problems, a linear threshold function, and maps a vector x to an approximation of the label y. </p><p>The Amazon SageMaker linear learner algorithm provides a solution for both classification and regression problems. With the SageMaker algorithm, you can simultaneously explore different training objectives and choose the best solution from a validation set.</p><p>Linear learner cannot be used as a framework to run your own customized scripts.</p><p><strong>Object2Vec</strong> - Object2Vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space.</p><p>Object2Vec cannot be used as a framework to run your own customized scripts.</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p>Factorization Machines cannot be used as a framework to run your own customized scripts.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a></p>", "answers": ["<p>Linear Learner</p>", "<p>Object2Vec</p>", "<p>Factorization Machines</p>", "<p>XGBoost</p>"]}, "correct_response": ["d"], "section": "ML Implementation and Operations", "question_plain": "Which of the following SageMaker algorithms can be used both as a built-in-algorithm as well as a framework to run your own customized scripts?", "related_lectures": []}, {"_class": "assessment", "id": 52820550, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A research agency is developing a robotic submarine to map the marine life forms in the pacific ocean. The robot should be able to classify the images of marine life forms in an autonomous way with low latency. </p><p>Which Amazon SageMaker architecture would you recommend for this use-case?</p>", "relatedLectureIds": "", "answers": ["<p>Use Kinesis Video Streams to classify the images of the marine life forms</p>", "<p>Use AWS Rekognition to classify the images of the marine life forms</p>", "<p>Use SageMaker Neo to compile and package the classification model on the underlying runtime infrastructure on the robotic device</p>", "<p>Use Kinesis Data Streams to process the video stream and invoke a lambda to infer via a classification model to classify the images of the marine life forms</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use SageMaker Neo to compile and package the classification model on the underlying runtime infrastructure on the robotic device</strong></p><p>Neo is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and <em>at the edge</em>.</p><p>Since the robot should be able to classify the images of marine life forms in an autonomous way with low latency, Neo is the right fit for the given use case. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_13-52-09-6a7a547b7323967dbe08889dadf94143.jpg\"></p><p>Incorrect options:</p><p><strong>Use Kinesis Video Streams to classify the images of the marine life forms - </strong>Amazon Kinesis Video Streams makes it easy to securely stream media from connected devices to AWS for storage, analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming media from millions of devices. It durably stores, encrypts, and indexes media in your streams, and allows you to access your media through easy-to-use APIs.</p><p>How Kinesis Video Streams Work:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_14-06-46-70122bc04c0fa1d07ef72d5d189dea1f.jpg\"><p><strong>Use AWS Rekognition to classify the images of the marine life forms</strong> - Amazon Rekognition is a service that makes it easy to add powerful visual analysis to your applications. Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them.</p><p><strong>Use Kinesis Data Streams to process the video stream and invoke a lambda to infer via a classification model to classify the images of the marine life forms</strong> - Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources. Within seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.</p><p>These three options, that use Kinesis Video Streams, Rekognition and Kinesis Data Streams respectively, are over-the-air options, hence ruled out for the given use-case. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a></p>"}, "correct_response": ["c"], "section": "ML Implementation and Operations", "question_plain": "A research agency is developing a robotic submarine to map the marine life forms in the pacific ocean. The robot should be able to classify the images of marine life forms in an autonomous way with low latency. Which Amazon SageMaker architecture would you recommend for this use-case?", "related_lectures": []}, {"_class": "assessment", "id": 52820552, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data science team at an email marketing company has created a data lake with raw and refined zones. The raw zone has the data as it arrives from the source, however, the team wants to de-duplicate the data before it is written into the refined zone. </p><p>What is the best way to accomplish this with the least amount of development time and infrastructure maintenance effort?</p>", "relatedLectureIds": "", "answers": ["<p>Create an Apache Spark based EMR job and run it once a day to de-duplicate the records from raw zone into refined zone</p>", "<p>Invoke an AWS Glue ML Transforms job when new data arrives into raw zone so that de-duplicated records can be written into the refined zone</p>", "<p>Create a Lambda function with the code to handle all possible data duplication use cases. Trigger a Lambda function when new files arrive in the S3 raw zone</p>", "<p>Ingest the raw zone data in Kinesis Data Firehose and process the data using a Lambda function before it is finally dumped into the refined zone</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Invoke an AWS Glue ML Transforms job when new data arrives into raw zone so that de-duplicated records can be written into the refined zone</strong></p><p>You can create machine learning transforms to cleanse your data using AWS Glue ML&nbsp;Transforms. You can call these transforms from your ETL script. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_14-32-41-e1eb7dbbfcc87b4ee30a394919d48577.jpg\"><p>AWS Glue ML Transforms job can perform deduplication in a serverless fashion and that\u2019s the correct choice for this use-case.</p><p>Incorrect options:</p><p><strong>Create an Apache Spark based EMR job and run it once a day to de-duplicate the records from raw zone into refined zone</strong> - Apache Spark is a distributed processing framework and programming model that helps you do machine learning, stream processing, or graph analytics using Amazon EMR clusters. Executing Apache Spark jobs on EMR would require configuring and managing the cluster manually, so this option is ruled out.</p><p><strong>Create a Lambda function with the code to handle all possible data duplication use cases. Trigger a Lambda function when new files arrive in the S3 raw zone</strong> - Using a Lambda function involves significant development and maintenance effort, so this option is ruled out. </p><p><strong>Ingest the raw zone data in Kinesis Data Firehose and process the data using a Lambda function before it is finally dumped into the refined zone</strong> - Kinesis Data Firehose facilitates loading streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics. As Kinesis Data Firehose is used for streaming data analytics, so it\u2019s not the right fit for the given use case. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\">https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html</a></p>"}, "correct_response": ["b"], "section": "Data Engineering", "question_plain": "The data science team at an email marketing company has created a data lake with raw and refined zones. The raw zone has the data as it arrives from the source, however, the team wants to de-duplicate the data before it is written into the refined zone. What is the best way to accomplish this with the least amount of development time and infrastructure maintenance effort?", "related_lectures": []}, {"_class": "assessment", "id": 52820554, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are building a forecasting model using the SageMaker DeepAR algorithm. The following graph captures the Actual vs Forecast comparison from the model output for the revenue data for the last few years:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-26_17-41-17-86b923ab3773fe6c89965d79431be625.png\"></p><p>What can you conclude about the forecast shown in the graph above?</p>", "relatedLectureIds": "", "answers": ["<p>The forecast captures the trend well but misses the seasonality</p>", "<p>The forecast captures the seasonality well but misses the trend</p>", "<p>The forecast captures both the trend and seasonality well</p>", "<p>The forecast captures neither the trend nor the seasonality</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>The forecast captures both the trend and seasonality well</strong></p><p>Trends are continuous increases or decreases in a metric's value. Seasonality, on the other hand, reflects periodic (cyclical) patterns that occur in a system. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_15-02-32-e5207afcd48ec2dbc9d9d3392e4033e2.png\"><p><em>We can infer from the graph shown in the question that the forecast captures both the trend and seasonality well.</em></p><p>Incorrect options:</p><p><strong>The forecast captures the trend well but misses the seasonality</strong></p><p><strong>The forecast captures the seasonality well but misses the trend</strong></p><p><strong>The forecast captures neither the trend nor the seasonality</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "You are building a forecasting model using the SageMaker DeepAR algorithm. The following graph captures the Actual vs Forecast comparison from the model output for the revenue data for the last few years:What can you conclude about the forecast shown in the graph above?", "related_lectures": []}, {"_class": "assessment", "id": 52820556, "assessment_type": "multi-select", "prompt": {"question": "<p>You are creating a classification model using one of the Amazon SageMaker built-in algorithms and you want to use GPUs for both training and inference. </p><p>Which of the following steps would you follow to meet this requirement? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Select a built-in algorithm that supports GPUs for both training and inference</p>", "<p>Specify gpu=True as the parameter in the create_training_job boto3 API call on Jupyter Notebook</p>", "<p>Select the correct instance type that supports GPUs</p>", "<p>Enable GPU support in the docker image for the SageMaker algorithm</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Select a built-in algorithm that supports GPUs for both training and inference</strong></p><p><strong>Select the correct instance type that supports GPUs</strong></p><p>You need to select the appropriate built-in SageMaker algorithm as well as choose the correct instance type for GPU support.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_15-16-09-3cbb52a517c78280ab895488e9ef4573.jpg\"><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_15-13-37-e9b88b46727d9e08c4c6cfc597485ff8.png\"><p>Incorrect options:</p><p><strong>Specify gpu=True as the parameter in the create_training_job boto3 API call on Jupyter Notebook</strong> - create_training_job has no such parameter as gpu, this option has been added as a distractor. </p><p><strong>Enable GPU support in the docker image for the SageMaker algorithm</strong> - You can\u2019t enable GPU support in docker image as that\u2019s a made up option.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/cmn-info-instance-types.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/cmn-info-instance-types.html</a></p>"}, "correct_response": ["a", "c"], "section": "ML Implementation and Operations", "question_plain": "You are creating a classification model using one of the Amazon SageMaker built-in algorithms and you want to use GPUs for both training and inference. Which of the following steps would you follow to meet this requirement? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820558, "assessment_type": "multiple-choice", "prompt": {"question": "<p>For the upcoming festive season, an e-commerce company is anticipating a major shift in the expected workload for its product recommendation engine hosted on SageMaker. </p><p>Which solution would you recommend to address this issue?</p>", "relatedLectureIds": "", "answers": ["<p>Use elastic inference to manage the workload</p>", "<p>Use automatic scaling for the production variants to manage the workload</p>", "<p>Use inference pipelines to manage the workload</p>", "<p>SageMaker hosting has a built-in mechanism to address this issue. Nothing else needs to be done</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use automatic scaling for the production variants to manage the workload</strong></p><p>Amazon SageMaker supports automatic scaling (autoscaling) for your hosted models. <em>Autoscaling</em> dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, autoscaling brings more instances online. When the workload decreases, autoscaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.</p><p>When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, SageMaker launches them in multiple Availability Zones. This ensures continuous availability.</p><p>Automatic scaling is the correct option to address any changes in workload. </p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html</a></p><p>Incorrect options:</p><p><strong>Use elastic inference to manage the workload</strong> - By using Amazon Elastic Inference (EI), you can speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as Amazon SageMaker hosted models, but at a fraction of the cost of using a GPU instance for your endpoint. EI allows you to add inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. </p><p>Elastic inference is used to improve the inference throughput, so this option is not the right fit for the given use case.</p><p><strong>Use inference pipelines to manage the workload</strong> -&nbsp; An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed. </p><p>Inference Pipelines are used to define and deploy a combination of algorithms in SageMaker, so this option is not the right fit for the given use case.</p><p><strong>SageMaker hosting has a built-in mechanism to address this issue. Nothing else needs to be done</strong> - This option has been added as a distractor.</p>"}, "correct_response": ["b"], "section": "ML Implementation and Operations", "question_plain": "For the upcoming festive season, an e-commerce company is anticipating a major shift in the expected workload for its product recommendation engine hosted on SageMaker. Which solution would you recommend to address this issue?", "related_lectures": []}, {"_class": "assessment", "id": 52820560, "assessment_type": "multi-select", "prompt": {"question": "<p>The AI research department at a university is collaborating with a consultancy firm. A research assistant at the department would like to allow developers from the consultancy firm to access some of the SageMaker resources created in the AWS account of the research department. </p><p>As an AWS ML&nbsp;Specialist, which of the following solutions would you recommend to grant this access? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Create a new AWS user account and share the username and password via email</p>", "<p>Create a role to delegate access to your resources with the third-party AWS account</p>", "<p>Provide access to externally authenticated users through identity federation</p>", "<p>Create SageMaker resource based policies to allow this access</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Create a role to delegate access to your resources with the third-party AWS account</strong></p><p>When third parties require access to your organization's AWS resources, you can use roles to delegate access to them. For example, a third party might provide a service for managing your AWS resources. With IAM roles, you can grant these third parties access to your AWS resources without sharing your AWS security credentials. Instead, the third party can access your AWS resources by assuming a role that you create in your AWS account.</p><p>You can create a role to delegate access and meet the requirements of this use case.</p><p><strong>Provide access to externally authenticated users through identity federation</strong></p><p>Your users might already have identities outside of AWS, such as in your corporate directory. If those users need to work with AWS resources (or work with applications that access those resources), then those users also need AWS security credentials. You can use an IAM role to specify permissions for users whose identity is federated from your organization or a third-party identity provider (IdP).</p><p>For the given use case, you can provide access via identity federation. </p><p>Incorrect options:</p><p><strong>Create a new AWS user account and share the username and password via email</strong> - Sharing user account credentials via email violates the recommended security best practices, so this option is incorrect.</p><p><strong>Create SageMaker resource based policies to allow this access</strong> - Resource-based policies are attached to a resource. For example, you can attach resource-based policies to Amazon S3 buckets, Amazon SQS queues, etc. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_15-35-18-f95bdf7147ec4f04a1ec8d56c2150cf3.jpg\"><p>SageMaker does not support resource based policies, this option has been added as a distractor.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_troubleshoot.html#security_iam_troubleshoot-cross-account-access\">https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_troubleshoot.html#security_iam_troubleshoot-cross-account-access</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p>"}, "correct_response": ["b", "c"], "section": "ML Implementation and Operations", "question_plain": "The AI research department at a university is collaborating with a consultancy firm. A research assistant at the department would like to allow developers from the consultancy firm to access some of the SageMaker resources created in the AWS account of the research department. As an AWS ML&nbsp;Specialist, which of the following solutions would you recommend to grant this access? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820562, "assessment_type": "multi-select", "prompt": {"question": "<p>Which of the following represent the correct statements regarding the Amazon SageMaker logging and monitoring options on CloudWatch and CloudTrail? (Select four)</p>", "relatedLectureIds": "", "answers": ["<p>CloudWatch keeps the SageMaker monitoring statistics for 15 months. However, the Amazon CloudWatch console limits the search to metrics that were updated in the last 2 weeks</p>", "<p>SageMaker monitoring metrics are available on CloudWatch at a 2-minute frequency</p>", "<p>CloudTrail monitors calls to InvokeEndpoint</p>", "<p>AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Amazon SageMaker. CloudTrail keeps this record for a period of 90 days</p>", "<p>CloudTrail does not monitor calls to InvokeEndpoint</p>", "<p>SageMaker monitoring metrics are available on CloudWatch at a 1-minute frequency</p>"], "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>CloudWatch keeps the SageMaker monitoring statistics for 15 months. However, the Amazon CloudWatch console limits the search to metrics that were updated in the last 2 weeks</strong></p><p><strong>SageMaker monitoring metrics are available on CloudWatch at a 1-minute frequency</strong></p><p>Monitoring is an important part of maintaining the reliability, availability, and performance of SageMaker. AWS provides the following monitoring tools to watch SageMaker, report when something is wrong, and take automatic actions when appropriate:</p><p>Amazon CloudWatch monitors your AWS resources and the applications that you run on AWS in real time. You can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a specified metric reaches a threshold that you specify. You can further use Amazon CloudWatch Logs and CloudWatch Events to augment your monitoring processes.</p><p>AWS CloudTrail captures API calls and related events made by or on behalf of your AWS account and delivers the log files to an Amazon S3 bucket that you specify. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_16-12-43-6d4deb76fc41228775700472eb8c5309.jpg\"><p><strong>CloudTrail does not monitor calls to InvokeEndpoint</strong></p><p><strong>AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Amazon SageMaker. CloudTrail keeps this record for a period of 90 days </strong></p><p>Amazon SageMaker is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in SageMaker. <em>CloudTrail captures all API calls for SageMaker, with the exception of InvokeEndpoint, as events.</em> The calls captured include calls from the SageMaker console and code calls to the SageMaker API operations. </p><p><em>You can troubleshoot operational and security incidents over the past 90 days in the CloudTrail console by viewing Event history. </em></p><p>Incorrect options:</p><p><strong>CloudTrail monitors calls to InvokeEndpoint</strong></p><p><strong>SageMaker monitoring metrics are available on CloudWatch at a 2-minute frequency</strong></p><p>These two options contradict the explanation provided above, so these options are incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-incident-response.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-incident-response.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/logging-using-cloudtrail.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/logging-using-cloudtrail.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html</a></p>"}, "correct_response": ["a", "d", "e", "f"], "section": "ML Implementation and Operations", "question_plain": "Which of the following represent the correct statements regarding the Amazon SageMaker logging and monitoring options on CloudWatch and CloudTrail? (Select four)", "related_lectures": []}, {"_class": "assessment", "id": 52820564, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Silicon Valley startup intends to provide real estate recommendations to millions of home-buyers. The data science team is grappling with thousands of features that could go into the model, so the team wants to consider only the most relevant derived features. </p><p>As an AWS&nbsp;Certified ML Specialist, which of the following solutions would you recommend to get the team started?</p>", "relatedLectureIds": "", "answers": ["<p>Use K-means to identify the most relevant derived features</p>", "<p>Use Latent Dirichlet Allocation to identify the most relevant derived features</p>", "<p>Use Principal Component Analysis to identify the most relevant derived features</p>", "<p>Use Neural Topic Model to identify the most relevant derived features</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Use Principal Component Analysis to identify the most relevant derived features</strong></p><p><em>Dimensionality reduction</em> is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. There are various methods for dimensionality reduction such as - Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), etc. </p><p>PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. PCA can help in identifying the most relevant derived features for the given use case.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-17-56-ede2b8e5b21458eac7985aa4e25b15a6.jpeg\"></p><p>Incorrect options:</p><p><strong>Use K-means to identify the most relevant derived features</strong> - Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. Data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-13-51-dfd58c135e25df41652ce71bab01371d.jpg\"></p><p>K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. </p><p>K-means is used for clustering and it cannot be used to identify the most relevant derived features for the given use case.</p><p><strong>Use Latent Dirichlet Allocation to identify the most relevant derived features</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents.</p><p>Latent Dirichlet Allocation is used for topic modeling and it cannot be used to identify the most relevant derived features for the given use case.</p><p><strong>Use Neural Topic Model to identify the most relevant derived features</strong> - Amazon SageMaker NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. Documents that contain frequent occurrences of words such as \"bike\", \"car\", \"train\", \"mileage\", and \"speed\" are likely to share a topic on \"transportation\" for example. Topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities. </p><p>SageMaker NTM is used for topic modeling and it cannot be used to identify the most relevant derived features for the given use case.</p>"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "A Silicon Valley startup intends to provide real estate recommendations to millions of home-buyers. The data science team is grappling with thousands of features that could go into the model, so the team wants to consider only the most relevant derived features. As an AWS&nbsp;Certified ML Specialist, which of the following solutions would you recommend to get the team started?", "related_lectures": []}, {"_class": "assessment", "id": 52820566, "assessment_type": "multi-select", "prompt": {"question": "<p>Learning curve is a graph that compares the performance of a model on training datasets v/s validation datasets over a varying number of training instances. The following figure illustrates the learning curves for two separate models:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2020-10-03_10-38-36-1360ed7c3e1b03e7028e82140947f65a.png\"></p><p>The learning curve on the left was captured for a model running the Amazon SageMaker Linear Learner algorithm and the learning curve on the right was captured for another model running the Amazon SageMaker XGBoost algorithm. The horizontal line with dashes represents the desired performance. The curve in red represents the validation dataset and the curve in blue represents the training dataset. </p><p>Which of the following solutions would you use to fix the issues plaguing the model on the left? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Add more features to the model</p>", "<p>Remove features from the model</p>", "<p>Increase regularization parameters</p>", "<p>Decrease regularization parameters</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Add more features to the model</strong></p><p><strong>Decrease regularization parameters</strong></p><p>The learning curve on the left represents a model that is underfitting (has bias) because both the training and validation error are high. You can address this by adding more features or by decreasing regularization parameters. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_17-10-07-05882ec49a567118462ea5ded3b5bf43.jpg\"><p>Incorrect options:</p><p><strong>Remove features from the model</strong></p><p><strong>Increase regularization parameters</strong></p><p>These two options contradict the explanation provided above, so these options are incorrect</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p><p><a href=\"https://medium.com/@datalesdatales/why-you-should-be-plotting-learning-curves-in-your-next-machine-learning-project-221bae60c53\">https://medium.com/@datalesdatales/why-you-should-be-plotting-learning-curves-in-your-next-machine-learning-project-221bae60c53</a></p>"}, "correct_response": ["a", "d"], "section": "Modeling", "question_plain": "Learning curve is a graph that compares the performance of a model on training datasets v/s validation datasets over a varying number of training instances. The following figure illustrates the learning curves for two separate models:The learning curve on the left was captured for a model running the Amazon SageMaker Linear Learner algorithm and the learning curve on the right was captured for another model running the Amazon SageMaker XGBoost algorithm. The horizontal line with dashes represents the desired performance. The curve in red represents the validation dataset and the curve in blue represents the training dataset. Which of the following solutions would you use to fix the issues plaguing the model on the left? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820568, "assessment_type": "multi-select", "prompt": {"question": "<p>Which of the following represent the correct statements regarding the IAM features available with Amazon SageMaker? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Amazon SageMaker supports resource-based policies</p>", "<p>Amazon SageMaker supports authorization based on resource tags</p>", "<p>With IAM identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied for Amazon SageMaker</p>", "<p>Amazon SageMaker supports service linked roles</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Amazon SageMaker supports authorization based on resource tags</strong> - You can attach tags to SageMaker resources or pass tags in a request to SageMaker. To control access based on tags, you provide tag information in the condition element of a policy using the sagemaker:ResourceTag/key-name, aws:RequestTag/key-name, or aws:TagKeys condition keys. </p><p><strong>With IAM identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied for Amazon SageMaker - </strong>With IAM identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied. SageMaker supports specific actions, resources, and condition keys. Administrators can use AWS JSON policies to specify who has access to what. That is, which principal can perform actions on what resources, and under what conditions. The Action element of a JSON policy describes the actions that you can use to allow or deny access in a policy. </p><p>Incorrect options:</p><p><strong>Amazon SageMaker supports resource-based policies</strong> - SageMaker does not support resource-based policies, so this option is incorrect.</p><p><strong>Amazon SageMaker supports service linked roles</strong> - SageMaker doesn't support service-linked roles, so this option is incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html</a></p>"}, "correct_response": ["b", "c"], "section": "ML Implementation and Operations", "question_plain": "Which of the following represent the correct statements regarding the IAM features available with Amazon SageMaker? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820570, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An analytics company wants to create a text summarization model based on the SageMaker seq2seq algorithm. However the training data is in the form of 1TB of flat files whereas seq2seq only expects RecordIO-Protobuf format. </p><p>As an ML Specialist, which of the following solutions would you recommend?</p>", "relatedLectureIds": "", "answers": ["<p>Use AWS Glue to create an ETL job to write the data in RecordIO-Protobuf format on S3. This data can be used by seq2seq based model for training</p>", "<p>Spin-up an Apache Spark EMR cluster to transform the data from flat files into RecordIO-Protobuf format and save it on S3. This data can be used by seq2seq based model for training</p>", "<p>Use AWS Step function with Lambda to write the data in RecordIO-Protobuf format on S3. This data can be used by seq2seq based model for training</p>", "<p>Use Kinesis Data Firehose to transform the data into RecordIO-Protobuf format</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Spin-up an Apache Spark EMR cluster to transform the data from flat files into RecordIO-Protobuf format and save it on S3. This data can be used by seq2seq based model for training</strong></p><p>Apache Spark is a distributed processing framework and programming model that helps you do machine learning, stream processing, or graph analytics using Amazon EMR clusters. Amazon EMR is a web service that makes it easy for you to process and analyze vast amounts of data using applications in the Hadoop ecosystem, including Hive, Pig, HBase, Presto, Impala, and others.</p><p>Apache Spark (running on the EMR cluster in this use-case) can write the output in RecorIO-Protobuf format.</p><p>Incorrect options:</p><p><strong>Use AWS Glue to create an ETL job to write the data in RecordIO-Protobuf format on S3. This data can be used by seq2seq based model for training</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides both visual and code-based interfaces to make data integration easier. Users can easily find and access data using the AWS Glue Data Catalog. Data engineers and ETL (extract, transform, and load) developers can visually create, run, and monitor ETL workflows with a few clicks in AWS Glue Studio. <em>Glue cannot write the output in RecordIO-Protobuf format. </em></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_04-13-36-704716ef796952b824e0a4fa0dc2cd81.jpg\"><p><strong>Use AWS Step function with Lambda to write the data in RecordIO-Protobuf format on S3. This data can be used by seq2seq based model for training</strong> - AWS Step Functions is a low-code visual workflow service used to orchestrate AWS services, automate business processes, and build serverless applications. Lambda is not suited for long-running processes such as the task of transforming 1TB data into RecordIO-Protobuf format. </p><p><strong>Use Kinesis Data Firehose to transform the data into RecordIO-Protobuf format</strong> - Kinesis Data Firehose can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics. It is not meant to be used for batch processing use cases and it cannot write data in RecorIO-Protobuf format. </p><p>Reference:</p><p><a href=\"https://aws.amazon.com/emr/faqs/\">https://aws.amazon.com/emr/faqs/</a></p>"}, "correct_response": ["b"], "section": "Data Engineering", "question_plain": "An analytics company wants to create a text summarization model based on the SageMaker seq2seq algorithm. However the training data is in the form of 1TB of flat files whereas seq2seq only expects RecordIO-Protobuf format. As an ML Specialist, which of the following solutions would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 52820572, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The marketing analytics team at a leading bank has created a multi-class classification model to segment the bank\u2019s customers into three tiers - Platinum, Gold and Silver. The confusion matrix for the underlying model was reported as follows:</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-27_00-43-57-fb734a347084cca1312cd6a37829446c.png\"></p><p>What is the overall recall for this multi-class classification model?</p>", "relatedLectureIds": "", "answers": ["<p>0.30 </p>", "<p>0.20</p>", "<p>0.46</p>", "<p>0.57</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>0.57</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_04-22-49-1708c82b6d398bc2dd3e330bcbdcc1c4.jpg\"><p>Recall for Platinum Tier = (True Positives / (True Positives + False Negatives))</p><p>= (30/(30+ (50+20) )) = 30/100 = 0.30</p><p>Recall for Gold Tier = (True Positives / (True Positives + False Negatives))</p><p>= (60/(60+ (20+20) )) = 60/100 = 0.60</p><p>Recall for Silver Tier = (True Positives / (True Positives + False Negatives))</p><p>= (80/(80+ (10+10) )) = 80/100 = 0.80</p><p>Overall Recall = Average of the recall for Platinum, Gold and Silver Tiers</p><p>= (0.30+0.60+0.80)/3 = 0.57</p><p>Incorrect options:</p><p><strong>0.30 </strong></p><p><strong>0.20</strong></p><p><strong>0.46</strong></p><p>These three options do not match the calculations shown above, so these are incorrect.</p><p><br></p>"}, "correct_response": ["d"], "section": "Modeling", "question_plain": "The marketing analytics team at a leading bank has created a multi-class classification model to segment the bank\u2019s customers into three tiers - Platinum, Gold and Silver. The confusion matrix for the underlying model was reported as follows:What is the overall recall for this multi-class classification model?", "related_lectures": []}, {"_class": "assessment", "id": 52820574, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You would like to identify the \u201cglobal hotspots\u201d for UFO sightings over the last 50 years. The data is available from a globally reputed agency based out of Area 51. </p><p>Which visualization tool is best suited to bring out these locations on a geographic map?</p>", "relatedLectureIds": "", "answers": ["<p>Hashmap</p>", "<p>Treemap</p>", "<p>Heatmap</p>", "<p>Mindmap</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Heatmap</strong></p><p>Geographic heat maps are an interactive way to identify where something occurs, and demonstrate areas of high and low density.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_04-37-50-0c7a40845b286878e2bb047e3ba978be.jpg\"><p>Heatmaps can also be used to represent the data in a 2-dimensional form. The data values are represented as colors in the graph. The goal of the heatmap is to provide a colored visual summary of information.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_04-39-06-4af3d023ee88e25f7f2cec4bfaac0ce8.jpg\"><p>Incorrect options:</p><p><strong>Hashmap</strong></p><p><strong>Mindmap</strong></p><p>Hashmap is a programming construct and mindmap is a diagram used to visually organize information. Both of these options are not relevant to the given use-case. </p><p><strong>Treemap</strong></p><p>Treemaps are an alternative way of visualising the hierarchical structure of a Tree Diagram while also displaying quantities for each category via area size. Each category is assigned a rectangle area with their subcategory rectangles nested inside of it.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_04-42-26-aee6c569d1fb773f73958f865ae42877.png\">"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "You would like to identify the \u201cglobal hotspots\u201d for UFO sightings over the last 50 years. The data is available from a globally reputed agency based out of Area 51. Which visualization tool is best suited to bring out these locations on a geographic map?", "related_lectures": []}, {"_class": "assessment", "id": 52820576, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A bioinformatics company wants to automate the secondary analysis of the raw DNA reads into a complete genomic sequence by comparing the multiple overlapping reads and the reference sequence. </p><p>Which AWS service can be used to configure and schedule this secondary analysis on compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of this workflow?</p>", "relatedLectureIds": "", "answers": ["<p>AWS Glue</p>", "<p>AWS Batch</p>", "<p>Amazon SageMaker</p>", "<p>Amazon CloudWatch</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>AWS Batch</strong></p><p>AWS Batch is a set of batch management capabilities that dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted. </p><p>With AWS Batch, there is no need to install and manage batch computing software or server clusters, allowing you to instead focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads using Amazon EC2 (available with Spot Instances) and AWS compute resources with AWS Fargate or Fargate Spot.</p><p>AWS Batch can be used to configure and schedule resources for the given use case.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-32-31-463d3d7573026e1654db372285be0b97.jpg\"><p>Incorrect options:</p><p><strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue consists of a Data Catalog which is a central metadata repository; an ETL engine that can automatically generate Scala or Python code; a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue cannot be used to configure and schedule resources for the given use case.</p><p><strong>Amazon SageMaker</strong> - Amazon SageMaker is a fully managed service that provides the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker cannot be used to configure and schedule resources for the given use case.</p><p><strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. It cannot be used to configure and schedule resources for the given use case.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/batch/use-cases/\">https://aws.amazon.com/batch/use-cases/</a></p>"}, "correct_response": ["b"], "section": "Data Engineering", "question_plain": "A bioinformatics company wants to automate the secondary analysis of the raw DNA reads into a complete genomic sequence by comparing the multiple overlapping reads and the reference sequence. Which AWS service can be used to configure and schedule this secondary analysis on compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of this workflow?", "related_lectures": []}, {"_class": "assessment", "id": 52820578, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are analyzing the salary trends for Silicon Valley engineers over the last decade. The dataset has information on the age, professional experience in years, skill level from 1 to 10, gender, location (5 distinct locations) and salary. </p><p>What is the best way to visualize the average annual salary trend at each location over the last decade?</p>", "relatedLectureIds": "", "answers": ["<p>Pie Chart</p>", "<p>Bar Chart</p>", "<p>Scatter Plot</p>", "<p>Line Chart</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Line Chart</strong></p><p>The multi-series line chart is used to visualize trends and relationships in multiple datasets. It consists of various data points connected using line segments, where each point represents a single value. Multi series line chart is best suited for this job.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-44-54-52c639023735c8f6bb13153e5d5a4152.jpg\"><p>Incorrect options:</p><p><strong>Pie Chart</strong> - A pie chart is a circular statistical graphic, which is divided into slices to illustrate numerical proportion. In a pie chart, the arc length of each slice, is proportional to the quantity it represents. Pie Chart is ruled out for this use-case. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-46-13-d326c5adf73665095d7be62897d77e79.jpg\"><p><strong>Bar Chart - </strong>A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column chart.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-47-42-8d6dddea2d283e4a00c33aed70979d63.jpg\"><p><strong>Scatter Plot</strong> - A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-50-04-31c0d6d6ad2291602367c0a821b342aa.jpg\"><p>While you can use multi bar chart or multi scatter plot, but none of these will bring out the annual salary trend for each location, so both these options are incorrect.</p>"}, "correct_response": ["d"], "section": "Exploratory Data Analysis", "question_plain": "You are analyzing the salary trends for Silicon Valley engineers over the last decade. The dataset has information on the age, professional experience in years, skill level from 1 to 10, gender, location (5 distinct locations) and salary. What is the best way to visualize the average annual salary trend at each location over the last decade?", "related_lectures": []}, {"_class": "assessment", "id": 52820580, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team at a medical research company has handed over the cleaned and prepared dataset for DNA&nbsp;sequencing to the Machine Learning team. The ML team wants to use this dataset for building a regression model based on the SageMaker Linear Learner algorithm. </p><p>Which of the following represents the first step that the ML team needs to follow for the entire dataset?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Shuffle the dataset</strong></p><p>The first step would be to shuffle the dataset. Shuffling helps the training converge fast, prevents any bias during the training and prevents the model from learning the order of the training.</p><p>Incorrect options:</p><p><strong>Normalize the dataset</strong> </p><p>Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. </p><p><strong>Standardize the dataset</strong> </p><p>Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.</p><p>Normalization and Standardization are done on specific features, not on the entire dataset. Therefore both these options are incorrect. </p><p><strong>Create training set, validation set and test set</strong> </p><p><em>Training Set</em> - This refers to the actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data. </p><p><em>Validation Set</em> - The validation set is used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The validation set is also known as the Dev set or the Development set since this dataset helps during the \u201cdevelopment\u201d stage of the model.</p><p><em>Test Set</em> - The Test Set is used to provide an unbiased evaluation of a final model fit on the training dataset. </p><p>Once the data is shuffled, you should do the splits for training set, validation set and test set.</p>", "answers": ["<p>Create training set, validation set and test set</p>", "<p>Normalize the dataset</p>", "<p>Shuffle the dataset</p>", "<p>Standardize the dataset</p>"]}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "The data engineering team at a medical research company has handed over the cleaned and prepared dataset for DNA&nbsp;sequencing to the Machine Learning team. The ML team wants to use this dataset for building a regression model based on the SageMaker Linear Learner algorithm. Which of the following represents the first step that the ML team needs to follow for the entire dataset?", "related_lectures": []}, {"_class": "assessment", "id": 52820582, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data engineering team at an ecommerce company is migrating its data architecture from a data lake to a data warehouse. The current data lake is housed in S3. </p><p>Which is the fastest way to load this data into Amazon Redshift?</p>", "relatedLectureIds": "", "answers": ["<p>Use the INSERT command to load data from files in S3</p>", "<p>Use the LOAD command to load data from files in S3</p>", "<p>Use the UNLOAD command to load data from files in S3</p>", "<p>Use the COPY command to load data from files in S3</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use the COPY command to load data from files in S3</strong></p><p>Amazon Redshift is an enterprise-level, petabyte scale, fully managed data warehousing service. Amazon Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing, columnar data storage, and very efficient, targeted data compression encoding schemes.</p><p>The COPY command loads data in parallel from Amazon S3, Amazon EMR, Amazon DynamoDB, or multiple data sources on remote hosts. COPY loads large amounts of data much more efficiently than using INSERT statements, and stores the data more effectively as well. So this option is the right fit for the given use case.</p><p>Incorrect options:</p><p><strong>Use the LOAD command to load data from files in S3</strong> - There is no such thing as a \"LOAD\" command for Redshift. </p><p><strong>Use the INSERT command to load data from files in S3</strong> - COPY loads large amounts of data much more efficiently than using INSERT statements, so this option is not correct.</p><p><strong>Use the UNLOAD command to load data from files in S3</strong> - UNLOAD is used to write the results of a Redshift query to one or more text files on Amazon S3, so this option is not correct.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html</a></p>"}, "correct_response": ["d"], "section": "Data Engineering", "question_plain": "The data engineering team at an ecommerce company is migrating its data architecture from a data lake to a data warehouse. The current data lake is housed in S3. Which is the fastest way to load this data into Amazon Redshift?", "related_lectures": []}, {"_class": "assessment", "id": 52820584, "assessment_type": "multi-select", "prompt": {"question": "<p>What are the ideal characteristics of a good dataset for Machine Learning specific use cases? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>Should be representative of the underlying business problem to be solved</p>", "<p>Should have fair sampling with even distribution of outcomes</p>", "<p>Should have some bias so algorithm can learn the edge cases correctly</p>", "<p>Should have skewed distribution so algorithm can learn the edge cases correctly</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Should be representative of the underlying business problem to be solved</strong></p><p><strong>Should have fair sampling with even distribution of outcomes</strong></p><p>If data has bias or follows a skewed distribution, model will also demonstrate skewed results. Ideally the data should be representative of the underlying business use case to be solved and have a fair sampling with even distribution of outcomes. So both these options are correct.</p><p>Incorrect options:</p><p><strong>Should have some bias so algorithm can learn the edge cases correctly</strong></p><p><strong>Should have skewed distribution so algorithm can learn the edge cases correctly</strong></p><p>These two options contradict the explanation provided above, so these options are incorrect.</p>"}, "correct_response": ["a", "b"], "section": "Exploratory Data Analysis", "question_plain": "What are the ideal characteristics of a good dataset for Machine Learning specific use cases? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820586, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Tf-idf is a statistical technique frequently used in Machine Learning domains such as text-summarization and classification. Tf-idf measures the relevance of a word in a document compared to the entire corpus of documents. You have a corpus (D) containing the following documents:</p><p>Document 1 (d1) : \u201cA quick brown fox jumps over the lazy dog. What a fox!\u201d</p><p>Document 2 (d2) : \u201cA quick brown fox jumps over the lazy fox. What a fox!\u201d</p><p>Which of the following statements is correct?</p>", "relatedLectureIds": "", "answers": ["<p>Using tf-idf, the word \u201cfox\u201d is more relevant for document d2 than document d1</p>", "<p>Using tf-idf, the word \u201cfox\u201d is more relevant for document d1 than document d2</p>", "<p>Using tf-idf, the word \u201cfox\u201d is equally relevant for both document d1 and document d2</p>", "<p>Insufficient information has been provided to compute tf-idf</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Using tf-idf, the word \u201cfox\u201d is equally relevant for both document d1 and document d2</strong></p><p>tf is the frequency of any \"term\" in a given \"document\". Using this definition, we can compute the following:</p><p>tf(\u201cfox\u201d, d1) = 2/12 , as the word \"fox\" appears twice in the first document which has a total of 12 words</p><p>tf(\u201cfox\u201d, d2) = 3/12 , as the word \"fox\" appears thrice in the second document which has a total of 12 words</p><p>An idf is constant per corpus (in this case, the corpus consists of 2 documents) , and accounts for the ratio of documents that include that specific \"term\". Using this definition, we can compute the following:</p><p>idf(\u201cfox\u201d, D) = log(2/2) = 0 , as the word \"fox\" appears in both the documents in the corpus</p><p>Now, </p><p>tf-idf(\u201cfox\u201d, d1, D) = tf(\u201cfox\u201d, d1) * idf(\u201cfox\u201d, D) = (2/12) * 0 = 0</p><p>tf-idf(\u201cfox\u201d, d2, D) = tf(\u201cfox\u201d, d2) * idf(\u201cfox\u201d, D) = (3/12) * 0 = 0</p><p>Using tf-idf, the word \u201cfox\u201d is equally relevant (or just irrelevant!) for both document d1 and document d2</p><p>Highly recommend the following resource for a detailed review of tf-idf:</p><p><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">https://en.wikipedia.org/wiki/Tf\u2013idf</a></p><p>Incorrect options:</p><p><strong>Using tf-idf, the word \u201cfox\u201d is more relevant for document d2 than document d1</strong></p><p><strong>Using tf-idf, the word \u201cfox\u201d is more relevant for document d1 than document d2</strong></p><p><strong>Insufficient information has been provided to compute tf-idf</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "Tf-idf is a statistical technique frequently used in Machine Learning domains such as text-summarization and classification. Tf-idf measures the relevance of a word in a document compared to the entire corpus of documents. You have a corpus (D) containing the following documents:Document 1 (d1) : \u201cA quick brown fox jumps over the lazy dog. What a fox!\u201dDocument 2 (d2) : \u201cA quick brown fox jumps over the lazy fox. What a fox!\u201dWhich of the following statements is correct?", "related_lectures": []}, {"_class": "assessment", "id": 52820588, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The Machine Learning team at an e-commerce company is analyzing the sales data. The data is stored in a highly optimized data compression format and the daily volume of data is around 1TB. The team would like to reduce this volume to one-tenth of its original size without significantly compromising on the quality of data, so that they can complete the classification model training in a much shorter time-span. </p><p>As an ML Specialist, which of the following solutions would you recommend to the team?</p>", "relatedLectureIds": "", "answers": ["<p>Use a more efficient compression algorithm</p>", "<p>Use dimensionality reduction to reduce the data volume and still preserve a significant variance between observations</p>", "<p>Use clustering to reduce the data volume and still preserve a significant variance between observations</p>", "<p>The team needs to use better hardware to run the classification model training so that the original data can be processed in a short timespan</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Use dimensionality reduction to reduce the data volume and still preserve a significant variance between observations</strong></p><p>Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. There are various methods for dimensionality reduction such as - Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), etc.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-17-56-ede2b8e5b21458eac7985aa4e25b15a6.jpeg\"></p><p>You can use dimensionality reduction to reduce the data volume while still preserving a significant variance between observations so that data quality is not compromised.</p><p>Incorrect options:</p><p><strong>Use clustering to reduce the data volume and still preserve a significant variance between observations</strong> - Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. Data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-13-51-dfd58c135e25df41652ce71bab01371d.jpg\"></p><p>For the given use case, clustering cannot be used to reduce the data volume to meet the requirements.</p><p><strong>Use a more efficient compression algorithm - </strong>Adding another compression layer would not help an already compressed dataset, so this option is incorrect.</p><p><strong>The team needs to use better hardware to run the classification model training so that the original data can be processed in a short timespan</strong> - Throwing more hardware at the problem does not reduce the data volume. </p><p><br></p>"}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "The Machine Learning team at an e-commerce company is analyzing the sales data. The data is stored in a highly optimized data compression format and the daily volume of data is around 1TB. The team would like to reduce this volume to one-tenth of its original size without significantly compromising on the quality of data, so that they can complete the classification model training in a much shorter time-span. As an ML Specialist, which of the following solutions would you recommend to the team?", "related_lectures": []}, {"_class": "assessment", "id": 52820590, "assessment_type": "multi-select", "prompt": {"question": "<p>You are analyzing the income data provided in a case study as part of a data science competition. You observe that the data has several outliers. </p><p>Which techniques can you use to address outliers in the data? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Logarithm Transformation</strong></p><p>The Log Transform decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.</p><p>The logarithm, x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation with a major effect on distribution shape. It is commonly used for reducing right skewness and is often appropriate for measured variables. It can not be applied to zero or negative values. One unit on a logarithmic scale means a multiplication by the base of logarithms being used.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_06-41-32-6b17f6b9edc19580c45a4c83ad08e839.jpg\"></p><p><em>The Log Transform also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.</em></p><p><strong>Robust Standardization</strong></p><p>Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.</p><p>A better approach to standardizing input variables in the presence of outliers is to ignore the outliers from the calculation of the mean and standard deviation, then use the calculated values to scale the variable.</p><p>This is called robust standardization or robust data scaling.</p><p>Logarithm transformation and Robust Standardization are the correct techniques to address the outliers in data. </p><p>Highly recommend the following references for a deep-dive:</p><p><a href=\"https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\">https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</a></p><p><a href=\"https://machinelearningmastery.com/robust-scaler-transforms-for-machine-learning/\">https://machinelearningmastery.com/robust-scaler-transforms-for-machine-learning/</a></p><p>Incorrect options:</p><p><strong>One-hot encoding</strong> - One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column. One-hot encoding cannot be used to address the outliers.</p><p><strong>Normalization</strong> - Normalization scales all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, <em>the effects of the outliers increases.</em> So this option is incorrect.</p>", "answers": ["<p>One-hot encoding</p>", "<p>Logarithm Transformation</p>", "<p>Robust Standardization</p>", "<p>Normalization</p>"]}, "correct_response": ["b", "c"], "section": "Exploratory Data Analysis", "question_plain": "You are analyzing the income data provided in a case study as part of a data science competition. You observe that the data has several outliers. Which techniques can you use to address outliers in the data? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820592, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Which of the following statement represents the key difference between a data lake and a data warehouse?</p>", "relatedLectureIds": "", "answers": ["<p>A data warehouse can only store structured data whereas a data lake can store structured, semi-structured and unstructured data</p>", "<p>A data lake can only store structured data whereas a data warehouse can store structured, semi-structured and unstructured data</p>", "<p>Both data lake and data warehouse can store structured, semi-structured and unstructured data</p>", "<p>Both data lake and data warehouse can only store structured data</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>A data warehouse can only store structured data whereas a data lake can store structured, semi-structured and unstructured data</strong></p><p>A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics\u2014from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.</p><p>A data warehouse is a large collection of business data used to help an organization make decisions. A data warehouse periodically pulls data from the business apps and systems; then, the data goes through formatting and import processes to match the data already in the warehouse. </p><p><em>Data lakes and data warehouses are both widely used for storing big data, but they are not interchangeable terms. A data lake is a vast pool of raw data, the purpose for which is not yet defined. A data warehouse is a repository for structured, filtered data that has already been processed for a specific purpose.</em></p><p>A data warehouse can only store structured data whereas a data lake can store structured, semi-structured and unstructured data. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_06-52-11-1954b5847259b23eda98d1b2d7788474.jpg\"><p>Incorrect options:</p><p><strong>A data lake can only store structured data whereas a data warehouse can store structured, semi-structured and unstructured data</strong></p><p><strong>Both data lake and data warehouse can store structured, semi-structured and unstructured data</strong></p><p><strong>Both data lake and data warehouse can only store structured data</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>References:</p><p><a href=\"https://www.kdnuggets.com/2015/09/data-lake-vs-data-warehouse-key-differences.html\">https://www.kdnuggets.com/2015/09/data-lake-vs-data-warehouse-key-differences.html</a></p><p><a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/\">https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/</a></p>"}, "correct_response": ["a"], "section": "Exploratory Data Analysis", "question_plain": "Which of the following statement represents the key difference between a data lake and a data warehouse?", "related_lectures": []}, {"_class": "assessment", "id": 52820594, "assessment_type": "multiple-choice", "prompt": {"question": "<p>AWS Glue jobs can be used to create serverless ETL jobs. Which of the following ETL source input types are NOT supported by AWS Glue for ETL jobs?</p>", "relatedLectureIds": "", "answers": ["<p>DocumentDB</p>", "<p>Oracle</p>", "<p>PostgreSQL</p>", "<p>Timestream</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Timestream</strong></p><p>AWS Glue can run your ETL jobs as new data arrives. For example, you can use an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3. You can also register this new dataset in the AWS Glue Data Catalog as part of your ETL jobs.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_11-50-20-6dd05b5671d4a23c24633dbdab478d58.jpg\"><p>In AWS Glue, various PySpark and Scala methods and transforms specify the connection type using a <code>connectionType</code> parameter. AWS Glue does NOT support Timestream as the source input type:</p><p>Incorrect options:</p><p><strong>DocumentDB</strong></p><p><strong>Oracle</strong></p><p><strong>PostgreSQL</strong></p><p>AWS&nbsp;Glue supports these three options as the ETL source input type. So these options are incorrect.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_11-53-12-5939c329a3f5f6e9af51a26dc1dc2fb5.jpg\"><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-connect.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-connect.html</a></p>"}, "correct_response": ["d"], "section": "Data Engineering", "question_plain": "AWS Glue jobs can be used to create serverless ETL jobs. Which of the following ETL source input types are NOT supported by AWS Glue for ETL jobs?", "related_lectures": []}, {"_class": "assessment", "id": 52820596, "assessment_type": "multi-select", "prompt": {"question": "<p>The data science team at an investment bank is analyzing the stock price data for blue chip stocks over the last year. </p><p>Which visualization tools can be used to spot outliers? (Select three)</p>", "relatedLectureIds": "", "answers": ["<p>Box plot</p>", "<p>Histogram</p>", "<p>Scatter plot</p>", "<p>Line chart</p>", "<p>Heatmap</p>", "<p>Bar chart&nbsp; </p>"], "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Box plot - </strong>A box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. <em>Outliers may be plotted as individual points.</em></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_16-11-49-ad46106d8f112c2534e8a514e9b70824.jpg\"><p><strong>Histogram</strong> - A histogram is a graphical representation that organizes a group of data points into user-specified ranges. Similar in appearance to a bar graph, the histogram condenses a data series into an easily interpreted visual by taking many data points and grouping them into logical ranges or bins.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_16-13-33-e60b16e915b8c53404099ec196bfcad8.jpeg\"><p><strong>Scatter Plot</strong> - A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-50-04-31c0d6d6ad2291602367c0a821b342aa.jpg\"></p><p>Box plot, scatter plot and histogram can be used to spot outliers. </p><p>Incorrect options:</p><p><strong>Line Chart</strong></p><p>A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_16-23-42-cdc4c0d1e40c5a1be7f266c28c6fc4b5.jpg\"><p><strong>Bar Chart - </strong>A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column chart.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-27_05-47-42-8d6dddea2d283e4a00c33aed70979d63.jpg\"></p><p><strong>Heatmap</strong> - A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-29_16-24-43-135808d3f44f24a0fcd99e947064ddbe.jpg\"><p><em>Line chart, bar chart and heatmap cannot be used to spot outliers. </em></p><p>References:</p><p><a href=\"https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\">https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba</a></p><p><a href=\"https://humansofdata.atlan.com/2017/10/how-to-find-outliers-data-set/\">https://humansofdata.atlan.com/2017/10/how-to-find-outliers-data-set/</a></p>"}, "correct_response": ["a", "b", "c"], "section": "Exploratory Data Analysis", "question_plain": "The data science team at an investment bank is analyzing the stock price data for blue chip stocks over the last year. Which visualization tools can be used to spot outliers? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820482, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A media company needs to ingest and store a near real-time stream of social media data. The source data is in JSON format. The company does not want to manage the underlying infrastructure and it wants the data to be immediately available for ad-hoc analysis. The solution must be cost efficient and scalable. </p><p>As an ML specialist, what is your recommendation?</p>", "relatedLectureIds": "", "answers": ["<p>Ingest the data using Kinesis Firehose that further transforms the data into Parquet format while writing to S3. Use an AWS Glue Crawler to read this data via an Athena table for ad-hoc analysis</p>", "<p>Ingest the data using Kinesis Data Streams and use a Lambda function to write the stream into S3. Launch a Glue ETL Job every 15 minutes to convert the data from JSON format to Parquet format. Use an AWS Glue Crawler to read this data into an Athena table for ad-hoc analysis</p>", "<p>Ingest the data using a Spark Streaming application running on an EMR cluster. The output data is written in Parquet format on S3. Use an AWS Glue Crawler to read this data into an Athena table for ad-hoc analysis</p>", "<p>Ingest the data into S3 using Kinesis Firehose. Launch a Glue ETL Job every 15 minutes to write S3 data into RDS. Perform ad-hoc query analysis once data is ingested into RDS</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Ingest the data using Kinesis Firehose that further transforms the data into Parquet format while writing to S3. Use an AWS Glue Crawler to read this data via an Athena table for ad-hoc analysis</strong></p><p>Kinesis Firehose can transform data to Parquet format and store it on S3 without provisioning any servers. Also this transformed data can be read via an Athena Table (created by using a Glue Crawler) and then the underlying data is readily available for ad-hoc analysis. </p><p>Incorrect options:</p><p><strong>Ingest the data using Kinesis Data Streams and use a Lambda function to write the stream into S3. Launch a Glue ETL Job every 15 minutes to convert the data from JSON format to Parquet format. Use an AWS Glue Crawler to read this data into an Athena table for ad-hoc analysis</strong></p><p><strong>Ingest the data into S3 using Kinesis Firehose. Launch a Glue ETL Job every 15 minutes to write S3 data into RDS. Perform ad-hoc query analysis once data is ingested into RDS</strong></p><p>Although Glue ETL Job can transform the source data to Parquet format, it is best suited for batch ETL use cases and it\u2019s not meant to process near real-time data. Therefore both these options are incorrect.</p><p><strong>Ingest the data using a Spark Streaming application running on an EMR cluster. The output data is written in Parquet format on S3. Use an AWS Glue Crawler to read this data into an Athena table for ad-hoc analysis</strong> - Using EMR cluster is not an option, as the company does not want to manage the underlying infrastructure.</p>"}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "A media company needs to ingest and store a near real-time stream of social media data. The source data is in JSON format. The company does not want to manage the underlying infrastructure and it wants the data to be immediately available for ad-hoc analysis. The solution must be cost efficient and scalable. As an ML specialist, what is your recommendation?", "related_lectures": []}, {"_class": "assessment", "id": 52820484, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The management of a shopping mall wants to predict the number of footfalls next week. They have data for the average number of footfalls over the last 52 weeks. </p><p>Which of the following probability distributions is the best fit for this use case?</p>", "relatedLectureIds": "", "answers": ["<p>Binomial distribution</p>", "<p>Bernoulli distribution</p>", "<p>Poisson distribution</p>", "<p>Normal distribution</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Poisson distribution</strong></p><p>A Poisson distribution is a probability distribution that is used to show how many times an event is likely to occur over a specified period. In other words, it is a count distribution. Poisson distributions are often used to understand independent events that occur at a constant rate within a given interval of time.</p><p><em>The Poisson distribution can be used to predict the number of footfalls next week for the given use case.</em></p><p>Incorrect options:</p><p><strong>Binomial distribution</strong> - A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail.</p><p><strong>Bernoulli distribution</strong> - A Bernoulli distribution is a discrete probability distribution for a Bernoulli trial \u2014 a random experiment that has only two outcomes (usually called a \u201cSuccess\u201d or a \u201cFailure\u201d). For example, the probability of getting a heads (a \u201csuccess\u201d) while flipping a coin is 0.5.</p><p><em>The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.</em></p><p><strong>Normal distribution</strong> - Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.</p><p><em>None of these three options can be used to predict the number of footfalls next week for the given use case.</em></p><p>Highly recommend the following reference for a deep-dive: </p><p><a href=\"https://medium.com/@srowen/common-probability-distributions-347e6b945ce4\">https://medium.com/@srowen/common-probability-distributions-347e6b945ce4</a></p>"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "The management of a shopping mall wants to predict the number of footfalls next week. They have data for the average number of footfalls over the last 52 weeks. Which of the following probability distributions is the best fit for this use case?", "related_lectures": []}, {"_class": "assessment", "id": 52820486, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail organization wants to forecast the sales of its flagship products for the upcoming festive season. They have the last 5 years of sales data for these products. </p><p>As an ML specialist, which algorithm would you use to implement the forecasting solution?</p>", "relatedLectureIds": "", "answers": ["<p>Latent Dirichlet Allocation (LDA)</p>", "<p>Linear Learner</p>", "<p>Semantic Segmentation</p>", "<p>Random Cut Forest</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Linear Learner</strong></p><p>Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1. For multiclass classification problems, the labels must be from 0 to num_classes - 1. For regression problems, y is a real number. The algorithm learns a linear function, or, for classification problems, a linear threshold function, and maps a vector x to an approximation of the label y.</p><p>The given use case is for a regression problem that can be easily solved using the Linear Learner algorithm. </p><p>Incorrect options:</p><p><strong>Latent Dirichlet Allocation (LDA)</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.</p><p>LDA is used for topic modeling, so it is not the right fit for the given use case.</p><p><strong>Semantic Segmentation</strong> - The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.</p><p>Semantic Segmentation is used for image analysis, so it is not the right fit for the given use case.</p><p><strong>Random Cut Forest</strong> - Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the \"regular\" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the \"regular\" data can often be described with a simple model.</p><p>Random Cut Forest is used to detect outliers, so it is not the right fit for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "A retail organization wants to forecast the sales of its flagship products for the upcoming festive season. They have the last 5 years of sales data for these products. As an ML specialist, which algorithm would you use to implement the forecasting solution?", "related_lectures": []}, {"_class": "assessment", "id": 52820488, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Executives at a leading smartphone brand are contemplating the launch of a radical new phone model with never-before-seen features. Given the lack of historical data for this phone model, which AWS SageMaker algorithm can help the executives in predicting the product sales over the next quarter for this innovative phone?</p>", "relatedLectureIds": "", "answers": ["<p>XGBoost</p>", "<p>DeepAR</p>", "<p>Linear Learner</p>", "<p>Factorization Machines</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>DeepAR</strong></p><p>The Amazon SageMaker DeepAR forecasting algorithm is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN). Classical forecasting methods, such as autoregressive integrated moving average (ARIMA) or exponential smoothing (ETS), fit a single model to each individual time series. They then use that model to extrapolate the time series into the future.</p><p>In many applications, however, you have many similar time series across a set of cross-sectional units. For example, you might have time series groupings for demand for different products, server loads, and requests for webpages. For this type of application, you can benefit from training a single model jointly over all of the time series. DeepAR takes this approach. When your dataset contains hundreds of related time series, DeepAR outperforms the standard ARIMA and ETS methods. You can also use the trained model to generate forecasts for new time series that are similar to the ones it has been trained on.</p><p>SageMaker DeepAR algorithm specializes in forecasting new product performance, so this option is the right fit for the given use case.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_06-41-16-079734dec3e55a32087e6671556b5d15.jpg\"><p>Incorrect options:</p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p><strong>Linear Learner</strong> - Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1. For multiclass classification problems, the labels must be from 0 to num_classes - 1. For regression problems, y is a real number. The algorithm learns a linear function, or, for classification problems, a linear threshold function, and maps a vector x to an approximation of the label y.</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p>None of these three algorithms can be used to forecast new product performance, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "Executives at a leading smartphone brand are contemplating the launch of a radical new phone model with never-before-seen features. Given the lack of historical data for this phone model, which AWS SageMaker algorithm can help the executives in predicting the product sales over the next quarter for this innovative phone?", "related_lectures": []}, {"_class": "assessment", "id": 52820490, "assessment_type": "multi-select", "prompt": {"question": "<p>An entrepreneur wants to launch the next unicorn business with futuristic Business Intelligence features. The product would allow business managers to gather business insights using a voice based conversational interface rather than typing complex SQL queries. </p><p>Which AWS services would you use to build this product with the least amount of time and resources? (Select three)</p>", "relatedLectureIds": "", "answers": ["<p>Connect</p>", "<p>Lex</p>", "<p>Translate</p>", "<p>Polly</p>", "<p>Comprehend</p>"], "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Lex</strong></p><p>Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Alexa, Amazon Lex provides high quality speech recognition and language understanding capabilities, enabling addition of sophisticated, natural language \u2018chatbots\u2019 to new and existing applications. </p><p><strong>Polly</strong></p><p>Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech. </p><p><strong>Comprehend</strong></p><p>Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. </p><p>For the given use case, Lex can be used to convert speech to text via automatic speech recognition (ASR) and then further pipe this text to recognize the intent of the text via natural language understanding (NLU) by using the pre-configured Intents and Entities to come back with the most relevant text response. Comprehend can be used to uncover the insights and relationships in your input text. In the end, this text response would be converted to speech via Polly. </p><p>Incorrect options:</p><p><strong>Connect</strong> - Amazon Connect is an easy to use omnichannel cloud contact center that provides a seamless experience across voice and chat for your customers and agents. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_08-28-19-233d8552efb172bc2e01c6c1c38a63e0.jpg\"></p><p><strong>Translate</strong> - Amazon Translate is a neural machine translation service that delivers fast, high-quality, affordable, and customizable language translation. Neural machine translation is a form of language translation automation that uses deep learning models to deliver more accurate and more natural sounding translation than traditional statistical and rule-based translation algorithms.</p><p>You cannot use Connect or Translate for the given use case, so both options are incorrect. </p>"}, "correct_response": ["b", "d", "e"], "section": "Modeling", "question_plain": "An entrepreneur wants to launch the next unicorn business with futuristic Business Intelligence features. The product would allow business managers to gather business insights using a voice based conversational interface rather than typing complex SQL queries. Which AWS services would you use to build this product with the least amount of time and resources? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820492, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A sports enthusiast wants to build a mobile app that is able to recognize celebrity athletes. </p><p>Which AWS service can help build a solution for this requirement with the minimum possible effort?</p>", "relatedLectureIds": "", "answers": ["<p>Amazon Lex</p>", "<p>Amazon Rekognition</p>", "<p>Amazon Predict</p>", "<p>Amazon Polly</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Amazon Rekognition</strong></p><p>Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p><em>Key features of Rekognition:</em></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_08-41-25-3b5d6036fa8fa62cb17f2f44d7f7bf4d.jpg\"><p><em>Rekognition has a celebrity recognition feature that can be leveraged to build a solution for the given requirement. </em></p><p>Incorrect options:</p><p><strong>Amazon Lex</strong> - Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Alexa, Amazon Lex provides high quality speech recognition and language understanding capabilities, enabling addition of sophisticated, natural language \u2018chatbots\u2019 to new and existing applications. </p><p><strong>Amazon Polly</strong> - Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech. </p><p>Neither Lex nor Polly can be used to build a solution for the given scenario, so both options are incorrect.</p><p><strong>Amazon Predict</strong> - This is a made-up option, there is no such service as Amazon Predict. </p><p>References:</p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><a href=\"https://aws.amazon.com/machine-learning/ai-services/\">https://aws.amazon.com/machine-learning/ai-services/</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "A sports enthusiast wants to build a mobile app that is able to recognize celebrity athletes. Which AWS service can help build a solution for this requirement with the minimum possible effort?", "related_lectures": []}, {"_class": "assessment", "id": 52820494, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Neural Network is tasked with classifying the correct breed from a given pool of ten breeds of dogs for a scientific experiment. Which activation function should be used in the output layer of this Neural Network?</p>", "relatedLectureIds": "", "answers": ["<p>Sigmoid</p>", "<p>Tanh</p>", "<p>RELU</p>", "<p>Softmax</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Softmax</strong></p><p>Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output. </p><p><em>The softmax function is a more generalized sigmoid activation function which is used for multiclass classification., therefore this option is the right fit for the given use case.</em></p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_09-15-00-bd7cac33dfa4d13f24d86a79efafd59f.jpg\"></p><p>Highly recommend the following resource for a deep-dive on activation functions:</p><p><a href=\"https://medium.com/fintechexplained/neural-network-activation-function-types-a85963035196\">https://medium.com/fintechexplained/neural-network-activation-function-types-a85963035196</a></p><p>Incorrect options:</p><p><strong>Sigmoid - </strong>The sigmoid activation function is \u201cS\u201d shaped. It can add non-linearity to the output and returns a binary value of 0 or 1. </p><p><strong>RELU - </strong>RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer of a neural network. The output can range from 0 to infinity. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_09-15-19-ceb3bd8f369dd3ba1996ba20a8ecbdc8.jpg\"></p><p><strong>Tanh - </strong>Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. The tanh function is mainly used for classification between two classes.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-01_09-15-46-8417cb0d8eb3e5e4b1fcf68ccabf0c8a.jpg\"></p><p>Reference:</p><p><a href=\"https://en.wikipedia.org/wiki/Softmax_function\">https://en.wikipedia.org/wiki/Softmax_function</a></p><p><a href=\"https://en.wikipedia.org/wiki/Logistic_function\">https://en.wikipedia.org/wiki/Logistic_function</a></p>"}, "correct_response": ["d"], "section": "Modeling", "question_plain": "A Neural Network is tasked with classifying the correct breed from a given pool of ten breeds of dogs for a scientific experiment. Which activation function should be used in the output layer of this Neural Network?", "related_lectures": []}, {"_class": "assessment", "id": 52820496, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A SageMaker training job with 1TB of training data is taking too long to run. </p><p>As an AWS ML specialist, which of the following options would you suggest as the most cost effective solution that requires the least infrastructure management?</p>", "relatedLectureIds": "", "answers": ["<p>Run the training job on an EMR cluster having Amazon SageMaker Spark Library along with the training container image</p>", "<p>Add additional storage to the SageMaker Training Instance</p>", "<p>Since time is the limiting resource, upgrade the SageMaker Training Instance type to the highest possible type so that the job runs quickly</p>", "<p>Convert the training data to recordIO-protobuf file type and run the training job in pipe mode</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Convert the training data to recordIO-protobuf file type and run the training job in pipe mode</strong></p><p>The protobuf recordIO format, used for training data, is the optimal way to load data into your model for training.</p><p>When you use the protobuf recordIO format you can also take advantage of pipe mode when training your model. Pipe mode, used together with the protobuf recordIO format, gives you the best data load performance by streaming your data directly from S3.</p><p>Converting the data to recordIO-protobuf file type can significantly improve the training time with a marginal increase in cost to store the recordIO-protobuf data on S3. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_00-30-45-663a49a1d4dce2d44d434bce18cb87b8.jpg\"><p>Incorrect options:</p><p><strong>Run the training job on an EMR cluster having Amazon SageMaker Spark Library along with the training container image </strong>- Spinning up EMR clusters would be costly and require complex infrastructure maintenance. </p><p><strong>Add additional storage to the SageMaker Training Instance</strong> - This would not resolve the issue as the constraint is on runtime. So adding more storage would not help.</p><p><strong>Since time is the limiting resource, upgrade the SageMaker Training Instance type to the highest possible type so that the job runs quickly</strong> - Upgrading the existing instance type would incur higher costs. Additionally, data would still need to be copied into the instance. So this option is incorrect.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></p>"}, "correct_response": ["d"], "section": "Data Engineering", "question_plain": "A SageMaker training job with 1TB of training data is taking too long to run. As an AWS ML specialist, which of the following options would you suggest as the most cost effective solution that requires the least infrastructure management?", "related_lectures": []}, {"_class": "assessment", "id": 52820498, "assessment_type": "multi-select", "prompt": {"question": "<p>Which of the following options would you identify as the constituent steps of the three step process for deploying a model using SageMaker hosted services? (Select three)</p>", "relatedLectureIds": "", "answers": ["<p>Tune model hyperparameters</p>", "<p>Create the model</p>", "<p>Create the endpoint configuration for HTTPS endpoint</p>", "<p>Validate the model</p>", "<p>Create the HTTPS endpoint</p>"], "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Create the model</strong></p><p><strong>Create the endpoint configuration for HTTPS endpoint</strong></p><p><strong>Create the HTTPS endpoint</strong></p><p>Deploying a model using SageMaker hosting services is a three-step process with the following steps: </p><p><em>Create a model in SageMaker</em> - By creating a model, you tell SageMaker where it can find the model components. This includes the S3 path where the model artifacts are stored and the Docker registry path for the image that contains the inference code.</p><p><em>Create an endpoint configuration for an HTTPS endpoint</em> - You specify the name of one or more models in production variants and the ML compute instances that you want SageMaker to launch to host each production variant. SageMaker supports running (a) multiple models, (b) multiple variants of a model, or (c) combinations of models and variants on the same endpoint. Model variants can reference the same model inference container (i.e. run the same algorithm), but use different model artifacts (e.g., different model weight values based on other hyper-parameter configurations). In contrast, two different models may use the same algorithm, but focus on different business problems or underlying goals and may operate on different data sets.</p><p><em>Create an HTTPS endpoint</em> - Provide the endpoint configuration to SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_00-52-12-ecf620ca3397ef6732a468f18e7fbba4.jpg\"><p>Incorrect options:</p><p><strong>Tune model hyperparameters</strong></p><p><strong>Validate the model</strong></p><p>These options contradict the explanation provided above, so both these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html</a></p>"}, "correct_response": ["b", "c", "e"], "section": "ML Implementation and Operations", "question_plain": "Which of the following options would you identify as the constituent steps of the three step process for deploying a model using SageMaker hosted services? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 52820500, "assessment_type": "multiple-choice", "prompt": {"question": "<p>How many shards would a Kinesis Data Streams application need if the average record size is 500KB and 2 records per second are being written into this application that has 7 consumers?</p>", "relatedLectureIds": "", "answers": ["<p>7</p>", "<p>3</p>", "<p>4</p>", "<p>1</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>4</strong></p><p>number_of_shards = max (incoming_write_bandwidth_in_KB/1000, outgoing_read_bandwidth_in_KB/2000)</p><p>where</p><p>incoming_write_bandwidth_in_KB = average_data_size_in_KB multiplied by the number_of_records_per_seconds. = 500 * 2 = 1000</p><p>outgoing_read_bandwidth_in_KB = incoming_write_bandwidth_in_KB multiplied by the number_of_consumers = 1000 * 7 = 7000</p><p>So, number_of_shards = max(1000/1000, 7000/2000) = max(1, 3.5) = 4</p><p>So, 4 shards are needed to address this use case.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_01-03-19-e7554f2ebfd957c77edd03b67f0b5838.jpg\"><p><br></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_01-03-35-df3136b18d5d35b61aa3c7df40ac09e2.jpg\"><p>Incorrect options:</p><p><strong>7</strong></p><p><strong>3</strong></p><p><strong>1</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>References:</p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\">https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p>"}, "correct_response": ["c"], "section": "Data Engineering", "question_plain": "How many shards would a Kinesis Data Streams application need if the average record size is 500KB and 2 records per second are being written into this application that has 7 consumers?", "related_lectures": []}, {"_class": "assessment", "id": 52820502, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data preprocessing script uses the sklearn and numpy libraries in Python. Which of the following types of Glue job can be used to run this script?</p>", "relatedLectureIds": "", "answers": ["<p>PySpark</p>", "<p>Python Shell</p>", "<p>Zeppelin Shell</p>", "<p>Jupyter Shell</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Python Shell</strong></p><p>You can use Python scripts in AWS Glue to run small to medium-sized generic tasks that are often part of an ETL (extract, transform, and load) workflow. You can use Python shell jobs, for example, to submit SQL queries to services such as Amazon Redshift, Amazon Athena, or Amazon EMR, or run machine-learning and scientific analyses.</p><p>Python Shell supports Glue jobs relying on libraries such as numpy, pandas and sklearn. </p><p>Incorrect options:</p><p><strong>PySpark</strong></p><p>PySpark supports Glue jobs written primarily using Python API of Apache Spark. </p><p><strong>Zeppelin Shell</strong></p><p><strong>Jupyter Shell</strong></p><p>There is no such thing as Zeppelin Shell or Jupyter Shell, so both these options are incorrect.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/01/introducing-python-shell-jobs-in-aws-glue/\">https://aws.amazon.com/about-aws/whats-new/2019/01/introducing-python-shell-jobs-in-aws-glue/</a></p>"}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "A data preprocessing script uses the sklearn and numpy libraries in Python. Which of the following types of Glue job can be used to run this script?", "related_lectures": []}, {"_class": "assessment", "id": 52820504, "assessment_type": "multi-select", "prompt": {"question": "<p>Some of the built-in supervised learning algorithms on SageMaker require the training data to be in CSV format. Which of the following constraints should the CSV file meet? (Select two)</p>", "relatedLectureIds": "", "answers": ["<p>It should have a header record</p>", "<p>Target variable should be in the first column</p>", "<p>It should not have a header record</p>", "<p>Target variable is in the last column</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Target variable should be in the first column</strong></p><p><strong>It should not have a header record</strong></p><p>Many Amazon SageMaker algorithms support training with data in CSV format. To use data in CSV format for training, in the input data channel specification, specify text/csv as the ContentType. Amazon SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column. To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. For example, in this case 'content_type=text/csv;label_size=0'. </p><p>Incorrect options:</p><p><strong>It should have a header record</strong></p><p><strong>Target variable is in the last column</strong></p><p>These two options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a></p>"}, "correct_response": ["b", "c"], "section": "Data Engineering", "question_plain": "Some of the built-in supervised learning algorithms on SageMaker require the training data to be in CSV format. Which of the following constraints should the CSV file meet? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 52820506, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data science team is trying to port a legacy binary classification model to Amazon SageMaker. In the legacy workflow, the data engineering component was handled using Apache Spark and scikit-learn based preprocessors. </p><p>Which feature of Amazon SageMaker can be utilized for seamless integration of the legacy functionality into the new SageMaker model?</p>", "relatedLectureIds": "", "answers": ["<p>Batch Transform</p>", "<p>Elastic Inference</p>", "<p>Inference Pipeline</p>", "<p>Automatic Scaling</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Inference Pipeline</strong></p><p>An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.</p><p>You can add SageMaker Spark ML Serving and scikit-learn containers that reuse the data transformers developed for training models. The entire assembled inference pipeline can be considered as a SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.</p><p>Incorrect options:</p><p><strong>Batch Transform</strong> - To get inferences for an entire dataset, use batch transform. With batch transform, you create a batch transform job using a trained model and the dataset, which must be stored in Amazon S3. Amazon SageMaker saves the inferences in an S3 bucket that you specify when you create the batch transform job. Batch transform manages all of the compute resources required to get inferences. This includes launching instances and deleting them after the batch transform job has completed. Batch transform manages interactions between the data and the model with an object within the instance node called an agent.</p><p>Batch transform cannot be used to integrate the legacy functionality into the SageMaker model.</p><p><strong>Elastic Inference</strong> - Amazon Elastic Inference (EI) is a resource you can attach to your Amazon EC2 CPU instances to accelerate your deep learning (DL) inference workloads. Amazon EI accelerators come in multiple sizes and are a cost-effective method to build intelligent capabilities into applications running on Amazon EC2 instances.</p><p>Elastic Inference cannot be used to integrate the legacy functionality into the SageMaker model.</p><p><strong>Automatic Scaling</strong> - Amazon SageMaker supports automatic scaling (autoscaling) for your hosted models. Autoscaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, autoscaling brings more instances online. When the workload decreases, autoscaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.</p><p>Autoscaling cannot be used to integrate the legacy functionality into the SageMaker model.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</a></p>"}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "A data science team is trying to port a legacy binary classification model to Amazon SageMaker. In the legacy workflow, the data engineering component was handled using Apache Spark and scikit-learn based preprocessors. Which feature of Amazon SageMaker can be utilized for seamless integration of the legacy functionality into the new SageMaker model?", "related_lectures": []}, {"_class": "assessment", "id": 52820508, "assessment_type": "multiple-choice", "prompt": {"question": "<p>In order to run unsupervised algorithms on SageMaker you need to configure content type parameter. How should you specify the number of label columns in the content type?</p>", "relatedLectureIds": "", "answers": ["<p>application/csv;label_size=None</p>", "<p>text/csv;label_size=0</p>", "<p>application/csv;label_size=0</p>", "<p>text/csv;label_size=None</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>text/csv;label_size=0</strong></p><p>Many Amazon SageMaker algorithms support training with data in CSV format. To use data in CSV format for training, in the input data channel specification, specify <code><strong>text/csv</strong></code> as the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Channel.html#SageMaker-Type-Channel-ContentType\"><code>ContentType</code></a>. Amazon SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column. </p><p><em>To run unsupervised learning algorithms that don't have a target, specify the number of label columns in the content type. For example, in this case </em><code><strong><em>'content_type=text/csv;label_size=0'</em></strong></code><em>.</em></p><p>Incorrect options:</p><p><strong>application/csv;label_size=None</strong></p><p><strong>application/csv;label_size=0</strong></p><p><strong>text/csv;label_size=None</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "In order to run unsupervised algorithms on SageMaker you need to configure content type parameter. How should you specify the number of label columns in the content type?", "related_lectures": []}, {"_class": "assessment", "id": 52820598, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A medical diagnostics company specializes in cancer detection tests. The data scientists at the company are working on a classification model to detect cancer and they do not want any cases of cancer going undetected. The classification model\u2019s predicted value of 1 implies that the patient is predicted to have cancer.</p><p>Which of the following metrics should the data scientists focus on so that they can achieve the desired objective?</p>", "relatedLectureIds": "", "answers": ["<p>Accuracy</p>", "<p>Recall</p>", "<p>F1-score</p>", "<p>Precision</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Recall</strong></p><p>Recall measures the fraction of actual positives that are predicted as positive. The range is 0 to 1. A larger value indicates better predictive accuracy:</p><p>Recall = (True Positives / (True Positives + False Negatives))</p><p>The company would like to be extra sure that a patient does not have cancer before they pronounce them healthy. This implies that they want less false negatives. As false negatives decrease, the model would have a higher recall, so recall is the metric to focus on.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_09-58-34-3f60b24276c56de725bdc3a6d16d6831.jpg\"><p>Incorrect options:</p><p><strong>Accuracy</strong> - Accuracy measures the fraction of correct predictions.</p><p><strong>Precision</strong> - Precision measures the fraction of actual positives among those examples that are predicted as positive. </p><p><strong>F1-score</strong> - F1 score is a binary classification metric that considers both binary metrics precision and recall. It is the harmonic mean between precision and recall. </p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-09-02_10-00-15-7911cd63be4e9da578db0fca11be39ad.jpg\"><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>"}, "correct_response": ["b"], "section": "Modeling", "question_plain": "A medical diagnostics company specializes in cancer detection tests. The data scientists at the company are working on a classification model to detect cancer and they do not want any cases of cancer going undetected. The classification model\u2019s predicted value of 1 implies that the patient is predicted to have cancer.Which of the following metrics should the data scientists focus on so that they can achieve the desired objective?", "related_lectures": []}, {"_class": "assessment", "id": 52820600, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Researchers at a university claim a breakthrough in early cancer detection. The lab results for a series of trials yield the following confusion matrix. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2020-10-01_16-56-22-b1b1126e25f2988365c4067dc430ba03.png\"></p><p>What is the recall of the underlying model?</p>", "relatedLectureIds": "", "answers": ["<p>80%</p>", "<p>20%</p>", "<p>50%</p>", "<p>89%</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>89%</strong></p><p>Recall = (True Positives / (True Positives + False Negatives)) = (80/(80+10)) = 0.89 or 89%</p><p>Incorrect options:</p><p><strong>80%</strong></p><p><strong>20%</strong></p><p><strong>50%</strong></p><p>These three options do not match the calculations shown above, so these options are incorrect.</p>"}, "correct_response": ["d"], "section": "Modeling", "question_plain": "Researchers at a university claim a breakthrough in early cancer detection. The lab results for a series of trials yield the following confusion matrix. What is the recall of the underlying model?", "related_lectures": []}, {"_class": "assessment", "id": 52820602, "assessment_type": "multiple-choice", "prompt": {"question": "<p>How many containers can a SageMaker Inference Pipeline support?</p>", "relatedLectureIds": "", "answers": ["<p>1 container </p>", "<p>Between 2 and 5 containers</p>", "<p>Between 2 and 15 containers</p>", "<p>Between 2 and 20 containers</p>"], "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Between 2 and 15 containers</strong></p><p>An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.</p><p>You can add SageMaker Spark ML Serving and scikit-learn containers that reuse the data transformers developed for training models. The entire assembled inference pipeline can be considered as a SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.</p><p>Incorrect options:</p><p><strong>1 container </strong></p><p><strong>Between 2 and 5 containers</strong></p><p><strong>Between 2 and 20 containers</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</a></p>"}, "correct_response": ["c"], "section": "ML Implementation and Operations", "question_plain": "How many containers can a SageMaker Inference Pipeline support?", "related_lectures": []}, {"_class": "assessment", "id": 52820604, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An e-commerce company is looking for a solution that detects anomalies in order to identify fraudulent transactions. The company utilizes Amazon Kinesis to transfer JSON-formatted transaction records from its on-premises database to Amazon S3. The existing dataset comprises 200-column wide records for each transaction. To identify fraudulent transactions, the solution needs to analyze just 20 of these columns.</p><p>Which of the following would you suggest as the lowest-cost solution that needs the least development work and offers out-of-the-box anomaly detection functionality?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</strong></p><p>For the given use case, you can use an AWS Glue job to extract, transform, and load (ETL) data from the data source (in JSON format) to the data target (in Parquet format). You can then use an AWS Glue crawler, which is a program that connects to a data store (source or target) such as Amazon S3, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in the AWS Glue Data Catalog.</p><p><img src=\"https://assets-pt.media.datacumulus.com/aws-das-pt/assets/pt2-q5-i1.jpg\"></p><p>via - <a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p><p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run, thereby making this solution really low cost. You can also use Athena to build a table with only the subset of columns that are required for downstream analysis.</p><p><br></p><p>Finally, you can read the data in the given Athena table via Amazon QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection. QuickSight uses proven Amazon technology to continuously run ML-powered anomaly detection across millions of metrics to discover hidden trends and outliers in your data. This anomaly detection enables you to get deep insights that are often buried in the aggregates and not scalable with manual analysis. With ML-powered anomaly detection, you can find outliers in your data without the need for manual analysis, custom development, or ML domain expertise.</p><p><br></p><p>Incorrect options:</p><p><strong>Use Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries that compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</strong> - Using Kinesis Data Analytics involves some custom query development to analyze the incoming data to compute an anomaly score for all transactions. In addition, this solution processes all columns of the data instead of just the subset of columns required for the analysis. Therefore, this option is not the best fit for the given use case. </p><p><strong>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3 </strong>- Amazon SageMaker is a fully managed service to build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. Using SageMaker involves custom code development to build, develop, test, and deploy the anomaly detection model that is relevant to the given scenario. Instead, you can directly use QuickSight to identify fraudulent transactions using QuickSight's built-in machine learning-based anomaly detection functionality. Therefore, this option is not the right fit for the given use case.</p><p><strong>Use Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</strong> - This option involves significant custom code development on a Lambda function to examine the incoming stream from Firehose and then compute an anomaly score for all transactions. In addition, the lambda looks at all the fields in the data instead of just the subset of fields required for the analysis. Therefore, this option is incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html\">https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html</a></p>", "answers": ["<p>Use Kinesis Data Analytics to detect anomalies on a data stream from Kinesis Streams by running SQL queries that compute an anomaly score for all transactions and then store all fraudulent transactions in Amazon S3. Use Amazon QuickSight to visualize the results from Amazon S3</p>", "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon SageMaker to build an anomaly detection model that can detect fraudulent transactions by ingesting data directly from Amazon S3</p>", "<p>Use Kinesis Data Firehose to detect anomalies on a data stream from Kinesis Streams via a Lambda function which computes an anomaly score for all transactions and stores all fraudulent transactions in Amazon RDS. Use Amazon QuickSight to visualize the results from RDS</p>", "<p>Transform the data from JSON format to Apache Parquet format using an AWS Glue job. Configure AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalog. Leverage Amazon Athena to create a table with a subset of columns. Set up Amazon QuickSight for visual analysis of the data and identify fraudulent transactions using QuickSight's built-in machine learning-powered anomaly detection</p>"]}, "correct_response": ["d"], "section": "Data Engineering", "question_plain": "An e-commerce company is looking for a solution that detects anomalies in order to identify fraudulent transactions. The company utilizes Amazon Kinesis to transfer JSON-formatted transaction records from its on-premises database to Amazon S3. The existing dataset comprises 200-column wide records for each transaction. To identify fraudulent transactions, the solution needs to analyze just 20 of these columns.Which of the following would you suggest as the lowest-cost solution that needs the least development work and offers out-of-the-box anomaly detection functionality?", "related_lectures": []}]}
4755598
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 61582230, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>The traffic monitoring authorities at a city want to monitor the traffic at busy intersections and take corrective action at the earliest. An ML solutions company has developed a Proof-of-Concept for processing this video data and now it wants to productionalize it to cover all city intersections. </p><p>As an AWS&nbsp;ML&nbsp;specialist, which of the following solutions would you recommend so that it takes the LEAST amount of development effort and ongoing maintenance effort?</p>", "answers": ["<p>Process the incoming video data using Spark Streaming on EMR cluster to detect anomalous traffic situations and alert the authorities</p>", "<p>Process the incoming video data using Kinesis Data Streams, trigger a Lambda for each stream and then do frame analysis to detect anomalous traffic situations and alert the authorities</p>", "<p>Analyse the incoming video streams using Kinesis Video Streams in real time and then send an alert using a downstream EC2 instance</p>", "<p>Process the incoming video data using Kinesis Data Analytics to detect anomalies in real time</p>"], "explanation": "<p>Correct option:</p><p><strong>Analyse the incoming video streams using Kinesis Video Streams in real time and then send an alert using a downstream EC2 instance</strong></p><p>Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming video data from millions of devices. It durably stores, encrypts, and indexes video data in your streams, and allows you to access your data through easy-to-use APIs.</p><p>Kinesis Video Streams enables you to playback video for live and on-demand viewing, and quickly build applications that take advantage of computer vision and video analytics through integration with Amazon Rekognition Video, and libraries for ML frameworks such as Apache MxNet, TensorFlow, and OpenCV. Kinesis Video Streams also supports WebRTC, an open-source project that enables real-time media streaming and interaction between web browsers, mobile applications, and connected devices via simple APIs. Typical uses include video chat and peer-to-peer media streaming.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_05-04-42-3aca38e8c906d69742e200ad19f7a2db.jpg\"><p>You can use Amazon Kinesis Video Streams to securely and cost-effectively ingest, store, and analyze this massive volume of video data to help solve traffic problems mentioned in the given use-case.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_05-05-24-58ab850e0faa21b42de56004c1bcea8b.jpg\"><p>Incorrect options:</p><p><strong>Process the incoming video stream data directly using Kinesis Data Streams, trigger a Lambda for each stream and then do frame analysis to detect anomalous traffic situations and alert the authorities</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.</p><p>Kinesis Data Streams cannot directly consume the incoming video stream data. You will need to develop custom code to process the incoming video streams, so this option is incorrect.</p><p><strong>Process the incoming video stream data directly using Kinesis Data Analytics to detect anomalies in real time</strong> - Kinesis Data Analytics enables you to quickly build end-to-end stream processing applications for log analytics, clickstream analytics, Internet of Things (IoT), ad tech, gaming, and more. The four most common use cases are streaming extract-transform-load (ETL), continuous metric generation, responsive real-time analytics, and interactive querying of data streams. Kinesis Data Analytics cannot directly consume the incoming video stream data. You will need to develop custom code to process the incoming video streams via Kinesis Data Streams and then direct the resultant feed into Kinesis Data Analytics, so this option is incorrect.</p><p><strong>Process the incoming video data using Spark Streaming on EMR cluster to detect anomalous traffic situations and alert the authorities</strong> - Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3).</p><p>Amazon EMR is ideal for problems that necessitate the fast and efficient processing of large amounts of data. The web service interfaces allow you to build processing workflows, and programmatically monitor progress of running clusters. In addition, you can use the simple web interface of the AWS Management Console to launch your clusters and monitor processing-intensive computation on clusters of Amazon EC2 instances.</p><p>Using EMR cluster would imply managing the underlying infrastructure, therefore it would increase the maintenance effort. So this option is ruled out for the given use-case.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/kinesis/video-streams/faqs/\">https://aws.amazon.com/kinesis/video-streams/faqs/</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Data Engineering", "question_plain": "The traffic monitoring authorities at a city want to monitor the traffic at busy intersections and take corrective action at the earliest. An ML solutions company has developed a Proof-of-Concept for processing this video data and now it wants to productionalize it to cover all city intersections. As an AWS&nbsp;ML&nbsp;specialist, which of the following solutions would you recommend so that it takes the LEAST amount of development effort and ongoing maintenance effort?", "related_lectures": []}, {"_class": "assessment", "id": 61582232, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An Analytics Consulting Firm would like to capture and analyze the real time metrics for a cab hailing service. The Firm would like to identify \u201cdemand hotspots\u201d in real time so that additional cabs can be dispatched to meet the sudden spurt in demand. </p><p>Which of the following solutions would require the LEAST development and maintenance effort to build a real time analytics solution for this use case?</p>", "answers": ["<p>Ingest the data into Kinesis Data Streams and immediately write the stream into Kinesis Data Analytics for SQL based analysis so that appropriate alerts can be sent to the drivers. Once processing is done, the streams are dumped into S3 using Kinesis Data Firehose</p>", "<p>Ingest the source data directly into Kinesis Data Analytics so that real time analytics can be done without any processing delay. Once processing is done, the streams are dumped into S3 using Kinesis Data Firehose</p>", "<p>Ingest the data into Kinesis Data Firehose and write into S3, which triggers a Lambda that analyses the event data. The Lambda finally writes the output to S3</p>", "<p>Ingest the data into Kinesis Data Streams that writes the data into a Spark Streaming application running on an EMR cluster. Once the processing is done, the output is written on S3</p>"], "explanation": "<p>Correct option:</p><p><strong>Ingest the data into Kinesis Data Streams and immediately write the stream into Kinesis Data Analytics for SQL based analysis so that appropriate alerts can be sent to the drivers. Once processing is done, the streams are dumped into S3 using Kinesis Data Firehose</strong></p><p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_06-38-36-654a0f67d27a5ad168879954c3addf8a.jpg\"><p>Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services.</p><p>Amazon Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.</p><p>Kinesis Data Analytics cannot directly ingest source data as seen from the diagram below:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_06-39-09-43edb120271f00f7be5bb68533e4204f.jpg\"><p>Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_06-40-15-d10e9e1dc1048bb575754ec0ec7746b0.jpg\"><p>The correct solution is to ingest the data into Kinesis Data Streams and immediately write the stream into Kinesis Data Analytics for SQL based analysis so that appropriate alerts can be sent to the drivers. Once processing is done, the streams are dumped into S3 using Kinesis Data Firehose.</p><p>Incorrect options:</p><p><strong>Ingest the source data directly into Kinesis Data Analytics so that real time analytics can be done without any processing delay. Once processing is done, the streams are dumped into S3 using Kinesis Data Firehose</strong> - Kinesis Data Analytics cannot directly ingest source data. For the given use-case, you can feed data into Kinesis Data Analytics via Kinesis Data Streams.&nbsp; </p><p><strong>Ingest the data into Kinesis Data Firehose and write into S3, which triggers a Lambda that analyses the event data. The Lambda finally writes the output to S3</strong> - Using a combination of Kinesis Firehose with Lambda would introduce a buffering delay of at least 1 minute or 1MB of data, so the solution will not be real time. Therefore, this option is incorrect.</p><p><strong>Ingest the data into Kinesis Data Streams that writes the data into a Spark Streaming application running on an EMR cluster. Once the processing is done, the output is written on S3</strong></p><p>Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3).</p><p>Using EMR would significantly increase the development and maintenance effort since the cluster has to be managed manually. So it\u2019s not the right choice. </p><p><br></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "An Analytics Consulting Firm would like to capture and analyze the real time metrics for a cab hailing service. The Firm would like to identify \u201cdemand hotspots\u201d in real time so that additional cabs can be dispatched to meet the sudden spurt in demand. Which of the following solutions would require the LEAST development and maintenance effort to build a real time analytics solution for this use case?", "related_lectures": []}, {"_class": "assessment", "id": 61582234, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A plumbing company wants to better predict the sales of its flagship copper tubing for the next year. The sales data has copper tubing sizes captured as XS, S, M, L, XL and the retail price of the copper tubing varies with the size. </p><p>Which of the following data preparation steps need to be followed for the copper tubing size before it goes into the regression model for prediction?</p>", "answers": ["<p>One-hot Encoding</p>", "<p>Categorical Encoding</p>", "<p>Quantile Binning</p>", "<p>Interval Binning</p>"], "explanation": "<p>Correct option:</p><p><strong>Categorical Encoding</strong></p><p>Most of the machine learning algorithms can not handle categorical variables unless we convert them to numerical values. Many algorithm\u2019s performances vary based on how categorical variables are encoded.</p><p>Categorical variables can be divided into two categories: Nominal (no particular order) and Ordinal (follow some order).</p><p>As pricing varies with the size, we need to carry out categorical encoding that is representative of the size of the copper tubing. An example could be like so:</p><p>XS -&gt; 2</p><p>S -&gt; 4</p><p>M -&gt; 7</p><p>L -&gt; 10</p><p>XL -&gt; 12</p><p>Here is a good reference for understanding categorical encoding:</p><p><a href=\"https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\">https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02</a></p><p>Incorrect options:</p><p><strong>One-hot Encoding</strong> - A one hot encoding is a representation of categorical variables as binary vectors. Initially, categorical values are mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.</p><p>One-hot encoding would not capture the price variance with respect to size, so this option is incorrect.</p><p><strong>Quantile Binning</strong></p><p><strong>Interval Binning</strong></p><p>Binning: Binning is the process of converting numeric data into categorical data. It is one of the methods used in feature engineering. Binning comes in very handy for numeric features, especially when it is one with wide range.</p><p>Quantile Binning: Quantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data are smaller, and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc.</p><p>Interval Binning: Each bin contains a specific numeric range. For example, we can group a person\u2019s age into decades: 0\u20139 years old will be in bin 1, 10\u201319 years fall will be in bin 2.</p><p>Binning (of any type) is not relevant in this case, so both these options are incorrect.</p><p>Please refer to the link below for more information on binning:</p><p><a href=\"https://medium.com/hacktive-devs/feature-engineering-in-machine-learning-part-1-a3904769cd93\">https://medium.com/hacktive-devs/feature-engineering-in-machine-learning-part-1-a3904769cd93</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "A plumbing company wants to better predict the sales of its flagship copper tubing for the next year. The sales data has copper tubing sizes captured as XS, S, M, L, XL and the retail price of the copper tubing varies with the size. Which of the following data preparation steps need to be followed for the copper tubing size before it goes into the regression model for prediction?", "related_lectures": []}, {"_class": "assessment", "id": 61582236, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An electronics goods company wants to do sentiment analysis of the customer feedback for its latest product recently launched in France. However, the customer feedback is only available in the form of recorded audio snippets in the French language. </p><p>As an AWS&nbsp;ML specialist, which of the following AWS AI services would you use to build the simplest solution with the least development effort?</p>", "answers": ["<p>Transcribe -&gt; Translate -&gt; Comprehend</p>", "<p>Transcribe -&gt; Lex -&gt; Comprehend</p>", "<p>Transcribe -&gt; Comprehend</p>", "<p>Translate -&gt; Transcribe -&gt; Comprehend</p>"], "explanation": "<p>Correct option:</p><p><strong>Transcribe -&gt; Comprehend</strong></p><p>Amazon Transcribe: Amazon Transcribe makes it easy for developers to add speech to text capabilities to their applications. Amazon Transcribe uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately. Amazon Transcribe can be used to transcribe customer service calls, automate subtitling, and generate metadata for media assets to create a fully searchable archive. You can use Amazon Transcribe Medical to add medical speech to text capabilities to clinical documentation applications.</p><p>Amazon Comprehend: Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. Instead of combing through documents, the process is simplified and unseen information is easier to understand.</p><p>The service can identify critical elements in data, including references to language, people, and places, and the text files can be categorized by relevant topics. In real time, you can automatically and accurately detect customer sentiment in your content. This accelerates more informed, real-time decision making to improve customer experiences. Comprehend not only locates any content that contains personally identifiable information, it also redacts and masks that content.</p><p>For the given use-case, the recorded audio would have to be first transcribed into French Language and it then can be directly fed into the Comprehend service for sentiment analysis. So the correct order is : Transcribe -&gt; Comprehend.</p><p>Incorrect options:</p><p><strong>Transcribe -&gt; Translate -&gt; Comprehend</strong> - Comprehend supports multiple languages for sentiment analysis, so there is no need for adding the Translate service, unless there is a need to store the transcribed text based customer feedback in French language to another language such as English. </p><p><strong>Transcribe -&gt; Lex -&gt; Comprehend</strong> - Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Alexa, Amazon Lex provides high quality speech recognition and language understanding capabilities, enabling addition of sophisticated, natural language \u2018chatbots\u2019 to new and existing applications. Amazon Lex reduces multi-platform development effort, allowing you to easily publish your speech or text chatbots to mobile devices and multiple chat services. As the given use-case does not have a requirement of a conversational interface, so Lex is not needed.</p><p><strong>Translate -&gt; Transcribe -&gt; Comprehend </strong>- Since Comprehend supports multiple languages for sentiment analysis, so there is no need for adding the Translate service. In addition, the order of services, that is, Translate followed by Transcribe, is incorrect for this option.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "ML Implementation and Operations", "question_plain": "An electronics goods company wants to do sentiment analysis of the customer feedback for its latest product recently launched in France. However, the customer feedback is only available in the form of recorded audio snippets in the French language. As an AWS&nbsp;ML specialist, which of the following AWS AI services would you use to build the simplest solution with the least development effort?", "related_lectures": []}, {"_class": "assessment", "id": 61582238, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An ML specialist is examining the root cause for the under-performance of a regression model and the specialist believes that the model is consistently overestimating the outcome. </p><p>Which of the following metrics should the specialist track on a chart to help identify any pattern of model overestimation?</p>", "answers": ["<p>RMSE</p>", "<p>AUC</p>", "<p>Residuals</p>", "<p>Mean Absolute Error</p>"], "explanation": "<p>Correct option:</p><p><strong>Residuals</strong></p><p>A residual for an observation in the evaluation data is the difference between the true target and the predicted target. Residuals represent the portion of the target that the model is unable to predict. A positive residual indicates that the model is underestimating the target (the actual target is larger than the predicted target). A negative residual indicates an overestimation (the actual target is smaller than the predicted target).</p><p>The residuals plot would indicate any trend of underestimation or overestimation. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_07-56-25-d5f86c2f3a3c393cd89531ee3845613b.jpg\"><p>Incorrect options:</p><p><strong>RMSE</strong> - For regression tasks, Amazon ML uses the industry standard root mean square error (RMSE) metric. It is a distance measure between the predicted numeric target and the actual numeric answer (ground truth). The smaller the value of the RMSE, the better is the predictive accuracy of the model. A model with perfectly correct predictions would have an RMSE of 0.</p><p><strong>Mean Absolute Error</strong> - Given any test data-set, Mean Absolute Error (MAE) of your model refers to the mean of the absolute values of each prediction error on all instances of the test data-set.</p><p>Both Mean Absolute Error and RMSE would only give the magnitude of the error, so both these options are incorrect.</p><p><strong>AUC</strong></p><p>Amazon ML provides an industry-standard accuracy metric for binary classification models called Area Under the (Receiver Operating Characteristic) Curve (AUC). AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Because it is independent of the score cut-off, you can get a sense of the prediction accuracy of your model from the AUC metric without picking a threshold.</p><p>The AUC metric returns a decimal value from 0 to 1. AUC values near 1 indicate an ML model that is highly accurate. Values near 0.5 indicate an ML model that is no better than guessing at random. Values near 0 are unusual to see, and typically indicate a problem with the data. Essentially, an AUC near 0 says that the ML model has learned the correct patterns, but is using them to make predictions that are flipped from reality ('0's are predicted as '1's and vice versa).</p><p>AUC metric is used for classification models, so this option is not the right fit for the given use-case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression-model-insights.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "An ML specialist is examining the root cause for the under-performance of a regression model and the specialist believes that the model is consistently overestimating the outcome. Which of the following metrics should the specialist track on a chart to help identify any pattern of model overestimation?", "related_lectures": []}, {"_class": "assessment", "id": 61582240, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An Analytics Consulting Firm wants you to review a Classification Model trained on historical data and deployed about 6 months ago. At the time of deployment the model performance was up to the mark. Post deployment, the model has not been retrained on the incremental data coming in every day. Now the model performance has gone down significantly. </p><p>As an ML Specialist, which of the following solutions would you recommended to address the given use case?</p>", "answers": ["<p>Completely retrain the model using only the data for the last 6 months</p>", "<p>Completely retrain the model using the historical data along with the data for the last 6 months</p>", "<p>Completely retrain the model again using only the historical data</p>", "<p>Change the algorithm behind the model for better performance</p>"], "explanation": "<p>Correct option:</p><p><strong>Completely retrain the model using the historical data along with the data for the last 6 months</strong></p><p>This is an example of model deterioration because the training data has aged. The solution is to retrain the model using the historical data along with the data for the last 6 months. </p><p>The following process can be employed to maintain a model:</p><p>1. Assess: Assess the models periodically and proactively with new datasets and compare its performance with measures. Even if the performance deteriorates but still within the acceptable threshold, it should still be OK. However, this assessment has to be carried out at least once in 6 months.</p><p>2. Retrain: If the above assessment falls below the threshold, retrain the model with fresh sample of datasets - sometimes with more number of observations. However, during retraining, keep all the original and derived features of the original model. The fresh and added datasets may lead to change in coefficients of the model performing better.</p><p>3. Rebuild: In spite of retrain, if the performance of model does not improve, just scrap the original model entirely and start from scratch. This means, analyzing new and old features, imputing missing values, one-hot encoding and label encoding features, performing feature engineering, building diversified models, and ensemble them. Finally deploy this model into production and perform A/B testing to measure the performance of new model.</p><p>Incorrect options:</p><p><strong>Completely retrain the model using only the data for the last 6 months</strong></p><p><strong>Completely retrain the model again using only the historical data</strong></p><p>As mentioned in the explanation above, you should use the historical data along with the data for the last 6 months, so as not to introduce any bias in the model for recent data versus historic data.</p><p><strong>Change the algorithm behind the model for better performance - </strong>As mentioned earlier, this step should be taken only if the performance of model does not improve. Then you need to just scrap the original model entirely and start from scratch. </p><p>References:</p><p><a href=\"https://blog.socratesk.com/blog/2017/11/14/model-deterioration\">https://blog.socratesk.com/blog/2017/11/14/model-deterioration</a></p><p><a href=\"https://towardsdatascience.com/why-machine-learning-models-degrade-in-production-d0f2108e9214\">https://towardsdatascience.com/why-machine-learning-models-degrade-in-production-d0f2108e9214</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "ML Implementation and Operations", "question_plain": "An Analytics Consulting Firm wants you to review a Classification Model trained on historical data and deployed about 6 months ago. At the time of deployment the model performance was up to the mark. Post deployment, the model has not been retrained on the incremental data coming in every day. Now the model performance has gone down significantly. As an ML Specialist, which of the following solutions would you recommended to address the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 61582242, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A retail organization ingests 100GB of data into S3 from its global storefronts on a daily basis. This data needs to be cleaned, prepared and analyzed daily so that sales reports can be sent out to the business stakeholders. </p><p>Which option takes the least effort to make this data available for SQL queries?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Configure Glue crawlers to set up Athena tables that read data from S3. The daily data is then readily available for SQL queries in Athena as soon as it arrives</strong></p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately. You don\u2019t even need to load your data into Athena, it works directly with data stored in S3.</p><p>Amazon Athena uses a managed Data Catalog to store information and schemas about the databases and tables that you create for your data stored in Amazon S3. In regions where AWS Glue is available, you can upgrade to using the AWS Glue Data Catalog with Amazon Athena. In regions where AWS Glue is not available, Athena uses an internal Catalog.</p><p>You can modify the catalog using DDL statements or via the AWS Management Console. Any schemas you define are automatically saved unless you explicitly delete them. Athena uses schema-on-read technology, which means that your table definitions applied to your data in S3 when queries are being executed. There\u2019s no data loading or transformation required. You can delete table definitions and schema without impacting the underlying data stored on Amazon S3.</p><p>AWS Glue automatically crawls your data sources, identifies data formats, and suggests schemas and transformations. Crawlers can help automate table creation and automatic loading of partitions. </p><p>So for the given use case, configuring Glue crawlers is just a one-time set up to create the Athena tables and this is the least effort way of getting the daily S3 data available for SQL&nbsp;queries.</p><p>Incorrect options:</p><p><strong>Setup a daily Glue ETL&nbsp;job to write the incremental S3 data into Redshift and have it available for SQL queries</strong></p><p>Amazon Redshift is a cloud data warehouse. It makes it fast, simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query execution.</p><p><strong>Setup a daily Glue ETL&nbsp;job to write the incremental S3 data into RDS and have it available for SQL queries</strong></p><p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.</p><p>A Glue ETL&nbsp;job is the business logic that performs the Extract, Transform, and Load (ETL) work in AWS Glue. When you start a job, AWS Glue runs a script that extracts data from sources, transforms the data, and loads it into targets. You can create jobs in the ETL section of the AWS Glue console. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_08-38-44-37840a722268a32a750b05d7a38782e6.jpg\"></p><p>Using a daily Glue job adds unnecessary complexity into the solution, as the data will have to copied daily from S3 (source) to either Redshift or RDS (target) via the ETL&nbsp;job. So both these options are ruled out.</p><p><strong>Setup a daily Glue ETL&nbsp;job to write the incremental S3 data into DynamoDB and have it available for SQL queries</strong></p><p>Amazon DynamoDB is a key-value and document NoSQL&nbsp;database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. You can\u2019t use SQL queries with DynamoDB, so this option is incorrect.</p><p>References:</p><p><a href=\"https://aws.amazon.com/athena/faqs/\">https://aws.amazon.com/athena/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/crawler-running.html\">https://docs.aws.amazon.com/glue/latest/dg/crawler-running.html</a></p>", "answers": ["<p>Configure Glue crawlers to set up Athena tables that read data from S3. The daily data is then readily available for SQL queries in Athena as soon as it arrives</p>", "<p>Setup a daily Glue ETL&nbsp;job to write the incremental S3 data into Redshift and have it available for SQL queries</p>", "<p>Setup a daily Glue ETL job to write the incremental S3 data into RDS and have it available for SQL queries</p>", "<p>Setup a daily Glue ETL&nbsp;job to write the incremental S3 data into DynamoDB and have it available for SQL queries</p>"]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "A retail organization ingests 100GB of data into S3 from its global storefronts on a daily basis. This data needs to be cleaned, prepared and analyzed daily so that sales reports can be sent out to the business stakeholders. Which option takes the least effort to make this data available for SQL queries?", "related_lectures": []}, {"_class": "assessment", "id": 61582244, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>The compliance department at a major Financial Services Firm wants to monitor the SageMaker services used by the Data Science team for their ML jobs. </p><p>Which services can be used to achieve this objective? (Select two)</p>", "answers": ["<p>Amazon Inspector</p>", "<p>Amazon CloudWatch</p>", "<p>AWS CloudTrail</p>", "<p>AWS Config</p>"], "explanation": "<p>Correct options:</p><p><strong>Amazon CloudWatch </strong></p><p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so you can get a unified view of your AWS resources, applications, and services that run in AWS and on-premises. You can correlate your metrics and logs to better understand the health and performance of your resources. You can also create alarms based on metric value thresholds you specify, or that can watch for anomalous metric behavior based on machine learning algorithms. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_09-44-10-7f5b616724cc3788a34ff59e88109f80.jpg\"><p><strong>AWS CloudTrail</strong></p><p>With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_09-53-54-896f9736819ed151bd31947d256d1ff7.jpg\"><p>Both CloudWatch and CloudTrail can be used to monitor the SageMaker services:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_09-51-24-f4e17dbf4fff4c5751aea930d82d02ed.jpg\"><p>Incorrect options:</p><p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. You cannot use Inspector to monitor the SageMaker services.</p><p><strong>AWS&nbsp;Config</strong> - AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time. You cannot use Config to monitor the SageMaker services.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-overview.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-overview.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b", "c"], "section": "ML Implementation and Operations", "question_plain": "The compliance department at a major Financial Services Firm wants to monitor the SageMaker services used by the Data Science team for their ML jobs. Which services can be used to achieve this objective? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582246, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>An analyst is trying to create a box plot for the following data points :</p><p>10.2, 14.1, 14.4. 14.4, 14.4, 14.5, 14.5, 14.6, 14.7, 14.7, 14.7, 14.9, 15.1, 15.9, 16.4</p><p>Based on these data points, we have the following characteristics :</p><p>Q1(25th percentile) = 14.4</p><p>Q2(50th percentile) = 14.6</p><p>Q3(75th percentile) = 14.9</p><p><br></p><p>Identify the data points that would show up as outliers on the box plot (Select three):</p>", "answers": ["<p>10.2</p>", "<p>14.1</p>", "<p>14.4</p>", "<p>15.1</p>", "<p>15.9</p>", "<p>16.4</p>"], "explanation": "<p>Interquartile Range (IQR) = Q3-Q1 = 0.5</p><p>Minimum outlier cutoff = Q1 - 1.5 * IQR = 14.4 - (1.5*0.5) = 13.65</p><p>Maximum outlier cutoff = Q3 + 1.5 * IQR = 14.9 + (1.5*0.5) = 15.65</p><p>So the outlier would be anything less than 13.65 or anything more than 15.65. Thus the outliers are 10.2, 15.9, 16.4 for the given problem statement.</p><p>Here is an excellent reference for a deep-dive on the box plot statistical characteristics: </p><p><a href=\"https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51\">https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51</a></p>", "feedbacks": ["", "", "", "", "", ""]}, "correct_response": ["a", "e", "f"], "section": "Exploratory Data Analysis", "question_plain": "An analyst is trying to create a box plot for the following data points :10.2, 14.1, 14.4. 14.4, 14.4, 14.5, 14.5, 14.6, 14.7, 14.7, 14.7, 14.9, 15.1, 15.9, 16.4Based on these data points, we have the following characteristics :Q1(25th percentile) = 14.4Q2(50th percentile) = 14.6Q3(75th percentile) = 14.9Identify the data points that would show up as outliers on the box plot (Select three):", "related_lectures": []}, {"_class": "assessment", "id": 61582248, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A company wants to enhance the existing security procedure at its data center so that only the employees with the required privilege are able to access certain sensitive areas of the facility. The company wants to do a facial match so that only bona fide employees can access these sensitive areas. </p><p>Which service would you recommend to build this solution with the least possible effort?</p>", "answers": ["<p>SageMaker Object Detection</p>", "<p>SageMaker Image Classification</p>", "<p>Amazon Rekognition</p>", "<p>SageMaker Semantic Segmentation</p>"], "explanation": "<p>Correct option:</p><p><strong>Amazon Rekognition:</strong></p><p>Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p>Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them.</p><p>Rekognition Image uses deep neural network models to detect and label thousands of objects and scenes in your images, and continually adding new labels and facial recognition features to the service. With Rekognition Image, you only pay for the images you analyze and the face metadata you store.</p><p>Rekognition Video is a video recognition service that detects activities; understands the movement of people in frame; and recognizes objects, celebrities, and inappropriate content in videos stored in Amazon S3 and live video streams from Acuity. Rekognition Video detects persons and tracks them through the video even when their faces are not visible, or as the whole person might go in and out of the scene.</p><p>You can refer to the following AWS blog post on how to leverage Amazon&nbsp;Rekognition to build your own face recognition service:</p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/\">https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/</a></p><p>Incorrect options:</p><p><strong>SageMaker Object Detection</strong> - The Amazon SageMaker Object Detection algorithm detects and classifies objects in images using a single deep neural network. It is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene. The object is categorized into one of the classes in a specified collection with a confidence score that it belongs to the class.</p><p><strong>SageMaker Image Classification</strong> - The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. It uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available.</p><p><strong>SageMaker semantic segmentation</strong> - The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.</p><p>Using SageMaker algorithms involves custom development effort to develop, test and deploy the model, so these three options are not the right for the given use-case.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "A company wants to enhance the existing security procedure at its data center so that only the employees with the required privilege are able to access certain sensitive areas of the facility. The company wants to do a facial match so that only bona fide employees can access these sensitive areas. Which service would you recommend to build this solution with the least possible effort?", "related_lectures": []}, {"_class": "assessment", "id": 61582358, "assessment_type": "multi-select", "prompt": {"question": "<p>The data science team at a retail company wants to predict customer churn using a dataset that has 200 continuous numerical characteristics. The Sales department has offered no insights on which characteristics are important for the churn prediction. The Sales department wants to interpret the model and then determine the direct effect of significant characteristics on the model's output. While training a logistic regression model, the data science team has noticed a significant difference in the accuracy of the training and validation datasets.</p><p>Which of the following options can the data science team use to enhance the model's performance for addressing the given use case? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Leverage L1 regularization for the classifier</strong></p><p>Regularization helps prevent linear models from overfitting training data examples (that is, memorizing patterns instead of generalizing them) by penalizing extreme weight values. L1 regularization has the effect of reducing the number of features used in the model by pushing to zero the weights of features that would otherwise have small weights. As a result, L1 regularization results in sparse models and reduces the amount of noise in the model.</p><p>The given use case states that there is a significant difference in the accuracy of the training and validation datasets, so the model is overfitting. Therefore, you can use L1 regularization as a solution for the given scenario.</p><p><strong>Set up recursive feature elimination</strong></p><p>Recursive Feature Elimination, or RFE is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable. Hence it allows you to choose the best features without modifying the existing features, which is a key requirement of the given scenario, as the Sales department wants to interpret the model and then determine the direct effect of significant characteristics on the model's output.</p><p>Incorrect options:</p><p><strong>Add more features to the dataset</strong> - If you add more features to the dataset, it would further increase the model\u2019s overfitting. So this option is incorrect.</p><p><strong>Leverage t-distributed stochastic neighbor embedding (t-SNE)</strong> - t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration, dimensionality reduction and visualizing high-dimensional data. In simpler terms, t-SNE gives you a feel or intuition of how the data is arranged in a high-dimensional space. You won\u2019t be able to see the direct impact of relevant features on the model outcome. Note that although you could use t-SNE&nbsp;for dimensionality reduction, it would result in dimensions that are not directly interpretable. Remember the key requirement of the given scenario - the Sales department wants to interpret the model and then determine the direct effect of significant characteristics on the model's output. So this option is incorrect.</p><p><strong>Leverage linear discriminant analysis</strong> - Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u201ccurse of dimensionality\u201d) and also reduce computational costs. Note that although you could use LDA&nbsp;for dimensionality reduction, it would result in dimensions that are not directly interpretable. So for this option as well, you won\u2019t be able to see the direct impact of relevant features on the model outcome. Remember the key requirement of the given scenario - the Sales department wants to interpret the model and then determine the direct effect of significant characteristics on the model's output. So this option is incorrect.</p><p><br></p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/deploying-your-own-data-processing-code-in-an-amazon-sagemaker-autopilot-inference-pipeline/\">https://aws.amazon.com/blogs/machine-learning/deploying-your-own-data-processing-code-in-an-amazon-sagemaker-autopilot-inference-pipeline/</a></p><p><br></p>", "answers": ["<p>Add more features to the dataset</p>", "<p>Leverage L1 regularization for the classifier</p>", "<p>Set up recursive feature elimination</p>", "<p>Leverage t-distributed stochastic neighbor embedding (t-SNE)</p>", "<p>Leverage linear discriminant analysis</p>"]}, "correct_response": ["b", "c"], "section": "Data Engineering", "question_plain": "The data science team at a retail company wants to predict customer churn using a dataset that has 200 continuous numerical characteristics. The Sales department has offered no insights on which characteristics are important for the churn prediction. The Sales department wants to interpret the model and then determine the direct effect of significant characteristics on the model's output. While training a logistic regression model, the data science team has noticed a significant difference in the accuracy of the training and validation datasets.Which of the following options can the data science team use to enhance the model's performance for addressing the given use case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582250, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>Which of the following options can be used to train a SageMaker model? (Select three)</p>", "answers": ["<p>SageMaker Console</p>", "<p>SageMaker API</p>", "<p>AWS Glue</p>", "<p>SageMaker Notebook</p>", "<p>SageMaker Trainer</p>"], "explanation": "<p>Correct options:</p><p><strong>SageMaker Console</strong></p><p><strong>SageMaker API</strong></p><p>In machine learning, you \"teach\" a computer to make predictions, or inferences. First, you use an algorithm and example data to train a model. Then you integrate your model into your application to generate inferences in real time and at scale. In a production environment, a model typically learns from millions of example data items and produces inferences in hundreds to less than 20 milliseconds.</p><p>The following diagram shows how you train and deploy a model with Amazon SageMaker:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_11-45-41-01d4d53e5b285ec4eb160d821683a5df.jpg\"><p>To train a model in SageMaker, you create a training job. You can create a training job with the SageMaker console or the API.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_11-46-36-2ce2934d82aa4af148be4e44fac74cd4.jpg\"><p><strong>SageMaker Notebook</strong></p><p>An Amazon SageMaker notebook instance is a machine learning (ML) compute instance running the Jupyter Notebook App. SageMaker manages creating the instance and related resources. Use Jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to SageMaker hosting, and test or validate your models.</p><p>So you can use SageMaker Console, SageMaker API and SageMaker Notebook to train SageMaker models. </p><p>Incorrect options:</p><p><strong>SageMaker Trainer</strong> - There is no such thing as \"SageMaker Trainer\". This option has been added as a distractor.</p><p><strong>AWS Glue - </strong>AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue cannot be used to train SageMaker models</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html</a></p>", "feedbacks": ["", "", "", "", ""]}, "correct_response": ["a", "b", "d"], "section": "Modeling", "question_plain": "Which of the following options can be used to train a SageMaker model? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 61582252, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>A Financial Services company has asked you to finetune its SageMaker model training process. You observe that the company runs the training jobs multiple times in a day with a little tweaking of the training data for each run. </p><p>As an AWS&nbsp;ML specialist, which of the following steps would you recommend to improve the training performance so that the training jobs can complete faster in the MOST&nbsp;cost effective manner? (Select two) </p>", "answers": ["<p>Upgrade the training instance to the highest possible type</p>", "<p>Change the data format to protobuf recordIO format</p>", "<p>Use pipe mode to stream data from S3</p>", "<p>Spin up an EMR cluster to process the training job</p>"], "explanation": "<p>Correct options:</p><p><strong>Change the data format to protobuf recordIO format</strong></p><p><strong>Use pipe mode to stream data from S3</strong></p><p>AWS&nbsp;recommends using Pipe input mode for Amazon SageMaker algorithms. With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space. This approach shortens the lengthy download process and dramatically reduces startup time. Most first-party Amazon SageMaker algorithms work best with the optimized protobuf recordIO format. Therefore, both these options represent the correct answer.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_14-18-13-f38353ab8f57a9a9941e0752bbf40d88.jpg\"><p>Incorrect options:</p><p><strong>Upgrade the training instance to the highest possible type - </strong>Throwing more/better hardware is not a recommended solution to improve the training performance for the given use-case as it would increase the costs.</p><p><strong>Spin up an EMR cluster to process the training job</strong> - Amazon EMR is a big data platform for data processing, interactive analysis, and machine learning using open source frameworks such as Apache Spark, Apache Hive, and Presto. You can enhance the Amazon SageMaker capabilities by connecting the SageMaker notebook instance to an Apache Spark cluster running on Amazon EMR. The combination allows you to build models on large quantities of data. Using EMR cluster would result in increased costs but it may not result in a commensurate improvement in the training performance. This option is not the right fit for the given use case.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b", "c"], "section": "ML Implementation and Operations", "question_plain": "A Financial Services company has asked you to finetune its SageMaker model training process. You observe that the company runs the training jobs multiple times in a day with a little tweaking of the training data for each run. As an AWS&nbsp;ML specialist, which of the following steps would you recommend to improve the training performance so that the training jobs can complete faster in the MOST&nbsp;cost effective manner? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582254, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An analytics company is doing the sentiment analysis of tweets about a leading sports event. The company has prepared the following confusion matrix. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-27_07-10-43-a5aae1bb7d8991c097b9572843809630.png\"></p><p>What is the precision of the underlying model?</p>", "answers": ["<p>80%</p>", "<p>20%</p>", "<p>50%</p>", "<p>89%</p>"], "explanation": "<p>Correct option:</p><p><strong>80%</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-04-25-a339613918edf154e87b4fa662e81e09.jpg\"><p>Precision = (True Positives / (True Positives + False Positives))</p><p>= (800/(800+200)) = 0.8 or 80%</p><p>Incorrect options:</p><p><strong>20%</strong></p><p><strong>50%</strong></p><p><strong>89%</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "Modeling", "question_plain": "An analytics company is doing the sentiment analysis of tweets about a leading sports event. The company has prepared the following confusion matrix. What is the precision of the underlying model?", "related_lectures": []}, {"_class": "assessment", "id": 61582256, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A Silicon Valley startup has introduced a new email service that aims to address the problem of email spam. The startup wants to be extra sure that genuine emails are not marked as spam. The underlying machine learning model\u2019s predicted value of 1 implies that the model predicts a given email to be spam. The data scientists at the startup would like to analyze the results of the underlying model. </p><p>As an AWS&nbsp;ML&nbsp;specialist, which of the following would you identify as the most important evaluation metric for this model?</p>", "answers": ["<p>Accuracy</p>", "<p>Recall</p>", "<p>F1-score</p>", "<p>Precision</p>"], "explanation": "<p>Correct option:</p><p><strong>Precision</strong></p><p>Given, Precision = (True Positives / (True Positives + False Positives))</p><p>Precision shows the percentage of actual positive instances (as opposed to false positives) among those instances that have been retrieved (those predicted to be positive). In other words, how many selected items are positive?</p><p>The startup would like to be extra sure that an email is spam before potentially putting in the spam folder. According to the underlying model, a false positive implies that the email was actually genuine, but the model predicted it to be spam. Therefore in the case of a false positive, the user never sees this genuine email as the mail would go to the spam folder instead. This implies that the data scientists would want to have less false positives from the model. As false positives decrease, the model would have a higher precision per the formula quoted above. Hence precision is the most important evaluation metric for this model.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-15-43-575e817de71fe808b95f56d8ecc9f61f.jpg\"></p><p>Incorrect options:</p><p><strong>Accuracy - </strong>Accuracy measures the percentage of correct predictions.</p><p><strong>Recall</strong> - Recall shows the percentage of actual positives among the total number of relevant instances (actual positives). In other words, how many positive items are selected?</p><p><strong>F1-score - </strong>F1 score is a binary classification metric that considers both binary metrics precision and recall. It is the harmonic mean between precision and recall. The range is 0 to 1. A larger value indicates better predictive accuracy.</p><p>These three metrics are not the best fit for the given use case per the explanation provided above, so these options are incorrect. </p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-16-08-6d367a21b3a3520cf85eef43d2e0cf69.jpg\"></p><p>Here is a reference for a deep-dive on precision and recall: </p><p><a href=\"https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\">https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["d"], "section": "Modeling", "question_plain": "A Silicon Valley startup has introduced a new email service that aims to address the problem of email spam. The startup wants to be extra sure that genuine emails are not marked as spam. The underlying machine learning model\u2019s predicted value of 1 implies that the model predicts a given email to be spam. The data scientists at the startup would like to analyze the results of the underlying model. As an AWS&nbsp;ML&nbsp;specialist, which of the following would you identify as the most important evaluation metric for this model?", "related_lectures": []}, {"_class": "assessment", "id": 61582258, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>Which is the best evaluation metric for a binary classification model?</p>", "answers": ["<p>Accuracy</p>", "<p>AUC/ROC</p>", "<p>F1-Score</p>", "<p>Precision</p>"], "explanation": "<p>Correct option:</p><p><strong>AUC/ROC</strong></p><p>AWS&nbsp;recommends an industry-standard accuracy metric for binary classification models called Area Under the (Receiver Operating Characteristic - ROC) Curve (AUC). AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Because it is independent of the score cut-off, you can get a sense of the prediction accuracy of your model from the AUC metric without picking a threshold.</p><p>AUC/ROC is the correct choice. AUC/ROC metric does not require you to set a classification threshold. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-31-28-14b26d1f7df23802b49613e51b7402ee.jpg\"><p>For imbalanced datasets, you are better off using another metric called - <strong>PR AUC</strong> - that is also used in production systems for a highly imbalanced dataset, where the fraction of positive class is small, such as in case of credit card fraud detection.</p><p>Highly recommend to read the following blog on evaluation metrics:</p><p><a href=\"https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\">https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc</a></p><p>Incorrect options:</p><p><strong>Accuracy</strong> - Accuracy measures the percentage of correct predictions.</p><p><strong>F1-Score</strong> - F1 score is a binary classification metric that considers both binary metrics precision and recall. It is the harmonic mean between precision and recall. The range is 0 to 1. A larger value indicates better predictive accuracy:</p><p><strong>Precision</strong> - Precision shows the percentage of actual positive instances (as opposed to false positives) among those instances that have been retrieved (those predicted to be positive). In other words, how many selected items are positive?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-37-39-addc7caad207e671798ea2caa1efeb42.jpg\"><p>As mentioned in the explanation above, these three metrics are not the best metric for evaluating binary classification models, so these options are incorrect for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p><p><br></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Modeling", "question_plain": "Which is the best evaluation metric for a binary classification model?", "related_lectures": []}, {"_class": "assessment", "id": 61582260, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>After training a SageMaker model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data. </p><p>What can you say about the model?</p>", "answers": ["<p>Model is overfitting</p>", "<p>Model is underfitting</p>", "<p>Model is neither underfitting nor overfitting</p>", "<p>The model needs more training data</p>"], "explanation": "<p>Correct option:</p><p><strong>Model is underfitting</strong></p><p>Underfitting refers to a model that can neither model the training dataset nor generalize to new dataset. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training dataset.</p><p>When a model underfits, it exhibits low accuracy on both the training and test data.</p><p>Overfitting is a term used in statistics that refers to a modeling error that occurs when a function corresponds too closely to a dataset. As a result, overfitting may fail to fit additional data, and this may affect the accuracy of predicting future observations.</p><p>Overfitting happens when a model learns the detail and noise in the training dataset to the extent that it negatively impacts the performance of the model on a new dataset. This means that the noise or random fluctuations in the training dataset is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new datasets and negatively impact the model\u2019s ability to generalize.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_16-49-18-11b87ff4bff8bddf6e5c9fc96fe7625e.jpg\"><p>Incorrect options:</p><p><strong>Model is overfitting</strong></p><p><strong>Model is neither underfitting nor overfitting</strong></p><p>These two options contradict the explanation provided above which clearly concludes that the model is underfitting.</p><p><strong>The model needs more training data</strong> - This option has been added as a distractor. Adding more training data would not help address the root cause of the issue since the model is underfitting.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Modeling", "question_plain": "After training a SageMaker model over a huge training dataset, the data science team observed that it has low accuracy on the training data as well as low accuracy on the test data. What can you say about the model?", "related_lectures": []}, {"_class": "assessment", "id": 61582262, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>A leading ecommerce company is looking to improve the user experience by recommending the related product categories for its catalog of products. </p><p>As an ML Specialist, which SageMaker algorithms would you use to develop a solution for this use-case? (Select two)</p>", "answers": ["<p>Latent Dirichlet Allocation (LDA)</p>", "<p>XGBoost</p>", "<p>Factorization Machines</p>", "<p>K-means</p>"], "explanation": "<p>Correct options:</p><p><strong>Latent Dirichlet Allocation (LDA)</strong></p><p>The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. You can use LDA to figure out the right categories for each product. </p><p><strong>Factorization Machines</strong></p><p>The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. For the given use-case, you can use Factorization Machines to recommend the right related categories (computed via the LDA step above) for the given product\u2019s categories.</p><p>Incorrect options:</p><p><strong>XGBoost - </strong>The XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. It can be used for use cases such as predicting if an item belongs to a category: an email spam filter or predict numeric/continuous values such as estimating the value of a house. It cannot be used for recommending related product categories for the given use case.</p><p><strong>K-means</strong> - K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. Since the use case refers to related product categories and not the related products, so K-means cannot be used here.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-12_17-52-57-eeb6ff921516856c9b4bd44f6a60eee3.jpg\"><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a", "c"], "section": "Modeling", "question_plain": "A leading ecommerce company is looking to improve the user experience by recommending the related product categories for its catalog of products. As an ML Specialist, which SageMaker algorithms would you use to develop a solution for this use-case? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582264, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are creating a computer vision application to recognize truck brands. Your application uses Convolutional Neural Networks (CNN) but you do not have enough data to train the model. However, there are pre-trained third-party image recognition models available for similar tasks. </p><p>What steps will you take to build your solution in the shortest possible duration?</p>", "answers": ["<p>Use Kinesis Video Streams to identify the truck brand by using image manipulation algorithms and then do a pixel by pixel comparison</p>", "<p>Use transfer learning in your CNN by using the pre-trained third-party image recognition model as the convolutional base. Then remove the original classifier from the pre-trained model and add the new classifier for recognizing truck brands.</p>", "<p>Use transfer learning with Kinesis Video Streams</p>", "<p>Use transfer learning by retraining the pre-trained third-party image recognition model with your own data.</p>"], "explanation": "<p>Correct option:</p><p><strong>Use transfer learning in your CNN by using the pre-trained third-party image recognition model as the convolutional base. Then remove the original classifier from the pre-trained model and add the new classifier for recognizing truck brands.</strong></p><p>Image classification in Amazon SageMaker can be run in two modes: full training and transfer learning. In full training mode, the network is initialized with random weights and trained on user data from scratch. In transfer learning mode, the network is initialized with pre-trained weights and just the top fully connected layer is initialized with random weights. Then, the whole network is fine-tuned with new data. In this mode, training can be achieved even with a smaller dataset. This is because the network is already trained and therefore can be used in cases without sufficient training data.</p><p>The correct option is to use transfer learning in your CNN by using the pre-trained third-party image recognition model as the convolutional base. Then remove the original classifier from the pre-trained model and add the new classifier for recognizing truck brands. </p><p>Highly recommend this excellent reference on using transfer learning in computer vision applications:</p><p><a href=\"https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751\">https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751</a></p><p>Here is another blog that demonstrates use of Amazon SageMaker incremental learning features to perform transfer learning:</p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/now-easily-perform-incremental-learning-on-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/now-easily-perform-incremental-learning-on-amazon-sagemaker/</a></p><p>Incorrect options:</p><p><strong>Use Kinesis Video Streams to identify the truck brand by using image manipulation algorithms and then do a pixel by pixel comparison</strong> - Image manipulation algorithms that do a pixel by pixel comparison represent a non Machine Learning based approach to solving this problem. Since the existing solution uses CNNs (which is a Machine Learning based approach), so this option is ruled out, as it cannot accelerate the development of the existing solution.</p><p><strong>Use transfer learning with Kinesis Video Streams</strong></p><p>Amazon Kinesis Video Streams makes it easy to securely stream media from connected devices to AWS for storage, analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming media from millions of devices. Kinesis Video Streams provides a library to integrate ML frameworks such as Apache MxNet, TensorFlow, and OpenCV with video streams to build machine learning applications. Kinesis Video Streams is integrated with Amazon Rekognition Video, enabling you to build computer vision applications that detect objects, events, and people.</p><p>You cannot directly use transfer learning with Kinesis Video Streams. </p><p><strong>Use transfer learning by retraining the pre-trained third-party image recognition model with your own data</strong> - Retraining the pre-trained model with your own data is not correct because you do not have enough data to train. </p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Modeling", "question_plain": "You are creating a computer vision application to recognize truck brands. Your application uses Convolutional Neural Networks (CNN) but you do not have enough data to train the model. However, there are pre-trained third-party image recognition models available for similar tasks. What steps will you take to build your solution in the shortest possible duration?", "related_lectures": []}, {"_class": "assessment", "id": 61582266, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are working on a computer vision application (using Convolutional Neural Networks) to recognize an endangered species of tigers. 70% of the incorrectly classified images from the CNN were in a 90-degrees counter-clockwise rotated state. </p><p>Which of the following corrective actions you will take to address this issue?</p>", "answers": ["<p>Add more hidden layers to the CNN</p>", "<p>Perform rigorous hyperparameter tuning to achieve better classification accuracy</p>", "<p>Procure more training images that are in 90-degrees counter-clockwise rotated state. Retrain the CNN with this new dataset.</p>", "<p>Use Recurrent Neural Network (RNN) for correct classification of all image orientations.</p>"], "explanation": "<p>Correct option:</p><p><strong>Procure more training images that are in 90-degrees counter-clockwise rotated state. Retrain the CNN with this new dataset.</strong></p><p>In machine learning, a classifier assigns a class label to a data point. For example, an image classifier produces a class label (e.g, bird, plane) for what objects exist within an image. A convolutional neural network, or CNN for short, is also a type of classifier. A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assigns weights and biases to various aspects/objects in the image and be able to differentiate one from the other. A CNN&nbsp;can capture the spatial and temporal dependencies in an image through the application of relevant filters. </p><p>For the given use case, the CNN needs to be retrained with more images that are in a 90-degrees counter-clockwise rotated state since a feature in itself in a CNN is not scale or rotation invariant.</p><p>Here is a good reference link: </p><p><a href=\"https://stats.stackexchange.com/questions/239076/about-cnn-kernels-and-scale-rotation-invariance\">https://stats.stackexchange.com/questions/239076/about-cnn-kernels-and-scale-rotation-invariance</a></p><p>Incorrect options:</p><p><strong>Add more hidden layers to the CNN</strong> - The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers, and normalization layers. Here it simply means that instead of using the normal activation functions defined above, convolution and pooling functions are used as activation functions.</p><p><strong>Perform rigorous hyperparameter tuning to achieve better classification accuracy - </strong>Hyperparameter tuning refers to choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. </p><p>Adding more hidden layers or hyperparameter tuning are just meant to serve as distractors, as these cannot help the CNN&nbsp;to address the rotational variance of the images. Therefore both these options are incorrect.</p><p><strong>Use Recurrent Neural Network (RNN) for correct classification of all image orientations</strong> - Recurrent neural networks (RNN) are a class of neural networks that are helpful in modeling sequence data. RNNs are not the right fit for computer vision applications, so this option is ruled out for the given use case.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Data Engineering", "question_plain": "You are working on a computer vision application (using Convolutional Neural Networks) to recognize an endangered species of tigers. 70% of the incorrectly classified images from the CNN were in a 90-degrees counter-clockwise rotated state. Which of the following corrective actions you will take to address this issue?", "related_lectures": []}, {"_class": "assessment", "id": 61582268, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A Sports Analytics Company wants to analyze the game-plays for the coming NBA season. They would like to track the movement of each athlete for post-game analysis. </p><p>Which of the following options can be used to build a solution in the least possible time?</p>", "answers": ["<p>AWS Rekognition</p>", "<p>Kinesis Video Streams</p>", "<p>SageMaker Image Classification</p>", "<p>Kinesis Data Stream with Lambda based video frame processing</p>"], "explanation": "<p>Correct option:</p><p><strong>AWS Rekognition</strong></p><p>Amazon Rekognition Video has a feature called Pathing that can create a track of the path people take in videos and provide information such as:</p><p>The location of the person in the video frame at the time their path is tracked.</p><p>Facial landmarks such as the position of the left eye, when detected.</p><p>So, AWS Rekognition can be used to build a solution for this use-case.</p><p>Incorrect options:</p><p><strong>Kinesis Video Streams</strong> - Amazon Kinesis Video Streams makes it easy to securely stream media from connected devices to AWS for storage, analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams automatically provisions and elastically scales all the infrastructure needed to ingest streaming media from millions of devices. It durably stores, encrypts, and indexes media in your streams, and allows you to access your media through easy-to-use APIs. Kinesis Video Streams is ideal for building media streaming applications for camera-enabled IoT devices and for building real-time computer vision-enabled ML applications that are becoming prevalent in a wide range of use cases. </p><p>Kinesis Video Streams cannot be used to create a track of the path people take in videos, so this option is incorrect.</p><p><strong>SageMaker Image Classification</strong> - The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. It uses a convolutional neural network (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available.</p><p>The Image Classification algorithm cannot be used to create a track of the path people take in videos, so this option is incorrect.</p><p><strong>Kinesis Data Stream with Lambda based video frame processing</strong> - Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources. </p><p>Although it is possible to combine Kinesis Data Streams with Lambda to build a custom application that can create a track of the path people take in videos, however, it will not meet the requirement of building a solution in the least possible time. So this option is incorrect.</p><p>References:</p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/persons.html\">https://docs.aws.amazon.com/rekognition/latest/dg/persons.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "A Sports Analytics Company wants to analyze the game-plays for the coming NBA season. They would like to track the movement of each athlete for post-game analysis. Which of the following options can be used to build a solution in the least possible time?", "related_lectures": []}, {"_class": "assessment", "id": 61582270, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>You would like to tune the hyperparameters for the Sagemaker XGBoost algorithm. </p><p>Which of the following would you identify as the correct options for the model validation techniques? (Select two)</p>", "answers": ["<p>Validation using a holdout set</p>", "<p>K-fold validation</p>", "<p>Validation using training set</p>", "<p>Validation using SageMaker Ground Truth</p>"], "explanation": "<p>Correct options:</p><p><strong>Validation using a holdout set</strong></p><p>Machine learning practitioners often set aside a part of the data as a \"holdout set.\" They don\u2019t use this data for model training.</p><p>With this approach, you evaluate how well your model provides inferences on the holdout set. You then assess how effectively the model generalizes what it learned in the initial training, as opposed to using model memory. This approach to validation gives you an idea of how often the model is able to infer the correct answer.</p><p>Typically, the holdout dataset is of 20-30% of the training data.</p><p><strong>K-fold validation</strong></p><p>In this validation approach, you split the example dataset into <em>k</em> parts. You treat each of these parts as a holdout set for<em> k</em> training runs, and use the other <em>k</em>-1 parts as the training set for that run. You produce<em> k</em> models using a similar process, and aggregate the models to generate your final model. The value <em>k</em> is typically in the range of 5-10.</p><p>So, you can use validation via a holdout set or K-fold validation to tune the hyperparameters for the Sagemaker XGBoost algorithm.</p><p>Incorrect options:</p><p><strong>Validation using training set - </strong></p><p><strong>Validation using SageMaker Ground Truth </strong></p><p>Validation using training set and validation using SageMaker Ground Truth are made-up options. So both these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-model-validation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-model-validation.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a", "b"], "section": "Modeling", "question_plain": "You would like to tune the hyperparameters for the Sagemaker XGBoost algorithm. Which of the following would you identify as the correct options for the model validation techniques? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582272, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are working on a click prediction system and you want to capture the underlying click rate patterns when ads from a certain ad-category are placed on pages from a certain page-category. </p><p>Which SageMaker algorithm can be used to meet this requirement?</p>", "answers": ["<p>XGBoost</p>", "<p>BlazingText</p>", "<p>Latent Dirichlet Analysis</p>", "<p>Factorization Machines</p>"], "explanation": "<p>Correct option:</p><p><strong>Factorization Machines</strong></p><p>The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. <em>Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</em></p><p>Incorrect options:</p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems. </p><p>XGBoost algorithm cannot be used to build a click prediction system.</p><p><strong>BlazingText</strong> - The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification. </p><p>BlazingText algorithm cannot be used to build a click prediction system.</p><p><strong>Latent Dirichlet Analysis</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics. </p><p>Latent Dirichlet Allocation (LDA) algorithm cannot be used to build a click prediction system.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["d"], "section": "Modeling", "question_plain": "You are working on a click prediction system and you want to capture the underlying click rate patterns when ads from a certain ad-category are placed on pages from a certain page-category. Which SageMaker algorithm can be used to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 61582274, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A marketing analyst wants to group current and prospective customers into 10 groups based on their attributes. The analyst wants to send mailings to prospective customers in the group which has the highest percentage of current customers. </p><p>As an AWS ML Specialist, which SageMaker algorithm would you recommend to meet this requirement?</p>", "answers": ["<p>KNN</p>", "<p>Principal Component Analysis</p>", "<p>K-means</p>", "<p>Latent Dirichlet Allocation</p>"], "explanation": "<p>Correct option:</p><p><strong>K-means</strong></p><p>K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity.</p><p>Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time. To do this, the version used by Amazon SageMaker streams mini-batches (small, random subsets) of the training data.</p><p>K-means is the right algorithm to uncover discrete groupings within the given dataset.</p><p>Incorrect options:</p><p><strong>KNN</strong> - Amazon SageMaker k-nearest neighbors (k-NN) algorithm is an index-based algorithm. It uses a non-parametric method for classification or regression. For classification problems, the algorithm queries the <em>k</em> points that are closest to the sample point and returns the most frequently used label of their class as the predicted label. For regression problems, the algorithm queries the <em>k</em> closest points to the sample point and returns the average of their feature values as the predicted value.</p><p>As there is no historic data with labels, so KNN is ruled out. </p><p><strong>Principal Component Analysis</strong>&nbsp;- Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.</p><p>PCA is used for dimensionality reduction, so it is not the right fit for the given use case.</p><p><strong>Latent Dirichlet Allocation</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.</p><p>LDA is used for topic modeling, so it is not the right fit for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "A marketing analyst wants to group current and prospective customers into 10 groups based on their attributes. The analyst wants to send mailings to prospective customers in the group which has the highest percentage of current customers. As an AWS ML Specialist, which SageMaker algorithm would you recommend to meet this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 61582276, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>What is the format in which Amazon Sagemaker models are stored?</p>", "answers": ["<p>model.tar.gz</p>", "<p>model.zip</p>", "<p>model.tar.gzip</p>", "<p>model.gzip</p>"], "explanation": "<p>Correct option:</p><p><strong>model.tar.gz</strong></p><p>Amazon SageMaker models are stored as model.tar.gz in the S3 bucket specified in <code>OutputDataConfig</code> <code>S3OutputPath</code> parameter of the <code>create_training_job</code> call. You can specify most of these model artifacts when creating a hosting model. You can also open and review them in your notebook instance. When <code>model.tar.gz</code> is untarred, it contains <code>model_algo-1</code>, which is a serialized Apache MXNet object.</p><p>Incorrect options:</p><p><strong>model.zip</strong></p><p><strong>model.tar.gzip</strong></p><p><strong>model.gzip</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "What is the format in which Amazon Sagemaker models are stored?", "related_lectures": []}, {"_class": "assessment", "id": 61582278, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>Which of the following is the only mandatory hyperparameter for <strong>both</strong> the Word2Vec (unsupervised) and Text Classification (supervised) modes of the SageMaker BlazingText algorithm?</p>", "answers": ["<p>buckets</p>", "<p>epochs</p>", "<p>mode</p>", "<p>learning_rate</p>"], "explanation": "<p>Correct option:</p><p><strong>mode</strong></p><p><em>mode</em> represents the Word2vec architecture used for training. Valid values are: batch_skipgram, skipgram, or cbow.</p><p><em>mode</em> is the only mandatory hyperparameter for both the Word2Vec (unsupervised) and Text Classification (supervised) modes of the SageMaker BlazingText algorithm.</p><p>Incorrect options:</p><p><strong>buckets</strong> -&nbsp; This represents the number of hash buckets to use for subwords.</p><p><strong>epochs</strong> - This represents the number of complete passes through the training data.</p><p><strong>learning_rate</strong> - This represents the step size used for parameter updates.</p><p>These three are optional hyperparameters.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "Which of the following is the only mandatory hyperparameter for both the Word2Vec (unsupervised) and Text Classification (supervised) modes of the SageMaker BlazingText algorithm?", "related_lectures": []}, {"_class": "assessment", "id": 61582280, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are building a feature for a web application such that when a user attempts to log in from an anomalous IP address, a web login server would trigger a multi-factor authentication system. </p><p>Which SageMaker algorithm would you use for this feature?</p>", "answers": ["<p>Random Cut Forest</p>", "<p>XGBoost</p>", "<p>IP Insights</p>", "<p>Factorization Machines</p>"], "explanation": "<p>Correct option:</p><p><strong>IP Insights</strong></p><p>Amazon SageMaker IP Insights is an unsupervised learning algorithm that learns the usage patterns for IPv4 addresses. It is designed to capture associations between IPv4 addresses and various entities, such as user IDs or account numbers. You can use it to identify a user attempting to log into a web service from an anomalous IP address, for example. Or you can use it to identify an account that is attempting to create computing resources from an unusual IP address. Trained IP Insight models can be hosted at an endpoint for making real-time predictions or used for processing batch transforms.</p><p>SageMaker IP insights ingests historical data as (entity, IPv4 Address) pairs and learns the IP usage patterns of each entity. When queried with an (entity, IPv4 Address) event, a SageMaker IP Insights model returns a score that infers how anomalous the pattern of the event is. <em>For example, when a user attempts to log in from an IP address, if the IP Insights score is high enough, a web login server might decide to trigger a multi-factor authentication system.</em></p><p>Incorrect options:</p><p><strong>Random Cut Forest</strong> - Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the \"regular\" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the \"regular\" data can often be described with a simple model.</p><p>Random Cut Forest cannot be used to address this use case.</p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p>XGBoost cannot be used to address this use case.</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation. </p><p>Factorization Machines cannot be used to address this use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "You are building a feature for a web application such that when a user attempts to log in from an anomalous IP address, a web login server would trigger a multi-factor authentication system. Which SageMaker algorithm would you use for this feature?", "related_lectures": []}, {"_class": "assessment", "id": 61582282, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>Which of the following are the mandatory hyperparameters for the SageMaker K-means algorithm? (Select two)</p>", "answers": ["<p>feature_dim</p>", "<p>mini_batch_size</p>", "<p>k</p>", "<p>epochs</p>"], "explanation": "<p>Correct options:</p><p><strong>feature_dim</strong> - This represents the number of features in the input data.</p><p><strong>k</strong> - This represents the number of required clusters.</p><p>feature_dim and k are the only mandatory hyperparameters for the SageMaker K-means algorithm.</p><p>Incorrect options:</p><p><strong>mini_batch_size</strong> - This represents the number of observations per mini-batch for the data iterator.</p><p><strong>epochs</strong> - This represents the number of passes done over the training data.</p><p>Both of these are optional hyperparameters.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means-api-config.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a", "c"], "section": "Modeling", "question_plain": "Which of the following are the mandatory hyperparameters for the SageMaker K-means algorithm? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582284, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The data science team at a SaaS CRM company wants to improve its customer support workflow. The team wants to identify duplicate support tickets or route tickets to the correct support queue based on similarity of the text found in a ticket. </p><p>As an AWS&nbsp;ML Specialist, which SageMaker algorithm would you recommend to help solve this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Object2Vec</strong></p><p>The Amazon SageMaker Object2Vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space. You can use the learned embeddings to efficiently compute nearest neighbors of objects and to visualize natural clusters of related objects in low-dimensional space, for example. You can also use the embeddings as features of the corresponding objects in downstream supervised tasks, such as classification or regression.</p><p>Object2Vec can be used to find semantically similar objects such as tickets. For example, in a customer support workflow, you might need to identify duplicate support tickets or route tickets to the correct support queue based on similarity of the text found in the ticket.</p><p>Incorrect options:</p><p><strong>Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p>Factorization Machines cannot be used to find semantically similar objects.</p><p><strong>Latent Dirichlet Allocation</strong> - The Amazon SageMaker Latent Dirichlet Allocation (LDA) algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.<strong> </strong></p><p>Latent Dirichlet Allocation cannot be used to find semantically similar objects.</p><p><strong>XGBoost - </strong>The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p>XGBoost cannot be used to find semantically similar objects.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html</a></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/\">https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/</a></p>", "answers": ["<p>Factorization Machines</p>", "<p>Latent Dirichlet Allocation</p>", "<p>Object2Vec</p>", "<p>XGBoost</p>"]}, "correct_response": ["c"], "section": "Modeling", "question_plain": "The data science team at a SaaS CRM company wants to improve its customer support workflow. The team wants to identify duplicate support tickets or route tickets to the correct support queue based on similarity of the text found in a ticket. As an AWS&nbsp;ML Specialist, which SageMaker algorithm would you recommend to help solve this problem?", "related_lectures": []}, {"_class": "assessment", "id": 61582286, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>Only three of the built-in SageMaker algorithms support incremental training. Can you identify those three algorithms? (Select three)</p>", "answers": ["<p>Object Detection</p>", "<p>XGBoost</p>", "<p>Image Classification</p>", "<p>Linear Learner</p>", "<p>Semantic Segmentation</p>"], "explanation": "<p>Correct options:</p><p><strong>Object Detection</strong></p><p><strong>Image Classification</strong></p><p><strong>Semantic Segmentation</strong></p><p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_12-11-22-cf6705c000c27841cdd4e749756b03bb.jpg\"><p>Incorrect options:</p><p><strong>Linear Learner</strong></p><p><strong>XGBoost</strong></p><p>Per the note mentioned above, these two algorithms do not support incremental training.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html</a></p>", "feedbacks": ["", "", "", "", ""]}, "correct_response": ["a", "c", "e"], "section": "ML Implementation and Operations", "question_plain": "Only three of the built-in SageMaker algorithms support incremental training. Can you identify those three algorithms? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 61582288, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>The ML&nbsp;team at a social media company wants to avoid overfitting their SageMaker model by using early stopping. What is the criteria on which early stopping works in Amazon SageMaker?</p>", "answers": ["<p>If the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</p>", "<p>If the value of the objective metric for the current training job is better (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</p>", "<p>If the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the mean value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</p>", "<p>If the value of the objective metric for the current training job is better (higher when minimizing or lower when maximizing the objective metric) than the mean value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</p>"], "explanation": "<p>Correct option:</p><p><strong>If the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</strong></p><p>To optimize your SageMaker training job, you can stop the training job early (that a hyperparameter tuning job launches) when they are not improving significantly as measured by the objective metric. Stopping training jobs early can help reduce compute time and helps you avoid overfitting your model.</p><p>When you enable early stopping for a hyperparameter tuning job, SageMaker uses the following criteria:</p><p>If the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job. </p><p>Incorrect options:</p><p><strong>If the value of the objective metric for the current training job is better (higher when minimizing or lower when maximizing the objective metric) than the median value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</strong> - The value of the objective metric for the current training job should be <em>worse (not better, as mentioned for this option) </em>than the median value of running averages of the objective metric for SageMaker to stop the current training job.</p><p><strong>If the value of the objective metric for the current training job is worse (higher when minimizing or lower when maximizing the objective metric) than the mean value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</strong></p><p><strong>If the value of the objective metric for the current training job is better (higher when minimizing or lower when maximizing the objective metric) than the mean value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker stops the current training job</strong></p><p>Both these options use the <em>mean</em> of the running averages of the objective metric for previous training jobs. The correct criteria uses <em>median</em> of the running averages of the objective metric for previous training jobs. Therefore, both these options are incorrect. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "The ML&nbsp;team at a social media company wants to avoid overfitting their SageMaker model by using early stopping. What is the criteria on which early stopping works in Amazon SageMaker?", "related_lectures": []}, {"_class": "assessment", "id": 61582290, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "explanation": "<p>Correct option:</p><p><strong>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of HTTP requests</strong></p><p>An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.</p><p>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of HTTP requests.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_12-45-21-9950ac1bf5a021cf37490f68f505792f.jpg\"><p>Incorrect options:</p><p><strong>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of HTTPS requests</strong></p><p><strong>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of RPC requests</strong></p><p><strong>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of MQTT requests</strong></p><p>These three options contradict the explanation provided above. The invocations should be <em>HTTP</em>&nbsp;requests. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html</a></p>", "answers": ["<p>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of HTTP requests</p>", "<p>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of HTTPS requests</p>", "<p>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of RPC requests</p>", "<p>Within an inference pipeline model, Amazon SageMaker handles invocations as a sequence of MQTT requests</p>"], "question": "<p>Which of the following is correct regarding an inference pipeline on Amazon SageMaker?</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "Which of the following is correct regarding an inference pipeline on Amazon SageMaker?", "related_lectures": []}, {"_class": "assessment", "id": 61582292, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An ML engineer has built a deep learning model and now wants to deploy it using the SageMaker Hosting Services. For inference, the engineer wants a cost-effective option that guarantees low latency but still comes at a fraction of the cost of using a GPU instance for the endpoint. </p><p>As an AWS ML Specialist, which of the following would you recommend for the given use case?</p>", "answers": ["<p>SageMaker Neo</p>", "<p>Elastic Inference</p>", "<p>Automatic Scaling</p>", "<p>Inference Pipeline</p>"], "explanation": "<p>Correct option:</p><p><strong>Elastic Inference</strong></p><p>ML inference is the process of using a trained machine learning model to make predictions. By using Amazon Elastic Inference (EI), you can speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as Amazon SageMaker hosted models, but at a fraction of the cost of using a GPU instance for your endpoint. Elastic Inference is supported in EI-enabled versions of TensorFlow, Apache MXNet, and PyTorch.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-13_12-55-45-89b634cfe6e7c54caa80a3ea19b99ab2.jpg\"><p>Incorrect options:</p><p><strong>SageMaker Neo</strong> - Amazon SageMaker Neo enables developers to optimize machine learning (ML) models for inference on SageMaker in the cloud and supported devices at the edge.</p><p>After training a model for high accuracy, developers often spend a lot of time and effort tuning the model for high performance. For inference in the cloud, developers often turn to large instances with lots of memory and powerful processing capabilities at higher costs to achieve better throughput. For inference on edge devices with limited compute and memory, developers often spend months hand-tuning the model to achieve acceptable performance within the device hardware constraints.</p><p>Amazon SageMaker Neo automatically optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy.</p><p>SageMaker Neo cannot be used to accelerate real-time inferences calls for deep learning models that are deployed as Amazon SageMaker hosted models.</p><p><strong>Automatic Scaling</strong> - Amazon SageMaker supports automatic scaling (autoscaling) for your hosted models. <em>Autoscaling</em> dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, autoscaling brings more instances online. When the workload decreases, autoscaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.</p><p>Autoscaling cannot be used to accelerate real-time inferences calls for deep learning models that are deployed as Amazon SageMaker hosted models.</p><p><strong>Inference Pipeline</strong> - An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. </p><p>Inference pipeline cannot be used to accelerate real-time inferences calls for deep learning models that are deployed as Amazon SageMaker hosted models.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "ML Implementation and Operations", "question_plain": "An ML engineer has built a deep learning model and now wants to deploy it using the SageMaker Hosting Services. For inference, the engineer wants a cost-effective option that guarantees low latency but still comes at a fraction of the cost of using a GPU instance for the endpoint. As an AWS ML Specialist, which of the following would you recommend for the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 61582294, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are building a Deep Learning based context-sensitive spelling correction functionality for a consumer facing application. For example, consider the following misspelled food description: \u201clow tat milk\u201d. A traditional spell checker might correct it to \u201clow tar milk\u201d, which is an incorrect suggestion for the domain of food text, therefore, it should be corrected to \u201clow fat milk\u201d. </p><p>Which of the following options would you use to build your context aware model?</p>", "answers": ["<p>Seq2seq</p>", "<p>N-grams</p>", "<p>Levenstein distance</p>", "<p>Word2Vec</p>"], "explanation": "<p>Correct option:</p><p><strong>Seq2seq - </strong>Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French). In simple terms, the way seq2seq works is that it reads in a sentence, gets a sense of the entire sentence\u2019s \u201cmeaning\u201d or context and then performs its assigned task. So, for the given use case, instead of translating from one language to another, you can \u201ctranslate\u201d from possibly misspelled lines in English to their corrected versions. The model reads in a possibly misspelled line, encodes it into a representation of its \u201cmeaning\u201d and then outputs the corrected line.</p><p>Highly recommend this case study for a deep-dive:</p><p><a href=\"https://makers.underarmour.com/context-sensitive-spell-correction-with-deep-learning/\">https://makers.underarmour.com/context-sensitive-spell-correction-with-deep-learning/</a></p><p>Incorrect options:</p><p><strong>N-grams</strong> - An N-gram is simply a sequence of N words. N-grams cannot be used to build a context aware model.</p><p><strong>Levenstein distance</strong> - The Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. Levenshtein distance cannot be used to build a context aware model.</p><p><strong>Word2Vec</strong> -&nbsp; Word2vec is a two-layer neural net that processes text by \u201cvectorizing\u201d words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand. Word2vec cannot be used to build a context aware model.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "Modeling", "question_plain": "You are building a Deep Learning based context-sensitive spelling correction functionality for a consumer facing application. For example, consider the following misspelled food description: \u201clow tat milk\u201d. A traditional spell checker might correct it to \u201clow tar milk\u201d, which is an incorrect suggestion for the domain of food text, therefore, it should be corrected to \u201clow fat milk\u201d. Which of the following options would you use to build your context aware model?", "related_lectures": []}, {"_class": "assessment", "id": 61582296, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A car insurance company wants to automate the claims process. The company wants the customers to upload the video footage of the damaged car. This video footage is then assessed by an Amazon SageMaker model as part of the damage evaluation process. The company has no prior training data to get started on this endeavor. </p><p>As an AWS ML Specialist, which of the following solutions would you recommend to the company?</p>", "answers": ["<p>Use an unsupervised learning algorithm to label the videos which can be used in the downstream Amazon SageMaker model for the damage evaluation process</p>", "<p>Use Amazon SageMaker Ground Truth to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</p>", "<p>Use AWS Rekognition to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</p>", "<p>Use Kinesis Video Streams to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</p>"], "explanation": "<p>Correct option:</p><p><strong>Use Amazon SageMaker Ground Truth to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</strong></p><p>To train a machine learning model, you need a large, high-quality, labeled dataset. Ground Truth helps you build high-quality training datasets for your machine learning models. Amazon SageMaker Ground Truth is a fully managed data labeling service that makes it easy to build highly accurate training datasets for machine learning. </p><p>So for the given scenario, you can use Amazon SageMaker Ground Truth to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process.</p><p>Incorrect options:</p><p><strong>Use an unsupervised learning algorithm to label the videos which can be used in the downstream Amazon SageMaker model for the damage evaluation process</strong> - An unsupervised learning algorithm cannot be used to create labels for the training videos, so this option is ruled out</p><p><strong>Use AWS Rekognition to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</strong> - Amazon Rekognition is a service that makes it easy to add powerful visual analysis to your applications. Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them. </p><p>Rekognition cannot be used to create labels for the training videos, so this option is not relevant for the given use case.</p><p><strong>Use Kinesis Video Streams to create the labels for the training videos. The labeled videos can be used to train the downstream Amazon SageMaker model for the damage evaluation process</strong> - Amazon Kinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing. Kinesis Video Streams is ideal for building media streaming applications for camera-enabled IoT devices and for building real-time computer vision-enabled ML applications that are becoming prevalent in a wide range of use cases. </p><p>Kinesis Video Streams cannot be used to create labels for the training videos, so this option is not relevant for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "A car insurance company wants to automate the claims process. The company wants the customers to upload the video footage of the damaged car. This video footage is then assessed by an Amazon SageMaker model as part of the damage evaluation process. The company has no prior training data to get started on this endeavor. As an AWS ML Specialist, which of the following solutions would you recommend to the company?", "related_lectures": []}, {"_class": "assessment", "id": 61582298, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>Considering the following ROC curve generated for the Amazon SageMaker XGBoost algorithm for a binary classification use-case</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/2019-11-27_09-02-53-9fcf115e7051ecb6c40861da5da95e0e.png\"></p><p>Which of the following statements are correct?</p>", "answers": ["<p>The model represented by the black ROC curve is best at distinguishing the two classes. The model represented by the blue ROC curve is worst at distinguishing the two classes</p>", "<p>The model represented by the red ROC curve is best at distinguishing the two classes. The model represented by the blue ROC curve is worst at distinguishing the two classes</p>", "<p>The model represented by the black ROC curve is best at distinguishing the two classes. The model represented by the red ROC curve is worst at distinguishing the two classes</p>", "<p>The model represented by the blue ROC curve is best at distinguishing the two classes. The model represented by the black ROC curve is worst at distinguishing the two classes</p>"], "explanation": "<p>Correct option:</p><p><strong>The model represented by the blue ROC curve is best at distinguishing the two classes. The model represented by the black ROC curve is worst at distinguishing the two classes</strong></p><p>AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting positive classes as positive and negative classes as negative.</p><p>An excellent model has AUC near 1 (the blue curve in the figure) which means it has a good measure of separability. A poor model has an AUC near 0 which means it has the worst measure of separability. In fact, it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5 (the black curve), it means the model has no class separation capacity whatsoever. The model represented by the red curve has performance in between the model represented by the blue curve and that of the black curve.</p><p>Highly recommend the following blog for a deep-dive on AUC-ROC:</p><p><a href=\"https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\">https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5</a></p><p>Incorrect options:</p><p><strong>The model represented by the black ROC curve is best at distinguishing the two classes. The model represented by the blue ROC curve is worst at distinguishing the two classes</strong></p><p><strong>The model represented by the red ROC curve is best at distinguishing the two classes. The model represented by the blue ROC curve is worst at distinguishing the two classes</strong></p><p><strong>The model represented by the black ROC curve is best at distinguishing the two classes. The model represented by the red ROC curve is worst at distinguishing the two classes</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p><br></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["d"], "section": "Modeling", "question_plain": "Considering the following ROC curve generated for the Amazon SageMaker XGBoost algorithm for a binary classification use-caseWhich of the following statements are correct?", "related_lectures": []}, {"_class": "assessment", "id": 61582300, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>The data science team at an e-commerce company wants to analyze and visualize the clickstream data as it arrives into the data lake. </p><p>Which of the following combination of AWS serverless services would you recommend for the given use case?</p>", "answers": ["<p>S3, Lambda, ElasticSearch, Kibana</p>", "<p>S3, EMR, ElasticSearch, Kibana</p>", "<p>S3, EC2, D3.js</p>", "<p>S3, Glue, Athena, QuickSight</p>"], "explanation": "<p>Correct option:</p><p><strong>S3, Glue, Athena, QuickSight</strong></p><p>Amazon S3 is object storage built to store and retrieve any amount of data from anywhere. Using this service, you can easily build applications that make use of cloud native storage. S3 is a serverless service.</p><p>AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p><p>Amazon QuickSight is a scalable, serverless, embeddable, machine learning-powered business intelligence (BI) service built for the cloud.</p><p>Therefore S3, Glue, Athena, QuickSight is the correct choice of services for the given use case.</p><p>Incorrect options:</p><p><strong>S3, Lambda, ElasticSearch, Kibana</strong></p><p><strong>S3, EMR, ElasticSearch, Kibana</strong></p><p><strong>S3, EC2, D3.js</strong></p><p>Amazon Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. ElasticSearch is not serverless.</p><p>Amazon EMR is the industry-leading cloud big data platform for data processing, interactive analysis, and machine learning using open source frameworks such as Apache Spark, Apache Hive, and Presto. You can deploy your workloads to EMR using Amazon EC2, Amazon Elastic Kubernetes Service (EKS), or on-premises AWS Outposts. EMR&nbsp;is not a serverless service.</p><p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. EC2 is not a serverless service.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["d"], "section": "Exploratory Data Analysis", "question_plain": "The data science team at an e-commerce company wants to analyze and visualize the clickstream data as it arrives into the data lake. Which of the following combination of AWS serverless services would you recommend for the given use case?", "related_lectures": []}, {"_class": "assessment", "id": 61582302, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "explanation": "<p>Correct options:</p><p><strong>Create a VPC interface endpoint to use PrivateLink for your notebook instance and then configure Security Groups for your VPC that allow outbound connections</strong></p><p>You can connect to your notebook instance from your VPC through an interface endpoint in your Virtual Private Cloud (VPC) instead of connecting over the public internet. When you use a VPC interface endpoint, communication between your VPC and the notebook instance is conducted entirely and securely within the AWS network.</p><p><strong>Setup NAT gateway for your VPC and configure Security Groups for your VPC that allow outbound connections</strong></p><p>The instances in a private subnet can access the internet by using a network address translation (NAT) gateway that resides in the public subnet.</p><p>As the notebook instance is in a VPC, so the internet access is disabled. The notebook instance won't be able to train or host models unless your VPC has an interface endpoint (PrivateLink) or a NAT gateway and your security groups allow outbound connections. </p><p>Incorrect options:</p><p><strong>Setup NAT gateway for your VPC along with Security Groups for your VPC that allow inbound connections</strong> - The security group should allow outbound connections as the outbound connection would be initiated to read the data from S3. </p><p><strong>Setup S3 gateway for your VPC - </strong>There is no such thing as S3 gateway for a VPC. A VPC endpoint enables private connections between your VPC and supported AWS services. You specify a gateway endpoint as a route table target for traffic that is destined for the supported AWS services. You need to use a gateway endpoint to access S3 data using a private connection from within a VPC. So this option is incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html</a></p>", "answers": ["<p>Create a VPC interface endpoint to use PrivateLink for your notebook instance and then configure Security Groups for your VPC that allow outbound connections</p>", "<p>Setup NAT gateway for your VPC and configure Security Groups for your VPC that allow outbound connections</p>", "<p>Setup NAT gateway for your VPC along with Security Groups for your VPC that allow inbound connections</p>", "<p>Setup S3 gateway for your VPC</p>"], "question": "<p>You have disabled direct internet access to your Amazon SageMaker notebook instance while connecting to your VPC in order to prevent unauthorized access to your data. As there is no data access via the internet, the notebook instance is not able to train or host models. </p><p>Which of the following solutions can be used to address this issue? (Select two)</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a", "b"], "section": "ML Implementation and Operations", "question_plain": "You have disabled direct internet access to your Amazon SageMaker notebook instance while connecting to your VPC in order to prevent unauthorized access to your data. As there is no data access via the internet, the notebook instance is not able to train or host models. Which of the following solutions can be used to address this issue? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582304, "assessment_type": "multi-select", "prompt": {"question": "<p>As a security policy, the data science team at an e-commerce company does not want Amazon SageMaker to provide external network access to the training or inference containers, so network isolation is enabled for all containers. </p><p>Which of the following Amazon SageMaker containers do NOT support network isolation? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Chainer</strong></p><p><strong>Amazon SageMaker Reinforcement Learning</strong></p><p>If you enable network isolation, the containers can't make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment. In the case of a training job with multiple instances, network inbound and outbound traffic is limited to the peers of each training container. SageMaker still performs download and upload operations against Amazon S3 using your SageMaker execution role in isolation from the training or inference container.</p><p>Network isolation is NOT supported by the following managed Amazon SageMaker containers as they require access to Amazon S3:</p><p>Chainer</p><p>Amazon SageMaker Reinforcement Learning</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/mkt-algo-model-internet-free.html</a></p><p>Incorrect options:</p><p><strong>MXNet</strong></p><p><strong>TensorFlow</strong></p><p><strong>Scikit-learn</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>", "answers": ["<p>Chainer </p>", "<p>MXNet</p>", "<p>Scikit-learn</p>", "<p>Amazon SageMaker Reinforcement Learning</p>", "<p>TensorFlow</p>"]}, "correct_response": ["a", "d"], "section": "ML Implementation and Operations", "question_plain": "As a security policy, the data science team at an e-commerce company does not want Amazon SageMaker to provide external network access to the training or inference containers, so network isolation is enabled for all containers. Which of the following Amazon SageMaker containers do NOT support network isolation? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582306, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "explanation": "<p>Correct option:</p><p><strong>Drop the heritage houses from the training data and then train the model</strong></p><p>Data preparation (also referred to as \u201cdata preprocessing\u201d) is the process of transforming raw data to make it ready for downstream ML&nbsp;algorithms. As the heritage houses are clear outliers in terms of price and will never be listed, it is best to drop these from the training data and then train the model.</p><p>Incorrect options:</p><p><strong>Normalize the data for all houses in this city and then train the model</strong></p><p><strong>Standardize the data for all houses in this city and then train the model</strong></p><p>Normalization typically means rescaling the values into a range of [0,1]. </p><p>Standardization typically means rescaling values to have a mean of 0 and a standard deviation of 1. </p><p>While normalizing and standardizing is a valid strategy but for this use-case it would end up injecting noise into the model due to the data from the heritage houses. So both these options are incorrect.</p><p><strong>One-hot encode the data for all houses in this city and then train the model - </strong>One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms for processing. One-hot encoding is used only for nominal (without any ordering of values, which are also known as ordinals) categorical features. The housing data such as house size, house age should not be one-hot encoded. So this option is incorrect.</p>", "answers": ["<p>Normalize the data for all houses in this city and then train the model</p>", "<p>Standardize the data for all houses in this city and then train the model</p>", "<p>Drop the heritage houses from the training data and then train the model</p>", "<p>One-hot encode the data for all houses in this city and then train the model</p>"], "question": "<p>An online real estate database company provides information on the housing prices for all states in the US by capturing information such as house size, age, location etc. The company is capturing data for a city where the typical housing prices are around $200K except for some houses that are more than 100 years old with an asking price of about $1 million. These heritage houses will never be listed on the platform. </p><p>Which of the following data processing steps would you recommend to address this use case?</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "An online real estate database company provides information on the housing prices for all states in the US by capturing information such as house size, age, location etc. The company is capturing data for a city where the typical housing prices are around $200K except for some houses that are more than 100 years old with an asking price of about $1 million. These heritage houses will never be listed on the platform. Which of the following data processing steps would you recommend to address this use case?", "related_lectures": []}, {"_class": "assessment", "id": 61582308, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>An upcoming music streaming service wants to build a Minimum Viable Product and would like to have the underlying music recommendation engine developed at the earliest with the least development effort. </p><p>As an AWS ML Specialist, which AWS service would you suggest for the music recommendation engine?</p>", "answers": ["<p>Amazon SageMaker Factorization Machines</p>", "<p>Amazon Personalize</p>", "<p>Amazon SageMaker XGBoost</p>", "<p>Amazon SageMaker Neural Topic Model</p>"], "explanation": "<p>Correct option:</p><p><strong>Amazon Personalize</strong></p><p>Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications. </p><p>Amazon Personalize makes it easy for developers to build applications capable of delivering a wide array of personalization experiences, including specific product recommendations, personalized product re-ranking, and customized direct marketing. </p><p>There is no need for development, training and testing of custom models when using Personalize.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_18-12-45-bcc4398bce5f26b94bd00a2c280eb6e3.jpg\"><p>Incorrect options:</p><p><strong>Amazon SageMaker Factorization Machines</strong> - The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p><strong>Amazon SageMaker XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p><strong>Amazon SageMaker Neural Topic Model</strong> - Amazon SageMaker NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. Documents that contain frequent occurrences of words such as \"bike\", \"car\", \"train\", \"mileage\", and \"speed\" are likely to share a topic on \"transportation\" for example. Topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities. </p><p>These three options require significant effort to train and test the models, so these options are incorrect for the given use case.</p><p>Reference:</p><p><a href=\"https://aws.amazon.com/personalize/\">https://aws.amazon.com/personalize/</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Modeling", "question_plain": "An upcoming music streaming service wants to build a Minimum Viable Product and would like to have the underlying music recommendation engine developed at the earliest with the least development effort. As an AWS ML Specialist, which AWS service would you suggest for the music recommendation engine?", "related_lectures": []}, {"_class": "assessment", "id": 61582310, "assessment_type": "multi-select", "prompt": {"relatedLectureIds": "", "question": "<p>The data science team at an analytics company is working on a linear regression model and it observes that the training error as well as the test error are high, implying that the model has a bias. </p><p>Which of the following L1 and L2 regularization optimizations may be done to resolve this issue? (Select two)</p>", "answers": ["<p>Increase L1 regularization</p>", "<p>L1 and L2 regularization are not required, just get more training data</p>", "<p>Use L2 regularization and drop L1 regularization</p>", "<p>Decrease L1 regularization</p>"], "explanation": "<p>Correct options:</p><p><strong>Use L2 regularization and drop L1 regularization</strong></p><p><strong>Decrease L1 regularization</strong></p><p>You can think of L1 as reducing the number of features in the model altogether. Think of \u201cDecreasing L1 regularization\u201d as keeping more features in the model, thereby removing the bias. </p><p>L2 \u201cregulates\u201d the feature weight instead of just dropping them, so you can use&nbsp; L2 regularization to address the bias.</p><p>Highly recommend to review L1 and L2 regularizations in more detail here:</p><p><a href=\"https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\">https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c</a></p><p>Incorrect options:</p><p><strong>Increase L1 regularization </strong>- This option contradicts the explanation provided above, so this option is incorrect.</p><p><strong>L1 and L2 regularization are not required, just get more training data - </strong>Getting more training data alone will not address the model's bias, so this option is incorrect.</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c", "d"], "section": "Modeling", "question_plain": "The data science team at an analytics company is working on a linear regression model and it observes that the training error as well as the test error are high, implying that the model has a bias. Which of the following L1 and L2 regularization optimizations may be done to resolve this issue? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582312, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>You are working on a fraud detection model based on SageMaker IP Insights algorithm with a training data set of 1TB in CSV format. Your Sagemaker Notebook instance has only 5GB of space. </p><p>Given these constraints, how would you go about building your model?</p>", "answers": ["<p>Shuffle the training data and create a 5GB slice of this shuffled data. Build your model on the Jupyter Notebook using this slice of training data. Once the evaluation metric looks good, create a training job on SageMaker infrastructure with the appropriate instance types and instance counts to handle the entire training data</p>", "<p>Create an AWS Glue job to compress the training data into parquet format using an appropriate compression codec. This should allow you to use the entire compressed training data on your notebook instance</p>", "<p>Spin-up an EMR Cluster running Apache Spark to transform the CSV data into recordIO-protobuf format. Read the entire transformed data in recordIO-protobuf format from S3 in your Jupyter Notebook instance while training your model</p>", "<p>Create an AWS Glue job to transform the training data into recordIO-protobuf format. Read the entire transformed data in recordIO-protobuf format from S3 in your Jupyter Notebook instance while training your model</p>"], "explanation": "<p>Correct option:</p><p><strong>Shuffle the training data and create a 5GB slice of this shuffled data. Build your model on the Jupyter Notebook using this slice of training data. Once the evaluation metric looks good, create a training job on SageMaker infrastructure with the appropriate instance types and instance counts to handle the entire training data</strong></p><p>The correct option is to shuffle the training data and create a 5GB slice of this shuffled data. Then you need to build your model on the Jupyter Notebook using this slice of training data. Once the evaluation metric looks good, you need to create a training job on SageMaker infrastructure with the appropriate instance types and instance counts to handle the entire training data.</p><p>Incorrect options:</p><p><strong>Create an AWS Glue job to transform the training data into recordIO-protobuf format. Read the entire transformed data in recordIO-protobuf format from S3 in your Jupyter Notebook instance while training your model</strong> - AWS Glue job cannot write output in recordIO-protobuf format, so this option is ruled out.</p><p><strong>Create an AWS Glue job to compress the training data into parquet format using an appropriate compression codec. This should allow you to use the entire compressed training data on your notebook instance</strong></p><p><strong>Spin-up an EMR Cluster running Apache Spark to transform the CSV data into recordIO-protobuf format. Read the entire transformed data in recordIO-protobuf format from S3 in your Jupyter Notebook instance while training your model</strong></p><p>IP Insights algorithm supports only CSV file type as training data, so both these options using parquet or recordIO-protobuf are ruled out. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights-training-data-formats.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights-training-data-formats.html</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "Modeling", "question_plain": "You are working on a fraud detection model based on SageMaker IP Insights algorithm with a training data set of 1TB in CSV format. Your Sagemaker Notebook instance has only 5GB of space. Given these constraints, how would you go about building your model?", "related_lectures": []}, {"_class": "assessment", "id": 61582314, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "explanation": "<p>Correct option:</p><p><strong>Use exclude pattern **.metadata in the crawler definition to ignore the metadata</strong></p><p>AWS Glue crawler supports exclude patterns. Correct option is to use exclude pattern <code>**.metadata</code> in the crawler definition to ignore the metadata. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-18_18-43-53-9a83700c269d8878b1455a9c038ad3b8.jpg\"><p>Here is a deep-dive for the exclude patterns:</p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude\">https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude</a></p><p>Incorrect options:</p><p><strong>It is not possible to ignore the metadata in crawler. Create a daily ETL job to transfer only the transaction data specific CSV files into a new location and then read this cleansed transaction data into Athena</strong> - This option has been added as a distractor.</p><p><strong>Use exclude pattern .metadata/** in the crawler definition to ignore the metadata</strong></p><p><strong>Use exclude pattern .**metadata in the crawler definition to ignore the metadata</strong></p><p>The correct pattern has been described in the explanation above, so both these options are incorrect.</p>", "answers": ["<p>It is not possible to ignore the metadata in crawler. Create a daily ETL job to transfer only the transaction data specific CSV files into a new location and then read this cleansed transaction data into Athena</p>", "<p>Use exclude pattern **.metadata in the crawler definition to ignore the metadata</p>", "<p>Use exclude pattern .metadata/** in the crawler definition to ignore the metadata</p>", "<p>Use exclude pattern .**metadata in the crawler definition to ignore the metadata</p>"], "question": "<p>You want to create an AWS Glue crawler to read the transaction data dumped into an S3 based data lake in the s3://mybucket/myfolder/ location. The transaction data is in CSV format however there are some additional metadata files with .metadata extension in the same location. The metadata needs to be ignored while reading the transaction data via Athena. </p><p>How would you implement this solution?</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "You want to create an AWS Glue crawler to read the transaction data dumped into an S3 based data lake in the s3://mybucket/myfolder/ location. The transaction data is in CSV format however there are some additional metadata files with .metadata extension in the same location. The metadata needs to be ignored while reading the transaction data via Athena. How would you implement this solution?", "related_lectures": []}, {"_class": "assessment", "id": 61582316, "assessment_type": "multiple-choice", "prompt": {"relatedLectureIds": "", "question": "<p>A leading technology company offers a fast-track leadership program to the best performing executives at the company. At any point in time, more than a thousand executives are part of this leadership program. </p><p>Which of the following is the best visualization type to analyze the salary distribution for these executives?</p>", "answers": ["<p>Pie Chart</p>", "<p>Bar Chart</p>", "<p>Histogram</p>", "<p>Bubble Chart</p>"], "explanation": "<p>Correct option:</p><p><strong>Histogram</strong></p><p>Histogram is best suited to analyze the underlying distribution of data for the given use-case.</p><p>Highly recommend this resource for a deep-dive on visualizations in ML:</p><p><a href=\"https://medium.com/data-science-bootcamp/data-visualization-in-machine-learning-beyond-the-basics-baf2cbea8989\">https://medium.com/data-science-bootcamp/data-visualization-in-machine-learning-beyond-the-basics-baf2cbea8989</a></p><p>Incorrect options:</p><p><strong>Pie Chart</strong></p><p><strong>Bar Chart</strong></p><p><strong>Bubble Chart</strong></p><p>These three chart types are not the right fit for analyzing the underlying distribution of data.</p><p>Reference:</p><p><a href=\"https://chartio.com/learn/charts/essential-chart-types-for-data-visualization/\">https://chartio.com/learn/charts/essential-chart-types-for-data-visualization/</a></p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "A leading technology company offers a fast-track leadership program to the best performing executives at the company. At any point in time, more than a thousand executives are part of this leadership program. Which of the following is the best visualization type to analyze the salary distribution for these executives?", "related_lectures": []}, {"_class": "assessment", "id": 61582318, "assessment_type": "multi-select", "prompt": {"explanation": "<p>Correct options:</p><p><strong>Over-sample from the positive class</strong></p><p>In case of a binary classification model with strongly unbalanced classes, we need to over-sample from the minority class (which is the positive class for the given use-case). Oversampling randomly replicates minority instances to increase their population. </p><p><strong>Collect more training data for the positive class</strong></p><p>You can collect more training data for the minority class, as that would improve the model performance.</p><p>Here are a few good references : </p><p><a href=\"http://www.svds.com/learning-imbalanced-classes/\">http://www.svds.com/learning-imbalanced-classes/</a></p><p><a href=\"https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes\">https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes</a></p><p>Incorrect options:</p><p><strong>Over-sample from the negative class</strong> - For the given use-case, we need to oversample from the positive class and NOT the negative class. </p><p><strong>Collect more training data for the negative class </strong>- For the given use-case, we need to collect more training data for the minority class and NOT the majority class. </p>", "answers": ["<p>Over-sample from the negative class</p>", "<p>Collect more training data for the negative class</p>", "<p>Collect more training data for the positive class</p>", "<p>Over-sample from the positive class</p>"], "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "question": "<p>Researchers at NASA are creating a model on Amazon SageMaker to analyze images for detecting strong gravitational lensing, a phenomenon in which an accumulation of matter in space is dense enough that it bends light waves as they travel around it. The training data contains 200K images of the negative class (images with no gravitational lensing) and only 2000 images of the positive class (images with gravitational lensing). The final model has 85% accuracy, but poor recall. </p><p>How can you improve the model performance? (Select two)</p>"}, "correct_response": ["c", "d"], "section": "Exploratory Data Analysis", "question_plain": "Researchers at NASA are creating a model on Amazon SageMaker to analyze images for detecting strong gravitational lensing, a phenomenon in which an accumulation of matter in space is dense enough that it bends light waves as they travel around it. The training data contains 200K images of the negative class (images with no gravitational lensing) and only 2000 images of the positive class (images with gravitational lensing). The final model has 85% accuracy, but poor recall. How can you improve the model performance? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582320, "assessment_type": "multi-select", "prompt": {"answers": ["<p>Remove stop words</p>", "<p>Create n-gram vector for each word</p>", "<p>Create one-hot encoding for each word</p>", "<p>Tokenize each word</p>", "<p>Lowercase each word</p>", "<p>Remove archaic words such as doth and methinks</p>"], "explanation": "<p>Correct options:</p><p><strong>Remove stop words</strong> - Stop words are the most commonly occurring words which are not relevant in the context of the data and do not contribute any deeper meaning to the phrase. These contain no sentiment, so can be removed as part of the pre-processing steps.</p><p><strong>Tokenize each word</strong> - Tokenization is the process of converting text into tokens before transforming it into vectors. It is also easier to filter out unnecessary tokens. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_12-11-37-9b7bf837136cccf739e35c0b6d346d9a.jpg\"><p><strong>Lowercase each word</strong> - As part of this step, we convert character to the same case so the same words are indeed recognised as the same.</p><p>Removing stop words, tokenizing each word and \u201clowercase-ing\u201d each word are the recommended pre-processing steps for the given use case. </p><p>Highly recommend a deep-dive of the following common text preprocessing steps:</p><p><a href=\"https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb\">https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb</a></p><p>Incorrect options:</p><p><strong>Create n-gram vector for each word</strong> - An N-gram means a sequence of N words. </p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_12-22-10-6580864081e8dc4c69eba6a9d602a45f.png\"><p>You cannot create a N-gram vector for \"each word\", so this option is incorrect.</p><p><strong>Create one-hot encoding for each word</strong> - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_12-24-27-e95b28daa37884ff08c17bf1c4687159.jpg\"><p>Creating one hot encoding for \"each word\" would not help in solving the sentiment analysis problem for the given use-case. </p><p><strong>Remove archaic words such as doth and methinks</strong> - Archaic words should not be removed, since these play a crucial role to determine the sentiment of the sentence. Therefore, this option is incorrect.</p>", "question": "<p>The research team at a University wants to do sentiment analysis of the most famous quotes from the classical english literature over the last 500 years. Some of the sample quotes from the corpus are like so : <em>\u201cAll that glitters is not gold\u201d, \u201cBrevity is the soul of wit\u201d, \u201cThe lady doth protest too much, methinks.\u201d, \u201cLove all, trust a few, do wrong to none.\u201d </em></p><p>As an AWS&nbsp;ML Specialist, which of the following data pre-processing steps would you recommend before the team starts building the model? (Select three)</p>", "feedbacks": ["", "", "", "", "", ""], "relatedLectureIds": ""}, "correct_response": ["a", "d", "e"], "section": "Exploratory Data Analysis", "question_plain": "The research team at a University wants to do sentiment analysis of the most famous quotes from the classical english literature over the last 500 years. Some of the sample quotes from the corpus are like so : \u201cAll that glitters is not gold\u201d, \u201cBrevity is the soul of wit\u201d, \u201cThe lady doth protest too much, methinks.\u201d, \u201cLove all, trust a few, do wrong to none.\u201d As an AWS&nbsp;ML Specialist, which of the following data pre-processing steps would you recommend before the team starts building the model? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 61582322, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>These variations should be acceptable as part of the model performance</p>", "<p>Make sure that the training data distribution is similar to the expected distribution for the production inference data</p>", "<p>Replace XGBoost with Linear Learner algorithm</p>", "<p>Use Elastic Inference to improve the inference results</p>"], "relatedLectureIds": "", "explanation": "<p>Correct option:</p><p><strong>Make sure that the training data distribution is similar to the expected distribution for the production inference data</strong></p><p>Given the scenario, it seems like the data distribution may have been different for the training job, hence model shows major variation on production inference. So the team needs to make sure that the training data distribution is similar to the expected distribution for the production inference data.</p><p>Incorrect options:</p><p><strong>Use Elastic Inference to improve the inference results - </strong>Elastic Inference is used for low latency and high throughput model inference and it has nothing to do with inference results accuracy.</p><p><strong>These variations should be acceptable as part of the model performance</strong> - There should be no significant variations just for production inference, so this option is ruled out. </p><p><strong>Replace XGBoost with Linear Learner algorithm</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.</p><p>Replacing algorithm will not address the root cause, so this option is ruled out.</p>", "question": "<p>The ML team at an ecommerce company has trained a SageMaker XGBoost model on a large dataset. The evaluation metric looks good for the training job. However, post production deployment, the team observes that the inference results are not correct. </p><p>As an ML Specialist, which of the following solutions would you recommend to resolve this issue?</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["b"], "section": "Exploratory Data Analysis", "question_plain": "The ML team at an ecommerce company has trained a SageMaker XGBoost model on a large dataset. The evaluation metric looks good for the training job. However, post production deployment, the team observes that the inference results are not correct. As an ML Specialist, which of the following solutions would you recommend to resolve this issue?", "related_lectures": []}, {"_class": "assessment", "id": 61582324, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Drop the name and one-hot-encode gender and employment status</strong></p><p>One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. A categorical variable can take on a limited, and usually fixed, number of possible values.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_12-24-27-e95b28daa37884ff08c17bf1c4687159.jpg\"></p><p>As gender and employment status are categorical they need to be one-hot-encoded. Name has no bearing as a useful feature for the model, so it can be discarded. </p><p>Incorrect options:</p><p><strong>Drop the age and one-hot-encode name</strong></p><p><strong>Drop the age and one-hot-encode name and credit score</strong></p><p>You cannot one-hot encode \"name\" as it's not a categorical variable, so both these options are incorrect.</p><p><strong>Drop the name and one-hot-encode annual salary and credit score</strong> - You cannot one-hot encode annual salary as it's not a categorical variable.</p>", "answers": ["<p>Drop the name and one-hot-encode gender and employment status</p>", "<p>Drop the name and one-hot-encode annual salary and credit score</p>", "<p>Drop the age and one-hot-encode name</p>", "<p>Drop the age and one-hot-encode name and credit score</p>"], "question": "<p>The data science team at an analytics company is working on a credit score model using SageMaker Linear Learner algorithm. The training data consists of these fields : name, age, annual salary, gender, employment status and credit score. The model needs to predict the credit score label. </p><p>Which of the following data preparation steps need to be completed before working on the model?</p>", "relatedLectureIds": ""}, "correct_response": ["a"], "section": "Exploratory Data Analysis", "question_plain": "The data science team at an analytics company is working on a credit score model using SageMaker Linear Learner algorithm. The training data consists of these fields : name, age, annual salary, gender, employment status and credit score. The model needs to predict the credit score label. Which of the following data preparation steps need to be completed before working on the model?", "related_lectures": []}, {"_class": "assessment", "id": 61582326, "assessment_type": "multiple-choice", "prompt": {"explanation": "<p>Correct option:</p><p><strong>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and represent these features as (x,y) coordinates on a circle using sin and cos transformations. This transformed data should then be used to train the model</strong></p><p>The best way to uncover any cyclical sales patterns is to engineer the cyclical features by representing these as (x,y) coordinates on a circle using sin and cos functions. </p><p>Highly recommend this deep-dive on feature engineering for cyclical features -</p><p><a href=\"http://blog.davidkaleko.com/feature-engineering-cyclical-features.html\">http://blog.davidkaleko.com/feature-engineering-cyclical-features.html</a></p><p>Incorrect options:</p><p><strong>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and use these features in one-hot encoded format for training the model</strong> - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. A categorical variable can take on a limited, and usually fixed, number of possible values.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_12-24-27-e95b28daa37884ff08c17bf1c4687159.jpg\"></p><p>You cannot use one-hot encoding to uncover any cyclical sales patterns.</p><p><strong>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and use these features in label encoded format for training the model</strong> - Label Encoding refers to converting the labels into numeric form like so:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-24_16-00-34-4d915bb0d43355083bc0d96104dcfa76.jpg\"><p>You cannot use label encoding to uncover any cyclical sales patterns.</p><p><strong>No need for data preprocessing as the underlying algorithm can detect the cyclical patterns on its own</strong> - This option has been added as a distractor. The underlying algorithm cannot detect any cyclical pattern on its own.</p>", "feedbacks": ["", "", "", ""], "answers": ["<p>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and use these features in one-hot encoded format for training the model</p>", "<p>No need for data preprocessing as the underlying algorithm can detect the cyclical patterns on its own</p>", "<p>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and use these features in label encoded format for training the model</p>", "<p>Preprocess the order date to create new features such as hour of the day, day of the week, week of the month, week of the year, date of the month, month of the year and represent these features as (x,y) coordinates on a circle using sin and cos transformations. This transformed data should then be used to train the model</p>"], "question": "<p>The data science team at an ecommerce company is working on a training dataset for a forecasting model. The dataset represents the sales data for the last 5 years and has the following features : item description, item price, order date, quantity ordered, shipping address, order amount. The team would like to uncover any cyclical sales patterns such as hourly, daily, weekly, monthly, yearly from this data. </p><p>As an ML Specialist, which of the following solutions would you recommend?</p>", "relatedLectureIds": ""}, "correct_response": ["d"], "section": "Exploratory Data Analysis", "question_plain": "The data science team at an ecommerce company is working on a training dataset for a forecasting model. The dataset represents the sales data for the last 5 years and has the following features : item description, item price, order date, quantity ordered, shipping address, order amount. The team would like to uncover any cyclical sales patterns such as hourly, daily, weekly, monthly, yearly from this data. As an ML Specialist, which of the following solutions would you recommend?", "related_lectures": []}, {"_class": "assessment", "id": 61582328, "assessment_type": "multi-select", "prompt": {"explanation": "<p>Correct options:</p><p><strong>Drop a feature if it has a lot of missing values</strong></p><p><strong>Drop a feature if it has a low correlation to the target label</strong></p><p><strong>Drop a feature if it has low variance</strong></p><p>Variance measures how far each number in the set is from the mean and thus from every other number in the set. In other words, it refers to a statistical measurement of the spread between numbers in a data set.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_05-27-13-207b619e14fda876d8c5cf922590dfd6.png\"></p><p>\"Correlation\" is a statistical term describing the degree to which two variables move in coordination with one-another. If the two variables move in the same direction, then those variables are said to have a positive correlation. If they move in opposite directions, then they have a negative correlation.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_05-24-57-94df7fcc28067ee90566a0af91f21712.jpg\"></p><p>The rule of thumb is that you drop a feature that will not help a model to learn. Any feature that has low variance will not \"contribute\" to help the model learn to predict the target label, so it can be dropped. Similarly, a feature with a lot of missing values or a feature that has a low/no correlation to the target label ought to be dropped.</p><p>Incorrect options:</p><p><strong>Drop a feature if it has a high correlation to the target label</strong></p><p><strong>Drop a feature if it has high variance</strong></p><p><strong>Drop a feature if it has a few missing values</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>", "feedbacks": ["", "", "", "", "", ""], "relatedLectureIds": "", "answers": ["<p>Drop a feature if it has a low correlation to the target label</p>", "<p>Drop a feature if it has a high correlation to the target label</p>", "<p>Drop a feature if it has low variance</p>", "<p>Drop a feature if it has high variance</p>", "<p>Drop a feature if it has a lot of missing values</p>", "<p>Drop a feature if it has a few missing values</p>"], "question": "<p>You are pre-processing a training dataset to be used on the Amazon SageMaker Linear Learner algorithm. The dataset has hundreds of features and you need to decide which features to drop. </p><p>Which of the following guidelines would you follow to accomplish this goal? (Select three)</p>"}, "correct_response": ["a", "c", "e"], "section": "Exploratory Data Analysis", "question_plain": "You are pre-processing a training dataset to be used on the Amazon SageMaker Linear Learner algorithm. The dataset has hundreds of features and you need to decide which features to drop. Which of the following guidelines would you follow to accomplish this goal? (Select three)", "related_lectures": []}, {"_class": "assessment", "id": 61582330, "assessment_type": "multiple-choice", "prompt": {"question": "<p>The marketing analytics team at a financial services company is working on creating a customer loyalty program targeted at specific groups of customers. </p><p>Which data analysis technique should be used for this goal?</p>", "explanation": "<p>Correct option:</p><p><strong>Clustering</strong></p><p>Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. Data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-13-51-dfd58c135e25df41652ce71bab01371d.jpg\"><p>For the given use case, clustering is the best way to uncover similar groups. These groups can then be further analyzed to customize the customer loyalty program.</p><p>Here is a great reference for clustering algorithms:</p><p><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</a></p><p>Incorrect options:</p><p><strong>Dimensionality Reduction</strong> - Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. There are various methods for dimensionality reduction such as - Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), etc.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-17-56-ede2b8e5b21458eac7985aa4e25b15a6.jpeg\"><p>Dimensionality reduction cannot be used to group data points, so this option is not relevant for the given use case.</p><p><strong>Bivariate visualizations</strong> - For bivariate analysis, we are only concerned with analyzing two data attributes or variables and visualizing the same (two dimensions).</p><p><strong>Multivariate visualizations</strong> - For multivariate analysis, we analyze multiple data dimensions or attributes (2 or more). Multivariate analysis not only involves just checking out distributions but also potential relationships, patterns and correlations amongst these attributes.</p><p>Neither multivariate nor bivariate visualizations can be used to group data points, so these two options are not relevant for the given use case.</p><p>Highly recommend the following reference for a deep dive on visualizing multi-dimensional data:</p><p><a href=\"https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57\">https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57</a></p><p><br></p>", "answers": ["<p>Dimensionality Reduction</p>", "<p>Multivariate visualizations</p>", "<p>Clustering</p>", "<p>Bivariate visualizations</p>"], "feedbacks": ["", "", "", ""], "relatedLectureIds": ""}, "correct_response": ["c"], "section": "Exploratory Data Analysis", "question_plain": "The marketing analytics team at a financial services company is working on creating a customer loyalty program targeted at specific groups of customers. Which data analysis technique should be used for this goal?", "related_lectures": []}, {"_class": "assessment", "id": 61582332, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Kinesis Producer Library</strong></p><p>An Amazon Kinesis Data Streams producer is an application that puts user data records into a Kinesis data stream (also called <em>data ingestion</em>). The Kinesis Producer Library (KPL) simplifies producer application development, allowing developers to achieve high write throughput to a Kinesis data stream.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-25_07-46-39-dce7581afeffc4400f85e3999c833267.jpg\"><p>Incorrect options:</p><p><strong>Spark Streaming - </strong>Spark Streaming is an extension of the core Apache Spark API that allows data engineers and data scientists to process real-time data from various sources including (but not limited to) Kafka, Flume, and Amazon Kinesis. This processed data can be pushed out to file systems, databases, and live dashboards. You need to manage the underlying infrastructure via Amazon EMR service or via vanilla installation on EC2 instances. Spark Streaming is used to ingest and process real time data. It cannot be used on the source system to produce real time data. </p><p><strong>Kinesis Client Library - </strong>KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding. The KCL takes care of all of these subtasks so that you can focus your efforts on writing your custom record-processing logic. It cannot be used on the source system to produce real time data. </p><p><strong>Kinesis Data Streams API - </strong>The Kinesis Data Streams APIs help you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records. It cannot be used on the source system to produce real time data. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage\">https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage</a></p>", "question": "<p>The data engineering team at an ecommerce company wants to ingest the clickstream data from the source system in a reliable way. The solution should provide built-in performance benefits and ease of use on the client side as well. </p><p>Which solution would you implement on the source system to achieve high write throughput with reliability?</p>", "relatedLectureIds": "", "answers": ["<p>Kinesis Producer Library</p>", "<p>Spark Streaming</p>", "<p>Kinesis Data Streams API</p>", "<p>Kinesis Client Library</p>"]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "The data engineering team at an ecommerce company wants to ingest the clickstream data from the source system in a reliable way. The solution should provide built-in performance benefits and ease of use on the client side as well. Which solution would you implement on the source system to achieve high write throughput with reliability?", "related_lectures": []}, {"_class": "assessment", "id": 61582334, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "explanation": "<p>Correct options:</p><p><strong>Kinesis Data Streams PutRecord API uses name of the stream, a partition key and the data blob whereas Kinesis Data Firehose PutRecord API uses the name of the delivery stream and the data record</strong></p><p><em>For Amazon Kinesis Data Firehose,</em> PutRecord&nbsp; writes a single data record into an Amazon Kinesis Data Firehose delivery stream. To write multiple data records into a delivery stream, use PutRecordBatch. By default, each delivery stream can take in up to 2,000 transactions per second, 5,000 records per second, or 5 MB per second. If you use PutRecord and PutRecordBatch, the limits are an aggregate across these two operations for each delivery stream. </p><p><em>You must specify the name of the delivery stream and the data record when using PutRecord for Kinesis Data Firehose.</em></p><p><em>For Amazon Kinesis Data Streams,</em> PutRecord writes a single data record into an Amazon Kinesis data stream. Call <code>PutRecord</code> to send data into the stream for real-time ingestion and subsequent processing, one record at a time. Each shard can support writes up to 1,000 records per second, up to a maximum data write total of 1 MiB per second. </p><p><em>You must specify the name of the stream that captures, stores, and transports the data; a partition key; and the data blob itself when using PutRecord for Kinesis Data Streams.</em></p><p>The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs. </p><p>Incorrect options:</p><p><strong>Kinesis Data Firehose PutRecord API uses name of the stream, a partition key and the data blob whereas Kinesis Data Streams PutRecord API uses the name of the delivery stream and the data record</strong></p><p><strong>Both Kinesis Data Firehose PutRecord API and Kinesis Data Streams PutRecord API use the name of the stream, a partition key and the data blob</strong></p><p><strong>Both Kinesis Data Firehose PutRecord API and Kinesis Data Streams PutRecord API use the name of the delivery stream and the data record</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecord.html\">https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecord.html</a></p><p><a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html\">https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html</a></p>", "answers": ["<p>Kinesis Data Firehose PutRecord API uses name of the stream, a partition key and the data blob whereas Kinesis Data Streams PutRecord API uses the name of the delivery stream and the data record</p>", "<p>Kinesis Data Streams PutRecord API uses name of the stream, a partition key and the data blob whereas Kinesis Data Firehose PutRecord API uses the name of the delivery stream and the data record</p>", "<p>Both Kinesis Data Firehose PutRecord API and Kinesis Data Streams PutRecord API use the name of the stream, a partition key and the data blob</p>", "<p>Both Kinesis Data Firehose PutRecord API and Kinesis Data Streams PutRecord API use the name of the delivery stream and the data record</p>"], "relatedLectureIds": "", "question": "<p>The data engineering team at a social media company ingests the clickstream data into the Kinesis Data Streams using the PutRecord API in the source system. Now, the team wants to ingest this data into Kinesis Data Firehose instead and they want to use the PutRecord API for Firehose. </p><p>Which of the following represents the key differences between the PutRecord API call for Kinesis Data Stream v/s that of Kinesis Data Firehose?</p>"}, "correct_response": ["b"], "section": "Data Engineering", "question_plain": "The data engineering team at a social media company ingests the clickstream data into the Kinesis Data Streams using the PutRecord API in the source system. Now, the team wants to ingest this data into Kinesis Data Firehose instead and they want to use the PutRecord API for Firehose. Which of the following represents the key differences between the PutRecord API call for Kinesis Data Stream v/s that of Kinesis Data Firehose?", "related_lectures": []}, {"_class": "assessment", "id": 61582336, "assessment_type": "multi-select", "prompt": {"explanation": "<p>Correct options:</p><p><strong>AWS Data Pipeline</strong> - AWS Data Pipeline is a web service that makes it easy to schedule regular data movement and data processing activities in the AWS cloud. Using AWS Data Pipeline, you can quickly and easily provision pipelines that remove the development and maintenance effort required to manage your daily data operations, letting you focus on generating insights from that data. Simply specify the data sources, schedule, and processing activities required for your data pipeline. AWS Data Pipeline handles running and monitoring your processing activities on a highly reliable, fault-tolerant infrastructure. Additionally, to further ease your development process, AWS Data Pipeline provides built-in activities for common actions such as copying data between Amazon Amazon S3 and Amazon RDS, or running a query against Amazon S3 log data.</p><p><strong>AWS Glue ETL job</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. You should use AWS Glue to discover properties of the data you own, transform it, and prepare it for analytics. Glue can automatically discover both structured and semi-structured data stored in your data lake on Amazon S3, data warehouse in Amazon Redshift, and various databases running on AWS. It provides a unified view of your data via the Glue Data Catalog that is available for ETL, querying and reporting using services like Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum. Glue automatically generates Scala or Python code for your ETL jobs that you can further customize using tools you are already familiar with.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_07-40-14-b23494be10d7afe1e2a1d7917288cc4e.jpg\"><p>Both AWS Data Pipeline and AWS Glue ETL job are fully managed AWS&nbsp;services and require no infrastructure management, so both of these are the correct choices for the given use-case.</p><p>Incorrect options:</p><p><strong>Apache Spark ETL script running on EMR cluster - </strong>SparkSQL is built on top of the Spark Core, which leverages in-memory computations and RDDs that allow it to be much faster than Hadoop MapReduce. Amazon EMR is a managed service for the Hadoop and Spark ecosystem that allows customers to quickly focus on the analytics they want to run, not the heavy lifting of cluster management.</p><p>Since the EMR cluster needs to be provisioned and managed, so this option is ruled out for the given use-case.</p><p><img src=\"https://dmhnzl5mp9mj6.cloudfront.net/bigdata_awsblog/images/Spark_SQL_Ben_Image_1.PNG\"></p><p><strong>Lambda functions orchestrated by AWS Step Function</strong> - AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Lambda functions are not meant to handle ETL workloads, so this option is also ruled out. </p><p>References:</p><p><a href=\"https://aws.amazon.com/datapipeline/\">https://aws.amazon.com/datapipeline/</a></p><p><a href=\"https://stackoverflow.com/questions/47304870/scheduling-data-extraction-from-aws-redshift-to-s3\">https://stackoverflow.com/questions/47304870/scheduling-data-extraction-from-aws-redshift-to-s3</a></p>", "question": "<p>A financial services company wants to migrate its data architecture from a data warehouse to a data lake. It wants to use a solution that takes the least amount of development time and needs no infrastructure management. </p><p>What options would you recommend to transfer the data from AWS Redshift to Amazon S3? (Select two)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "answers": ["<p>AWS Data Pipeline</p>", "<p>Apache Spark ETL script running on EMR cluster</p>", "<p>Lambda functions orchestrated by AWS Step Function</p>", "<p>AWS Glue ETL job</p>"]}, "correct_response": ["a", "d"], "section": "Data Engineering", "question_plain": "A financial services company wants to migrate its data architecture from a data warehouse to a data lake. It wants to use a solution that takes the least amount of development time and needs no infrastructure management. What options would you recommend to transfer the data from AWS Redshift to Amazon S3? (Select two)", "related_lectures": []}, {"_class": "assessment", "id": 61582338, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "relatedLectureIds": "", "question": "<p>An e-commerce company wants to optimize the cost structure for its Redshift data warehouse by moving out some of the infrequently accessed data to S3. </p><p>Which of the following solutions requires minimum maintenance and development effort, so that the company can still access this infrequently accessed data from Redshift whenever required?</p>", "answers": ["<p>Create an AWS Glue ETL job that writes the data from S3 back into Redshift. The job needs to be triggered every time the data needs to be analysed in Redshift</p>", "<p>Create a Glue crawler to read the S3 data via Athena so there is no need to use Redshift</p>", "<p>Use Redshift Spectrum so that the infrequently accessed data in S3 can be queried from Redshift</p>", "<p>Create an EMR based Spark ETL job that writes the data from S3 back into Redshift. The job needs to be triggered every time the data needs to be analysed in Redshift</p>"], "explanation": "<p>Correct option:</p><p><strong>Use Redshift Spectrum so that the infrequently accessed data in S3 can be queried from Redshift</strong></p><p>Using Amazon Redshift Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum queries employ massive parallelism to execute very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Multiple clusters can concurrently query the same dataset in Amazon S3 without the need to make copies of the data for each cluster.</p><p>Redshift Spectrum is a built-in feature of Amazon Redshift, and your existing queries and BI tools will continue to work seamlessly. Under the covers, AWS manages a fleet of thousands of Redshift Spectrum nodes spread across multiple Availability Zones. These are transparently scaled and allocated to your queries based on the data that you need to process, with no provisioning or commitments. Redshift Spectrum is also highly concurrent\u2014you can access your Amazon S3 data from any number of Amazon Redshift clusters.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_07-50-31-3bb0cddc9049fdc20eee5190f0de2dad.jpg\"></p><p><br></p><p>Incorrect options:</p><p><strong>Create an AWS Glue ETL job that writes the data from S3 back into Redshift. The job needs to be triggered every time the data needs to be analysed in Redshift</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. You should use AWS Glue to discover properties of the data you own, transform it, and prepare it for analytics. Glue can automatically discover both structured and semi-structured data stored in your data lake on Amazon S3, data warehouse in Amazon Redshift, and various databases running on AWS. It provides a unified view of your data via the Glue Data Catalog that is available for ETL, querying and reporting using services like Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum. Glue automatically generates Scala or Python code for your ETL jobs that you can further customize using tools you are already familiar with.</p><p><strong>Create an EMR based Spark ETL job that writes the data from S3 back into Redshift. The job needs to be triggered every time the data needs to be analysed in Redshift</strong> - SparkSQL is built on top of the Spark Core, which leverages in-memory computations and RDDs that allow it to be much faster than Hadoop MapReduce. Amazon EMR is a managed service for the Hadoop and Spark ecosystem that allows customers to quickly focus on the analytics they want to run, not the heavy lifting of cluster management.</p><p>EMR and Glue based ETL jobs are not practical as the job needs to be invoked every time data needs to be queried in Redshift. Once the query is done, that data needs to be deleted again to save costs. Therefore both these options are incorrect as these are inefficient and require significant development effort.</p><p><strong>Create a Glue crawler to read the S3 data via Athena so there is no need to use Redshift - </strong>Using Athena is not an option as the query needs to be executed in Redshift to meet the requirements of the given scenario. </p><p>References:</p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\">https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/\">https://aws.amazon.com/blogs/big-data/amazon-redshift-spectrum-extends-data-warehousing-out-to-exabytes-no-loading-required/</a></p>"}, "correct_response": ["c"], "section": "Data Engineering", "question_plain": "An e-commerce company wants to optimize the cost structure for its Redshift data warehouse by moving out some of the infrequently accessed data to S3. Which of the following solutions requires minimum maintenance and development effort, so that the company can still access this infrequently accessed data from Redshift whenever required?", "related_lectures": []}, {"_class": "assessment", "id": 61582340, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are developing a computer vision system for a factory assembly line. The system can trigger a robotic arm to correct the orientation of the product components whenever it detects a misalignment. </p><p>Which SageMaker algorithm should be used in this computer vision system?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Semantic Segmentation</strong></p><p>The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes. Tagging is fundamental for understanding scenes, which is critical to an increasing number of computer vision applications, such as self-driving vehicles, medical imaging diagnostics, and robot sensing.</p><p>Because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a <em>segmentation mask</em>. A segmentation mask is a grayscale image with the same shape as the input image.</p><p><em>Semantic Segmentation is used for pixel level analysis of an image and it can be used in this computer vision system to detect misalignment</em></p><p>Incorrect options:</p><p><strong>Reinforcement Learning (RL) - </strong>RL enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences. Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior. RL is not relevant for the given use case. </p><p><strong>Image Classification</strong> - The SageMaker Image Classification algorithm is a supervised learning algorithm that analyzes only whole images, classifying them into one of multiple output categories. This algorithm cannot do pixel level analysis, so it cannot be used to detect a misalignment in the orientation of the product components for the given use case.</p><p><strong>Object Detection</strong> - The Object Detection Algorithm is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box. This algorithm cannot do pixel level analysis, so it cannot be used to detect a misalignment in the orientation of the product components for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html</a></p>", "answers": ["<p>Reinforcement Learning (RL)</p>", "<p>Image Classification</p>", "<p>Object Detection</p>", "<p>Semantic Segmentation</p>"]}, "correct_response": ["d"], "section": "Modeling", "question_plain": "You are developing a computer vision system for a factory assembly line. The system can trigger a robotic arm to correct the orientation of the product components whenever it detects a misalignment. Which SageMaker algorithm should be used in this computer vision system?", "related_lectures": []}, {"_class": "assessment", "id": 61582342, "assessment_type": "multiple-choice", "prompt": {"explanation": "<p>Correct option:</p><p><strong>Set the S3DataDistributionType field to ShardedByS3Key</strong></p><p>S3DataDistributionType is a field used to describe the S3DataSource for SageMaker.</p><p>If you want Amazon SageMaker to replicate a subset of data on each ML compute instance that is launched for model training, specify <code>ShardedByS3Key</code> for S3DataDistributionType field.</p><p>Incorrect options:</p><p><strong>Set the S3DataType field to ShardedByS3Key</strong> - S3DataType is a field used to describe the S3DataSource for SageMaker. This option has been added as a distractor since the value ShardedByS3Key is associated with the S3DataDistributionType field.</p><p><strong>Set the S3Uri field to ShardedByS3Key</strong> - S3Uri is a field used to describe the S3DataSource for SageMaker. This option has been added as a distractor since the value ShardedByS3Key is associated with the S3DataDistributionType field.</p><p><strong>Set the S3DataDistributionType field to FullyReplicated</strong> - If you want Amazon SageMaker to replicate the entire dataset on each ML compute instance that is launched for model training, specify <code>FullyReplicated</code>.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html\">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html</a></p>", "answers": ["<p>Set the S3DataDistributionType field to ShardedByS3Key</p>", "<p>Set the S3DataType field to ShardedByS3Key</p>", "<p>Set the S3Uri field to ShardedByS3Key</p>", "<p>Set the S3DataDistributionType field to FullyReplicated</p>"], "relatedLectureIds": "", "question": "<p>When you create a training job with the SageMaker API, Amazon SageMaker replicates the entire dataset from S3 to ML compute instances by default. </p><p>What of the following options would you use to replicate only a subset of the data on each ML compute instance?</p>", "feedbacks": ["", "", "", ""]}, "correct_response": ["a"], "section": "Data Engineering", "question_plain": "When you create a training job with the SageMaker API, Amazon SageMaker replicates the entire dataset from S3 to ML compute instances by default. What of the following options would you use to replicate only a subset of the data on each ML compute instance?", "related_lectures": []}, {"_class": "assessment", "id": 61582344, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Enable inter-container traffic encryption from the console</p>", "<p>Use SSH for inter-node traffic encryption</p>", "<p>There are no inter-node communications for batch processing, so inter-node traffic encryption is not required</p>", "<p>Use AWS-SSE for inter-node traffic encryption</p>"], "relatedLectureIds": "", "question": "<p>You are training a batch transformation job in Amazon SageMaker. You have protected data at rest by using AWS KMS key on S3. Amazon SageMaker ensures that machine learning (ML) model artifacts and other system artifacts are encrypted in transit and at rest. </p><p>What measures would you take to make sure that the data is protected in-transit even for inter-node training communications?</p>", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>There are no inter-node communications for batch processing, so inter-node traffic encryption is not required</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_08-48-26-44f649d6a40ddaa54ad4914107f142da.jpg\"><p>Incorrect options:</p><p><strong>Enable inter-container traffic encryption from the console</strong> - This option is not relevant to the given use case. </p><p><strong>Use SSH for inter-node traffic encryption</strong></p><p><strong>Use AWS-SSE for inter-node traffic encryption</strong></p><p>SSH and AWS-SSE are not used for inter-node traffic encryption, as SageMaker uses TLS 1.2 for this encryption. </p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-in-transit.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/encryption-in-transit.html</a></p>"}, "correct_response": ["c"], "section": "ML Implementation and Operations", "question_plain": "You are training a batch transformation job in Amazon SageMaker. You have protected data at rest by using AWS KMS key on S3. Amazon SageMaker ensures that machine learning (ML) model artifacts and other system artifacts are encrypted in transit and at rest. What measures would you take to make sure that the data is protected in-transit even for inter-node training communications?", "related_lectures": []}, {"_class": "assessment", "id": 61582346, "assessment_type": "multiple-choice", "prompt": {"explanation": "<p>Correct option:</p><p><strong>Validation Set</strong></p><p>A hyperparameter is a parameter whose value is used to control the learning process for a machine learning model. Examples of algorithm hyperparameters are <em>learning rate</em> and <em>mini-batch</em> size.</p><p>The validation set is used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The validation set is also known as the Dev set or the Development set since this dataset helps during the \u201cdevelopment\u201d stage of the model.</p><p>Hyperparameters should be tuned against the Validation Set.</p><p><em>Difference between parameters and hyperparameters:</em></p><p>Parameters are the ones that the model uses to make predictions. The values of parameters are derived via training. For example, the weight coefficients in a linear regression model. Hyperparameters are the ones that help with the learning process. For example,<em> number of clusters </em>in K-Means. </p><p>Incorrect options:</p><p><strong>Training Set</strong> - This refers to the actual dataset that we use to train the model (weights and biases in the case of a Neural Network). The model sees and learns from this data. Hyperparameters should not be tuned against the Training Set.</p><p><strong>Test Set</strong> - The Test Set is used to provide an unbiased evaluation of a final model fit on the training dataset. Hyperparameters should not be tuned against the Test Set.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_09-08-49-8f5be160224487b948d2c5942654e6ee.jpg\"><p><strong>Any of the Training Set, Validation Set or Test Set can be used</strong> - This option has been added as a distractor.</p><p>Reference:</p><p><a href=\"https://machinelearningmastery.com/difference-test-validation-datasets/\">https://machinelearningmastery.com/difference-test-validation-datasets/</a></p>", "answers": ["<p>Training Set</p>", "<p>Validation Set</p>", "<p>Test Set</p>", "<p>Any of the Training Set, Validation Set or Test Set can be used</p>"], "feedbacks": ["", "", "", ""], "question": "<p>Which data set should you use for hyperparameter tuning?</p>", "relatedLectureIds": ""}, "correct_response": ["b"], "section": "Data Engineering", "question_plain": "Which data set should you use for hyperparameter tuning?", "related_lectures": []}, {"_class": "assessment", "id": 61582348, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>/home/ec2-user/SageMaker</p>", "<p>/home/ec2-user/data</p>", "<p>/home/ec2-user/model</p>", "<p>/home/ec2-user/code</p>"], "question": "<p>You have launched a new Jupyter Notebook instance and you want to make sure that you don\u2019t lose any files and data when the notebook instance restarts. </p><p>Where should you save your files and data so that they are not overwritten when the instance restarts?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>/home/ec2-user/SageMaker</strong></p><p>Only files and data saved within the /home/ec2-user/SageMaker folder persist between notebook instance sessions. Files and data that are saved outside this directory are overwritten when the notebook instance stops and restarts.</p><p>Incorrect options:</p><p><strong>/home/ec2-user/data</strong></p><p><strong>/home/ec2-user/model</strong></p><p><strong>/home/ec2-user/code</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html</a></p>"}, "correct_response": ["a"], "section": "ML Implementation and Operations", "question_plain": "You have launched a new Jupyter Notebook instance and you want to make sure that you don\u2019t lose any files and data when the notebook instance restarts. Where should you save your files and data so that they are not overwritten when the instance restarts?", "related_lectures": []}, {"_class": "assessment", "id": 61582350, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data scientist at an e-commerce company is working on a solution to present individualized storefronts that best match each user\u2019s specific interests and tastes.</p><p>As an AWS ML Specialist, which SageMaker algorithm would you recommend as a solution?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Factorization Machines</strong></p><p>The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.</p><p>Therefore, Factorization Machines algorithm is the right fit for the given use case so that the company can create individualized storefronts having product recommendations that match each user\u2019s specific interests and tastes.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/test_question_description/2021-08-26_10-09-34-05b2fc79cbd15bb147fd765e6ad22716.jpg\"></p><p>Incorrect options:</p><p><strong>XGBoost</strong> - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems. </p><p>XGBoost is not the right fit to build recommendation system for the given use case.</p><p><strong>Linear Learner</strong> - Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1. For multiclass classification problems, the labels must be from 0 to num_classes - 1. For regression problems, y is a real number. The Amazon SageMaker linear learner algorithm provides a solution for both classification and regression problems.</p><p>Linear Learner is not the right fit to build recommendation system for the given use case.</p><p><strong>BlazingText</strong> - The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification.</p><p>BlazingText is not the right fit to build recommendation system for the given use case.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html</a></p>", "answers": ["<p>XGBoost</p>", "<p>Linear Learner</p>", "<p>BlazingText</p>", "<p>Factorization Machines</p>"]}, "correct_response": ["d"], "section": "Modeling", "question_plain": "A data scientist at an e-commerce company is working on a solution to present individualized storefronts that best match each user\u2019s specific interests and tastes.As an AWS ML Specialist, which SageMaker algorithm would you recommend as a solution?", "related_lectures": []}, {"_class": "assessment", "id": 61582352, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>Hyperparameter tuning might not improve your model and look at other options such as data engineering or alternate algorithm to improve the model</strong></p><p>Hyperparameter tuning is not a panacea for model under-performance and you may need to look at other options such as data engineering or alternate algorithm for the model. The other options are actually the upper limits for some of the hyperparameter tuning resources, they have no direct bearing on this use-case. You can read more on this reference link :</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html</a></p><p>Incorrect options:</p><p><strong>Increase the number of concurrent hyperparameter tuning jobs to 100</strong></p><p><strong>Increase the maximum run time for a hyperparameter tuning job to 30 days</strong></p><p><strong>Increase the number of hyperparameters that can be searched to 20</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p>", "question": "<p>The data science team at an e-commerce company has been tasked to improve the underlying recommendations engine. The team has tried to use both the random search and bayesian search approaches to hyperparameter tuning, but the results have been inconsistent. </p><p>As an ML Specialist, what would be your suggestion to the team?</p>", "answers": ["<p>Hyperparameter tuning might not improve your model and look at other options such as data engineering or alternate algorithm to improve the model</p>", "<p>Increase the number of concurrent hyperparameter tuning jobs to 100</p>", "<p>Increase the maximum run time for a hyperparameter tuning job to 30 days</p>", "<p>Increase the number of hyperparameters that can be searched to 20</p>"], "relatedLectureIds": ""}, "correct_response": ["a"], "section": "Modeling", "question_plain": "The data science team at an e-commerce company has been tasked to improve the underlying recommendations engine. The team has tried to use both the random search and bayesian search approaches to hyperparameter tuning, but the results have been inconsistent. As an ML Specialist, what would be your suggestion to the team?", "related_lectures": []}, {"_class": "assessment", "id": 61582354, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "relatedLectureIds": "", "question": "<p>You are doing topic modeling using the SageMaker Latent Dirichlet Allocation (LDA) algorithm. Which of the following are correct?</p>", "answers": ["<p>LDA is a not a \"bag-of-words\" model, which means that the order of words does matter</p>", "<p>LDA is a not a \"bag-of-words\" model, which means that the order of words does not matter</p>", "<p>LDA is a \"bag-of-words\" model, which means that the order of words does matter</p>", "<p>LDA is a \"bag-of-words\" model, which means that the order of words does not matter</p>"], "explanation": "<p>Correct option:</p><p><strong>LDA is a \"bag-of-words\" model, which means that the order of words does not matter</strong></p><p>Amazon SageMaker LDA is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of different categories. These categories are themselves a probability distribution over the features. LDA is a generative probability model, which means it attempts to provide a model for the distribution of outputs and inputs based on latent variables. This is opposed to discriminative models, which attempt to learn how inputs map to outputs. <em>LDA is a \"bag-of-words\" model, which means that the order of words does not matter.</em></p><p>Incorrect options:</p><p><strong>LDA is a not a \"bag-of-words\" model, which means that the order of words does matter</strong></p><p><strong>LDA is a not a \"bag-of-words\" model, which means that the order of words does not matter</strong></p><p><strong>LDA is a \"bag-of-words\" model, which means that the order of words does matter</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html</a></p>"}, "correct_response": ["d"], "section": "Modeling", "question_plain": "You are doing topic modeling using the SageMaker Latent Dirichlet Allocation (LDA) algorithm. Which of the following are correct?", "related_lectures": []}, {"_class": "assessment", "id": 61582356, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>When you use automatic model tuning, the linear learner internal tuning mechanism is turned on automatically. This sets the number of parallel models, num_models, to 1</p>", "<p>When you use automatic model tuning, the linear learner internal tuning mechanism is turned on automatically. This sets the number of parallel models, num_models, to 0</p>", "<p>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 1</p>", "<p>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 0</p>"], "relatedLectureIds": "", "question": "<p>Which of the following statements is true for the Sagemaker Linear Learner algorithm?</p>", "feedbacks": ["", "", "", ""], "explanation": "<p>Correct option:</p><p><strong>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 1</strong></p><p>Automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. You choose the tunable hyperparameters, a range of values for each, and an objective metric. You choose the objective metric from the metrics that the algorithm computes. Automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric. <em>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 1.</em></p><p>Incorrect options:</p><p><strong>When you use automatic model tuning, the linear learner internal tuning mechanism is turned on automatically. This sets the number of parallel models, num_models, to 1</strong></p><p><strong>When you use automatic model tuning, the linear learner internal tuning mechanism is turned on automatically. This sets the number of parallel models, num_models, to 0</strong></p><p><strong>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 0</strong></p><p>These three options contradict the explanation provided above, so these options are incorrect.</p><p>Reference:</p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html</a></p>"}, "correct_response": ["c"], "section": "Modeling", "question_plain": "Which of the following statements is true for the Sagemaker Linear Learner algorithm?", "related_lectures": []}]}
4774522
~~~
{"count": 65, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 53418458, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>AWS</p>", "<p>EC2 Support</p>", "<p>Customer</p>", "<p>AMI Provider</p>"], "relatedLectureIds": "", "explanation": "<p>For Infrastructure as a Service (IaaS) products like EC2, the customer who launched the instance is responsible for adequately patching the instance. AWS is responsible for keeping AMI up-to-date.&nbsp; Once the EC2 instance is launched, only the customer can patch the instance. Reference: Security is Job Zero https://youtu.be/T7MnJOfOVcY</p><p><br></p>", "question": "<p>You have launched an EC2 instance using Deep Learning AMI.&nbsp; Under AWS Shared Responsibility Model, who is responsible for applying critical security patches on EC2 instances?</p>"}, "correct_response": ["c"], "section": "AWS", "question_plain": "You have launched an EC2 instance using Deep Learning AMI.&nbsp; Under AWS Shared Responsibility Model, who is responsible for applying critical security patches on EC2 instances?", "related_lectures": []}, {"_class": "assessment", "id": 53418460, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Read-only access to all resources in your account</p>", "<p>Read-Write access to all resources in your account</p>", "<p>Read-only access in the region where IAM user was created</p>", "<p>User cannot access AWS resources until explicit allow access is granted</p>"], "relatedLectureIds": "", "explanation": "<p>When you create a new IAM user without attaching any policies, the user is not allowed access to any AWS resource. User needs to be granted permissions by assigning policies or by adding them to a Group with necessary permissions. IAM is a global resource \u2013 when you create a policy, role, user, or group, they can granted permission to AWS resources in any region.</p>", "question": "<p>What privileges does a newly created Identity and Access Management (IAM) user have?&nbsp; This User does not have any policy attached and does not belong to any IAM Groups.</p>"}, "correct_response": ["d"], "section": "AWS", "question_plain": "What privileges does a newly created Identity and Access Management (IAM) user have?&nbsp; This User does not have any policy attached and does not belong to any IAM Groups.", "related_lectures": []}, {"_class": "assessment", "id": 53418462, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Patching Host Operating System</p>", "<p>Physical security of hardware</p>", "<p>Virtualization infrastructure</p>", "<p>Configuring Access to S3 bucket based on job role</p>"], "relatedLectureIds": "", "explanation": "<p>Under the shared responsibility model, data security is the responsibility of the customer.&nbsp; AWS provides capabilities to manage data security; however, it is up to the customer to take advantage of security capabilities based on their individual needs.&nbsp; Physical infrastructure, Facilities, Host Computers (underlying physical servers on which virtual instances run), Network infrastructure are all responsibilities of AWS.&nbsp; </p><p>Patching Host Operating System is AWS responsibility - here, host refers to the Physical server.</p><p>Patching Guest/Instance Operating System is Customer responsibility - here, instance refers to the virtual instance that customer created.</p><p>Additional reading and references: https://wa.aws.amazon.com/wat.concept.shared-resp-model.en.html, </p><p>Security is Job Zero https://youtu.be/T7MnJOfOVcY</p>", "question": "<p>Under the AWS Shared Responsibility Model, the customer is responsible for which of these tasks?</p>"}, "correct_response": ["d"], "section": "AWS", "question_plain": "Under the AWS Shared Responsibility Model, the customer is responsible for which of these tasks?", "related_lectures": []}, {"_class": "assessment", "id": 53418464, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>IAM</p>", "<p>SageMaker</p>", "<p>S3</p>", "<p>CloudWatch</p>"], "relatedLectureIds": "", "explanation": "<p>IAM is a global resource, and any policy or user or group or role that you create are available across all regions.&nbsp; With SageMaker, you need to pick a region to launch notebook instances, or for training and hosting models. S3 requires you to specify a region to create a bucket.&nbsp; CloudWatch is a repository of all metrics for monitoring resources in the region</p>", "question": "<p>Which of these services require you to select an AWS region when using it (choose three)?</p>"}, "correct_response": ["b", "c", "d"], "section": "AWS", "question_plain": "Which of these services require you to select an AWS region when using it (choose three)?", "related_lectures": []}, {"_class": "assessment", "id": 53418466, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>SageMaker Endpoint with a single instance</p>", "<p>S3</p>", "<p>SageMaker Endpoint with multiple instances</p>", "<p>Artificial Intelligence Services like Rekognition</p>"], "relatedLectureIds": "", "explanation": "<p>Each AWS region consists of three or more availability zones. Availability Zones are physically separate infrastructure. Among the choices presented, a SageMaker Endpoint that has only one instance to handle inference requests may be impacted if that instance is running in that Availability Zone.&nbsp; To improve Availability, for production workloads, you need to use at least two instances behind a SageMaker Endpoint \u2013 SageMaker will ensure that the instances are deployed in different availability zones.&nbsp; S3 automatically replicates data in three or more availability zones, and S3 can transparently handle availability zone failure.&nbsp; Managed AI Services like Rekognition is also multi-availability zone enabled and can handle availability zone failures automatically</p>", "question": "<p>Which one of the services may be impacted when a single availability zone goes down in an AWS region?</p>"}, "correct_response": ["a"], "section": "AWS", "question_plain": "Which one of the services may be impacted when a single availability zone goes down in an AWS region?", "related_lectures": []}, {"_class": "assessment", "id": 53418468, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Implement IAM Access Policy to remove delete access or modify access</p>", "<p>Use Vault Lock to implement write once, read many type policies</p>", "<p>Enforce controls like these at the application level</p>", "<p>Replicate Data to another read-only bucket</p>"], "relatedLectureIds": "", "explanation": "<p>Vault Lock allows you to set immutable policies to enforce compliance controls.&nbsp; With the IAM Access policy, you can define who has access to storage and type of access. However, the IAM policy on its own is not sufficient for compliance-related controls as someone could change the policy to grant write permissions</p>", "question": "<p>Your legal department has asked your team to ensure that historical manufacturing data are not deleted or tampered for a 5-year period.&nbsp; Your team is currently using Glacier for long term storage.&nbsp; What option would you pick to enforce this policy?</p>"}, "correct_response": ["b"], "section": "AWS", "question_plain": "Your legal department has asked your team to ensure that historical manufacturing data are not deleted or tampered for a 5-year period.&nbsp; Your team is currently using Glacier for long term storage.&nbsp; What option would you pick to enforce this policy?", "related_lectures": []}, {"_class": "assessment", "id": 53418470, "assessment_type": "multiple-choice", "prompt": {"question": "<p>You are using a lambda function to invoke SageMaker Endpoints.&nbsp; This function can accept a batch of records as input and returns the list of predicted values.&nbsp; You are testing a new model that requires compute-intensive pre-processing of incoming data.&nbsp; You want to use a higher-performing instance for your lambda function.&nbsp; What option does AWS provide to improve performance?</p>", "relatedLectureIds": ["14616954"], "feedbacks": ["", "", "", ""], "explanation": "<p>With Lambda, you must choose the memory needed to execute your function. Based on the memory configuration, proportional CPU capacity is allocated. You can also increase the timeout for up to 15 minutes. Using a compute-optimized instance would require a rewrite of the solution as you need to host the application and handle scaling and availability issues. Adjusting the Lamdba function's allocated memory is a much easier option to optimize your existing solution</p>", "answers": ["<p>Increase allocated vCPU </p>", "<p>Increase allocated memory</p>", "<p>Increase timeout</p>", "<p>Use a compute-optimized instance</p>"]}, "correct_response": ["b"], "section": "AWS", "question_plain": "You are using a lambda function to invoke SageMaker Endpoints.&nbsp; This function can accept a batch of records as input and returns the list of predicted values.&nbsp; You are testing a new model that requires compute-intensive pre-processing of incoming data.&nbsp; You want to use a higher-performing instance for your lambda function.&nbsp; What option does AWS provide to improve performance?", "related_lectures": [{"_class": "lecture", "id": 14616954, "title": "Microservice - Lambda to Endpoint - Payload", "created": "2019-05-03T23:05:42Z", "is_published": true, "asset": {"_class": "asset", "id": 17887714, "asset_type": "Video", "title": "Boto3sdkJSON_voice.mp4", "created": "2019-05-03T23:05:56Z"}, "object_index": 100, "url": "/course/aws-machine-learning-a-complete-guide-with-python/learn/lecture/14616954"}]}, {"_class": "assessment", "id": 53418472, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Avro</p>", "<p>JSON</p>", "<p>CSV</p>", "<p>Parquet</p>"], "relatedLectureIds": "", "explanation": "<p>Parquet is a columnar storage format that transparently compresses data.&nbsp; It is a very efficient format for querying a subset of columns across a large number of records.&nbsp; Avro is a suitable binary format that uses row storage and optimized for use cases that need to access the entire row.&nbsp; JSON and CSV are text formats that use Row storage</p>", "question": "<p>A startup is analyzing social media trends with data stored in S3.&nbsp; For analysis, it is common to access a subset of attributes across a large number of records.&nbsp; Which of these formats can lower the cost of storage while improving query performance?</p>"}, "correct_response": ["d"], "section": "AWS", "question_plain": "A startup is analyzing social media trends with data stored in S3.&nbsp; For analysis, it is common to access a subset of attributes across a large number of records.&nbsp; Which of these formats can lower the cost of storage while improving query performance?", "related_lectures": []}, {"_class": "assessment", "id": 53418474, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Enable Cross-Region Replication and restore objects from the replicated site</p>", "<p>Enable Versioning on the bucket</p>", "<p>Enable Lifecycle Policies on the bucket</p>", "<p>Move the deleted object to a temporary bucket and use it for restoring</p>"], "relatedLectureIds": "", "explanation": "<p>You can enable S3 versioning to keep the older version of the objects. You can create life cycle policies to remove the older version after 30 days.&nbsp; Cross-region can help in protecting against accidental deletion and disaster recovery by keeping a copy of data in a different region.&nbsp; But it is more expensive as a full copy of your bucket is maintained in another region.&nbsp; Moving the deleted objects to another bucket is unnecessary and requires other components.</p>", "question": "<p>Your company uses S3 for storing data collected from a variety of sources.&nbsp; The users are asking for a feature similar to a trash can or recycle bin.&nbsp; Deleted files should be available for restore for up to 30 days.&nbsp; &nbsp;How would you implement this? (Choose Two)</p>"}, "correct_response": ["b", "c"], "section": "AWS", "question_plain": "Your company uses S3 for storing data collected from a variety of sources.&nbsp; The users are asking for a feature similar to a trash can or recycle bin.&nbsp; Deleted files should be available for restore for up to 30 days.&nbsp; &nbsp;How would you implement this? (Choose Two)", "related_lectures": []}, {"_class": "assessment", "id": 53418476, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Athena</p>", "<p>Redshift Spectrum</p>", "<p>EMR Hive</p>", "<p>EMR Spark</p>"], "relatedLectureIds": "", "explanation": "<p>With Athena, you can query the data in S3 using SQL.&nbsp; You can either create the table structure in Glue Catalog or let the Glue Crawler collect the metadata and create the table.&nbsp; Athena automatically provisions the resources required for running queries.&nbsp; Redshift Spectrum also provides a capability similar to Athena; however, the query is executed in your Cluster.&nbsp; So, you would need to provision the servers.&nbsp; EMR Hive and Spark are also good options, but it would require provisioning your cluster, and you would also need to figure out how to load the data to the cluster.</p>", "question": "<p>An organization is consolidating data in S3, and data scientists need access to this data for initial exploration.&nbsp; They are well versed in SQL and would prefer to access the data in S3 using SQL.&nbsp; Which of these options provides the lowest cost without requiring to provision any servers?</p>"}, "correct_response": ["a"], "section": "AWS", "question_plain": "An organization is consolidating data in S3, and data scientists need access to this data for initial exploration.&nbsp; They are well versed in SQL and would prefer to access the data in S3 using SQL.&nbsp; Which of these options provides the lowest cost without requiring to provision any servers?", "related_lectures": []}, {"_class": "assessment", "id": 53418496, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Retrain the image classification model with new data</p>", "<p>Use Transfer learning by removing the output layer of the image classification model, reinitialize the weights of last hidden layer and retrain the model</p>", "<p>Use Transfer learning and remove the first hidden layer of image classification model and retrain the model</p>", "<p>Use Transfer learning by removing the output layer of the image classification model, reinitialize the weights of all layers and retrain the model</p>"], "relatedLectureIds": "", "explanation": "<p>Transfer learning is an approach of reusing a model that works well for a similar problem.&nbsp; With neural networks and deep learning, some domains like speech recognition, image recognition, and so forth require an extensive dataset for the algorithm to learn all patterns.&nbsp; You can reuse these models for more specialized tasks by using transfer learning.&nbsp; Commonly, with transfer learning, you remove the output layer of one model and feed the hidden layer to a different set of neurons that assess performance with the new dataset.&nbsp; The algorithm is now retrained to learn new patterns and adjust the weight.&nbsp; You can start by randomly initializing the weights of the last layer of the existing, and for more complex use cases, you may need to random initialize of weights of the final few layers. Reference: NIPS 2016 tutorial: Nuts and bolts of building AI applications by Dr. Andrew Ng</p>", "question": "<p>You are working on developing a solution to identify specific breeds of cats and dogs from an image.&nbsp; The dataset you have is small.&nbsp; You noticed that an existing image classification neural network that was trained on a large dataset has an excellent ability to classify images.&nbsp; You would like to reuse the network to make it work for the new problem.&nbsp; What steps can you take to accomplish this?</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "You are working on developing a solution to identify specific breeds of cats and dogs from an image.&nbsp; The dataset you have is small.&nbsp; You noticed that an existing image classification neural network that was trained on a large dataset has an excellent ability to classify images.&nbsp; You would like to reuse the network to make it work for the new problem.&nbsp; What steps can you take to accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 53418498, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Sigmoid</p>", "<p>Softmax</p>", "<p>None</p>", "<p>ReLU</p>"], "relatedLectureIds": "", "explanation": "<p>Softmax activation is used for predicting a single label from a set of possible labels.&nbsp; Softmax returns the probability for each label, and the sum of all probabilities adds up to a 1.&nbsp; The class with the highest probability is used as the final class for the example.</p>", "question": "<p>Which activation function would you use in the output layer for a Multi-class Classification neural network that predicts a single label from a set of possible labels?</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "Which activation function would you use in the output layer for a Multi-class Classification neural network that predicts a single label from a set of possible labels?", "related_lectures": []}, {"_class": "assessment", "id": 53418500, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>More often</p>", "<p>Less often</p>", "<p>Weight adjustment depends on the number of examples</p>", "<p>Weight adjustment is not dependent on mini-batch size</p>"], "relatedLectureIds": "", "explanation": "<p>Weights are adjusted based on error observed in a mini-batch.&nbsp; The training set is divided into mini-batches, and when you increase the number of examples in a mini-batch, you have fewer mini-batches.&nbsp; And this would result in less-frequent weight adjustment.</p>", "question": "<p>When you increase the mini-batch size, for every iteration of the training set, the weights of features are adjusted</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "When you increase the mini-batch size, for every iteration of the training set, the weights of features are adjusted", "related_lectures": []}, {"_class": "assessment", "id": 53418502, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Use 10,000 hours of clean speech data for training the model. Divide 100 hours of noisy data into validation and test sets. Optimize the model to improve validation performance and perform the final test using the test set</p>", "<p>Split the 10,000 hours of clean speech data into training and validation sets. Optimize the model to improve validation performance. Use 100 hours of noisy data for final testing</p>", "<p>Split the 10,000 hours of clean speech data into training and validation set.&nbsp; Divide 100 hours of noisy speech data, add some to the validation set and keep the rest in the test set</p>", "<p>Use 100 hours of noisy data for training and split the general speech data for validation and testing</p>"], "relatedLectureIds": "", "explanation": "<p>The objective of this model is to recognize speech in a noisy environment.&nbsp; Since there is very little noisy data available when compared to clean data, one approach that can be used is to train the model on clean data, split the noisy data into validation and test set.&nbsp; Use the noisy validation data to tune the model performance and perform the final check with test data.</p><p>Another option is to split the clean speech data into training and validation sets. Add some of the noisy data to the validation dataset and keep the remaining noisy data for the test set.</p><p>If you keep split the clean data into training and validation sets and tune model based on validation performance, this model only performs well with clean data and would perform poorly with noisy test data.&nbsp; That is because the distribution of clean and noisy data is different.</p><p>Just training on 100 hours of noisy data may not be enough for this use case. Reference: NIPS 2016 tutorial: Nuts and bolts of building AI applications by Dr. Andrew Ng</p>", "question": "<p>A team of machine learning experts is building a speech recognition system that can work in a noisy factory environment.&nbsp; The dataset consists of 10,000 hours of clean speech data and another dataset with 100 hours of noisy speech data recorded inside the factory. </p><p>How do you define training, validation, and test set? (Select Two)</p>"}, "correct_response": ["a", "c"], "section": "Machine Learning Concepts", "question_plain": "A team of machine learning experts is building a speech recognition system that can work in a noisy factory environment.&nbsp; The dataset consists of 10,000 hours of clean speech data and another dataset with 100 hours of noisy speech data recorded inside the factory. How do you define training, validation, and test set? (Select Two)", "related_lectures": []}, {"_class": "assessment", "id": 53418504, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>1%</p>", "<p>5%</p>", "<p>2.5%</p>", "<p>Average of the error rates</p>"], "relatedLectureIds": "", "explanation": "<p>1% should be used as the human-level performance and it is a good proxy for Bayes optimal error (theoretical best possible error rate).</p><p>Reference: NIPS 2016 tutorial: Nuts and bolts of building AI applications by Dr. Andrew Ng (Starting at 1:24:00) https://www.youtube.com/watch?v=wjqaz6m42wU</p>", "question": "<p>An organization has human experts who perform manual classification of products by visual inspection.&nbsp; A Machine Learning specialist is building a classification system to match human-level performance.&nbsp; When reviewing the error rate of humans, the specialist observes the following:&nbsp; </p><p>Newly trained employees had a misclassification error rate of 5%, Experienced employee had an error rate of 2.5%, and when a team of experienced employees worked together, they had a misclassification rate of 1%.&nbsp; </p><p>What should be considered as human-level performance?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "An organization has human experts who perform manual classification of products by visual inspection.&nbsp; A Machine Learning specialist is building a classification system to match human-level performance.&nbsp; When reviewing the error rate of humans, the specialist observes the following:&nbsp; Newly trained employees had a misclassification error rate of 5%, Experienced employee had an error rate of 2.5%, and when a team of experienced employees worked together, they had a misclassification rate of 1%.&nbsp; What should be considered as human-level performance?", "related_lectures": []}, {"_class": "assessment", "id": 53418506, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", ""], "answers": ["<p>Train with more data</p>", "<p>Increase Regularization</p>", "<p>New neural network architecture</p>", "<p>Decrease regularization</p>", "<p>Increase the number of epochs</p>"], "relatedLectureIds": "", "explanation": "<p>The training error is low, and the test error is high.&nbsp; So, the model has a variance problem.&nbsp; The model is overfitting the training data, or the data distribution between test and train data sets is different.&nbsp; You can train with more data to handle issues with different data distribution between train and test data sets.&nbsp; More data is also useful if the model is not detecting all the patterns.&nbsp; If you suspect overfitting, you can also increase regularization to simplify the model.&nbsp; Finally, you can also try different neural network architecture (for example reduce number of neurons, change the number of hidden layers and so forth).&nbsp; However, decreasing regularization would cause the model to overfit more.&nbsp; You can reduce the number of epochs to minimize variance.&nbsp; This would prevent the model from memorizing too much about training data.</p>", "question": "<p>The training error is low, but the test error is high.&nbsp; Among the choices presented, which one of these options can correct the issue? (Choose Three)</p>"}, "correct_response": ["a", "b", "c"], "section": "Machine Learning Concepts", "question_plain": "The training error is low, but the test error is high.&nbsp; Among the choices presented, which one of these options can correct the issue? (Choose Three)", "related_lectures": []}, {"_class": "assessment", "id": 53418508, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Build a more complex model</p>", "<p>Train longer</p>", "<p>New neural network architecture</p>", "<p>Increase regularization</p>"], "relatedLectureIds": "", "explanation": "<p>Since the gap between human-level performance and training error is large, the model is underfitting.&nbsp; When a model underfits, it is not learning from training data.&nbsp; To fix the high training error, you can increase the model complexity, train the model longer (more epochs), and use a different network architecture. However, increasing regularization would reduce the model complexity (by suppressing the importance of features), and it will underfit more. Reference: NIPS 2016 tutorial: Nuts and bolts of building AI applications by Dr. Andrew Ng</p>", "question": "<p>The human level error rate is 2%, and the model training error rate is 8%.&nbsp; What steps can you take to optimize the model? (Choose Three)</p>"}, "correct_response": ["a", "b", "c"], "section": "Machine Learning Concepts", "question_plain": "The human level error rate is 2%, and the model training error rate is 8%.&nbsp; What steps can you take to optimize the model? (Choose Three)", "related_lectures": []}, {"_class": "assessment", "id": 53418510, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Overfitting</p>", "<p>Underfitting</p>", "<p>Normal</p>", "<p>Performing close to human-level performance</p>"], "relatedLectureIds": "", "explanation": "<p>Here, the model training error is comparable to human-level performance.&nbsp; So, the model is doing well with training data.&nbsp; However, it is not generalizing well for unseen data, and the test error is much larger. So, it is showing signs of overfitting. (memorized too much about training data)</p>", "question": "<p>A model has the following errors: Training Error is 2%, Test Error is 5%.&nbsp; The benchmark is human-level performance, and the human error is 1%.&nbsp; &nbsp;</p><p>The model is:</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "A model has the following errors: Training Error is 2%, Test Error is 5%.&nbsp; The benchmark is human-level performance, and the human error is 1%.&nbsp; &nbsp;The model is:", "related_lectures": []}, {"_class": "assessment", "id": 53418512, "assessment_type": "multiple-choice", "prompt": {"explanation": "<p>XGBoost requires all numeric features. Tree-based algorithms can handle features with different scales. It also handles numeric categorical features (does not require one-hot encoding). However, XGBoost also supports One-hot encoded features. For a binary feature like Lawn (yes or no), only label encoding is needed (i.e., convert to 0 and 1). You should not perform one-hot encoding on binary features. For non-binary categorical features, you can test with label encoding first and then optionally, test performance with one-hot encoding.</p>", "answers": ["<p>Scale all numeric features to similar range and scale</p>", "<p>One-Hot encode categorical features</p>", "<p>Transform non-numeric categories to equivalent numeric categories</p>", "<p>Normalize all numeric features</p>"], "feedbacks": ["", "", "", ""], "relatedLectureIds": "", "question": "<p>A utility company wants to forecast water consumption per household.&nbsp; The historical data set contains the following attributes:</p><p>* Year - Numeric</p><p>* Month - Numeric</p><p>* Floor Size SqFt \u2013 numeric </p><p>* Lot Size SqFt - numeric</p><p>* Number of Bathrooms \u2013 numeric</p><p>* Lawn \u2013 categorical with values YES or NO</p><p>* Consumption \u2013 numeric (target)</p><p>To train using XGBoost, what data transformation step do you need to perform?</p>"}, "correct_response": ["c"], "section": "Machine Learning Concepts", "question_plain": "A utility company wants to forecast water consumption per household.&nbsp; The historical data set contains the following attributes:* Year - Numeric* Month - Numeric* Floor Size SqFt \u2013 numeric * Lot Size SqFt - numeric* Number of Bathrooms \u2013 numeric* Lawn \u2013 categorical with values YES or NO* Consumption \u2013 numeric (target)To train using XGBoost, what data transformation step do you need to perform?", "related_lectures": []}, {"_class": "assessment", "id": 53418514, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Accuracy</p>", "<p>Squared Error</p>", "<p>F1 Score</p>", "<p>ROC AUC Metric</p>"], "relatedLectureIds": "", "explanation": "<p>Receiver Operating Characteristic (ROC) curve compares the true positive rate and the false positive rate at different thresholds.&nbsp; AUC metrics measure the area formed by such a curve, and the ROC AUC is used for summarizing the model performance with a range of tradeoffs</p>", "question": "<p>You are training a model to predict the probability of leaving the mobile operator.&nbsp; You would like to assess the quality of the metrics at various cut-off thresholds.&nbsp; Which metric gives you insight into the model performance over a range of tradeoffs between true positive rate and false-positive rate?</p>"}, "correct_response": ["d"], "section": "Machine Learning Concepts", "question_plain": "You are training a model to predict the probability of leaving the mobile operator.&nbsp; You would like to assess the quality of the metrics at various cut-off thresholds.&nbsp; Which metric gives you insight into the model performance over a range of tradeoffs between true positive rate and false-positive rate?", "related_lectures": []}, {"_class": "assessment", "id": 53418516, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Positive: 10, Negative: 9</p>", "<p>Positive: 12, Negative: 7</p>", "<p>Positive: 13, Negative: 6</p>", "<p>Positive: 6, Negative: 13</p>"], "relatedLectureIds": "", "explanation": "<p>Positive = True Positive + False Negative</p><p>Negative = True Negative + False Positive</p>", "question": "<p>A binary classifier metrics for validation data has the following values:</p><p>&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 4, FP: 5</p><p>How many positive and negative samples are there in the validation dataset?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "A binary classifier metrics for validation data has the following values:&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 4, FP: 5How many positive and negative samples are there in the validation dataset?", "related_lectures": []}, {"_class": "assessment", "id": 53418518, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>0.8</p>", "<p>0.5</p>", "<p>0.6</p>", "<p>0.3</p>"], "relatedLectureIds": "", "explanation": "<p>Recall or true positive rate = TP/(TP+FN)</p>", "question": "<p>A binary classifier metrics for validation data has the following values:</p><p>&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 3, FP: 5</p><p>What is the Recall for this model?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "A binary classifier metrics for validation data has the following values:&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 3, FP: 5What is the Recall for this model?", "related_lectures": []}, {"_class": "assessment", "id": 53418520, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>0.8</p>", "<p>0.5</p>", "<p>0.6</p>", "<p>0.3</p>"], "relatedLectureIds": "", "explanation": "<p>Precision = TP / (TP + FP)</p>", "question": "<p>A binary classifier metrics for validation data has the following values:</p><p>&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 3, FP: 5</p><p>What is the Precision for this model?</p>"}, "correct_response": ["c"], "section": "Machine Learning Concepts", "question_plain": "A binary classifier metrics for validation data has the following values:&nbsp; &nbsp; &nbsp; &nbsp;TP: 8, FN: 2, TN: 3, FP: 5What is the Precision for this model?", "related_lectures": []}, {"_class": "assessment", "id": 53418522, "assessment_type": "multiple-choice", "prompt": {"answers": ["<p>Accuracy</p>", "<p>Precision</p>", "<p>Recall</p>", "<p>F1 Score</p>"], "question": "<p>A highly unbalanced dataset has 95% normal data and 5% positive data. What is a good performance metric to use for assessing the quality of the model?</p>", "explanation": "<p>Accuracy is not a useful metric for skewed data sets.&nbsp; Recall on its own is not enough as the model that predicts everything as positive will have a very high Recall.&nbsp; Precision on its own is not enough as the model can have very high precision even if it predicts only one positive correctly and misclassifies everything as negative.&nbsp; F1 Score is a useful metric as it considers both recall and precision.</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""]}, "correct_response": ["d"], "section": "Machine Learning Concepts", "question_plain": "A highly unbalanced dataset has 95% normal data and 5% positive data. What is a good performance metric to use for assessing the quality of the model?", "related_lectures": []}, {"_class": "assessment", "id": 53418524, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Model 1</p>", "<p>Model 2</p>", "<p>Model 3</p>", "<p>Model 4</p>"], "relatedLectureIds": "", "explanation": "<p>Recall needs to be at least 60%.&nbsp; Recall = TP/(TP+FN).&nbsp; </p><p>Model 1 recall is over 0.6, and Model 2 recall is 0.3; model 3 recall is 1/15 and is a small value and Model 4 recall is 0.6.&nbsp; So, the answer has to be either Model 1 or Model 4.&nbsp; </p><p>The cost of misclassifying a positive sample is three times more than misclassifying a negative sample. </p><p>Total Cost = 3 * FN + 1 * FP</p><p>Model 1 cost = 3*5 + 10 = 25. Model 4 cost is = 3 * 6 + 1 * 15 = 18 + 15 = 33.&nbsp; </p><p>So, Model 1 has the lowest cost while meeting 0.6 recall.</p>", "question": "<p>For a binary classification problem, the cost of misclassifying a positive sample is three times more than the cost of misclassifying a negative example.</p><p>Which model has the lowest cost with at least 60% recall?</p><p>Model 1 \u2013 TP: 10, FN: 5, TN: 25, FP: 10</p><p>Model 2 \u2013 TP: 5, FN: 10, TN: 20, FP: 15</p><p>Model 3 \u2013 TP: 1, FN: 14, TN: 30, FP: 5</p><p>Model 4 \u2013 TP: 9, FN: 6, TN: 20, FP: 15</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "For a binary classification problem, the cost of misclassifying a positive sample is three times more than the cost of misclassifying a negative example.Which model has the lowest cost with at least 60% recall?Model 1 \u2013 TP: 10, FN: 5, TN: 25, FP: 10Model 2 \u2013 TP: 5, FN: 10, TN: 20, FP: 15Model 3 \u2013 TP: 1, FN: 14, TN: 30, FP: 5Model 4 \u2013 TP: 9, FN: 6, TN: 20, FP: 15", "related_lectures": []}, {"_class": "assessment", "id": 53418526, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Training</p>", "<p>Validation</p>", "<p>Testing</p>", "<p>Hosting</p>"], "relatedLectureIds": "", "explanation": "<p>Hyperparameters are used when training the model \u2013 These parameters control how a model learns.&nbsp; Once a model is trained, you cannot change the hyperparameters \u2013 you need to retrain it</p>", "question": "<p>You want to test new values for hyperparameters for an algorithm.&nbsp; At what point in the model lifecycle can you change hyperparameters?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "You want to test new values for hyperparameters for an algorithm.&nbsp; At what point in the model lifecycle can you change hyperparameters?", "related_lectures": []}, {"_class": "assessment", "id": 53418528, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Scaling</p>", "<p>Normalization</p>", "<p>Remove one feature from every highly correlated feature pairs</p>", "<p>Data transformation is not needed for this dataset</p>"], "relatedLectureIds": "", "explanation": "<p>Decision Tree-based algorithms like XGBoost automatically handles correlated features, numeric features on a different scale, and numeric-categorical variables. Other algorithms like a neural network and the linear model would require features on a similar scale and range, and you need to keep only one feature in every highly correlated feature pairs and one-hot encode categorical features.</p>", "question": "<p>A data scientist is exploring the use of the XGBoost algorithm for a regression problem.&nbsp; </p><p>The dataset consists of numeric features.&nbsp; </p><p>Some of the features are highly correlated, and almost all the features are on different orders of magnitude.&nbsp; </p><p>What data-transformation is required to train on XGBoost?</p>"}, "correct_response": ["d"], "section": "Machine Learning Concepts", "question_plain": "A data scientist is exploring the use of the XGBoost algorithm for a regression problem.&nbsp; The dataset consists of numeric features.&nbsp; Some of the features are highly correlated, and almost all the features are on different orders of magnitude.&nbsp; What data-transformation is required to train on XGBoost?", "related_lectures": []}, {"_class": "assessment", "id": 53418530, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", ""], "answers": ["<p>One-Hot encode Day of Week</p>", "<p>Scale Temperature, Humidity, Precipitation, Windspeed, Pollen features</p>", "<p>Label encode AirQuality and Holiday features</p>", "<p>Transform using Principal Component Analysis</p>", "<p>Use numeric data without any transformation, and one hot encode categorical features</p>"], "relatedLectureIds": "", "explanation": "<p>The categorical features need to be one-hot encoded for algorithms like the linear model and neural network. </p><p>XGBoost and tree-based algorithms can work with numeric categorical features as well as one-hot encoded categorical features (one-hot encoding is not required; however, XGBoost can handle one-hot encoded data).&nbsp; </p><p>For binary feature like holiday, you need to convert to numeric value using label-encoding.</p><p>For numeric features, convert them to a similar range and scale. </p><p>AirQuality is the label, and the model needs to learn to predict one of two outcomes.</p><p>Since the label contains text, it needs to be converted to a numeric value and we can do that using label encoding.</p><p>Note: For multi-class problem, neural networks require one-hot encoding of labels</p>", "question": "<p>A dataset consists of following features along with the type of values it can contain</p><p>*&nbsp;DayOfWeek \u2013 Sunday, Monday, Tuesday and so forth</p><p>*&nbsp;Holiday \u2013 True or False</p><p>*&nbsp;Temperature \u2013 in Fahrenheit </p><p>*&nbsp;Humidity \u2013 0 to 100</p><p>*&nbsp;Precipitation \u2013 0 to 100</p><p>*&nbsp;Windspeed \u2013 0 to 150 </p><p>*&nbsp;Pollen \u2013 0 to 1</p><p>* AirQuality \u2013 Good, Bad&nbsp; </p><p>AirQuality is the label</p><p>The Machine Learning Analyst is planning to compare a variety of algorithms and would like to reuse the same transformed dataset for training and testing.&nbsp; </p><p>What data transformation is recommended? (Select Three)</p>"}, "correct_response": ["a", "b", "c"], "section": "Machine Learning Concepts", "question_plain": "A dataset consists of following features along with the type of values it can contain*&nbsp;DayOfWeek \u2013 Sunday, Monday, Tuesday and so forth*&nbsp;Holiday \u2013 True or False*&nbsp;Temperature \u2013 in Fahrenheit *&nbsp;Humidity \u2013 0 to 100*&nbsp;Precipitation \u2013 0 to 100*&nbsp;Windspeed \u2013 0 to 150 *&nbsp;Pollen \u2013 0 to 1* AirQuality \u2013 Good, Bad&nbsp; AirQuality is the labelThe Machine Learning Analyst is planning to compare a variety of algorithms and would like to reuse the same transformed dataset for training and testing.&nbsp; What data transformation is recommended? (Select Three)", "related_lectures": []}, {"_class": "assessment", "id": 53418532, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>5</p>", "<p>10</p>", "<p>8</p>", "<p>6</p>"], "relatedLectureIds": "", "explanation": "<p>With unigram transformation, each unique word is a feature.&nbsp; There are five unique words: disappointed, is, not, this, working.&nbsp; With bigram transformation, you need to include consecutive two-word combinations like \u201cthis is\u201d, \u201cis working\u201d and so forth.</p>", "question": "<p>You are using unigram text transformation to convert words to the frequency of occurrence.&nbsp; There are two sentences in the text.</p><p>\u201cthis is working - not disappointed\u201d&nbsp; </p><p>\u201cthis is not working - disappointed\u201d</p><p>How many features would the transformed dataset have?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "You are using unigram text transformation to convert words to the frequency of occurrence.&nbsp; There are two sentences in the text.\u201cthis is working - not disappointed\u201d&nbsp; \u201cthis is not working - disappointed\u201dHow many features would the transformed dataset have?", "related_lectures": []}, {"_class": "assessment", "id": 53418534, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>When using Linear Regression algorithm, it easily handles this dataset with very low RMSE error on the validation dataset</p>", "<p>When using XGBoost Regression algorithm, it easily handles this dataset with very low RMSE error on the validation dataset</p>", "<p>Instead of using Machine Learning, implement the logic in code as the conversion logic is simple</p>", "<p>Use either Linear Regression or XGBoost</p>"], "relatedLectureIds": "", "explanation": "<p>This problem is an example of where you don\u2019t want to use Machine Learning.&nbsp; Temperature conversion is easily doable with a simple formula, whereas implementing as an ML solution is more complicated and has a lot of overhead in terms of model training, validation, hosting, and ongoing maintenance. </p>", "question": "<p>You have a requirement to convert temperature from Celsius to Fahrenheit.&nbsp; You have a dataset of a few hundred rows that contain examples of Celsius and equivalent Fahrenheit.&nbsp; These are results observed using different approaches.&nbsp; </p><p>Which option would you pick?</p>"}, "correct_response": ["c"], "section": "Machine Learning Concepts", "question_plain": "You have a requirement to convert temperature from Celsius to Fahrenheit.&nbsp; You have a dataset of a few hundred rows that contain examples of Celsius and equivalent Fahrenheit.&nbsp; These are results observed using different approaches.&nbsp; Which option would you pick?", "related_lectures": []}, {"_class": "assessment", "id": 53418536, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Convolutional Neural Network</p>", "<p>Recurrent Neural Network</p>", "<p>General Purpose Neural Network</p>", "<p>Try different neural network architectures</p>"], "relatedLectureIds": "", "explanation": "<p>CNN's are ideal for image and video analysis applications \u2013 it considers a pixel and surround pixels to identify patterns.&nbsp; RNNs are used for applications where the predicted value depends on previously seen values \u2013 time-series forecasting, speech recognition and so forth.&nbsp; The general-purpose neural network considers each pixel as a separate feature and may require a very complex network for image analysis applications \u2013 whereas a simple CNN can easily outperform a general-purpose neural network for visual analysis application.</p>", "question": "<p>You are building a neural network for image analysis \u2013 What type of network would you use?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "You are building a neural network for image analysis \u2013 What type of network would you use?", "related_lectures": []}, {"_class": "assessment", "id": 53418538, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>It can help optimization algorithm jump local minima and explore other areas for global minima</p>", "<p>Optimization algorithm uses all samples for every weight adjustment</p>", "<p>It will make smoother and more gradual adjustments to the weight</p>", "<p>Smaller mini-batch will force the algorithm to converge and get stuck in local minima</p>"], "relatedLectureIds": "", "explanation": "<p>Mini-batch has the effect of making more substantial changes to weight as it uses a smaller set of samples to determine the gradient.&nbsp; In a deep learning network, the loss curve is very complex with multiple local minima.&nbsp; To prevent the optimizer from getting stuck in local minima, you can reduce the batch size to jump over local minima</p>", "question": "<p>When training a deep learning network, what is the impact of using smaller mini-batch sizes?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "When training a deep learning network, what is the impact of using smaller mini-batch sizes?", "related_lectures": []}, {"_class": "assessment", "id": 53418540, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Do nothing \u2013 algorithms can handle missing values if you provide examples in the training set</p>", "<p>Add substitute variables for each feature \u2013 when the feature has a missing value for a sample, set the substitute variable to 1 for that feature, and when the feature has a valid value, set the variable to 0</p>", "<p>Replace missing values with the average value for that feature</p>", "<p>Replace missing values with 0</p>"], "relatedLectureIds": "", "explanation": "<p>Substitute variables are Boolean features that capture if a feature contains a missing value for the sample.&nbsp; This allows the algorithm to learn from missing values</p><p>https://docs.aws.amazon.com/machine-learning/latest/dg/data-insights.html#missing-values</p>", "question": "<p>A data scientist is working on a problem to classify incoming data into one of five categories: Good, DefectA, DefectB, DefectC, and DefectD.&nbsp; The dataset consists of primarily numeric features, and some of the samples have missing values for features.&nbsp; This missing values in features can help predict the defect class.&nbsp; </p><p>How do you train the model to learn from missing values?</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "A data scientist is working on a problem to classify incoming data into one of five categories: Good, DefectA, DefectB, DefectC, and DefectD.&nbsp; The dataset consists of primarily numeric features, and some of the samples have missing values for features.&nbsp; This missing values in features can help predict the defect class.&nbsp; How do you train the model to learn from missing values?", "related_lectures": []}, {"_class": "assessment", "id": 53418542, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>linear regression</p>", "<p>xgboost</p>", "<p>decision tree</p>", "<p>neural network</p>"], "relatedLectureIds": "", "explanation": "<p>Tree-based algorithms like decision tree, random forest, and xgboost have a lower and upper bound it can predict for regression.&nbsp; The lower and upper bound is determined based on the range of values seen during training.</p>", "question": "<p>For a regression problem, which of these algorithms cap the output to a range of values seen in the training set? (Choose two) </p>"}, "correct_response": ["b", "c"], "section": "Machine Learning Concepts", "question_plain": "For a regression problem, which of these algorithms cap the output to a range of values seen in the training set? (Choose two)", "related_lectures": []}, {"_class": "assessment", "id": 53418544, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>L1 Regularization</p>", "<p>L2 Regularization</p>", "<p>Either L1 or L2 Regularization</p>", "<p>Learning Rate</p>"], "relatedLectureIds": "", "explanation": "<p>Regularization is used to control how a feature can influence the outcome.&nbsp; </p><p>When the model overfits, you can increase regularization to reduce the relative weight of each feature.&nbsp; </p><p>Similarly, when a model underfits, you can reduce regularization to allow features to assist in predicting the outcome more actively.&nbsp; </p><p>L1 Regularization works by eliminating features that are not important.&nbsp; </p><p>L2 Regularization keeps all the features but simply assigns a very small weight to features that are not important</p>", "question": "<p>A dataset contains a large number of features.&nbsp; You would like the algorithm to aggressively prune features that are not relevant. What hyperparameter can you use for this?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "A dataset contains a large number of features.&nbsp; You would like the algorithm to aggressively prune features that are not relevant. What hyperparameter can you use for this?", "related_lectures": []}, {"_class": "assessment", "id": 53418546, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Use algorithms like Factorization Machines that are optimized for very large datasets</p>", "<p>Reduce Dimension using Principal Component Analysis</p>", "<p>Compress using GZIP algorithm</p>", "<p>Store data in Parquet format</p>"], "relatedLectureIds": "", "explanation": "<p>Principal Component Analysis (PCA) is a dimensionality reduction technique \u2013 it works by capturing information contained in the original dataset using far fewer features known as components. The newly generated features (component) can then be used for model training.&nbsp; The one drawback with PCA is the newly generated components cannot be mapped to real-world features as there is no easy way to figure how each feature contributes to a component. You also need to standardize or normalize data before you perform PCA. </p><p>The factorization algorithm works very well with large sparse datasets.&nbsp; Since this is a dense dataset, this option is not valid.&nbsp; </p><p>GZIP compression merely reduces the storage needed \u2013 it does not reduce the number of features.&nbsp; </p><p>Parquet format is an efficient binary storage format for columnar access \u2013 it is useful for scenarios where you want to extract only some columns from 1000s of columns</p>", "question": "<p>You have a dense dataset with 1000s of features.&nbsp; You are using a custom training algorithm that has difficulty handling large datasets; you would like to reduce this dataset to a few important features.&nbsp; </p><p>The transformed dataset needs to retain as much information as possible from the original dataset.&nbsp; </p><p>What approach can you use for this problem?</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "You have a dense dataset with 1000s of features.&nbsp; You are using a custom training algorithm that has difficulty handling large datasets; you would like to reduce this dataset to a few important features.&nbsp; The transformed dataset needs to retain as much information as possible from the original dataset.&nbsp; What approach can you use for this problem?", "related_lectures": []}, {"_class": "assessment", "id": 53418548, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>1</p>", "<p>100</p>", "<p>4</p>", "<p>32</p>"], "relatedLectureIds": "", "explanation": "<p>\u201cRunning more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time.\u201d</p><p>https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html</p><p><br></p>", "question": "<p>You are using SageMaker\u2019s Automatic Hyperparameter tuning to find an optimal set of parameters for a deep learning network.&nbsp; You are using the Bayesian search with a maximum number of training jobs set to 100.&nbsp; What is the recommended amount of concurrent tuning jobs that you can run for the best results?</p>"}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "You are using SageMaker\u2019s Automatic Hyperparameter tuning to find an optimal set of parameters for a deep learning network.&nbsp; You are using the Bayesian search with a maximum number of training jobs set to 100.&nbsp; What is the recommended amount of concurrent tuning jobs that you can run for the best results?", "related_lectures": []}, {"_class": "assessment", "id": 53418550, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", ""], "answers": ["<p>Configure the tuning job to explore all hyperparameters supported by the algorithm</p>", "<p>Configure the tuning job to search a smaller number of hyperparameters</p>", "<p>Use fewer concurrent tuning jobs</p>", "<p>Use Logarithmic Scaling for hyperparameter that spans several orders of magnitude</p>", "<p>Use Linear Scaling for hyperparameter that spans several orders of magnitude</p>"], "relatedLectureIds": "", "explanation": "<p>\u201cyou can simultaneously use up to 20 variables in a hyperparameter tuning job, limiting your search to a much smaller number is likely to give better results\u201d</p><p>\u201ca tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time\u201d</p><p>\u201cChoose logarithmic scaling when you are searching a range that spans several orders of magnitude\u201d </p><p>\u201cyou specify a range of values between .0001 and 1.0 for the&nbsp;learning_rate&nbsp;hyperparameter, searching uniformly on a logarithmic scale gives you a better sample of the entire range than searching on a linear scale would, because searching on a linear scale would, on average, devote 90 percent of your training budget to only the values between .1 and 1.0, leaving only 10 percent of your training budget for the values between .0001 and .1\u201d</p><p>https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html</p><p>https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html</p>", "question": "<p>You are using SageMaker Automatic Hyperparameter tuning to search for optimal parameters for a learning algorithm.&nbsp; </p><p>What are the best practices when running a hyperparameter tuning job? (Choose three)</p>"}, "correct_response": ["b", "c", "d"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "You are using SageMaker Automatic Hyperparameter tuning to search for optimal parameters for a learning algorithm.&nbsp; What are the best practices when running a hyperparameter tuning job? (Choose three)", "related_lectures": []}, {"_class": "assessment", "id": 53418552, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Host your models on EC2 web server instances, and load balance using Elastic Load Balancing. Setup autoscaling to scale web servers</p>", "<p>API Gateway, Lambda, SageMaker Endpoint with Auto Scaling</p>", "<p>Use Lambda function to invoke machine learning models and invoke the Lambda function from the client application</p>", "<p>Invoke Machine Learning model endpoint from your Client application</p>"], "relatedLectureIds": "", "explanation": "<p>To streamline access to the backend services, you can API Gateway. You can use API Gateway as a Gatekeeper to ensure only authorized users and applications have access to the services. Configure API Gateway to invoke the Lambda function.&nbsp; Lambda function, in turn, invokes SageMaker endpoint.&nbsp; You need to configure Autoscaling to ensure SageMaker endpoint scales on demand.&nbsp; This approach also makes it easy to try different versions of Machine Learning Models without requiring code changes in the client application.</p>", "question": "<p>Your company has a portfolio of machine learning models that are used by web applications and mobile apps.&nbsp; What is the best mechanism to integrate machine learning models with your application?&nbsp; The solution also needs to scale on demand.</p>"}, "correct_response": ["b"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "Your company has a portfolio of machine learning models that are used by web applications and mobile apps.&nbsp; What is the best mechanism to integrate machine learning models with your application?&nbsp; The solution also needs to scale on demand.", "related_lectures": []}, {"_class": "assessment", "id": 53418554, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>4,500</p>", "<p>9,000</p>", "<p>18,000</p>", "<p>2,250</p>"], "relatedLectureIds": "", "explanation": "<p>SageMakerVariantInvocationsPerInstance is a per minute metric that you can monitor with CloudWatch to trigger Auto Scaling actions.&nbsp; When this value exceeds 4500, Autoscaling needs to add a server to handle the increased workload.</p><p>SageMakerVariantInvocationsPerInstance = (MAX_RPS * SAFETY_FACTOR) * 60 = 150 * 0.5 * 60 = 4500</p>", "question": "<p>You need to configure the SageMaker Endpoint to Scale on demand. Based on load testing, you have determined that one instance can handle 150 requests per second.&nbsp; Assume a safety factor of 0.5. </p><p>What value do you need to set for SageMakerVariantInvocationsPerInstance to trigger auto-scaling action?&nbsp; </p><p>Note: SageMakerVariantInvocationsPerInstance is a per minute metric.</p>"}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "You need to configure the SageMaker Endpoint to Scale on demand. Based on load testing, you have determined that one instance can handle 150 requests per second.&nbsp; Assume a safety factor of 0.5. What value do you need to set for SageMakerVariantInvocationsPerInstance to trigger auto-scaling action?&nbsp; Note: SageMakerVariantInvocationsPerInstance is a per minute metric.", "related_lectures": []}, {"_class": "assessment", "id": 53418556, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>SageMaker Endpoint</p>", "<p>SageMaker Batch Transform</p>", "<p>Autoscaling</p>", "<p>S3 Analytics</p>"], "relatedLectureIds": "", "explanation": "<p>Batch Transform is a cost-effective way to get large scale inference using SageMaker.&nbsp; Batch transform is ideal for situations where you don\u2019t need a persistent real-time endpoint, scenarios where you don\u2019t need sub-second latency performance.&nbsp; SageMaker manages all resources required for batch transform.&nbsp; SageMaker Endpoint is used for real-time inference.&nbsp; Autoscaling allows you to maintain capacity, handle instance failures and scale based on workload.&nbsp; S3 Analytics is used for analyzing storage access patterns, which in turn can help you to transition data to the right storage class in S3.</p>", "question": "<p>A machine learning specialist needs to get inference for the entire dataset that is stored in S3. The Machine Learning Model was trained on SageMaker.&nbsp; </p><p>Which of these options provides a managed infrastructure that is cost-effective for large scale inference?</p>"}, "correct_response": ["b"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A machine learning specialist needs to get inference for the entire dataset that is stored in S3. The Machine Learning Model was trained on SageMaker.&nbsp; Which of these options provides a managed infrastructure that is cost-effective for large scale inference?", "related_lectures": []}, {"_class": "assessment", "id": 53418558, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>General Purpose family</p>", "<p>Compute Optimized family</p>", "<p>Accelerated Computing family</p>", "<p>Memory-Optimized family</p>"], "relatedLectureIds": "", "explanation": "<p>Accelerated computing family (P and G type instances) come with GPUs, and these are ideal for algorithms that are optimized for GPUs. </p><p>General Purpose family are some of the lowest cost instances and offer balanced performance and memory configuration (T and M type instances). </p><p>Compute Optimized family comes with the latest generation CPUs and is a higher performance system. These are suitable for CPU intensive model training and hosting (C type instances).&nbsp; </p><p>Memory-optimized family are optimized for workloads that process large datasets in memory (R type instances).&nbsp; </p><p>Besides, the sagemaker also has Elastic Inference Acceleration (partial GPUs) that provides fractional GPU capacity at a fraction of the cost of accelerated computing family.&nbsp; </p><p>Elastic inference Acceleration is suitable for inference workloads that can benefit from GPUs and can be easily added to other instance families.</p>", "question": "<p>A data scientist has a large dataset that needs to be trained on the AWS SageMaker service.&nbsp; The training algorithm is optimized for GPU processing and can benefit from substantial speed-up when trained on instances with GPUs.&nbsp; Which instance family can you use for a training job for the best performance?</p>"}, "correct_response": ["c"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A data scientist has a large dataset that needs to be trained on the AWS SageMaker service.&nbsp; The training algorithm is optimized for GPU processing and can benefit from substantial speed-up when trained on instances with GPUs.&nbsp; Which instance family can you use for a training job for the best performance?", "related_lectures": []}, {"_class": "assessment", "id": 53418560, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>File Mode</p>", "<p>Pipe Mode</p>", "<p>SageMaker does not copy data to local instance volumes \u2013 all data resides in S3</p>", "<p>Explore compressed storage</p>"], "relatedLectureIds": "", "explanation": "<p>In Pipe Mode, training job streams data from S3 to your training instance.&nbsp; </p><p>Streaming can provide faster start times and better throughput. It also reduces the storage needed on your training instances as you need storage only for the final model artifacts.&nbsp; </p><p>In File mode, training job copies entire data from S3 to your training instance volumes.&nbsp; </p><p>So, you would need to allocate enough disk space in your training instances to store your full training dataset and for the final model artifacts</p>", "question": "<p>A machine learning specialist is using a SageMaker algorithm to train a model.&nbsp; The dataset is large, and the training job is distributed across multiple training instances.&nbsp; What mechanism does SageMaker provide to minimize temporary storage required in the training instance volumes?</p>"}, "correct_response": ["b"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A machine learning specialist is using a SageMaker algorithm to train a model.&nbsp; The dataset is large, and the training job is distributed across multiple training instances.&nbsp; What mechanism does SageMaker provide to minimize temporary storage required in the training instance volumes?", "related_lectures": []}, {"_class": "assessment", "id": 53418562, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>CSV must have column headers and target variable must be the last column</p>", "<p>CSV must have column headers with the target variable in the first column</p>", "<p>CSV must not have a column header record.&nbsp; Target variable must be the last column</p>", "<p>CSV must not have a column header record.&nbsp; Target variable must be the first column</p>"], "relatedLectureIds": "", "explanation": "<p>With CSV format, SageMaker XGBoost expects the target variable in the first column and without a column header</p>", "question": "<p>You are using CSV formatted files to train on SageMaker\u2019s built-in XGBoost algorithm. </p><p>SageMaker expects your training and validation to follow this convention:</p>"}, "correct_response": ["d"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "You are using CSV formatted files to train on SageMaker\u2019s built-in XGBoost algorithm. SageMaker expects your training and validation to follow this convention:", "related_lectures": []}, {"_class": "assessment", "id": 53418564, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Translate, Polly</p>", "<p>Transcribe, Translate, Polly</p>", "<p>Translate</p>", "<p>Transcribe, Polly, Translate</p>"], "relatedLectureIds": "", "explanation": "<p>Transcribe, Translate, Polly \u2013 Translation step requires text data. So, the first step is to transcribe the text from audio and then translate the text. Finally, to convert to speech, use Polly</p>", "question": "<p>A company has several audio files that must be converted to other languages.&nbsp; </p><p>What is the best way to complete this task?</p>"}, "correct_response": ["b"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A company has several audio files that must be converted to other languages.&nbsp; What is the best way to complete this task?", "related_lectures": []}, {"_class": "assessment", "id": 53418566, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Translate to German with source language set to auto-detect</p>", "<p>Translate to English with source language set to auto-detect and then translate the output to German</p>", "<p>Transcribe to English, Translate to German</p>", "<p>Transcribe to German with Source language set to auto-detect</p>"], "relatedLectureIds": "", "explanation": "<p>Translate to German with Source Language set to auto-detect.&nbsp; Translate service can auto-detect source language and convert it to a target language.&nbsp; However, both source and target languages must be on the supported list.&nbsp; https://docs.aws.amazon.com/translate/latest/dg/how-it-works.html</p>", "question": "<p>A company has received an email from a customer with product feedback.&nbsp; Feedback is in an unknown language, and the company\u2019s product team has requested a German version of the email.&nbsp; </p><p>What steps are needed to accomplish this?</p>"}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A company has received an email from a customer with product feedback.&nbsp; Feedback is in an unknown language, and the company\u2019s product team has requested a German version of the email.&nbsp; What steps are needed to accomplish this?", "related_lectures": []}, {"_class": "assessment", "id": 53418568, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Rekognition</p>", "<p>Textract</p>", "<p>Custom Classification with Comprehend</p>", "<p>Sentiment Analysis with Comprehend</p>"], "relatedLectureIds": "", "explanation": "<p>Sentiment Analysis with Comprehend. Sentiment analysis can evaluate text based on the content and provide a confidence score for Positive, Negative, Neutral, Mixed.&nbsp; You could use this to assess reviews based on the sentiment and shortlist strong positive and strong negative reviews.</p>", "question": "<p>An online marketplace wants to help customers make an informed choice when purchasing products.&nbsp; They would like to present the most positive and most critical customer reviews side-by-side in the product summary page.&nbsp; </p><p>Which capability can you use for this purpose?</p>"}, "correct_response": ["d"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "An online marketplace wants to help customers make an informed choice when purchasing products.&nbsp; They would like to present the most positive and most critical customer reviews side-by-side in the product summary page.&nbsp; Which capability can you use for this purpose?", "related_lectures": []}, {"_class": "assessment", "id": 53418570, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Rekognition Text Extraction</p>", "<p>Textract</p>", "<p>Comprehend</p>", "<p>Transcribe</p>"], "relatedLectureIds": "", "explanation": "<p>Comprehend.&nbsp; With Comprehend, you can analyze a document to extract entities and key phrases along with confidence scores.&nbsp; You can use it to provide a summary of the talking points of a document.&nbsp; Seq2Seq algorithm supported by SageMaker is another algorithm that you could for this purpose.</p>", "question": "<p>A customer has 1000s of documents, and they would like to create a summary of each document.&nbsp; </p><p>Which of these services is best suited for this requirement?</p>"}, "correct_response": ["c"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A customer has 1000s of documents, and they would like to create a summary of each document.&nbsp; Which of these services is best suited for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 53418572, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Use correct Region and Language</p>", "<p>Use batch streaming for highest quality outputs</p>", "<p>Use real-time streaming for highest quality output</p>", "<p>Use Speech Synthesis Markup Language</p>"], "relatedLectureIds": "", "explanation": "<p>With Polly, you can use Speech Synthesis Markup Language (SSML) to \u201ccontrol aspects of speech, such as pronunciation, volume, pitch, speed rate, etc.\u201d&nbsp; Reference: https://aws.amazon.com/polly/</p>", "question": "<p>A customer is using Polly to generate audio for text.&nbsp; However, Polly is not pronouncing some of the words correctly. What option would help you control the speech output?</p>"}, "correct_response": ["d"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A customer is using Polly to generate audio for text.&nbsp; However, Polly is not pronouncing some of the words correctly. What option would help you control the speech output?", "related_lectures": []}, {"_class": "assessment", "id": 53418574, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An Auto Show organizer wants to detect celebrities who are among the audience.&nbsp; The event center has several cameras that are recording the event live.&nbsp; What combination of service and order of processing can help achieve this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Use Kinesis Video Streams to capture the video feed from cameras.&nbsp; Rekognition service can directly consume Kinesis Video Streams, and you can configure Rekognition service to detect celebrities. The output of streaming analysis is stored in a Kinesis Data Stream.</p><p>Reference: https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/</p><p>For offline video analysis (video stored in s3), you need to start a job and once the job completes, it will notify using SNS (notification service). You can then pick up the results. Or, you can periodically poll by calling GetCelebrityRecognition.</p><p>Working with stored videos: <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/video.html\">https://docs.aws.amazon.com/rekognition/latest/dg/video.html</a></p><p><br></p>", "answers": ["<p>Kinesis Video Streams, Amazon Rekognition, Kinesis Data Stream</p>", "<p>Kinesis Firehose, Lambda, and Amazon Rekognition</p>", "<p>Kinesis Data Streams, Amazon Rekognition, Kinesis Video Stream</p>", "<p>Kinesis Firehose, Kinesis Analytics, and Amazon Rekognition</p>"]}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "An Auto Show organizer wants to detect celebrities who are among the audience.&nbsp; The event center has several cameras that are recording the event live.&nbsp; What combination of service and order of processing can help achieve this task?", "related_lectures": []}, {"_class": "assessment", "id": 53418576, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Factorization Machines</p>", "<p>BlazingText</p>", "<p>Comprehend</p>", "<p>DeepAR</p>"], "relatedLectureIds": "", "explanation": "<p>Factorization Machines algorithm is used for building recommender systems and for collaborative filtering.&nbsp; </p><p>Collaborative filtering algorithms learn the likelihood of a customer purchasing a product based on other customer purchase behavior.&nbsp; </p><p>BlazingText is used for text analysis and classification problems.&nbsp; </p><p>Comprehend is used for natural language processing and not suitable for this use case.&nbsp; </p><p>DeepAR is used for time series forecasting.</p>", "question": "<p>A grocery store has a robust online presence. The store wants to improve product recommendations using machine learning and suggest products that are purchased together.&nbsp; </p><p>Which of these algorithms can be used for this requirement?</p>"}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A grocery store has a robust online presence. The store wants to improve product recommendations using machine learning and suggest products that are purchased together.&nbsp; Which of these algorithms can be used for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 53418578, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>ObjectDetection</p>", "<p>ImageClassification</p>", "<p>Rekognition</p>", "<p>Semantic Segmentation</p>"], "relatedLectureIds": "", "explanation": "<p>The semantic Segmentation algorithm is useful for this use case \u2013 it can detect objects in an image, shape of each object along with location and pixels that are part of the object.&nbsp; </p><p>ImageClassification algorithm is used for classifying a whole image. </p><p>ObjectDetection is used for detecting objects and classifying them; you can also get a bounding box for each object \u2013 however, this algorithm does not gives you information about the exact shape of the object. </p><p>Rekognition AI Service can help you analyze images and videos; however, object shape detection is not one of the capabilities</p>", "question": "<p>A team of students is building an application that can blur or remove unwanted objects in an image.&nbsp; Users can pick the objects on which action needs to be performed.&nbsp; </p><p>Which one of the AWS machine learning capabilities can you use for this?</p>"}, "correct_response": ["d"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A team of students is building an application that can blur or remove unwanted objects in an image.&nbsp; Users can pick the objects on which action needs to be performed.&nbsp; Which one of the AWS machine learning capabilities can you use for this?", "related_lectures": []}, {"_class": "assessment", "id": 53418580, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Seq2Seq</p>", "<p>LDA</p>", "<p>K-Means</p>", "<p>Random Cut Forest</p>"], "relatedLectureIds": "", "explanation": "<p>Seq2Seq algorithm is used for text summarization \u2013 It accepts a series of tokens as input and outputs another sequence of tokens. LDA is an unsupervised algorithm for topic modeling \u2013 it can generate probabilities of a document belonging to a number of specified topics.&nbsp; K-Means is a clustering algorithm that is used for identifying grouping within data.&nbsp; Random Cut Forest is used for detecting anomalous data points</p>", "question": "<p>A machine learning specialist needs to come up with an approach to automatically summarize the content of large text documents. Which algorithm can be used for this use case?</p>"}, "correct_response": ["a"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A machine learning specialist needs to come up with an approach to automatically summarize the content of large text documents. Which algorithm can be used for this use case?", "related_lectures": []}, {"_class": "assessment", "id": 53418582, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>LDA</p>", "<p>Neural Topic Modeling (NTM)</p>", "<p>Comprehend</p>", "<p>Seq2Seq</p>"], "relatedLectureIds": "", "explanation": "<p>LDA and NTM are used for topic modeling; however, they are unsupervised and generally used in exploratory setting for understanding data.&nbsp; </p><p>You have the flexibility to specify the number of topics \u2013 however, the algorithms automatically assign topics \u2013 it may not match with what we consider as topics: travel, food, transportation, and so forth.&nbsp; It will automatically generate appropriate topics. </p><p>For example, LDA/NTM may come with a topic that groups travel and food together.&nbsp; </p><p>For this problem, Comprehend service can be used to train a classifier that can map text content to a topic. Seq2Seq is used for translation, summarization and so forth</p>", "question": "<p>You have a collection of documents that has text about a variety of different topics: animals, plants, transportation, travel, food, and so forth.&nbsp; You want to train an algorithm to categorize the documents into one of the above categories.&nbsp; </p><p>Which of these algorithms can you use for this requirement?</p>"}, "correct_response": ["c"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "You have a collection of documents that has text about a variety of different topics: animals, plants, transportation, travel, food, and so forth.&nbsp; You want to train an algorithm to categorize the documents into one of the above categories.&nbsp; Which of these algorithms can you use for this requirement?", "related_lectures": []}, {"_class": "assessment", "id": 53418584, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>SageMaker Neo</p>", "<p>Rekognition</p>", "<p>SageMaker GroundTruth</p>", "<p>ImageClassification</p>"], "relatedLectureIds": "", "explanation": "<p>SageMaker GroundTruth service provides two capabilities to manage labeling process \u2013 Automatic Labeling can learn from examples that you provide and label all instances.&nbsp; </p><p>Manual labeling uses Mechanical Turk service to distribute the task across human labelers, and GroundTruth provides the capability to manage the entire workflow.&nbsp; </p><p>Neo is used for deploying your Machine Learning algorithm anywhere in the Cloud and at Edge Locations \u2013 it is a cross-compilation capability to compile your Machine Learning Algorithm to run on specified hardware.&nbsp; </p><p>Rekognition is not used for manual labeling even though you can use this service to train to classify images by providing labeled data.&nbsp; </p><p>ImageClassification also expects labeled data as input. This question is about how to create labeled data</p>", "question": "<p>A manufacturing company has a collection of images that contains examples of normal and defective products.&nbsp; These images need to be manually labeled by human experts for model training, and they need a solution to manage the workflow to distribute images among human experts for manual labeling.&nbsp; </p><p>What capability can you for this?</p>"}, "correct_response": ["c"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "A manufacturing company has a collection of images that contains examples of normal and defective products.&nbsp; These images need to be manually labeled by human experts for model training, and they need a solution to manage the workflow to distribute images among human experts for manual labeling.&nbsp; What capability can you for this?", "related_lectures": []}, {"_class": "assessment", "id": 53418586, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Launch EC2 instance with Deep Learning AMIs</p>", "<p>Use pre-built TensorFlow docker images provided by SageMaker to train and host models on SageMaker infrastructure</p>", "<p>Launch EC2 instance, download and install required Machine Learning Frameworks</p>", "<p>Built custom docker image that conforms to SageMaker specification to develop and host models using SageMaker infrastructure</p>"], "relatedLectureIds": "", "explanation": "<p>SageMaker provides pre-built TensorFlow docker images that you can use to train, and host models on SageMaker managed infrastructure efficiently. </p><p>Deep Learning AMIs are another option, and you can use it to launch desired EC2 instances pre-configured with necessary tools.&nbsp; However, this requires you to manage and patch EC2 instances.&nbsp; </p><p>Launching desired EC2 instances and installing machine learning frameworks is another option \u2013 however, you need to manage and patch EC2 instances, and besides, you need to validate and patch ML framework. </p><p>Custom Docker images are required when we need to deploy custom models or use a machine learning framework not supported by SageMaker</p>", "question": "<p>An organization is using TensorFlow Machine Learning Framework for building models and would like to migrate the machine learning infrastructure to AWS.&nbsp; </p><p>Which one of these options takes the least effort to train, host, and manage TensorFlow models in AWS?</p>"}, "correct_response": ["b"], "section": "AWS SageMaker, AI and Frameworks", "question_plain": "An organization is using TensorFlow Machine Learning Framework for building models and would like to migrate the machine learning infrastructure to AWS.&nbsp; Which one of these options takes the least effort to train, host, and manage TensorFlow models in AWS?", "related_lectures": []}, {"_class": "assessment", "id": 53418478, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Ensure Athena queries are scheduled to run periodically to update metadata</p>", "<p>Ensure Glue Crawlers are configured as a scheduled job to scan the files and update metadata</p>", "<p>Create a new table in the Glue Catalog to capture the changes</p>", "<p>Configure the Lambda function to monitor S3 and to capture the metadata changes</p>"], "relatedLectureIds": "", "explanation": "<p>Glue Crawler is used for automated collection and maintenance of metadata in the Glue Catalog.&nbsp; You would need to configure the Crawler to periodically scan the source data to detect any structural changes in the files and keep the metadata in sync with data.&nbsp; With Athena service, you can query files in S3 using SQL \u2013 however, the metadata about the files needs to be created in Glue Catalog first. Lambda function is used for do-it-yourself catalog management. This requires more effort when compared to using Glue Crawler</p>", "question": "<p>You are using AWS provided services for maintaining metadata about your data files stored in S3. The incoming files to S3 have additional attributes that are collected, and they are not showing up in the metadata.&nbsp; What is the recommended approach to address this issue?</p>"}, "correct_response": ["b"], "section": "AWS", "question_plain": "You are using AWS provided services for maintaining metadata about your data files stored in S3. The incoming files to S3 have additional attributes that are collected, and they are not showing up in the metadata.&nbsp; What is the recommended approach to address this issue?", "related_lectures": []}, {"_class": "assessment", "id": 53418480, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Use Glue ETL to run Spark ETL scripts and configure it as a scheduled job</p>", "<p>Configure S3 to invoke Lambda function when a new file is added, perform the transformation in Lambda, and store the results back in S3</p>", "<p>Use Kinesis Datastreams for collecting the data from S3 and use built-in transformation to store the results in Parquet format</p>", "<p>Use Kinesis Firehose for reading the data from S3 and use built-in transformation to store the results in Parquet format</p>"], "relatedLectureIds": "", "explanation": "<p>Glue ETL provides an easy option to automatically generate ETL scripts and run the script as a scheduled job.&nbsp; Glue ETL provisions required Spark infrastructure to run the job and automatically terminates the environment after the job is completed.</p><p>A solution involving Kinesis Firehose requires an additional component to read data from S3 and add it firehose stream. For large files, you would also need to chunk into many messages when adding to the firehose.</p>", "question": "<p>You need to read the CSV files in S3, transform the content to Parquet format, and store the processed data back in S3.&nbsp; Which of these options is recommended for this solution?</p>"}, "correct_response": ["a"], "section": "AWS", "question_plain": "You need to read the CSV files in S3, transform the content to Parquet format, and store the processed data back in S3.&nbsp; Which of these options is recommended for this solution?", "related_lectures": []}, {"_class": "assessment", "id": 53418482, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>For each individual, keep four audio files in the training set and one in the test set</p>", "<p>Randomly split data between training and test set</p>", "<p>For each individual, keep three audio files in the training set, one in validation set and one in the test set</p>", "<p>Ensure some individuals are only in the test set \u2013 use the remaining data for training and validation</p>"], "relatedLectureIds": "", "explanation": "<p>The objective is to ensure the model generalizes well for unheard voices.&nbsp; &nbsp;So, the test set should not contain any individuals from the training or validation set.&nbsp; If we have the same individuals in the training and test set \u2013 the model may memorize voice for that individual and may artificially show improved performance. Reference: NIPS 2016 tutorial: Nuts and bolts of building AI applications by Dr. Andrew Ng.</p>", "question": "<p>You are developing a deep learning network for converting speech to text.&nbsp; The dataset has recordings of 1,000 individuals, with everyone providing five different audio files along with the transcribed text.&nbsp; &nbsp;(for a total 5,000 audio samples).&nbsp; The trained model must generalize well for new individuals.&nbsp; How would you use this data for developing a model?</p>"}, "correct_response": ["d"], "section": "Machine Learning Concepts", "question_plain": "You are developing a deep learning network for converting speech to text.&nbsp; The dataset has recordings of 1,000 individuals, with everyone providing five different audio files along with the transcribed text.&nbsp; &nbsp;(for a total 5,000 audio samples).&nbsp; The trained model must generalize well for new individuals.&nbsp; How would you use this data for developing a model?", "related_lectures": []}, {"_class": "assessment", "id": 53418484, "assessment_type": "multi-select", "prompt": {"feedbacks": ["", "", "", "", ""], "answers": ["<p>Use Accuracy as a measure for the unbalanced dataset</p>", "<p>Use ROC AUC as a metric for the unbalanced dataset</p>", "<p>Oversample by duplicating positive data</p>", "<p>Oversample positive data using techniques like SMOTE</p>", "<p>Collect more positive samples</p>"], "relatedLectureIds": "", "explanation": "<p>For the unbalanced dataset, accuracy is not a good measure as a model that predicts all instances as normal will be 99% accurate.&nbsp; ROC AUC metric considers True Positive Rate and False positive Rates at all possible cutoff thresholds.&nbsp; It is a useful metric for binary classifiers.&nbsp; However, they are not suitable for highly imbalanced datasets as ROC&nbsp;curve considers only true positive and false positive rates.&nbsp; ROC does not account for negatives and does not measure the performance well.&nbsp; Instead precision-recall curve is used for imbalanced datasets.&nbsp; Oversampling by duplicating data is not going to improve quality as it does not add any new patterns that algorithms can learn.&nbsp; Synthetic Minority Over-sampling Technique&nbsp; (SMOTE) provides a mechanism for artificial data generation that has shown to improve the accuracy of unbalanced classifiers. To generate data similar to an existing instance, SMOTE uses the nearest neighbors, and generate synthetic data along the lines that connect the neighbors.&nbsp; This method ensures data is close to other similar instances. &nbsp; Reference (ROC&nbsp;Imbalance): <a href=\"https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\">https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/</a>&nbsp; <a href=\"https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8\">https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8</a></p>", "question": "<p>You are working on a model to differentiate positive and negative classes \u2013 the dataset that was provided to you is highly unbalanced. 99% of the data is normal, with only 1% positive.&nbsp; What steps can you go through to handle this unbalanced dataset? (select two)</p>"}, "correct_response": ["d", "e"], "section": "Machine Learning Concepts", "question_plain": "You are working on a model to differentiate positive and negative classes \u2013 the dataset that was provided to you is highly unbalanced. 99% of the data is normal, with only 1% positive.&nbsp; What steps can you go through to handle this unbalanced dataset? (select two)", "related_lectures": []}, {"_class": "assessment", "id": 53418486, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Increase the learning rate</p>", "<p>Decrease the learning rate</p>", "<p>Keep the learning rate same as batch size</p>", "<p>Learning rate and batch size are independent of each other</p>"], "relatedLectureIds": "", "explanation": "<p>&nbsp;As described in the below article, batch size and learning rate should be adjusted by the same factor.&nbsp; In a deep learning network, the loss curve is very complex and has several local minima.&nbsp; Imagine you need to find the deepest point in a large land area, and you don\u2019t know where the deepest point is or how deep it is.&nbsp; You would come across smaller valleys (local minima), and we don\u2019t want to conclude a small valley as the deepest point incorrectly. Our goal is for the optimization algorithm to explore different areas to find the deepest valley (global minima).&nbsp; &nbsp;Large batch sizes appear to cause the model to get stuck in local minima whereas smaller batch sizes make the algorithm jump out of local minima and go towards global minima. To minimize the effect of large batches, when increasing batch size, you also need to increase the learning rate by the same factor.&nbsp; Similarly, if you decrease the batch size, you need to reduce the learning rate by the same factor. Reference: https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/</p>", "question": "<p>When training a deep learning model, if you increase the batch size, you should also</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "When training a deep learning model, if you increase the batch size, you should also", "related_lectures": []}, {"_class": "assessment", "id": 53418488, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Train</p>", "<p>Test</p>", "<p>Validation</p>", "<p>Use a random sample from train, validation and test sets</p>"], "relatedLectureIds": "", "explanation": "<p>Tune model using validation data. To prevent the model from overfitting the validation data, you need to plan to do a final check with an unseen test data</p>", "question": "<p>You are exploring different parameters for tuning the model.&nbsp; What dataset should you use to guide with this tuning exercise?</p>"}, "correct_response": ["c"], "section": "Machine Learning Concepts", "question_plain": "You are exploring different parameters for tuning the model.&nbsp; What dataset should you use to guide with this tuning exercise?", "related_lectures": []}, {"_class": "assessment", "id": 53418490, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>The network would not learn from missing values</p>", "<p>The network would automatically learn insights about missing values </p>", "<p>Behavior depends on the number of layers</p>", "<p>The response depends on activation function</p>"], "relatedLectureIds": "", "explanation": "<p>The system would not learn insights from missing values.&nbsp; You would need to create new examples in training data with missing values so that the model can learn to ignore missing values</p>", "question": "<p>Training data has values for all features.&nbsp; With Test data, some of the features have missing values.&nbsp; If you build a neural network with training data and use test data to verify performance, how would the neural network behave?</p>"}, "correct_response": ["a"], "section": "Machine Learning Concepts", "question_plain": "Training data has values for all features.&nbsp; With Test data, some of the features have missing values.&nbsp; If you build a neural network with training data and use test data to verify performance, how would the neural network behave?", "related_lectures": []}, {"_class": "assessment", "id": 53418492, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Ensure data is shuffled before creating train and test set</p>", "<p>Ensure there are no duplicates</p>", "<p>Ensure all duplicates are in train data</p>", "<p>Ensure all duplicates are in test data</p>"], "relatedLectureIds": "", "explanation": "<p>Duplicates can accidentally leak into validation and test sets when you split your data.&nbsp; This can cause artificially better performance on validation and test sets.&nbsp; You should clean up the data so that all examples are distinct.</p>", "question": "<p>A labeled dataset contains a lot of duplicate examples.&nbsp; How should you handle duplicate data?</p>"}, "correct_response": ["b"], "section": "Machine Learning Concepts", "question_plain": "A labeled dataset contains a lot of duplicate examples.&nbsp; How should you handle duplicate data?", "related_lectures": []}, {"_class": "assessment", "id": 53418494, "assessment_type": "multiple-choice", "prompt": {"feedbacks": ["", "", "", ""], "answers": ["<p>Shuffle data and perform a random split to keep 80% for training and 20% for testing</p>", "<p>Split data into 80% for training and 20% for testing</p>", "<p>Split data in such a way that first 80% of the days in a month are part of the training set and the remaining 20% of each month is set aside in the test set</p>", "<p>Order data by time and set aside first 80% for training and the remaining 20% for testing</p>"], "relatedLectureIds": "", "explanation": "<p>For time-series forecasting, our objective is to predict the values in the future.&nbsp; To get a realistic assessment of model performance, you need to split the dataset based on time. Set aside the first 70-80% for training and keep the most recent data (toward the end) for testing the accuracy of predictions.&nbsp; Random shuffling is not recommended for time series forecasting</p>", "question": "<p>A Machine Learning Expert is working on a time series forecasting problem to predict future demand for products.&nbsp; The dataset consists of two years\u2019 worth of historical data. What is the recommended way to split the training and test set?</p>"}, "correct_response": ["d"], "section": "Machine Learning Concepts", "question_plain": "A Machine Learning Expert is working on a time series forecasting problem to predict future demand for products.&nbsp; The dataset consists of two years\u2019 worth of historical data. What is the recommended way to split the training and test set?", "related_lectures": []}]}
5824438
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70596488, "assessment_type": "multi-select", "prompt": {"question": "<p>A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of 1,000 positive samples and 20,000 negative samples where positive samples indicate that the patient had cancer. A random forest model was chosen, and the model achieved an accuracy of 98% on the training set, however it performed poorly on the test set scoring 67% which was far below the company\u2019s business metric.</p><p>What should the Data Science team do to approach the business metric? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The data is completely unbalanced as the number of the negative class examples is far greater than the positive class. SMOTE \u201c<strong>Synthetic Minority Oversampling Technique\u201d </strong>is a technique used to increase the number of the minority class by creating synthetic data points. This will help with the problem of unbalanced dataset. Also, changing the cost function to have a higher impact on the minority class (positive class here) will penalize the cost of a minority class.</p><p><strong>CORRECT: </strong>\"Change the cost function so that false negatives have a higher impact on the cost value than false positives <strong>AND</strong> Use SMOTE technique to oversample the positive class\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Change the cost function so that false positives have a higher impact on the cost value than false negatives.\" is incorrect.</p><p>This answer worsens the problem of the unbalanced data as the false positives already have a higher impact due to the large number of data points in this class.</p><p><strong>INCORRECT:</strong> \"Increase the number of epochs\" is incorrect.</p><p>The model achieved an accuracy of 98% on the training set, so increasing the number of epochs will have a slight impact on the training accuracy but won\u2019t solve the problem of unbalanced data.</p><p><strong>INCORRECT:</strong> \" Use SMOTE technique to under sample the negative class to balance the training data.\" is incorrect.</p><p>SMOTE technique is used to oversample the minority class. Remember the name \u201c<strong>Synthetic Minority Oversampling Technique\u201d</strong></p>", "answers": ["<p>Change the cost function so that false negatives have a higher impact on the cost value than false positives.</p>", "<p>Change the cost function so that false positives have a higher impact on the cost value than false negatives.</p>", "<p>Increase the number of epochs.</p>", "<p>Use SMOTE technique to oversample the positive class.</p>", "<p>Use SMOTE technique to under sample the negative class to balance the training data.</p>"]}, "correct_response": ["a", "d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of 1,000 positive samples and 20,000 negative samples where positive samples indicate that the patient had cancer. A random forest model was chosen, and the model achieved an accuracy of 98% on the training set, however it performed poorly on the test set scoring 67% which was far below the company\u2019s business metric.What should the Data Science team do to approach the business metric? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70596490, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer needs to run SQL queries on data collected from various sensors. The data is stored in a JSON file format in an S3 bucket. SQL commands are not determined beforehand, however, they are performed upon request to gain data insights.</p><p>What is the cheapest and most efficient way to perform this operation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>You can use the AWS Glue Data Catalog to quickly discover and search across multiple AWS data sets without moving the data. Once the data is catalogued, it is immediately available for search and query using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum.</p><p>Remember:</p><ul><li><p>Athena can query data that sits in an S3 bucket <strong>provided that</strong> they have been crawled and the <strong>schema is provided.</strong></p></li><li><p>Amazon <strong>Glue data catalog</strong> extracts the <strong>schema</strong> of the data.</p></li><li><p><strong>Redshift</strong> stores <strong>structured</strong> data while <strong>S3</strong> stores <strong>structured</strong> and <strong>unstructured</strong> data.</p></li></ul><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_07-55-21-e7c743987391417cf0415b900ccdb9cd.jpg\"><p><strong>CORRECT: </strong>\"Use Amazon Glue to extract the data\u2019s schema and Amazon Athena to run SQL queries on demand \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Redshift for storing the data and running SQL queries upon request.\" is incorrect.</p><p>Redshift cannot store unstructured data.</p><p><strong>INCORRECT:</strong> \"Use Amazon Athena for both extracting the data\u2019s schema and running SQL queries on demand.\" is incorrect.</p><p>Amazon Athena cannot extract the data\u2019s schema; however, it can run SQL queries from the Glue data catalog (schema of the data).</p><p><strong>INCORRECT:</strong> \" Use Amazon Glue for both extracting the data\u2019s schema and running SQL queries on demand.\" is incorrect.</p><p>While Amazon Glue is used to extract the data\u2019s schema, it does not run SQL queries <strong>except</strong> for ETL jobs.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>", "answers": ["<p>Use Redshift for storing the data and running SQL queries upon request.</p>", "<p>Use Amazon Athena for both extracting the data\u2019s schema and running SQL queries on demand.</p>", "<p>Use Amazon Glue for both extracting the data\u2019s schema and running SQL queries on demand.</p>", "<p>Use Amazon Glue to extract the data\u2019s schema and Amazon Athena to run SQL queries on demand.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A machine learning engineer needs to run SQL queries on data collected from various sensors. The data is stored in a JSON file format in an S3 bucket. SQL commands are not determined beforehand, however, they are performed upon request to gain data insights.What is the cheapest and most efficient way to perform this operation?", "related_lectures": []}, {"_class": "assessment", "id": 70596492, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. 10,000 income\u2019s data are found missing which resemble 10% of the total income data.</p><p>Income data are in USD and range from $2,000 to $5,000,000. The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias.</p><p>Which action should be taken to overcome this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Missing data are one of the main problems which are inevitable in most of the datasets. There are many ways to overcome this problem such as:</p><ol><li><p>Deleting the missing rows entirely. While this is the easiest solution, it simply deletes many valuable information.</p></li><li><p>Imputing missing data using mean. This is a bad technique to be used if the dataset contains outliers.</p></li><li><p>Imputing missing data using mode. This is suitable for categorical features.</p></li><li><p>Imputing missing data using KNN. This is suitable for numerical features.</p></li><li><p>Imputing missing data using deep learning. This is suitable and accurate for categorical features.</p></li></ol><p><strong>CORRECT: </strong>\"Impute missing data using KNN.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Impute missing data using mean values of the column.\" is incorrect.</p><p>The income\u2019s range is 2,000 to 5,000,000. So, applying the mean values of the column will introduce bias. For instance, if you take the mean value of the given set (5000, 1000000, 750000) the result would be 585000. Now imagine if we were inserting this value to all the missing income\u2019s data, we will find that a 20-year-old person have an income of 585000 which we just substituted. This is illogical.</p><p><strong>NCORRECT:</strong> \"Impute missing data using deep learning.\" is incorrect.</p><p>This technique is best suitable for categorical features not numerical ones.</p><p><strong>INCORRECT:</strong> \" Delete the rows corresponding to the 10,000 missing income data.\" is incorrect.</p><p>While this is the simplest solution, 10% of the data is a large quantity. There are other techniques to deal with them instead of deleting them entirely.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/data-preprocessing.html\">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/data-preprocessing.html</a></p>", "answers": ["<p>Delete the rows corresponding to the 10,000 missing income data.</p>", "<p>Impute missing data using mean values of the column.</p>", "<p>Impute missing data using KNN.</p>", "<p>Impute missing data using deep learning.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. 10,000 income\u2019s data are found missing which resemble 10% of the total income data.Income data are in USD and range from $2,000 to $5,000,000. The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias.Which action should be taken to overcome this problem?", "related_lectures": []}, {"_class": "assessment", "id": 70596494, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist built two machine learning models that predicts if a person carries a certain virus. A positive label indicates that the person has the virus, while a negative label indicates that they are virus-free. A company made a contract with the specialist which states that it will adopt the model under some rules:</p><p>Penalties:</p><p>\u00b7 If the model predicted that a patient is infected while virus-free, then the specialist should pay $200 for the patient\u2019s virus test.</p><p>\u00b7 If the model predicted that a patient has no infection while having the virus, then the specialist should pay $3,000 for his treatment.</p><p>Compensation:</p><p>\u00b7 Otherwise, the specialist receives a compensation for each true negative or true positive.</p><p>The two figures below resemble the specialist\u2019s top models\u2019 performance based on a test set the company provided.</p><p>Which model should the specialist submit that decreases the penalty received and why?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_08-00-05-eeac98ab43a40abece761913708da137.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The specialist is searching for the model which will penalize less; therefore, he/she is searching for the model which costs the least. Model A has total cost of (60*3,000 + 100*200) = $200,000. Model B has total cost of (100*3,000+90*200) = $318,000.</p><p><strong>CORRECT: </strong>\"Model A as the total cost accompanied by the model is lower than the total cost for model B.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Model A as it has the highest recall.\" is incorrect.</p><p>While model A has the highest recall, however, the specialist is searching for the best model to lower the overall penalty.</p><p><strong>INCORRECT:</strong> \"Model B as it has the highest recall.\" is incorrect.</p><p>The specialist is searching for the best model to lower the overall penalty.</p><p><strong>INCORRECT:</strong> \"Model B as the total cost accompanied by the model is lower than the total cost for model B.\" is incorrect.</p><p>Model B has a higher overall cost than model A.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>Model A as it has the highest recall.</p>", "<p>Model B as it has the highest recall.</p>", "<p>Model A as the total cost accompanied by the model is lower than the total cost for model B.</p>", "<p>Model B as the total cost accompanied by the model is lower than the total cost for model B.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist built two machine learning models that predicts if a person carries a certain virus. A positive label indicates that the person has the virus, while a negative label indicates that they are virus-free. A company made a contract with the specialist which states that it will adopt the model under some rules:Penalties:\u00b7 If the model predicted that a patient is infected while virus-free, then the specialist should pay $200 for the patient\u2019s virus test.\u00b7 If the model predicted that a patient has no infection while having the virus, then the specialist should pay $3,000 for his treatment.Compensation:\u00b7 Otherwise, the specialist receives a compensation for each true negative or true positive.The two figures below resemble the specialist\u2019s top models\u2019 performance based on a test set the company provided.Which model should the specialist submit that decreases the penalty received and why?", "related_lectures": []}, {"_class": "assessment", "id": 70596496, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has just released a new machine learning model (B) and wants to deploy it using the canary deployment method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.</p><p>How should the company update the Sagemaker\u2019s endpoint?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The canary deployment releases the new version to a small set of users, then gradually increases its traffic. So, traffic at the beginning is 0:1, then begins to update periodically. This mitigates the risk of changes to the production. Other types of deployment are A/B, Blue/Green, Rolling.</p><p><strong>CORRECT: </strong>\" Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create an endpoint configuration with a ratio of 50:50 for production variants and do not update the weights.\" is incorrect.</p><p>Canary deployment involves increasing weights gradually.</p><p><strong>INCORRECT:</strong> \" Create two separate but identical environments and only serve one model at a time.\" is incorrect.</p><p>This is not a canary deployment technique as weights should be increased gradually.</p><p><strong>INCORRECT:</strong> \" Create a new endpoint which will hold model (B) only.\" is incorrect.</p><p>This is not a canary deployment technique.</p>", "answers": ["<p>Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.</p>", "<p>Create an endpoint configuration with a ratio of 50:50 for production variants and do not update the weights.</p>", "<p>Create two separate but identical environments and only serve one model at a time.</p>", "<p>Create a new endpoint which will hold model (B) only.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has just released a new machine learning model (B) and wants to deploy it using the canary deployment method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.How should the company update the Sagemaker\u2019s endpoint?", "related_lectures": []}, {"_class": "assessment", "id": 70596498, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data scientist trained a binary classification model to predict if it will rain in the near future based on some features. Using the confusion matrix below, determine the model\u2019s sensitivity and accuracy.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_08-04-52-5e3b2d8b0846c4aa20b4b68d78307fc2.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_08-04-52-d87820c13b9629774d22a0b3becadf94.jpg\"><p><strong>CORRECT: </strong>\"Recall: 0.75, Accuracy: 0.75\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Recall: 2/3, Accuracy: 0.75\" is incorrect.</p><p>Precision = 2/3, Recall = 0.75, Accuracy = 0.75</p><p><strong>INCORRECT:</strong> \" Recall: 0.75, Accuracy: 2/3\" is incorrect.</p><p>Precision = 2/3, Recall = 0.75, Accuracy = 0.75</p><p><strong>INCORRECT:</strong> \" Recall: 0.3, Accuracy = 0.75\" is incorrect.</p><p>Precision = 2/3, Recall = 0.75, Accuracy = 0.75</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>Recall: 2/3, Accuracy: 0.75</p>", "<p>Recall: 0.75, Accuracy: 0.75</p>", "<p>Recall: 0.75, Accuracy: 2/3</p>", "<p>Recall: 0.3, Accuracy = 0.75</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A data scientist trained a binary classification model to predict if it will rain in the near future based on some features. Using the confusion matrix below, determine the model\u2019s sensitivity and accuracy.", "related_lectures": []}, {"_class": "assessment", "id": 70596500, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A perfume dataset contains features such as \u201cSize\u201d, \u201cType\u201d, \u201cOrigin\u201d. The \u201cSize\u201d column contains the values: \u201cSmall\u201d, \u201cMedium\u201d and \u201cLarge\u201d. A machine learning specialist wants to build a model to predict the prices of perfumes given those features, however, he wants to first apply a feature engineering technique on the \u201cSize\u201d column without losing the inherent relationship between the different values of \u201cSize\u201d column.</p><p>Which technique should you recommend him?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is an example of an ordinal data. The data has a specific hierarchy level in which \u201cSmall\u201d is the smallest and \u201cLarge\u201d is the highest. The most suitable feature engineering method to be applied is the mapping technique where each label is mapped to a number representing its order between others.</p><p><strong>CORRECT: </strong>\" Apply a mapping technique where the 3 labels are mapped to 3 numbers representing their order.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Normalization.\" is incorrect.</p><p>Normalization is applied on numerical features not categorical ones.</p><p><strong>INCORRECT:</strong> \"One-hot encoding.\" is incorrect.</p><p>If we simply used one-hot encoding, we would lose the ordering of the labels.</p><p><strong>INCORRECT:</strong> \"Leave the labels as is.\" is incorrect.</p><p>Machine learning models are easier to train using numerical features rather than categorical ones.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html</a></p>", "answers": ["<p>Normalization.</p>", "<p>One-hot encoding.</p>", "<p>Apply a mapping technique where the 3 labels are mapped to 3 numbers representing their order.</p>", "<p>Leave the labels as is.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A perfume dataset contains features such as \u201cSize\u201d, \u201cType\u201d, \u201cOrigin\u201d. The \u201cSize\u201d column contains the values: \u201cSmall\u201d, \u201cMedium\u201d and \u201cLarge\u201d. A machine learning specialist wants to build a model to predict the prices of perfumes given those features, however, he wants to first apply a feature engineering technique on the \u201cSize\u201d column without losing the inherent relationship between the different values of \u201cSize\u201d column.Which technique should you recommend him?", "related_lectures": []}, {"_class": "assessment", "id": 70596502, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A burger franchise was obliged to reduce their workforce due to rising labor costs. They wanted to increase efficiency by introducing a smart system which takes orders from their customers automatically without human involvement. The system should recognize what is being ordered by customers, understand their intentions, ask them questions related to this particular order and finally submit the order.</p><p>Which services should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Transcribe is an AI service which uses Automatic Speech Recognition \u201cASR\u201d to convert speech into text.</p><p>Amazon Lex is an AWS service for building conversational interfaces for applications using voice and text. It can realize the customer\u2019s intent and behave subsequently.</p><p>Amazon Polly is an AI service which transforms text into speech.</p><p><strong>CORRECT: </strong>\" Amazon Transcribe to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Polly to ask the customer questions related to his order. \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon Rekognition to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Polly to ask the customer questions related to his order.\" is incorrect.</p><p>Amazon Rekognition is a computer vision AI service which automates image and video analysis.</p><p><strong>INCORRECT:</strong> \"Amazon Polly to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Transcribe to ask the customer questions related to his order. \" is incorrect.</p><p>Amazon Polly transforms text to speech not vice versa. Amazon Transcribe transforms speech to text.</p><p><strong>INCORRECT:</strong> \" Amazon Lex to recognize customer\u2019s speech, Amazon Polly to spot the customer\u2019s intent and Amazon Transcribe to ask the customer questions related to his order. \" is incorrect.</p><p>Amazon Lex spots the customer\u2019s intent; Amazon Polly transforms text to speech and Amazon transcribe transforms speech to text.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/transcribe/\">https://aws.amazon.com/transcribe/</a></p><p><a href=\"https://aws.amazon.com/lex/\">https://aws.amazon.com/lex/</a></p><p><a href=\"https://aws.amazon.com/polly/\">https://aws.amazon.com/polly/</a></p>", "answers": ["<p>Amazon Rekognition to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Polly to ask the customer questions related to his order.</p>", "<p>Amazon Polly to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Transcribe to ask the customer questions related to his order.</p>", "<p>Amazon Lex to recognize customer\u2019s speech, Amazon Polly to spot the customer\u2019s intent and Amazon Transcribe to ask the customer questions related to his order.</p>", "<p>Amazon Transcribe to recognize customer\u2019s speech, Amazon Lex to spot the customer\u2019s intent and Amazon Polly to ask the customer questions related to his order.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A burger franchise was obliged to reduce their workforce due to rising labor costs. They wanted to increase efficiency by introducing a smart system which takes orders from their customers automatically without human involvement. The system should recognize what is being ordered by customers, understand their intentions, ask them questions related to this particular order and finally submit the order.Which services should be used?", "related_lectures": []}, {"_class": "assessment", "id": 70596504, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer is building a neural network to identify different class labels within an image. The source data comes from images taken from a car while it is driving. The camera takes separate frames, and these frames are saved to train them in a model later. The neural network consists of 100 hidden layers. The output consists of 10 labels and some of these labels are: \u201cTraffic sign\u201d, \u201cPedestrian\u201d and \u201cCross walk\u201d. The input image could contain more than one label and all labels present should be predicted.</p><p>Concerning the output layer, which activation function should the engineer use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Softmax activation is used in the output layer when we want to predict only one class among other classes, so it makes a probability distribution among the outputs so that we can choose the highest probable output. On the other hand, sigmoid is used when we want to predict all classes present. The sigmoid outputs a probability for each class independent of one another. RelU activation is not used in the output layer as it outputs a value between \u201c0\u201d and \u201cinf\u201d. Also, Tanh is not used as it outputs a value between \u201c-1\u201d and \u201c1\u201d.</p><p><strong>CORRECT: </strong>\"Sigmoid activation.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Tanh activation \" is incorrect.</p><p>Tanh does not output probability for the classes as it outputs a value between \u201c-1\u201d and \u201c1\u201d.</p><p><strong>INCORRECT:</strong> \"RelU activation.\" is incorrect.</p><p>RelU does not output probability for the classes as it outputs a value between \u201c0\u201d and \u201cinf\u201d.</p><p><strong>INCORRECT:</strong> \"Softmax activation.\" is incorrect.</p><p>Softmax is used when we want to predict only one class among other classes, however, in this scenario we want to predict multiple classes if present in the same image.</p>", "answers": ["<p>RelU activation.</p>", "<p>Tanh activation.</p>", "<p>Sigmoid activation.</p>", "<p>Softmax activation.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning engineer is building a neural network to identify different class labels within an image. The source data comes from images taken from a car while it is driving. The camera takes separate frames, and these frames are saved to train them in a model later. The neural network consists of 100 hidden layers. The output consists of 10 labels and some of these labels are: \u201cTraffic sign\u201d, \u201cPedestrian\u201d and \u201cCross walk\u201d. The input image could contain more than one label and all labels present should be predicted.Concerning the output layer, which activation function should the engineer use?", "related_lectures": []}, {"_class": "assessment", "id": 70596506, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck.</p><p>The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method. She wants to use her own TensorFlow model with custom dependencies for training and hosting. However, she wants to try the model locally first to ensure that everything is working fine before initiating powerful instances for training.</p><p>What should the specialist do to train locally?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A SageMaker estimator is called and one of the parameters passed is \u201cinstance_type\u201d where we should define if we want to train the model locally or using an ec2 training instance.</p><p><strong>CORRECT: </strong>\"Initiate an Estimator from sagemaker.estimator and provide the Estimator object with instance_type=\u201dlocal\u201d for training locally.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Initiate an Estimator from sagemaker.estimator and provide the Estimator object with hyperparameters=\u201dlocal\u201d for training locally.\" is incorrect.</p><p>\u201clocal\u201d should be provided with instance_type not hyperparameters.</p><p><strong>INCORRECT:</strong> \" Initiate an Estimator from sagemaker.estimator and provide instance_type=\u201dlocal\u201d when calling estimator.fit().\" is incorrect.</p><p>Instance_type=\u201dlocal\u201d should be provided to sagemaker.estimator not estimator.fit().</p><p><strong>INCORRECT:</strong> \" Initiate an Estimator from sagemaker.estimator and provide instance_type=\u201dlocal\u201d when calling estimator.deploy().\" is incorrect.</p><p>While instance_type is a valid argument for estimator.deploy(), however, the specialist is looking for <strong>training</strong> <strong>locally</strong> not deploying locally.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html</a></p>", "answers": ["<p>Initiate an Estimator from sagemaker.estimator and provide the Estimator object with instance_type=\u201dlocal\u201d for training locally.</p>", "<p>Initiate an Estimator from sagemaker.estimator and provide the Estimator object with hyperparameters=\u201dlocal\u201d for training locally.</p>", "<p>Initiate an Estimator from sagemaker.estimator and provide instance_type=\u201dlocal\u201d when calling estimator.fit().</p>", "<p>Initiate an Estimator from sagemaker.estimator and provide instance_type=\u201dlocal\u201d when calling estimator.deploy().</p>"]}, "correct_response": ["a"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck.The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method. She wants to use her own TensorFlow model with custom dependencies for training and hosting. However, she wants to try the model locally first to ensure that everything is working fine before initiating powerful instances for training.What should the specialist do to train locally?", "related_lectures": []}, {"_class": "assessment", "id": 70596508, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data scientist is in the process of building a classification model using logistic regression. The aim of this model is to classify an email as being spam or not based on some features. The scientist began training the model using the \u201clog loss\u201d cost function and noticed that the model tends to get stuck in a local minimum.</p><p>What should the scientist do to overcome this problem while training?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Small batch size tends not to get stuck in local minimum. This is a serious problem because the model could get stuck in a local minimum repeatedly. Local minimum is defined as a point which has the lowest value for the cost function locally which, in turn, tricks the algorithm into thinking that this is the best point to lower the cost function, however, while training the model we are searching for the lowest global minimum (lowest cost) to converge on.</p><p>AWS states: \u201cOne of the leading theories argues that the non-convex surface of the loss function contains many local minima and saddle points. With a smaller mini-batch, the gradient of the loss per mini-batch is noisier and can result in the optimization process <em>bouncing out</em> of a local minimum or saddle point. A large mini-batch, however, results in a gradient with less stochasticity and optimization may <em>get stuck</em> in a local minimum or on a saddle point.\u201d</p><p>The Batch size should be chosen wisely as large batch sizes could suffer from the problem of being stuck in local minima, so to overcome this problem we should decrease the batch size number which makes the update noisier and has less chance to get stuck.</p><p><strong>CORRECT: </strong>\"Decrease batch size.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Change the cost function.\" is incorrect.</p><p>The \u201clog loss\u201d is usually the cost function used to train logistic regression models, so this is not the problem here.</p><p><strong>INCORRECT:</strong> \" Increase batch size.\" is incorrect.</p><p>Increasing batch size will worsen the problem as it needs to be decreased in order to escape the local minima.</p><p><strong>INCORRECT:</strong> \"Increase the number of epochs.\" is incorrect.</p><p>If the model is stuck on a local minimum, increasing the epochs won\u2019t help. The model is trained on epochs to try converging on the lowest loss. The model is tricked by the local minima that it has fully converged to the global minima, that\u2019s why increasing the epochs won\u2019t help.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/\">https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/</a></p>", "answers": ["<p>Change the cost function.</p>", "<p>Increase batch size.</p>", "<p>Decrease batch size.</p>", "<p>Increase the number of epochs.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A data scientist is in the process of building a classification model using logistic regression. The aim of this model is to classify an email as being spam or not based on some features. The scientist began training the model using the \u201clog loss\u201d cost function and noticed that the model tends to get stuck in a local minimum.What should the scientist do to overcome this problem while training?", "related_lectures": []}, {"_class": "assessment", "id": 70596510, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to start building a machine learning model for predictive maintenance purposes. This simply means that they can detect if their machine will fail up to 2 hours before failing so they can repair it immediately without further damages for the machine or the production line. They decided to collect their own dataset from various sensors. The data collected resembled raw measurements such as \u201cTemperature\u201d, \u201cPressure\u201d, \u201cFrequency\u201d and many more.</p><p>The domain expert says that the factors affecting the failure of the machine are not based on raw measurements, however a mathematical combination from these measurements forming 3 features which decide whether the machine will fail or not.</p><p>The company needs to ingest the data from various sensors, send it to a suitable data lake, initiate an ETL job on a daily basis to extract those features from the data residing in this data lake and store the result in the same lake along with running SQL commands on the result using a managed service if necessary.</p><p>Which combination of services should the company use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>S3 is the data lake storage in AWS. AWS Glue is used to extract schema of the data, save the results in a database, and run ETL jobs and much more. Amazon Athena is a managed service which could be used to run SQL commands on data residing on S3.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_08-19-58-93b3f33062f468a141fcaf955c7f08a8.jpg\"><p><strong>CORRECT: </strong>\"Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Athena to run SQL commands on the output.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use EBS as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to EBS for storage purposes and use Amazon Athena to run SQL commands on the output.\" is incorrect.</p><p><strong>INCORRECT:</strong> \" Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Neptune to run SQL commands on the output.\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Redshift to run SQL commands on the output.\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/ug/tutorial-add-crawler.html\">https://docs.aws.amazon.com/glue/latest/ug/tutorial-add-crawler.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/\">https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/</a></p>", "answers": ["<p>Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Athena to run SQL commands on the output.</p>", "<p>Use EBS as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to EBS for storage purposes and use Amazon Athena to run SQL commands on the output.</p>", "<p>Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Neptune to run SQL commands on the output.</p>", "<p>Use S3 as the data lake, use Glue Crawler to extract the data\u2019s schema, use Glue scheduled ETL jobs to extract the new features, dump the output to S3 for storage purposes and use Amazon Redshift to run SQL commands on the output.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company wants to start building a machine learning model for predictive maintenance purposes. This simply means that they can detect if their machine will fail up to 2 hours before failing so they can repair it immediately without further damages for the machine or the production line. They decided to collect their own dataset from various sensors. The data collected resembled raw measurements such as \u201cTemperature\u201d, \u201cPressure\u201d, \u201cFrequency\u201d and many more.The domain expert says that the factors affecting the failure of the machine are not based on raw measurements, however a mathematical combination from these measurements forming 3 features which decide whether the machine will fail or not.The company needs to ingest the data from various sensors, send it to a suitable data lake, initiate an ETL job on a daily basis to extract those features from the data residing in this data lake and store the result in the same lake along with running SQL commands on the result using a managed service if necessary.Which combination of services should the company use?", "related_lectures": []}, {"_class": "assessment", "id": 70596512, "assessment_type": "multi-select", "prompt": {"question": "<p>A company has a massive dataset of documents each containing information on a specific topic. There are about 30 different topics across the documents. They hired a machine learning engineer to create a model that differentiates these documents based on different topics. Neither the company nor the machine learning engineer know the topic names, they just want to categorize documents into 30 topics.</p><p>Which two SageMaker algorithms could the machine learning engineer choose to train the models? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Topic modelling is used to organize a corpus of documents into topics; however, it does not output the names of the topics. It just produces a number of different topics and could assign future documents into one of those topics. Neural Topic Modelling \u201cNTM\u201d and Latent Dirichlet Allocation \u201cLDA\u201d are both unsupervised machine learning algorithms which are used to organize documents into topics. The difference is that LDA is CPU dependent and do not use deep learning unlike NTM.</p><p><strong>CORRECT: </strong>\"Neural Topic Modelling <strong>AND</strong> Latent Dirichlet Allocation.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Blazing Text.\" is incorrect.</p><p>Blazing Text is used for text classification, predicting labels for sentences and creating vector representation of words.</p><p><strong>INCORRECT:</strong> \"DeepAR.\" is incorrect.</p><p>DeepAR algorithm is a supervised learning algorithm used for forecasting time series data using RNNs.</p><p><strong>INCORRECT:</strong> \"Object2Vec\" is incorrect.</p><p>This is a somehow general form of \u201cword2vec\u201d generalized to handle more complex objects than just words.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/lda.html</a></p>", "answers": ["<p>Blazing Text.</p>", "<p>Object2Vec.</p>", "<p>Neural Topic Modelling.</p>", "<p>DeepAR.</p>", "<p>Latent Dirichlet Allocation.</p>"]}, "correct_response": ["c", "e"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has a massive dataset of documents each containing information on a specific topic. There are about 30 different topics across the documents. They hired a machine learning engineer to create a model that differentiates these documents based on different topics. Neither the company nor the machine learning engineer know the topic names, they just want to categorize documents into 30 topics.Which two SageMaker algorithms could the machine learning engineer choose to train the models? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70596514, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of 1,000 positive samples and 200,000 negative samples where positive samples indicate that the patient had cancer. A random forest model along with an XGBoost model were proposed, and the company wanted the data science team to evaluate both of them to choose the best model.</p><p>What is the most suitable evaluation technique should be adopted by the team given the data imbalance?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Stratified K-fold cross validation technique is most suitable for unbalanced data to evaluate the model performance on unseen data.</p><p><strong>CORRECT: </strong>\" Stratified K-fold cross validation.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" K-fold cross validation.\" is incorrect.</p><p>Stratified K-fold cross validation technique is most suitable for unbalanced data.</p><p><strong>INCORRECT:</strong> \" Holdout cross-validation.\" is incorrect.</p><p>Stratified K-fold cross validation technique is most suitable for unbalanced data.</p><p><strong>INCORRECT:</strong> \" Accuracy.\" is incorrect.</p><p>Accuracy is not suitable when evaluating unbalanced data. A na\u00efve model could simply predict a constant label of 0 for all tests. Despite having an accuracy of (199000/200000) = 99.5%, it did not evaluate the model correctly as it classified ALL positive samples incorrectly.</p>", "answers": ["<p>K-fold cross validation.</p>", "<p>Stratified K-fold cross validation.</p>", "<p>Holdout cross-validation.</p>", "<p>Accuracy.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of 1,000 positive samples and 200,000 negative samples where positive samples indicate that the patient had cancer. A random forest model along with an XGBoost model were proposed, and the company wanted the data science team to evaluate both of them to choose the best model.What is the most suitable evaluation technique should be adopted by the team given the data imbalance?", "related_lectures": []}, {"_class": "assessment", "id": 70596516, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An agriculture company wants to integrate machine learning in their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take constant readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d.</p><p>The company believes that these sensors could be used to predict the crop\u2019s yield. The company wants to train the model using XGBoost, however they did not write any code yet for model training and inference.</p><p>Which of the following scenarios should the company adopt to train their model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>In this scenario, we do not have an existing training code for our model, so the best scenario is to use the ready-made built in XGBoost algorithm in SageMaker.</p><p><strong>CORRECT: </strong>\" Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Write the training code using python, dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.\" is incorrect.</p><p>This is a long process in which writing the training code could take a long time. There is a better method to just use the built-in algorithm provided.</p><p><strong>INCORRECT:</strong> \"Use SageMaker\u2019s script mode in which the company will supply the python code to the pre-built TensorFlow container. Write the training code using python and provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.\" is incorrect.</p><p>This is a long process in which writing the training code could take a long time. There is a better method to just use the built-in algorithm provided.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s XGBoost as a framework for predicting the crops\u2019 yield, write the necessary code and supply it to the XGBoost framework.\" is incorrect.</p><p>We could use XGBoost as a built-in algorithm to save time.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a></p>", "answers": ["<p>Write the training code using python, dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.</p>", "<p>Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield.</p>", "<p>Use SageMaker\u2019s script mode in which the company will supply the python code to the pre-built TensorFlow container. Write the training code using python and provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.</p>", "<p>Use SageMaker\u2019s XGBoost as a framework for predicting the crops\u2019 yield, write the necessary code and supply it to the XGBoost framework.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "An agriculture company wants to integrate machine learning in their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take constant readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d.The company believes that these sensors could be used to predict the crop\u2019s yield. The company wants to train the model using XGBoost, however they did not write any code yet for model training and inference.Which of the following scenarios should the company adopt to train their model?", "related_lectures": []}, {"_class": "assessment", "id": 70596518, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A start-up video streaming service company just purchased a massive data collection of a company from the same sector to train on. The data is in a JSON format. The company wants to predict if their future customers will churn based on the data they purchased from the other company. The start-up currently does not have a machine learning engineer to build them a model and none of them have a machine learning knowledge.</p><p>Which service and data format should the company use to train the model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Autopilot automatically builds, trains, and tunes the best machine learning models based on the data. It does not require any machine learning knowledge. On the other hand, Amazon SageMaker requires machine learning knowledge to choose the best algorithm and implementing the code to train the model. Also, AWS Autopilot only supports tabular data format at the time being.</p><p><strong>CORRECT: </strong>\"Use CSV format and AWS Autopilot to train the model.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use JSON format and AWS Autopilot to train the model.\" is incorrect.</p><p>Autopilot currently supports tabular data format only.</p><p><strong>INCORRECT:</strong> \"Use JSON format and Amazon SageMaker to train the model.\" is incorrect.</p><p>Amazon SageMaker requires prior machine learning knowledge.</p><p><strong>INCORRECT:</strong> \" Use CSV format and Amazon SageMaker to train the model.\" is incorrect.</p><p>Amazon SageMaker requires prior machine learning knowledge.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html</a></p>", "answers": ["<p>Use JSON format and AWS Autopilot to train the model.</p>", "<p>Use CSV format and AWS Autopilot to train the model.</p>", "<p>Use JSON format and Amazon SageMaker to train the model.</p>", "<p>Use CSV format and Amazon SageMaker to train the model.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A start-up video streaming service company just purchased a massive data collection of a company from the same sector to train on. The data is in a JSON format. The company wants to predict if their future customers will churn based on the data they purchased from the other company. The start-up currently does not have a machine learning engineer to build them a model and none of them have a machine learning knowledge.Which service and data format should the company use to train the model?", "related_lectures": []}, {"_class": "assessment", "id": 70596520, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has just released a new machine learning model (B) and wants to test it using the Blue/Green deployment method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.</p><p>How should the company update the Sagemaker\u2019s endpoint?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is an example of Blue/Green deployment. AWS quotes: \u201cA blue/green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a blue/green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.\u201d</p><p><strong>CORRECT: </strong>\" Create two separate but identical environments on the same endpoint where each environment holds a model variant. Once testing is finished on model (B), direct all live traffic to it.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.\" is incorrect.</p><p>This is a canary deployment.</p><p><strong>INCORRECT:</strong> \" Create an endpoint configuration with a ratio of 50:50 for production variants and compare the performance of both models.\" is incorrect.</p><p>A Blue/Green deployment requires 2 identical and separate environments.</p><p><strong>INCORRECT:</strong> \" Create an endpoint configuration with a ratio of 70:30 for production variants.\" is incorrect.</p><p>A Blue/Green deployment requires 2 identical and separate environments.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html\">https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html</a></p>", "answers": ["<p>Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.</p>", "<p>Create an endpoint configuration with a ratio of 50:50 for production variants and compare the performance of both models.</p>", "<p>Create two separate but identical environments on the same endpoint where each environment holds a model variant. Once testing is finished on model (B), direct all live traffic to it.</p>", "<p>Create an endpoint configuration with a ratio of 70:30 for production variants.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has just released a new machine learning model (B) and wants to test it using the Blue/Green deployment method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.How should the company update the Sagemaker\u2019s endpoint?", "related_lectures": []}, {"_class": "assessment", "id": 70596522, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A telecommunications company has a massive dataset of its customers where each customer has over 100 features including, but not limited to, \u201cAge\u201d, \u201cGender\u201d, \u201cmonthly_subscription\u201d and \u201clocation\u201d. Every new customer joining the company will have his/her information collected and added to the company\u2019s database. The company wants to segment their customers into different groups so as to link a new customer to one of those groups.</p><p>Given the massive number of features per customer, what should the company do to decrease the number of features and segment the customers into different groups?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>PCA (Principal Component Analysis) is a dimensionality reduction technique to reduce the number of features to the most significant ones. After the number of features is reduced, K-means algorithm, which is a clustering algorithm, segment the customers into different groups. The specific number of groups should be determined by the machine learning specialist using methods such as the elbow method.</p><p><strong>CORRECT: </strong>\"Apply PCA to reduce the number of features, then apply K-means algorithm to segment the customers.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Apply LDA to reduce the number of features, then apply KNN algorithm to segment the customers.\" is incorrect.</p><p>Latent Dirichlet allocation<a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a> \u201cLDA\u201d is a topic modelling algorithm. KNN is used for classification or regression problems.</p><p><strong>INCORRECT:</strong> \"Apply LDA to reduce the number of features, then apply K-means algorithm to segment the customers.\" is incorrect.</p><p>Latent Dirichlet allocation<a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a> \u201cLDA\u201d is a topic modelling algorithm.</p><p><strong>INCORRECT:</strong> \"Apply K-means algorithm to segment the customers, then PCA to reduce the number of features.\" is incorrect.</p><p>Dimensionality reduction should be applied before segmentation in order to decrease the number of features to be handled effectively by the clustering algorithm.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p>", "answers": ["<p>Apply LDA to reduce the number of features, then apply KNN algorithm to segment the customers.</p>", "<p>Apply LDA to reduce the number of features, then apply K-means algorithm to segment the customers.</p>", "<p>Apply PCA to reduce the number of features, then apply K-means algorithm to segment the customers.</p>", "<p>Apply K-means algorithm to segment the customers, then PCA to reduce the number of features.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A telecommunications company has a massive dataset of its customers where each customer has over 100 features including, but not limited to, \u201cAge\u201d, \u201cGender\u201d, \u201cmonthly_subscription\u201d and \u201clocation\u201d. Every new customer joining the company will have his/her information collected and added to the company\u2019s database. The company wants to segment their customers into different groups so as to link a new customer to one of those groups.Given the massive number of features per customer, what should the company do to decrease the number of features and segment the customers into different groups?", "related_lectures": []}, {"_class": "assessment", "id": 70596524, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A massive hypermarket owns a dataset of all the products sold in its branches. The owners want to gain insights on the group of products that are most likely to be purchased together. Each transaction that takes place in any of the hypermarket branches is recorded in a database. A machine learning specialist was hired to create a visualization model for the owners to observe similar groupings of products.</p><p>Which actions should the specialist undertake?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cThe Amazon SageMaker Object2Vec algorithm is a general-purpose neural embedding algorithm that is highly customizable. It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space. \u201c</p><p>The embeddings produced contain a lot of features so they cannot be visualized directly. An algorithm such as the Principal Component Analysis \u201cPCA\u201d is used to decrease the number of features to a suitable one to be represented in a 2D or 3D display.</p><p><strong>CORRECT: </strong>\" Create pairs of products that were purchased together. Each pair should either contain 2 products purchased together with a target label \u201c1\u201d or 2 products which were not purchased together with a target label of \u201c0\u201d. These pairs should be the input of Object2Vec algorithm in which embeddings are produced. PCA is applied to decrease the number of features and visualize the group of products.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create pairs of products that were purchased together. Each pair should either contain 2 products purchased together with a target label \u201c1\u201d or 2 products which were not purchased together with a target label of \u201c0\u201d. These pairs should be trained using XGBoost algorithm to output the embeddings which could be visualized directly.\" is incorrect.</p><p>XGBoost cannot produce embeddings. It\u2019s an algorithm used for classification or regression. However, it could use those embeddings in a classification or regression problem as inputs.</p><p><strong>INCORRECT:</strong> \" Separate products which were purchased together from those that were not purchased together into 2 files. Provide both files to the Object2Vec algorithm which will extract the embeddings. PCA is applied to decrease the number of features and visualize the group of products.\" is incorrect.</p><p>Object2Vec requires similar/different pairs of objects as input for producing embeddings.</p><p><strong>INCORRECT:</strong> \" Separate products which were purchased together from those that were not purchased together into 2 files. Provide both files to the Object2Vec algorithm which will extract the embeddings. Visualize the products directly from embeddings.\" is incorrect.</p><p>Object2Vec requires similar/different pairs of objects as input for producing embeddings. Also, PCA should be used for dimensionality reduction before visualizing the outputs.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html</a></p>", "answers": ["<p>Create pairs of products that were purchased together. Each pair should either contain 2 products purchased together with a target label \u201c1\u201d or 2 products which were not purchased together with a target label of \u201c0\u201d. These pairs should be the input of Object2Vec algorithm in which embeddings are to be produced. PCA is then applied to decrease the number of features and visualize the group of products.</p>", "<p>Create pairs of products that were purchased together. Each pair should either contain 2 products purchased together with a target label \u201c1\u201d or 2 products which were not purchased together with a target label of \u201c0\u201d. These pairs should be trained using XGBoost algorithm to output the embeddings which could be visualized directly.</p>", "<p>Separate products which were purchased together from those that were not purchased together into 2 files. Provide both files to the Object2Vec algorithm which will extract the embeddings. PCA is applied to decrease the number of features and visualize the group of products.</p>", "<p>Separate products which were purchased together from those that were not purchased together into 2 files. Provide both files to the Object2Vec algorithm which will extract the embeddings. Visualize the products directly from embeddings.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A massive hypermarket owns a dataset of all the products sold in its branches. The owners want to gain insights on the group of products that are most likely to be purchased together. Each transaction that takes place in any of the hypermarket branches is recorded in a database. A machine learning specialist was hired to create a visualization model for the owners to observe similar groupings of products.Which actions should the specialist undertake?", "related_lectures": []}, {"_class": "assessment", "id": 70596526, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist is training on a classification model that classifies different types of sea creatures from their physical and biological features. The specialist has a massive data which contains different features for each creature. The specialist noticed that the model sometimes gets stuck in local minima while training.</p><p>What should the specialist do to overcome this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cOne of the leading theories argues that the non-convex surface of the loss function contains many local minima and saddle points. With a smaller mini-batch, the gradient of the loss per mini-batch is noisier and can result in the optimization process <em>bouncing out</em> of a local minimum or saddle point. A large mini-batch, however, results in a gradient with less stochasticity and optimization may <em>get stuck</em> in a local minimum or on a saddle point.\u201d</p><p>The Batch size should be chosen wisely as large batch sizes could suffer from the problem of being stuck in local minima, so to overcome this problem we should decrease the batch size number which makes the update noisier and has less chance to get stuck.</p><p><strong>CORRECT: </strong>\"Decrease the batch size.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Increase the batch size.\" is incorrect.</p><p>Increasing the batch size will worsen the problem as the update will be less noisy and the probability of getting stuck in local minima is higher.</p><p><strong>INCORRECT:</strong> \" Use regularization.\" is incorrect.</p><p>Regularization helps prevent overfitting of the model. Here, we want to ensure that the model won\u2019t get stuck in local minima.</p><p><strong>INCORRECT:</strong> \" Train the model for more epochs.\" is incorrect.</p><p>If the model gets stuck in local minima, then training it for more epochs won\u2019t benefit it. In fact, it\u2019s already stuck at a point in which it\u2019s certain that this is the best point for convergence.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/\">https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/</a></p>", "answers": ["<p>Decrease the batch size.</p>", "<p>Increase the batch size.</p>", "<p>Use regularization.</p>", "<p>Train the model for more epochs.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist is training on a classification model that classifies different types of sea creatures from their physical and biological features. The specialist has a massive data which contains different features for each creature. The specialist noticed that the model sometimes gets stuck in local minima while training.What should the specialist do to overcome this problem?", "related_lectures": []}]}
5824440
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 70265864, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company needs to build a machine learning model to predict whether customers will leave in order to attract them with an incentive. The data is currently stored in an S3 bucket where it is encrypted. A machine learning specialist should build a model using amazon SageMaker contained within the company\u2019s own VPC. The data should not be transmitted through the public internet.</p><p>Which solution should the specialist adopt?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Therefore, your VPC is not exposed to the public internet. A <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway.html\"><strong>gateway endpoint</strong></a> is a gateway that is a target for a route in your route table used for traffic destined to either <strong>Amazon S3</strong> or <strong>DynamoDB</strong>.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_08-58-34-62e08e3f34457036e0f52079d5963bcc.jpg\"><p><strong>CORRECT: </strong>\" Create a VPC gateway endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create a VPC interface endpoint and apply a bucket access policy that allows access from the given VPC endpoint and an Amazon EC2 instance.\" is incorrect.</p><p>Amazon S3 uses a gateway endpoint not an interface endpoint.</p><p><strong>INCORRECT:</strong> \" Create a VPC interface and use Network Access Control Lists (NACLs) to allow traffic between only the given VPC endpoint and an Amazon EC2 instance.\" is incorrect.</p><p>Amazon S3 uses a gateway endpoint not an interface endpoint.</p><p><strong>INCORRECT:</strong> \" Create a VPC gateway endpoint and use security groups to restrict access to the given VPC endpoint and an Amazon EC2 instance.\" is incorrect.</p><p>Security groups are not used as a security layer for an S3 bucket. Further security practices are listed in the link below.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/security.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/security.html</a></p>", "answers": ["<p>Create a VPC interface endpoint and apply a bucket access policy that allows access from the given VPC endpoint and an Amazon EC2 instance.</p>", "<p>Create a VPC interface and use Network Access Control Lists (NACLs) to allow traffic between only the given VPC endpoint and an Amazon EC2 instance.</p>", "<p>Create a VPC gateway endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC.</p>", "<p>Create a VPC gateway endpoint and use security groups to restrict access to the given VPC endpoint and an Amazon EC2 instance.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company needs to build a machine learning model to predict whether customers will leave in order to attract them with an incentive. The data is currently stored in an S3 bucket where it is encrypted. A machine learning specialist should build a model using amazon SageMaker contained within the company\u2019s own VPC. The data should not be transmitted through the public internet.Which solution should the specialist adopt?", "related_lectures": []}, {"_class": "assessment", "id": 70265866, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning engineer trained a deep neural network for a complex classification problem. He realized that the model took so long to train with minimum changes to the weights. The activation function used in the hidden layer is sigmoid.</p><p>What could he do to overcome this problem? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>The vanishing gradient problem occurs when the neural network is deep (having many hidden layers). One of the main reasons for this problem is using an activation function such as the sigmoid function. This is due to the nature of the function, which forces the gradient to be small if the sigmoid function output is too high or too low. Also, initializing the weights using proper techniques avoids this problem. Also, batch normalization is used to overcome the vanishing gradient problem.</p><p>Remember:</p><ul><li><p><code>Batch normalization </code>overcomes <strong>vanishing</strong> and <strong>exploding</strong> <strong>gradient</strong> <strong>problems</strong>.</p></li><li><p>RelU does not have <strong>vanishing gradient </strong>problem.</p></li></ul><p><strong>CORRECT: </strong>\"Use batch normalization <strong>AND</strong> Apply a suitable initialization technique to the weights <strong>AND</strong> Use RelU as an activation function instead of sigmoid.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the number of layers.\" is incorrect.</p><p>Increasing the number of layers will make the model more complex and it has nothing to do with the vanishing gradient problem.</p><p><strong>INCORRECT:</strong> \"Increase the number of neurons per hidden layer.\" is incorrect.</p><p>Increasing the number of neurons per layer has nothing to do with the vanishing gradient problem.</p><p><strong>INCORRECT:</strong> \"Collect more training images.\" is incorrect.</p><p>The model is taking so long to train, so collecting more training images does not help.</p>", "answers": ["<p>Increase the number of layers.</p>", "<p>Use batch normalization.</p>", "<p>Increase the number of neurons per hidden layer.</p>", "<p>Apply a suitable initialization technique to the weights.</p>", "<p>Use RelU as an activation function instead of sigmoid.</p>", "<p>Collect more training images.</p>"]}, "correct_response": ["b", "d", "e"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning engineer trained a deep neural network for a complex classification problem. He realized that the model took so long to train with minimum changes to the weights. The activation function used in the hidden layer is sigmoid.What could he do to overcome this problem? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 70265868, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company needs to stream video data to AWS to pass through a custom computer vision model. The model currently resides on an EC2 instance. The company has a strict policy of wearing protective helmets.</p><p>A DynamoDB table currently stores this information and if any employee is not wearing a helmet, this database is automatically updated carrying the fine information. There are currently 20 installed cameras across the company. The company is looking for the cheapest possible solution to stream videos directly to AWS to be passed to their model.</p><p>Which solution should be adopted?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Kinesis video stream cannot stream data from multiple producers; therefore, a kinesis video stream should be setup for every producer. Also, Kinesis data streams can carry data coming from: website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. However, it cannot carry video data.</p><p>Remember:</p><ul><li><p><strong>ONE</strong> producer per video stream.</p></li></ul><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-02-22-47a01c6a79f138abe496484b12197be5.jpg\"><p><strong>CORRECT: </strong>\" Create a kinesis video stream for every camera installed where the destination should be the EC2 instance currently carrying the model.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create only one kinesis video stream for all the cameras installed where the destination should be the EC2 instance currently carrying the model.\" is incorrect.</p><p>Kinesis video stream cannot stream data from multiple producers</p><p><strong>INCORRECT:</strong> \" Create a kinesis data stream for all the cameras installed where the destination should be a kinesis video stream which then streams the data to the EC2 instance currently carrying the model.\" is incorrect.</p><p>Kinesis data streams cannot carry video data.</p><p><strong>INCORRECT:</strong> \" Create a kinesis data stream for every camera installed where the destination should be a kinesis video stream which then streams the data to the EC2 instance currently carrying the model.\" is incorrect.</p><p>Kinesis data streams cannot carry video data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html\">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html</a></p>", "answers": ["<p>Create a kinesis video stream for every camera installed where the destination should be the EC2 instance currently carrying the model.</p>", "<p>Create only one kinesis video stream for all the cameras installed where the destination should be the EC2 instance currently carrying the model.</p>", "<p>Create a kinesis data stream for all the cameras installed where the destination should be a kinesis video stream which then streams the data to the EC2 instance currently carrying the model.</p>", "<p>Create a kinesis data stream for every camera installed where the destination should be a kinesis video stream which then streams the data to the EC2 instance currently carrying the model.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company needs to stream video data to AWS to pass through a custom computer vision model. The model currently resides on an EC2 instance. The company has a strict policy of wearing protective helmets.A DynamoDB table currently stores this information and if any employee is not wearing a helmet, this database is automatically updated carrying the fine information. There are currently 20 installed cameras across the company. The company is looking for the cheapest possible solution to stream videos directly to AWS to be passed to their model.Which solution should be adopted?", "related_lectures": []}, {"_class": "assessment", "id": 70265870, "assessment_type": "multi-select", "prompt": {"question": "<p>A company currently has an Amazon SageMaker endpoint. The endpoint currently has 4 models running at evenly distributed weights such that each model serves 25% of the traffic. The endpoint is based on a c6g.2xlarge instance. The company\u2019s priority is to make sure the service reaches its users under all circumstances. In other words, the company wants the endpoint to be highly available.</p><p>Which of the following solutions would help meet these requirements? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>When it comes to ensuring that an instance is highly available on the AWS cloud, more than one instance should be deployed across multiple availability zones. Scaling the model horizontally means adding more instances. On the other hand, scaling vertically means that the instance is becoming more powerful by upgrading it.</p><p>In this scenario, we could either increase the number of instances and distribute them on different availability zones or use AWS auto scaling to remove the burden of scaling.</p><p>The figure below shows a highly available solution for deploying the models. Notice that the load balancer is used on a fleet of EC2 instances with or without automatic scaling. The point is that it would become useless if we used the elastic load balancer on one instance only, so we need to distribute the traffic to more than one instance. Also, the load balancer does not automatically scale the instances, however, it distributes traffic.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-04-05-9baeb1ec77d6160bcd5399ceb9df18c3.jpg\"><p><strong>CORRECT: </strong>\" Scale the instance serving the endpoint horizontally in different availability zones, thus creating more instances <strong>and</strong> Use AWS auto scaling. \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Scale the instance serving the endpoint vertically to a c6g.8xlarge instance.\" is incorrect.</p><p>Scaling the instance vertically just makes the instance more powerful, however if anything happened to this specific instance, the entire model would not be available.</p><p><strong>INCORRECT:</strong> \" Add an application load balancer to this instance.\" is incorrect.</p><p>The point is that it would become useless if we used the elastic load balancer on one instance only, so we need to distribute the traffic to more than one instance. Also, the load balancer does not automatically scale the instances, however, it distributes traffic.</p><p><strong>INCORRECT:</strong> \"Add an elastic inference to the endpoint.\" is incorrect.</p><p>An elastic inference gives a GPU-powered-acceleration to an ec2 instance which also does not make the model highly available.</p>", "answers": ["<p>Scale the instance serving the endpoint vertically to a c6g.8xlarge instance.</p>", "<p>Scale the instance serving the endpoint horizontally in different availability zones, thus creating more instances.</p>", "<p>Use AWS auto scaling.</p>", "<p>Add an application load balancer to this instance.</p>", "<p>Use Elastic load balancer on this instance.</p>"]}, "correct_response": ["b", "c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company currently has an Amazon SageMaker endpoint. The endpoint currently has 4 models running at evenly distributed weights such that each model serves 25% of the traffic. The endpoint is based on a c6g.2xlarge instance. The company\u2019s priority is to make sure the service reaches its users under all circumstances. In other words, the company wants the endpoint to be highly available.Which of the following solutions would help meet these requirements? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265872, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning specialist has just finished the data pre-processing phase for a regressive machine learning model. The specialist has limited time for the model\u2019s training and must choose a suitable optimizer to finish the training in the least amount of time.</p><p>Which of the following optimizers would be suitable? (Select THREE.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", "", ""], "explanation": "<p>Adam, RMSProp and Adagrad and Stochastic gradient descent (SGD) are all optimizers, however SGD is the slowest among them.</p><p>Adaboost is an ensemble method algorithm.</p><p>Xavier and normalized Xavier are weight initialization techniques.</p><p><strong>CORRECT: </strong>\"Adam <strong>AND</strong> Adagrad <strong>AND</strong> RMSProp\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Stochastic gradient descent (SGD)\" is incorrect.</p><p>SGD is the slowest optimizer algorithm among (Adam, Adagrad, RMSProp). The criteria here is the suitable algorithm to finish training the model in the least amount of time.</p><p><strong>INCORRECT:</strong> \"Adaboost\" is incorrect.</p><p>Adaboost is an ensemble method algorithm and is not an optimizer.</p><p><strong>INCORRECT:</strong> \"Normalized Xavier\" is incorrect.</p><p>Normalized Xavier is a weight initialization technique.</p>", "answers": ["<p>Stochastic gradient descent (SGD).</p>", "<p>Adam.</p>", "<p>RMSProp.</p>", "<p>Adaboost.</p>", "<p>Adagrad.</p>", "<p>Normalized Xavier.</p>"]}, "correct_response": ["b", "c", "e"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist has just finished the data pre-processing phase for a regressive machine learning model. The specialist has limited time for the model\u2019s training and must choose a suitable optimizer to finish the training in the least amount of time.Which of the following optimizers would be suitable? (Select THREE.)", "related_lectures": []}, {"_class": "assessment", "id": 70265874, "assessment_type": "multiple-choice", "prompt": {"question": "<p>Based on the diagram below, which of the following statements is correct?</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-07-11-385b9b3751ee197e4fce58bdcb0e062d.jpg\"></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The solution to this problem lies in the correct, incorrect prediction percentage diagram.</p><p><strong>CORRECT: </strong>\" The model correctly classified \u201cThriller\u201d genre in more than 20% of the \u201cThriller\u201d cases.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The model incorrectly classified \u201cThriller\u201d genre by \u201cAdventure\u201d genre in more than 70% of the \u201cThriller\u201d cases.\" is incorrect.</p><p><strong>INCORRECT:</strong> \" The model correctly classified \u201cAdventure\u201d genre in more than 80% of the \u201cAdventure\u201d cases.\" is incorrect.</p><p><strong>INCORRECT:</strong> \" The model incorrectly classified \u201cRomance\u201d genre by \u201cAdventure\u201d genre in more than 50% of the \u201cRomance\u201d cases.\" is incorrect.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html</a></p>", "answers": ["<p>The model incorrectly classified \u201cThriller\u201d genre by \u201cAdventure\u201d genre in more than 70% of the \u201cThriller\u201d cases.</p>", "<p>The model correctly classified \u201cAdventure\u201d genre in more than 80% of the \u201cAdventure\u201d cases.</p>", "<p>The model incorrectly classified \u201cRomance\u201d genre by \u201cAdventure\u201d genre in more than 50% of the \u201cRomance\u201d cases.</p>", "<p>The model correctly classified \u201cThriller\u201d genre in more than 20% of the \u201cThriller\u201d cases.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "Based on the diagram below, which of the following statements is correct?", "related_lectures": []}, {"_class": "assessment", "id": 70265876, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A multi-national company collects near real-time data globally from all its branches. The company\u2019s data engineers decided to stream the data through Amazon kinesis firehose which dumps the output in an S3 bucket. A team of machine learning engineers decide to use this data to train a model for classification purposes, however they suggest that the streamed data should be transformed, in the future, using a simple transformation technique.</p><p>Which solution requires the least amount of effort from the data engineers and transforms the streamed data on the spot?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Kinesis Data Analytics enables developers to run SQL code against streaming sources and stream the results to an S3 bucket. This is easier than configuring AWS Batch jobs which will also run after the data is streamed to S3.</p><p><br></p><p><strong>BEFORE:</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-09-06-5b3d54d63951fa00c17237410ff97567.jpg\"><p><strong>AFTER:</strong></p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-09-07-15128539e7f42c04e794b72d2440e5ee.jpg\"><p><strong>CORRECT: </strong>\"Apply the transformation using Amazon Kinesis Data Analytics which will be located downstream of the Kinesis Firehose. Send the stream output to the bucket S3.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use AWS Batch to initiate batch jobs which access the data on the S3 bucket, transform it and send it back to the S3 bucket.\" is incorrect.</p><p>While AWS Batch is used for running batch compute jobs, however this solution requires more effort than just adding kinesis data analytics to the stream.</p><p><strong>INCORRECT:</strong> \"Use AWS DMS to load the data from the S3 bucket, transform it and send it back to the S3 bucket.\" is incorrect.</p><p>AWS DMS is a database migration service and cannot be used to transform data.</p><p><strong>INCORRECT:</strong> \"Apply the transformation using Amazon Kinesis Data Streams which is located downstream of the Kinesis Firehose. Send the stream output to the bucket S3.\" is incorrect.</p><p>Amazon Kinesis Data Streams is used for streaming data only and cannot transform data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html</a></p>", "answers": ["<p>Use AWS Batch to initiate batch jobs which access the data on the S3 bucket, transform it and send it back to the S3 bucket.</p>", "<p>Use AWS DMS to load the data from the S3 bucket, transform it and send it back to the S3 bucket.</p>", "<p>Apply the transformation using Amazon Kinesis Data Analytics which will be located downstream of the Kinesis Firehose. Send the stream output to the bucket S3.</p>", "<p>Apply the transformation using Amazon Kinesis Data Streams which will be located downstream of the Kinesis Firehose. Send the stream output to the bucket S3.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A multi-national company collects near real-time data globally from all its branches. The company\u2019s data engineers decided to stream the data through Amazon kinesis firehose which dumps the output in an S3 bucket. A team of machine learning engineers decide to use this data to train a model for classification purposes, however they suggest that the streamed data should be transformed, in the future, using a simple transformation technique.Which solution requires the least amount of effort from the data engineers and transforms the streamed data on the spot?", "related_lectures": []}, {"_class": "assessment", "id": 70265878, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning specialist is implementing a machine learning model to produce a sentiment given a review. The specialist already extracted TF-IDF values for all the review sentences. There are approximately 10,000 TF-IDF value for each sentence, and this is making the model\u2019s training much harder.</p><p>Which algorithms should the specialist choose from to reduce the number of features? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>\u201cPCA\u201d and \u201cT-SNE\u201d are techniques used for dimensionality reduction. They simply decrease the number of features, hence decreasing dimensionality, while minimizing the loss of information resulting from this reduction.</p><p><strong>CORRECT: </strong>\"Apply \u201cPCA\u201d algorithm <strong>and</strong> Apply \u201cT-SNE\u201d algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Apply \u201cMICE\u201d algorithm.\" is incorrect.</p><p>MICE is multiple imputation by chained equations and as the name indicates, it is an algorithm used to impute missing data.</p><p><strong>INCORRECT:</strong> \"Apply \u201cSMOTE\u201d algorithm.\" is incorrect.</p><p>SMOTE is synthetic minority oversampling technique and is a technique used when dealing with unbalanced data.</p><p><strong>INCORRECT:</strong> \"Apply \u201cOne-hot encoding\u201d.\" is incorrect.</p><p>One-hot encoding is a data transformation technique which transforms categorical data into values of \u201c0\u201d or \u201c1\u201d.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p>", "answers": ["<p>Apply \u201cMICE\u201d algorithm.</p>", "<p>Apply \u201cPCA\u201d algorithm.</p>", "<p>Apply \u201cT-SNE\u201d algorithm.</p>", "<p>Apply \u201cSMOTE\u201d technique.</p>", "<p>Apply \u201cOne-hot encoding\u201d.</p>"]}, "correct_response": ["b", "c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist is implementing a machine learning model to produce a sentiment given a review. The specialist already extracted TF-IDF values for all the review sentences. There are approximately 10,000 TF-IDF value for each sentence, and this is making the model\u2019s training much harder.Which algorithms should the specialist choose from to reduce the number of features? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265880, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company launched a new product two weeks ago and they hired a machine learning specialist to design a new system to detect the sentiment of their customers\u2019 reviews automatically. The company does not have a labelled dataset for the specialist to train on.</p><p>The company insists that the specialist should come up with a solution as soon as possible and that they have no intent to buy labelled data. The specialist is provided with a csv file on an S3 bucket in which all the reviews of the new product reside. The specialist should devise a solution that:</p><ul><li><p>Analyzes the sentiment of the reviews and dumps the output in the S3 bucket.</p></li><li><p>Defines the schema of the data and runs SQL commands to clean the data if necessary, using a managed service.</p></li><li><p>Creates a pie chart using a business intelligence tool to segment the reviews.</p></li></ul><p>Which architecture should be implemented?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Comprehend: Uses Natural language processing to gain insights and relationships in text data. It could be used to detect the sentiment of a given sentence, that\u2019s why it\u2019s applicable in this scenario.</p><p>Amazon Glue: It could be used to extract the schema of the file and create a database which could be used later by Amazon Athena.</p><p>Amazon Athena: It is a managed service used to run SQL queries instantly on data residing in S3 or simply from tables residing in Amazon Glue Data Catalog.</p><p>Amazon Quicksight: It is an ML-powered business intelligence tool used to create visualizations and could be integrated with Amazon Athena.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-13-44-d94fadbf03f72578d62232f188be2325.jpg\"><p><strong>CORRECT: </strong>\"Run a sentiment analysis job using Amazon Comprehend -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Run a sentiment analysis job using Amazon Textract -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Access and transform the table using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.\" is incorrect.</p><p>Amazon Textract is an AI service which extracts printed text, handwriting and data from any document. It cannot be used for sentiment analysis.</p><p><strong>INCORRECT:</strong> \"Run a sentiment analysis job using Amazon Comprehend -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon EMR -&gt; Create the visualization using Amazon Quicksight.\" is incorrect.</p><p>While we could use Amazon EMR to create SQL commands, the company requires a managed service.</p><p><strong>INCORRECT:</strong> \"Run a sentiment analysis job using Amazon Translate -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.\" is incorrect.</p><p>Amazon Translate is an AI (NLP) service which is used in language translation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html\">https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html</a></p><p><a href=\"https://aws.amazon.com/glue\">https://aws.amazon.com/glue</a></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><a href=\"https://aws.amazon.com/quicksight/\">https://aws.amazon.com/quicksight/</a></p>", "answers": ["<p>Run a sentiment analysis job using Amazon Textract -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Access and transform the table using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.</p>", "<p>Run a sentiment analysis job using Amazon Comprehend -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon EMR -&gt; Create the visualization using Amazon Quicksight.</p>", "<p>Run a sentiment analysis job using Amazon Comprehend -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.</p>", "<p>Run a sentiment analysis job using Amazon Translate -&gt; Create an Amazon Glue Crawler to extract the schema of the data and save the result in AWS Glue Data Catalog -&gt; Run SQL commands for transformation using Amazon Athena -&gt; Create the visualization using Amazon Quicksight.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company launched a new product two weeks ago and they hired a machine learning specialist to design a new system to detect the sentiment of their customers\u2019 reviews automatically. The company does not have a labelled dataset for the specialist to train on.The company insists that the specialist should come up with a solution as soon as possible and that they have no intent to buy labelled data. The specialist is provided with a csv file on an S3 bucket in which all the reviews of the new product reside. The specialist should devise a solution that:Analyzes the sentiment of the reviews and dumps the output in the S3 bucket.Defines the schema of the data and runs SQL commands to clean the data if necessary, using a managed service.Creates a pie chart using a business intelligence tool to segment the reviews.Which architecture should be implemented?", "related_lectures": []}, {"_class": "assessment", "id": 70265882, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A well-known company has over 250 stores across the United States. The company currently produces nearly 30 products including \u201cMobile devices\u201d, \u201cMonitors\u201d, \u201cComputers\u201d and \u201cLaptops\u201d. The daily sales information for every store and product is stored on a daily basis in an S3 bucket. The CEO wants to be informed by any drops in revenue on a daily basis across all stores. Additionally, the drops in revenues should be sent to the CEO\u2019s email along with the specific products which contributed to this drop. The system should be launched as soon as possible.</p><p>What is the most appropriate service/method for this situation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Quicksight has an ML-powered anomaly detection feature which can detect anomalies in data whether they are below or above expected values. This, by far, is the easiest method which doesn\u2019t require ML expertise and can setup an alert directly to inform the targets.</p><p><strong>CORRECT: </strong>\" Use AWS Quicksight to detect data anomalies which are lower than the expected values and configure an alert to send the result directly to her email.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Launch an EC2 instance which uses the data on S3 to perform calculations of the revenues and send the result to an SNS topic using the appropriate IAM role.\" is incorrect.</p><p>This will take some time to setup the EC2 instance, write the anomaly detection code and configure the IAM roles to perform this operation.</p><p><strong>INCORRECT:</strong> \" Use AWS Batch with Fargate to launch batch jobs daily which performs revenue calculations and sends the result to an SNS topic using the appropriate IAM role.\" is incorrect.</p><p>This is a more managed service than just using EC2 instance, however Quicksight requires no code to be written.</p><p><strong>INCORRECT:</strong> \" Use AWS Athena to detect data anomalies which are lower than the expected values and configure an alert to send the result directly to her email.\" is incorrect.</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena cannot be used to set alert on its own and it is not faster than just using Quicksight to do all the work needed.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html\">https://docs.aws.amazon.com/quicksight/latest/user/anomaly-detection-function.html</a></p>", "answers": ["<p>Launch an EC2 instance which uses the data on S3 to perform calculations of the revenues and send the result to an SNS topic using the appropriate IAM role.</p>", "<p>Use AWS Batch with Fargate to launch batch jobs daily which performs revenue calculations and sends the result to an SNS topic using the appropriate IAM role.</p>", "<p>Use AWS Quicksight to detect data anomalies which are lower than the expected values and configure an alert to send the result directly to her email.</p>", "<p>Use AWS Athena to detect data anomalies which are lower than the expected values and configure an alert to send the result directly to her email.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A well-known company has over 250 stores across the United States. The company currently produces nearly 30 products including \u201cMobile devices\u201d, \u201cMonitors\u201d, \u201cComputers\u201d and \u201cLaptops\u201d. The daily sales information for every store and product is stored on a daily basis in an S3 bucket. The CEO wants to be informed by any drops in revenue on a daily basis across all stores. Additionally, the drops in revenues should be sent to the CEO\u2019s email along with the specific products which contributed to this drop. The system should be launched as soon as possible.What is the most appropriate service/method for this situation?", "related_lectures": []}, {"_class": "assessment", "id": 70265884, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck.</p><p>The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method.</p><p>The engineer built a neural network model that identifies the type of all the waste products found within the image. The algorithm should capture all types of waste products present.</p><p>Concerning the output layer, which activation function should the engineer use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Softmax activation is used in the output layer when we want to predict only one class among other classes, so it makes a probability distribution among the outputs so that we can choose the highest probable output. On the other hand, sigmoid is used when we want to predict all classes present. The sigmoid outputs a probability for each class independent of one another. RelU activation is not used in the output layer as it outputs a value between \u201c0\u201d and \u201cinf\u201d. Also, Tanh is not used as it outputs a value between \u201c-1\u201d and \u201c1\u201d.</p><p><strong>CORRECT: </strong>\"Sigmoid activation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Softmax activation\" is incorrect.</p><p>Softmax activation is used in the output layer when we want to predict only one class among other classes. Here we want to predict all the classes present.</p><p><strong>INCORRECT:</strong> \"Tanh activation\" is incorrect.</p><p>Tanh is not used as it outputs a value between \u201c-1\u201d and \u201c1\u201d and we want to output a value between \u201c0\u201d and \u201c1\u201d to resemble the probability that a product is present.</p><p><strong>INCORRECT:</strong> \"RelU activation\" is incorrect.</p><p>Tanh is not used as it outputs a value between \u201c0\u201d and \u201cinf\u201d and we want to output a value between \u201c0\u201d and \u201c1\u201d to resemble the probability that a product is present.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/softmax.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/softmax.html</a></p>", "answers": ["<p>Sigmoid activation.</p>", "<p>Softmax activation.</p>", "<p>Tanh activation.</p>", "<p>RelU activation.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck.The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method.The engineer built a neural network model that identifies the type of all the waste products found within the image. The algorithm should capture all types of waste products present.Concerning the output layer, which activation function should the engineer use?", "related_lectures": []}, {"_class": "assessment", "id": 70265886, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A district wants to reduce the crime rate throughout its vicinity. They hired a team of data scientists to create a model to detect sharp knives, automatic guns, pistols, or any dangerous weapon given a video stream. The data scientists used Amazon SageMaker to train 5 models and chose the best model.</p><p>The district will setup 4 cameras throughout the vicinity and wants a solution where they are informed, along with the police, immediately if a person is holding a dangerous weapon in order for them to hide.</p><p>Which of the following architectures is the most efficient for deployment given that they already have a containerized model, and they want a managed service to determine their compute capacity when applying inference?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>One Kinesis video stream cannot stream data from multiple producers; therefore, a kinesis video stream should be setup for every producer. Fargate is used instead of EC2 instances when deploying containerized models as it can compute the capacity needed for inference and it is a managed service as well. One Kinesis Data Streams is sufficient to ingest all the inferences across all the Fargate containers.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-17-09-63fbc46f79238f9bec4d362c18b59ccc.jpg\"><p><strong>CORRECT: </strong>\"Four cameras sending their streams to four kinesis video streams -&gt; The output is sent to 4 Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Four cameras sending their streams to one kinesis video streams -&gt; The output is sent to four Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.\" is incorrect.</p><p>One Kinesis video streams cannot ingest more than one stream.</p><p><strong>INCORRECT:</strong> \" Four cameras sending their streams to four kinesis video streams -&gt; The output is sent to 4 Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to four Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.\" is incorrect.</p><p>One Kinesis video streams cannot ingest more than one stream.</p><p>For efficiency, only one Kinesis Data Streams is sufficient to ingest the data.</p><p><strong>INCORRECT:</strong> \" Four cameras sending their streams to four kinesis video streams -&gt; The output is sent to 4 EC2 instances containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.\" is incorrect.</p><p>Fargate is a managed service which is capable of computing the resources needed to run a container, therefore it should be used instead of EC2.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/analyze-live-video-at-scale-in-real-time-using-amazon-kinesis-video-streams-and-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/analyze-live-video-at-scale-in-real-time-using-amazon-kinesis-video-streams-and-amazon-sagemaker/</a></p>", "answers": ["<p>Four cameras sending their streams to four kinesis video streams -&gt; The output is sent to 4 Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.</p>", "<p>Four cameras sending their streams to one kinesis video streams -&gt; The output is sent to four Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.</p>", "<p>Four cameras sending their streams to one kinesis video streams -&gt; The output is sent to 4 Fargate containers containing the model inference code and artifacts -&gt; The inference output is sent to four Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.</p>", "<p>Four cameras sending their streams to four kinesis video streams -&gt; The output is sent to 4 EC2 instances containing the model inference code and artifacts -&gt; The inference output is sent to one Kinesis data streams -&gt; Lambda function processes the output -&gt; If a dangerous weapon is found, lambda will send to the SNS topic where the subscribers will be notified by SMS.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A district wants to reduce the crime rate throughout its vicinity. They hired a team of data scientists to create a model to detect sharp knives, automatic guns, pistols, or any dangerous weapon given a video stream. The data scientists used Amazon SageMaker to train 5 models and chose the best model.The district will setup 4 cameras throughout the vicinity and wants a solution where they are informed, along with the police, immediately if a person is holding a dangerous weapon in order for them to hide.Which of the following architectures is the most efficient for deployment given that they already have a containerized model, and they want a managed service to determine their compute capacity when applying inference?", "related_lectures": []}, {"_class": "assessment", "id": 70265888, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A hypermarket wants to construct a computer vision model to classify different product images into 40 unique products so they can organize their warehouse using robots. An image dataset was already collected by the hypermarket and a machine learning specialist was hired for this role.</p><p>The specialist trained the model on Amazon SageMaker using its ready-made Pytorch container. The model should be deployed on a SageMaker endpoint in which the hypermarket could request it anytime from any of its branches. The model does not use intensive GPU, however, sometimes it needs moderate GPU processing power.</p><p>What is the cheapest and most appropriate instance type that the specialist should use to deploy the model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p><strong>Explanation:</strong></p><p>AWS states: \u201cAmazon Elastic Inference (Elastic Inference) is a resource you can attach to your Amazon Elastic Compute Cloud CPU instances, Amazon Deep Learning Containers, and SageMaker instances. Elastic Inference helps you accelerate your deep learning (DL) inference workloads.\u201d</p><p>Amazon Elastic inference is used to apply the GPU power to the EC2 instance at much lower cost than using a GPU based instance (P class instances).</p><p><strong>CORRECT: </strong>\" c6g.xlarge with eia1.medium\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"P3.2xlarge\" is incorrect.</p><p>This is a GPU based instance; however, the use case requires infrequent and slight use of the GPU processing power. As a result, this is not the cheapest instance for this use case.</p><p><strong>INCORRECT:</strong> \"c6g.xlarge\" is incorrect.</p><p>This is a compute optimized instance; however, we need to use the GPU power beside the computing for this use case.</p><p><strong>INCORRECT:</strong> \"r6g.large\" is incorrect.</p><p>This is a memory optimized instance which is best suited in the application of large datasets.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elastic-inference/latest/developerguide/basics.html\">https://docs.aws.amazon.com/elastic-inference/latest/developerguide/basics.html</a></p>", "answers": ["<p>c6g.xlarge with eia1.medium</p>", "<p>P3.2xlarge</p>", "<p>c6g.xlarge</p>", "<p>r6g.large</p>"]}, "correct_response": ["a"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A hypermarket wants to construct a computer vision model to classify different product images into 40 unique products so they can organize their warehouse using robots. An image dataset was already collected by the hypermarket and a machine learning specialist was hired for this role.The specialist trained the model on Amazon SageMaker using its ready-made Pytorch container. The model should be deployed on a SageMaker endpoint in which the hypermarket could request it anytime from any of its branches. The model does not use intensive GPU, however, sometimes it needs moderate GPU processing power.What is the cheapest and most appropriate instance type that the specialist should use to deploy the model?", "related_lectures": []}, {"_class": "assessment", "id": 70265890, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has just released a new machine learning model (B) and wants to test it using the A/B testing method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.</p><p>How should the company update the Sagemaker\u2019s endpoint?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The A/B testing method uses the 2 model variants with half the traffic serving each of them. Unlike the canary method which releases the new version to a small set of users, then gradually increases its traffic. So, traffic at the beginning is 0:1, then begins to update periodically.</p><p><strong>CORRECT: </strong>\" Create an endpoint configuration with a ratio of 50:50 for production variants and compare the performance of both models.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.\" is incorrect.</p><p>This is a canary deployment.</p><p><strong>INCORRECT:</strong> \" Create two separate but identical environments on the same endpoint and only serve one model at a time.\" is incorrect.</p><p>We want to use A/B testing method which basically tests both models in production.</p><p><strong>INCORRECT:</strong> \" Create a new endpoint which will hold model (B) only.\" is incorrect.</p><p>We want to use A/B testing method which basically tests both models in production.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/</a></p>", "answers": ["<p>Create an endpoint configuration with a ratio of 0:1 for production variants and update the weights periodically.</p>", "<p>Create an endpoint configuration with a ratio of 50:50 for production variants and compare the performance of both models.</p>", "<p>Create two separate but identical environments on the same endpoint and only serve one model at a time.</p>", "<p>Create a new endpoint which will hold model (B) only.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has just released a new machine learning model (B) and wants to test it using the A/B testing method. A Sagemaker\u2019s endpoint is already in service, and a model (A) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The two models classify a product\u2019s image as faulty or not.How should the company update the Sagemaker\u2019s endpoint?", "related_lectures": []}, {"_class": "assessment", "id": 70265892, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An agriculture company wants to integrate machine learning into their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d. The company believes that these sensors could be used to predict the crop\u2019s yield. The company sends the data to an S3 bucket on a daily basis. A machine learning specialist should use an algorithm to predict the crop\u2019s yield.</p><p>Which of the following algorithms is best suited for this use case?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Linear learner algorithm is a supervised algorithm used for classification or regression problems. Here we want to predict a numerical value which is the crop\u2019s yield, so a linear learner should be used.</p><p><strong>CORRECT: </strong>\"Linear learner algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"K-means algorithm.\" is incorrect.</p><p>K-means clustering is used when we want to segment different groups in the feature space carrying similar traits.</p><p><strong>INCORRECT:</strong> \"DeepAR algorithm\" is incorrect.</p><p>DeepAR algorithm is a supervised learning algorithm used for forecasting time series data using RNN.</p><p><strong>INCORRECT:</strong> \" Principal component analysis.\" is incorrect.</p><p>Principal component analysis (PCA) is a dimensionality reduction technique.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html</a></p>", "answers": ["<p>Linear learner algorithm.</p>", "<p>K-means clustering algorithm.</p>", "<p>DeepAR algorithm.</p>", "<p>Principal component analysis.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "An agriculture company wants to integrate machine learning into their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d. The company believes that these sensors could be used to predict the crop\u2019s yield. The company sends the data to an S3 bucket on a daily basis. A machine learning specialist should use an algorithm to predict the crop\u2019s yield.Which of the following algorithms is best suited for this use case?", "related_lectures": []}, {"_class": "assessment", "id": 70265894, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to ensure that all its employees are present with the recent face verification technology. The hired a machine learning specialist and handed her the task along with the dataset. The dataset contains different faces of employees, and the goal is to create a model to verify employees in the future. She started coding a python script to train on the TensorFlow framework with basic configurations. However, she wants to try the model locally first to ensure that everything is working fine before initiating powerful instances for training.</p><p>What is the most time saving scenario for the specialist to train her model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The specialist wants to use her own custom code with the TensorFlow framework with basic configurations, so the appropriate option to choose is attach the code with an existing pre-built TensorFlow container on SageMaker.</p><p><strong>CORRECT: </strong>\" Use SageMaker\u2019s script mode in which she will supply the python code to the pre-built TensorFlow container. Provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s built in algorithm for image classification and supply the training code.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just use as is to train on our datasets. Also, they are constantly upgraded with enhanced versions.</p><p><strong>INCORRECT:</strong> \" Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.\" is incorrect.</p><p>The specialist wants the most time saving method, so instead of dockerizing the script with the dependencies, she could just supply the script to the pre-built TensorFlow container.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s built in algorithm for image classification, dockerize the training code first then supply the training code to the image classification algorithm.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just use as is to train on our datasets. We cannot supply any codes to those algorithms.</p><p><strong>References:</strong></p><p><a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/tensorflow_bring_your_own\">https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/tensorflow_bring_your_own</a></p>", "answers": ["<p>Use SageMaker\u2019s built in algorithm for image classification and supply the training code.</p>", "<p>Use SageMaker\u2019s script mode in which she will supply the python code to the pre-built TensorFlow container. Provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.</p>", "<p>Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.</p>", "<p>Use SageMaker\u2019s built in algorithm for image classification, dockerize the training code first then supply the training code to the image classification algorithm.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company wants to ensure that all its employees are present with the recent face verification technology. The hired a machine learning specialist and handed her the task along with the dataset. The dataset contains different faces of employees, and the goal is to create a model to verify employees in the future. She started coding a python script to train on the TensorFlow framework with basic configurations. However, she wants to try the model locally first to ensure that everything is working fine before initiating powerful instances for training.What is the most time saving scenario for the specialist to train her model?", "related_lectures": []}, {"_class": "assessment", "id": 70265896, "assessment_type": "multi-select", "prompt": {"question": "<p>A weather company decided to collect daily temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entries, each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created a linear regression model using SageMaker\u2019s linear learner algorithm in which they produced a residual plot shown in the figure below.</p><p>What is the most suitable solution to this problem? (Select TWO.)</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-23-24-a7df03df0e88401ddf4959fa326748bd.JPG\">", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The graph shows that some data carry outlier values which could affect their predictions, so the model could have a high error between the predicted value and the actual outlier value. Also, some models could shift the graph to strongly favor the outlier points. A solution to this problem is to apply log-transformation to the data. Another solution which is applicable to this problem is to just delete these outliers as they represent a small percentage from the whole data, however this is not the best treatment.</p><p><strong>CORRECT: </strong>\"A log-transformation should be applied to the target data <strong>and</strong> Delete the data containing outlier values.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" The model should use a non-linear algorithm instead.\" is incorrect.</p><p>This is a perfect example for a linear algorithm output where residual values are distributed evenly above and below the residual plot along with being close to \u201c0\u201d error line.</p><p><strong>INCORRECT:</strong> \" Apply one-hot encoding to the data.\" is incorrect.</p><p>One-hot encoding is a data transformation applied on nominal categorical features so it\u2019s irrelevant to this regression problem.</p><p><strong>INCORRECT:</strong> \" The model should use less data to train on.\" is incorrect.</p><p>Using less data will make the model lose valuable information in which it could\u2019ve trained on.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html</a></p>", "answers": ["<p>The model should use a non-linear algorithm instead.</p>", "<p>Apply one-hot encoding to the data.</p>", "<p>The model should use less data to train on.</p>", "<p>A log-transformation should be applied to the target data.</p>", "<p>Delete the data containing outlier values.</p>"]}, "correct_response": ["d", "e"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A weather company decided to collect daily temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entries, each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created a linear regression model using SageMaker\u2019s linear learner algorithm in which they produced a residual plot shown in the figure below.What is the most suitable solution to this problem? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 70265898, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A well-known company wants to automate the process of filtering their applicants due to the large number of applicants applying each day. The company receives over 150 resumes per day. The resume includes the applicant\u2019s name, previous/current work, previous/current education, and some other information. The company saves these resumes on an S3 bucket in the company\u2019s VPC cloud. The company wants to extract personally identifiable information, locations, organizations, and dates from the resumes. However, the resumes are provided in PDF format not text format.</p><p>Which combination of services should the company use to extract the information from the PDF resumes?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Textract is a machine learning service that automatically extracts text, handwriting and data from scanned documents that goes beyond simple optical character recognition (OCR) to identify and extract data from forms and tables.</p><p>Amazon Comprehend is an NLP service that could extract key phrases, entities, and sentiments and much more.</p><p><strong>CORRECT: </strong>\" Use Amazon Textract to recognize the text from the image, then use Amazon Comprehend to extract the information required.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use Amazon Rekognition to recognize the text from the image, then use Amazon Comprehend to extract the information required.\" is incorrect.</p><p>Amazon Rekognition is a computer vision service that detect objects, scenes, etc.</p><p><strong>INCORRECT:</strong> \" Use Amazon Rekognition to recognize the text from the image, then use Amazon Textract to extract the information required.\" is incorrect.</p><p>Amazon Rekognition is a computer vision service that detect objects, scenes, etc.</p><p>Amazon Textract extracts data from scanned documents; however, it does not extract data insights such as key phrases, entities, etc.</p><p><strong>INCORRECT:</strong> \" Use Amazon Comprehend to recognize the text from the image, then use Amazon Textract to extract the information required.\" is incorrect.</p><p>Amazon Textract extracts data from scanned documents; however, it does not extract data insights such as key phrases, entities, etc.</p><p>Amazon Comprehend cannot extract text from a scanned document, however it could extract data insights such as key phrases, entities, etc.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/comprehend/features/\">https://aws.amazon.com/comprehend/features/</a></p><p><a href=\"https://aws.amazon.com/textract/features/\">https://aws.amazon.com/textract/features/</a></p>", "answers": ["<p>Use Amazon Rekognition to recognize the text from the image, then use Amazon Comprehend to extract the information required.</p>", "<p>Use Amazon Rekognition to recognize the text from the image, then use Amazon Textract to extract the information required.</p>", "<p>Use Amazon Comprehend to recognize the text from the image, then use Amazon Textract to extract the information required.</p>", "<p>Use Amazon Textract to recognize the text from the image, then use Amazon Comprehend to extract the information required.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A well-known company wants to automate the process of filtering their applicants due to the large number of applicants applying each day. The company receives over 150 resumes per day. The resume includes the applicant\u2019s name, previous/current work, previous/current education, and some other information. The company saves these resumes on an S3 bucket in the company\u2019s VPC cloud. The company wants to extract personally identifiable information, locations, organizations, and dates from the resumes. However, the resumes are provided in PDF format not text format.Which combination of services should the company use to extract the information from the PDF resumes?", "related_lectures": []}, {"_class": "assessment", "id": 70265900, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A large library decided to shift its content to be fully internet-based as they will no longer have a physical location. The books will be presented as a soft copy on their newly established website. The library also aims to build a machine learning model to help their users search for books faster. They want to recommend books to user\u2019s based on the recommendations made by other users.</p><p>Which type of algorithm should the library use to implement such service?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is a recommendation system machine learning problem. There are 2 types of recommendation systems mentioned in the above question:</p><ul><li><p>Collaborative filtering is used in recommender systems for calculating ratings based on ratings of similar users. Here, we have user A reading book X, if we got the books that other users recommend who read the same book X, then user A will most likely prefer those books. In fact, we do not know any features about other books, just that similar users liked them.</p></li><li><p>Content-based filtering is used when user A read book X and we recommend other books that have similar features as book X. Here, we know the features of other books, so we know that they could both be similar.</p></li></ul><p><strong>CORRECT: </strong>\" Collaborative filtering.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Content based filtering.\" is incorrect.</p><p>The library wants to recommend books to a certain user based on people\u2019s book choices who already read this user\u2019s book. This is collaborative filtering</p><p><strong>INCORRECT:</strong> \" Logistic regression.\" is incorrect.</p><p>Logistic regression is used for classification problems.</p><p><strong>INCORRECT:</strong> \" Reinforcement learning.\" is incorrect.</p><p>Reinforcement learning is used when we want to solve problems using trial and error, reward, and penalty.</p>", "answers": ["<p>Content based filtering.</p>", "<p>Collaborative filtering.</p>", "<p>Logistic regression.</p>", "<p>Reinforcement learning.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A large library decided to shift its content to be fully internet-based as they will no longer have a physical location. The books will be presented as a soft copy on their newly established website. The library also aims to build a machine learning model to help their users search for books faster. They want to recommend books to user\u2019s based on the recommendations made by other users.Which type of algorithm should the library use to implement such service?", "related_lectures": []}, {"_class": "assessment", "id": 70265902, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company specialized in ocean creatures\u2019 exploration received enough funds to initiate an exploration project in the Mariana Trench which is the deepest oceanic trench on Earth. The project\u2019s objective was to explore new creatures and also build a machine learning model to classify these creatures from images.</p><p>High resolution images were taken from a camera which was previously implanted by the company in the Mariana Trench. A machine learning specialist is about to train a model on these high-quality images using GPU based instances on Amazon SageMaker. A mini batch size of 128 was chosen for training on a single GPU which was sufficient, however the company wants to speed up the training of the project as per deadline.</p><p>What is the most efficient technique that the specialist should use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cParallel processing with multiple GPUs is an important step in scaling training of deep models. In each training iteration, typically a small subset of the dataset, called a mini batch, is processed. When a single GPU is available, processing of the mini batch in each training iteration is handled by this GPU. When training with multiple GPUs, the mini batch is split across available GPUs to evenly spread the processing load. To ensure that you fully use each GPU, you must increase the mini-batch size linearly with each additional GPU. Mini-batch size has an impact not only on training speed, but also on the quality of the trained model. As you increase the mini-batch size, it is important to tune other hyperparameters to ensure faster training with similar model quality.\u201d</p><p>The specialist chose a mini batch size of 128 which he stated was sufficient for the GPU. So, if he/she wants to increase the number of GPUs, he/she should increase the mini-batch size linearly. If a mini-batch size of 128 was sufficient for 1 GPU, then a mini-batch size of 512 would be a good fit for 4 GPUs.</p><p><strong>CORRECT: </strong>\" Use parallel processing with 4 GPUs, increase the mini batch size to 512 and distribute the processing with equal ratios to all GPUs. Also, increase the learning rate relatively.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use parallel processing with 4 GPUs, increase the mini batch size to 256 and distribute the processing with equal ratios to all GPUs. Also, increase the learning rate relatively.\" is incorrect.</p><p>The best practice is to increase the mini-batch size with the number of GPUs linearly in order to fully utilize all GPUs. If a mini-batch size of 128 was sufficient for 1 GPU, then a mini-batch size of 512 would be a good fit for 4 GPUs.</p><p><strong>INCORRECT:</strong> \" Increase the learning rate significantly to speed up the training.\" is incorrect.</p><p>This is not the most efficient technique as by doing this we\u2019ll just increase the rate in which we update our parameters, and we could overshoot the best convergence point containing the optimal weights if the learning rate was high enough.</p><p><strong>INCORRECT:</strong> \" Increase the mini batch size to 512 on the same GPU.\" is incorrect.</p><p>The specialist stated that a mini batch size of 128 on a single GPU was sufficient, so we must increase the number of GPUs if we want to increase the batch size.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/\">https://aws.amazon.com/blogs/machine-learning/the-importance-of-hyperparameter-tuning-for-scaling-deep-learning-training-to-multiple-gpus/</a></p>", "answers": ["<p>Use parallel processing with 4 GPUs, increase the mini batch size to 512 and distribute the processing with equal ratios to all GPUs. Also, increase the learning rate relatively.</p>", "<p>Use parallel processing with 4 GPUs, increase the mini batch size to 256 and distribute the processing with equal ratios to all GPUs. Also, increase the learning rate relatively.</p>", "<p>Increase the learning rate significantly to speed up the training.</p>", "<p>Increase the mini batch size to 512 on the same GPU.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company specialized in ocean creatures\u2019 exploration received enough funds to initiate an exploration project in the Mariana Trench which is the deepest oceanic trench on Earth. The project\u2019s objective was to explore new creatures and also build a machine learning model to classify these creatures from images.High resolution images were taken from a camera which was previously implanted by the company in the Mariana Trench. A machine learning specialist is about to train a model on these high-quality images using GPU based instances on Amazon SageMaker. A mini batch size of 128 was chosen for training on a single GPU which was sufficient, however the company wants to speed up the training of the project as per deadline.What is the most efficient technique that the specialist should use?", "related_lectures": []}]}
5824442
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 68126064, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Machine Learning Specialist is working on a project containing 3 TB of data which is currently on an S3 bucket. The company gave the specialist access to a SageMaker notebook with low specifications. The company also states that any training instance spun from the notebook should be a t3.2xlarge with 30 GB EBS storage at maximum. The specialist should provide a solution to train any model as fast as possible with minimum cost.</p><p>Which solution is applicable given these requirements?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>In this example we are facing 2 problems which are cost requirements and limitation of storage. When using pipe mode to stream data directly to training instances, we are overcoming the problem of having limited EBS storage. Also, this pipe mode option comes at no additional cost.</p><p><strong>CORRECT: </strong>\"Use pipe mode to stream data directly to your training instance.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Submit a request to the company to use a 3 TB EBS block storage instead of 30 GB.\" is incorrect.</p><p>While this is a correct option, the company requires the cheapest option.</p><p><strong>INCORRECT:</strong> \"Use FSx for lustre as a training data source instead of repetitively downloading it from S3.\" is incorrect.</p><p>FSx is used for massive datasets to reduce Amazon S3 download times, however, this option is expensive</p><p><strong>INCORRECT:</strong> \" Download data in chunks where each chunk holds a maximum of 30 GB of data.\" is incorrect.</p><p><br></p><p>There are two input modes for training: 1) File mode \u2013 2) Pipe mode</p><p>In File (default) mode, Amazon SageMaker copies the data from the input source onto the local Amazon Elastic Block Store (Amazon EBS) volumes before starting your training algorithm. This is the most commonly used input mode. In Pipe mode, Amazon SageMaker streams input data from the source directly to your algorithm without using the EBS volume. Therefore, there is no such option of downloading data in chunks.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></p>", "answers": ["<p>Submit a request to the company to use a 3 TB EBS block storage instead of 30 GB.</p>", "<p>Use FSx for lustre as a training data source instead of repetitively downloading it from S3.</p>", "<p>Download data in chunks, while training, where each chunk holds a maximum of 30 GB of data.</p>", "<p>Use pipe mode to stream data directly to your training instance.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A Machine Learning Specialist is working on a project containing 3 TB of data which is currently on an S3 bucket. The company gave the specialist access to a SageMaker notebook with low specifications. The company also states that any training instance spun from the notebook should be a t3.2xlarge with 30 GB EBS storage at maximum. The specialist should provide a solution to train any model as fast as possible with minimum cost.Which solution is applicable given these requirements?", "related_lectures": []}, {"_class": "assessment", "id": 68126066, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company needs to gain live insights through their Ip log traffic going to and from their network interfaces inside a VPC. The data engineering team decides to send their VPC\u2019s log data to Amazon CloudWatch. This data should be sent to a Splunk cluster for monitoring and searching purposes.</p><p>Which solution is possible to stream the data, transform it, and then stream the result to the specified Splunk cluster?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p><strong>Amazon Kinesis Firehose</strong> <strong>can</strong> transform the data from one format to another while <strong>Data Streams cannot. </strong>AWS Glue is a managed serverless ETL solution, however it cannot stream data. Also, Amazon Athena depends on AWS Glue to determine the data\u2019s schema in order to perform SQL commands on the database, also it cannot stream data.</p><p>Remember:</p><ul><li><p>Kinesis Firehose can transform streamed data.</p></li><li><p>Kinesis Data streams cannot transform streamed data.</p></li></ul><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-32-48-94af8a1b8ebe22dcdfe4d1cea20b1fc4.jpg\"><p><strong>CORRECT: </strong>\"Ingest the data from CloudWatch using Amazon Kinesis Data Firehose, enable the transformation option using lambda, then stream the results to the Splunk Cluster.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Ingest the data from CloudWatch and transform the data to an appropriate format using AWS Glue and then send the results to the Splunk cluster.\" is incorrect.</p><p>AWS Glue cannot stream data.</p><p><strong>INCORRECT:</strong> \" Ingest the data from CloudWatch and transform the data to an appropriate format using AWS Athena and then send the results to the Splunk cluster.\" is incorrect.</p><p>AWS Athena cannot stream data.</p><p><strong>INCORRECT:</strong> \"Ingest the data from CloudWatch using Amazon Kinesis Data Streams, enable the transformation option using lambda, then stream the results to the Splunk Cluster.\" is incorrect.</p><p>While Amazon Kinesis Data Streams can stream data, it cannot transform it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/vpc-splunk-tutorial.html\">https://docs.aws.amazon.com/firehose/latest/dev/vpc-splunk-tutorial.html</a></p>", "answers": ["<p>Ingest the data from CloudWatch and transform the data to an appropriate format using AWS Glue and then send the results to the Splunk cluster.</p>", "<p>Ingest the data from CloudWatch and transform the data to an appropriate format using AWS Athena and then send the results to the Splunk cluster.</p>", "<p>Ingest the data from CloudWatch using Amazon Kinesis Data Streams, enable the transformation option using lambda, then stream the results to the Splunk Cluster.</p>", "<p>Ingest the data from CloudWatch using Amazon Kinesis Data Firehose, enable the transformation option using lambda, then stream the results to the Splunk Cluster.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company needs to gain live insights through their Ip log traffic going to and from their network interfaces inside a VPC. The data engineering team decides to send their VPC\u2019s log data to Amazon CloudWatch. This data should be sent to a Splunk cluster for monitoring and searching purposes.Which solution is possible to stream the data, transform it, and then stream the result to the specified Splunk cluster?", "related_lectures": []}, {"_class": "assessment", "id": 68126068, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A hospital has a massive collection of PDF files containing patients\u2019 records. Each file contains text, handwritten notes and tables containing patient\u2019s information. The hospital wants to extract all these information without relying on human involvement to reduce cost. The data will be used for search purposes in the near future.</p><p>Which AWS service should they use to successfully extract these data?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Textract uses OCR technology to extract text and handwriting from a document. It can also extract tabular data in the form of rows and columns.</p><p>Remember:</p><ul><li><p>Textract for recognizing handwritten text, tabular data.</p></li><li><p>Amazon <strong>Polly. </strong>Polly is a person\u2019s name, so she converts text to spoken speech.</p></li><li><p>Transcribe does the opposite of Polly.</p></li></ul><p><strong>CORRECT: </strong>\"Amazon Textract\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon Comprehend\" is incorrect.</p><p>Amazon Comprehend is an NLP service used to extract valuable insights from a text such as sentiment analysis and extracting key phrases. It is not trained to recognize handwritten data.</p><p><strong>INCORRECT:</strong> \"Amazon Transcribe\" is incorrect.</p><p>Amazon Transcribe uses automatic speech recognition to convert speech to text.</p><p><strong>INCORRECT:</strong> \"Amazon Polly\" is incorrect.</p><p>Amazon Polly is the opposite of Amazon Transcribe where it converts text to speech.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/textract/latest/dg/what-is.html\">https://docs.aws.amazon.com/textract/latest/dg/what-is.html</a></p>", "answers": ["<p>Amazon Comprehend</p>", "<p>Amazon Textract</p>", "<p>Amazon Transcribe</p>", "<p>Amazon Polly</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A hospital has a massive collection of PDF files containing patients\u2019 records. Each file contains text, handwritten notes and tables containing patient\u2019s information. The hospital wants to extract all these information without relying on human involvement to reduce cost. The data will be used for search purposes in the near future.Which AWS service should they use to successfully extract these data?", "related_lectures": []}, {"_class": "assessment", "id": 68126070, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has sensitive data of its customers residing in a kms-encrypted S3 bucket. A machine learning engineer should build a classifier using Amazon SageMaker for pre-processing, training, and deployment of the model. The S3 bucket and SageMaker notebook are both located within the same VPC. The company demands that all traffic should be secured and within a private connection in the VPC.</p><p>Which solution is the most appropriate to begin experimenting with the model on SageMaker?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A VPC endpoint enables private connections between a virtual private cloud (VPC) and supported services. Therefore, your VPC is not exposed to the public internet. A <a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway.html\"><strong>gateway endpoint</strong></a> is a gateway that is a target for a route in your route table used for traffic destined to either <strong>Amazon S3</strong> or <strong>DynamoDB</strong>. Amazon SageMaker cannot access the S3 bucket without having the appropriate role. Also, the S3 bucket is secured using Amazon KMS, therefore granting the role read permission into S3 is not enough. A KMS key policy permission should be granted to decrypt the data in S3 and begin reading it.</p><p><strong>CORRECT: </strong>\"Create a VPC gateway endpoint to allow connection between S3 endpoint and SageMaker. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket and KMS key policy.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an AWS direct connect to initiate a private secure connection between the SageMaker instance endpoint and S3 endpoint. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket and KMS key policy.\" is incorrect.</p><p>AWS direct connect is used for hybrid cloud network connections. Here we are merely connecting 2 services inside of AWS.</p><p><strong>INCORRECT:</strong> \"Create an AWS direct connect to initiate a private secure connection between the SageMaker instance endpoint and S3 endpoint. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket.\" is incorrect.</p><p>AWS direct connect is used for hybrid cloud network connections. Here we are merely connecting 2 services inside of AWS.</p><p><strong>INCORRECT:</strong> \"Create a VPC gateway endpoint to allow connection between S3 endpoint and SageMaker. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket.\" is incorrect.</p><p>A kms key policy should be granted for the data to be decrypted first before reading it.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html</a></p>", "answers": ["<p>Create an AWS direct connect to initiate a private secure connection between the SageMaker instance endpoint and S3 endpoint. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket and KMS key policy.</p>", "<p>Create a VPC gateway endpoint to allow connection between S3 endpoint and SageMaker. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket and KMS key policy.</p>", "<p>Create a VPC gateway endpoint to allow connection between S3 endpoint and SageMaker. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket.</p>", "<p>Create an AWS direct connect to initiate a private secure connection between the SageMaker instance endpoint and S3 endpoint. Assign an IAM role to SageMaker and grant it read access permissions to the S3 bucket.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has sensitive data of its customers residing in a kms-encrypted S3 bucket. A machine learning engineer should build a classifier using Amazon SageMaker for pre-processing, training, and deployment of the model. The S3 bucket and SageMaker notebook are both located within the same VPC. The company demands that all traffic should be secured and within a private connection in the VPC.Which solution is the most appropriate to begin experimenting with the model on SageMaker?", "related_lectures": []}, {"_class": "assessment", "id": 68126072, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A hypermarket built a computer vision model to detect the facial expression of its customers on their shopping day. The hypermarket will use this data to give discounts for customers who are frowning for instance to boost their mood. There are currently 3 supported expressions: \u201cSmiling\u201d, \u201cFrowning\u201d and \u201cNeutral\u201d. Using the confusion matrix below, determine the accuracy of the model.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-36-17-272f307e4fafa9e713e394448a6c8e92.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-36-18-6a8787ab0410c296e2b3d9edc72efd12.jpg\"><p>Remember:</p><ul><li><p>Accuracy is always (the sum of a confusion matrix diagonal / Total dataset).</p></li><li><p>We choose the diagonal having the correct predictions either a true positive or a true negative.</p></li></ul><p><strong>CORRECT: </strong>\"Accuracy = 33.33%\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Accuracy = 70%\" is incorrect.</p><p>Accuracy is calculated above.</p><p><strong>INCORRECT:</strong> \"Accuracy = 50%\" is incorrect.</p><p>Accuracy is calculated above.</p><p><strong>INCORRECT:</strong> \"Accuracy = 55%\" is incorrect.</p><p>Accuracy is calculated above.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html</a></p>", "answers": ["<p>Accuracy = 33.33%</p>", "<p>Accuracy = 70%</p>", "<p>Accuracy = 50%</p>", "<p>Accuracy = 55%</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A hypermarket built a computer vision model to detect the facial expression of its customers on their shopping day. The hypermarket will use this data to give discounts for customers who are frowning for instance to boost their mood. There are currently 3 supported expressions: \u201cSmiling\u201d, \u201cFrowning\u201d and \u201cNeutral\u201d. Using the confusion matrix below, determine the accuracy of the model.", "related_lectures": []}, {"_class": "assessment", "id": 68126074, "assessment_type": "multi-select", "prompt": {"question": "<p>A government has a massive dataset for each of its citizens. Each citizen has 1,000 feature records and the total number of citizens in this city is 900,000. The government needs to segment their citizens into groups so that they can receive better health-related services.</p><p>Which algorithms should be used to provide a suitable solution given the number of features? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The number of features provided is massive; therefore, a suitable dimensionality reduction technique should be used first which is the <strong>Principal Component Analysis (PCA)</strong> algorithm. Then a segmentation algorithm should be used to segment citizens into some groups where each group carries somehow similar citizens in the feature space. The segmentation algorithm is the <strong>K-means</strong> algorithm.</p><p>Remember:</p><ul><li><p><strong>PCA</strong> for <strong>dimensionality reduction.</strong></p></li><li><p><strong>K-means </strong>for <strong>segmentation</strong>, <strong>KNN</strong> for <strong>classification/regression</strong>.</p></li></ul><p><strong>CORRECT: </strong>\"Principal Component Analysis (PCA) algorithm <strong>AND</strong> K-means algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"KNN algorithm.\" is incorrect.</p><p>KNN algorithm is used for classification/regression problems.</p><p><strong>INCORRECT:</strong> \"Latent Dirichlet Allocation algorithm (LDA).\" is incorrect.</p><p>LDA algorithm is a topic modelling algorithm used in NLP.</p><p><strong>INCORRECT:</strong> \"Random Cut Forest algorithm (RCF).\" is incorrect.</p><p>RCF is an unsupervised algorithm for anomaly detection.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html</a></p>", "answers": ["<p>KNN algorithm.</p>", "<p>K-means algorithm.</p>", "<p>Latent Dirichlet Allocation (LDA) algorithm.</p>", "<p>Principal Component Analysis (PCA) algorithm.</p>", "<p>Random Cut Forest (RCF) algorithm.</p>"]}, "correct_response": ["b", "d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A government has a massive dataset for each of its citizens. Each citizen has 1,000 feature records and the total number of citizens in this city is 900,000. The government needs to segment their citizens into groups so that they can receive better health-related services.Which algorithms should be used to provide a suitable solution given the number of features? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 68126076, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company hires a machine learning engineer to create models for classification purposes. The data ingested is currently not applicable for training and should undergo some transformation processes. The ETL process should be fully managed in terms of the provisioning and teardown of resources such as EMR clusters and EC2 instances. The ETL job should have the flexibility of using multiple engines if necessary.</p><p>Which is the most suitable ETL-specific service that he/she should use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p><strong>Managed</strong> ETL-specific service should shorten the choices to Glue and Data Pipeline which are both managed services. EMR is used in ETL jobs, however the process should have control over EMR clusters, so a higher service should control the process not EMR. Now, AWS Glue uses only Apache Spark as its engine in a serverless environment, so this will contradict the flexibility criteria for using multiple engines if necessary. AWS Data Pipeline is the best service for these constraints. While AWS Step Functions could be used along with other services to orchestrate an ETL process, it is not an ETL-specific service as it is built for more general workflows and using AWS Data Pipeline is much simpler.</p><p>The figure below shows an example of the usage of AWS Data Pipeline.</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-38-32-8d7d5c31a873fd1b3a1c2c88349ddd58.jpg\"></p><p><strong>CORRECT: </strong>\"AWS Data Pipeline\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS Glue\" is incorrect.</p><p>Glue is a managed ETL service, however it does work with only one engine (Apache Spark)</p><p><strong>INCORRECT:</strong> \"AWS EMR\" is incorrect.</p><p>EMR is an ETL service, however, you have access to the underlying infrastructure, so it is not managed.</p><p><strong>INCORRECT:</strong> \"AWS Step Functions\" is incorrect.</p><p>AWS quotes \u201cAWS Step Functions is a low-code visual workflow service used to orchestrate AWS services, automate business processes, and build serverless applications.\u201d</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\">https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html</a></p>", "answers": ["<p>AWS Data Pipeline</p>", "<p>AWS Glue</p>", "<p>AWS EMR</p>", "<p>AWS Step Functions</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company hires a machine learning engineer to create models for classification purposes. The data ingested is currently not applicable for training and should undergo some transformation processes. The ETL process should be fully managed in terms of the provisioning and teardown of resources such as EMR clusters and EC2 instances. The ETL job should have the flexibility of using multiple engines if necessary.Which is the most suitable ETL-specific service that he/she should use?", "related_lectures": []}, {"_class": "assessment", "id": 68126078, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A government has access to sensitive data which is currently on the cloud in an S3 bucket. The data includes data such as: citizens\u2019 social security number, addresses and income. The government will hire a team of machine learning engineers to train models based on this data in the future. The government needs a reliable encryption method to encrypt the data at rest and requires a method where they could manage and use AWS encryption keys.</p><p>Which encryption method should they use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>There are 2 types of encryption methods:</p><p>1) Server-Side Encryption:</p><ul><li><p>SSE-S3</p></li><li><p>SSE-KMS</p></li><li><p>SSE-C</p></li></ul><p>2) Client-Side Encryption:</p><ul><li><p>CSE</p></li></ul><p>The data is currently residing in an S3 bucket so we do not want to use a Client-Side Encryption, instead we shall use any of the 3 methods provided with the Server-Side Encryption. Since the governate needs a managed key service produced by the cloud provider and manage them, then SSE-KMS is the target.</p><p><strong>CORRECT: </strong>\"SSE-KMS\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"SSE-S3\" is incorrect.</p><p>This is the default encryption method for encrypting objects in an S3 bucket. Amazon produces and uses the keys.</p><p><strong>INCORRECT:</strong> \"SSE-C\" is incorrect.</p><p>This is a method for encrypting the data using customer-provided encryption keys.</p><p><strong>INCORRECT:</strong> \"CSE\" is incorrect.</p><p>The data is already on the S3 bucket, and this method is for encrypting data in transit before uploading it to the bucket.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html</a></p>", "answers": ["<p>SSE-S3</p>", "<p>SSE-KMS</p>", "<p>SSE-C</p>", "<p>CSE</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A government has access to sensitive data which is currently on the cloud in an S3 bucket. The data includes data such as: citizens\u2019 social security number, addresses and income. The government will hire a team of machine learning engineers to train models based on this data in the future. The government needs a reliable encryption method to encrypt the data at rest and requires a method where they could manage and use AWS encryption keys.Which encryption method should they use?", "related_lectures": []}, {"_class": "assessment", "id": 68126080, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. Figure (1) is a histogram showing information about the income\u2019s distribution in thousands.</p><p>What is the best description of this histogram and what sort of transformation is required to transform it to the desirable bell curve in Figure (2)?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-43-15-d01eb00bdcbb78d21cde61ec61118e58.jpg\"><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-43-15-dae6ba6f5395a28b7da8385e194410e0.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Do not get confused between left-skewed and right-skewed data. When the graph has a long right tail, it is right-skewed and vice versa. Log transform has the power of transforming the data presented above (right-skewed) to be more normally distributed.</p><p><strong>CORRECT: </strong>\"The income\u2019s data is right-skewed, and the transformation applied is log transform.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The Income\u2019s data is left-skewed, and the transformation applied is log transform.\" is incorrect.</p><p>The graph is right-skewed.</p><p><strong>INCORRECT:</strong> \"The income\u2019s data is left-skewed, and the transformation applied is PCA.\" is incorrect.</p><p>The data is right-skewed. PCA is a feature reduction technique where it reduces the feature dimension. For instance, a model containing 40 features could be reduced to the most powerful 10 features. The question is about transformation within one feature.</p><p><strong>INCORRECT:</strong> \"The income\u2019s data is right-skewed, and the transformation applied is PCA.\" is incorrect.</p><p>PCA is a feature reduction technique where it reduces the feature dimension. For instance, a model containing 40 features could be reduced to the most powerful 10 features. The question is about transformation within one feature.</p>", "answers": ["<p>The Income\u2019s data is left-skewed, and the transformation applied is log transform.</p>", "<p>The income\u2019s data is right-skewed, and the transformation applied is log transform.</p>", "<p>The income\u2019s data is left-skewed, and the transformation applied is PCA.</p>", "<p>The income\u2019s data is right-skewed, and the transformation applied is PCA.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. Figure (1) is a histogram showing information about the income\u2019s distribution in thousands.What is the best description of this histogram and what sort of transformation is required to transform it to the desirable bell curve in Figure (2)?", "related_lectures": []}, {"_class": "assessment", "id": 68126082, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A weather company hired a machine learning specialist to predict the weather in the near future. The specialist was informed by the domain expert with the most appropriate features that should be taken into consideration. The features included -but not limited to- \u201cTemperature\u201d, \u201cPressure\u201d, \u201cHumidity\u201d and \u201cWind speed\u201d. The specialist trained a binary classification model for this purpose, and he produced their confusion matrix.</p><p>What is the precision and F1 score to compare them with the business\u2019s metric?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-44-43-7f7362d1273bd3d38cd3ef6fa70a93cb.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-44-43-545136fffb9b4eacd82d5e8b7d8c8e56.jpg\"><p><strong>CORRECT: </strong>\" Precision = 1/4, F1 score = 3/8.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Precision = 3/4, F1 score = 1/4.\" is incorrect.</p><p>Precision = 1/4, Recall = 3/4 and F1 score = 3/8.</p><p><strong>INCORRECT:</strong> \" Precision = 1/4, Fl score = 3/4.\" is incorrect.</p><p>Precision = 1/4, Recall = 3/4 and F1 score = 3/8.</p><p><strong>INCORRECT:</strong> \" Precision = 3/8, F1 score = 1/4.\" is incorrect.</p><p>Precision = 1/4, Recall = 3/4 and F1 score = 3/8.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>Precision = 3/4, F1 score = 1/4.</p>", "<p>Precision = 1/4, Fl score = 3/4.</p>", "<p>Precision = 3/8, F1 score = 1/4.</p>", "<p>Precision = 1/4, F1 score = 3/8.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A weather company hired a machine learning specialist to predict the weather in the near future. The specialist was informed by the domain expert with the most appropriate features that should be taken into consideration. The features included -but not limited to- \u201cTemperature\u201d, \u201cPressure\u201d, \u201cHumidity\u201d and \u201cWind speed\u201d. The specialist trained a binary classification model for this purpose, and he produced their confusion matrix.What is the precision and F1 score to compare them with the business\u2019s metric?", "related_lectures": []}, {"_class": "assessment", "id": 68126084, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning company is collecting images for a rare chest disease called cystic fibrosis where it affects only 0.01% of the USA population. The company wants to automatically detect if the person has this disease from his/her X-ray image. The main obstacle remains in the lack of images containing the disease, patient\u2019s privacy where some patients refuse to cooperate. The company cannot obtain a massive dataset due to these restrictions.</p><p>Given these restrictions, what should the company do to somehow enhance their model\u2019s training?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The best method is to use data augmentation where the images containing the disease are flipped, rotated, scaled, cropped, translated, etc. This will increase the number of images containing the disease and will avoid overfitting the model as well.</p><p><strong>CORRECT: </strong>\" Perform data augmentation on images containing the disease.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use AWS Autopilot to automatically detect the disease.\" is incorrect.</p><p>Amazon SageMaker Autopilot currently does not train on complex models such as computer vision models.</p><p><strong>INCORRECT:</strong> \" Train the model for more epochs.\" is incorrect.</p><p>The problem remains in the lack of data. The company needs more data to train on rather than more time to train on the data.</p><p><strong>INCORRECT:</strong> \" Use AWS Rekognition to automatically detect the disease.\" is incorrect.</p><p>While Amazon Rekognition is a managed, high level AI service, it cannot simply detect a customizable problem such as the disease.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/iot/sagemaker-object-detection-greengrass-part-2-of-3/\">https://aws.amazon.com/blogs/iot/sagemaker-object-detection-greengrass-part-2-of-3/</a></p>", "answers": ["<p>Perform data augmentation on images containing the disease.</p>", "<p>Train the model for more epochs.</p>", "<p>Use AWS Rekognition to automatically detect the disease.</p>", "<p>Use AWS Autopilot to automatically detect the disease.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A machine learning company is collecting images for a rare chest disease called cystic fibrosis where it affects only 0.01% of the USA population. The company wants to automatically detect if the person has this disease from his/her X-ray image. The main obstacle remains in the lack of images containing the disease, patient\u2019s privacy where some patients refuse to cooperate. The company cannot obtain a massive dataset due to these restrictions.Given these restrictions, what should the company do to somehow enhance their model\u2019s training?", "related_lectures": []}, {"_class": "assessment", "id": 68126086, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An agriculture company wants to integrate machine learning in their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d. The company believes that these sensors could be used to predict the crop\u2019s yield. The company has a TensorFlow model which is built using \u201cR\u201d language.</p><p>Which of the following scenarios should the company adopt to train their model?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The company wants to use its own TensorFlow code built in R. Unfortunately, the prebuilt containers in SageMaker supports Python SDK not R, so the appropriate option to choose Is to first dockerize the training script with all the dependencies, upload it to ECR then use the image uri for training the model.</p><p><strong>CORRECT: </strong>\" Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield, dockerize the training code first then supply the training code to the algorithm.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just used as is to train on our datasets. Also, they are constantly upgraded with enhanced versions. There is an option for XGBoost algorithm to be used as a framework not a built-in algorithm.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s script mode in which she will supply the R code to the pre-built TensorFlow container. Provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.\" is incorrect.</p><p>The prebuilt containers in SageMaker supports Python SDK not R.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield and supply the training code.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just used as is to train on our datasets. There is an option for XGBoost algorithm to be used as a framework not a built-in algorithm.</p><p><strong>References:</strong></p><p><a href=\"https://sagemaker-examples.readthedocs.io/en/latest/r_examples/r_byo_r_algo_hpo/tune_r_bring_your_own.html\">https://sagemaker-examples.readthedocs.io/en/latest/r_examples/r_byo_r_algo_hpo/tune_r_bring_your_own.html</a></p>", "answers": ["<p>Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.</p>", "<p>Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield, dockerize the training code first then supply the training code to the algorithm.</p>", "<p>Use SageMaker\u2019s script mode in which the company will supply the python code to the pre-built TensorFlow container. Provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.</p>", "<p>Use SageMaker\u2019s XGBoost built in algorithm for predicting the crops\u2019 yield and supply the training code.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "An agriculture company wants to integrate machine learning in their system to maximize their crop yield. They currently have IOT sensors planted all over the farm perimeter. The sensors take readings of \u201cTemperature\u201d, \u201csoil moisture\u201d, \u201cfertility\u201d and \u201clight\u201d. The company believes that these sensors could be used to predict the crop\u2019s yield. The company has a TensorFlow model which is built using \u201cR\u201d language.Which of the following scenarios should the company adopt to train their model?", "related_lectures": []}, {"_class": "assessment", "id": 68126088, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to use anomaly detection to detect whenever their servers\u2019 sensors produce spiked readings. The company is willing to ingest real-time structured data containing features of their servers including -but not limited to- \u201cTemperature\u201d and \u201cUsage\u201d. The data is to be sent for live anomaly detection which should send an SMS to every technician if a faulty server was found.</p><p>Which combination of services should the company use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Kinesis Data streams is used when we want <strong>real-time</strong> data ingestion.</p><p>Kinesis Data Analytics can be used for <strong>anomaly detection</strong>.</p><p>Lambda function is used to analyze the output and could be integrated with Amazon SNS to send emails or SMS to the subscribed topics.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_09-48-42-98b0cd6fbc72cc16d2bb021357c738aa.JPG\"><p><strong>CORRECT: </strong>\" Data is ingested through Kinesis Data streams -&gt; Output is sent to Kinesis Data Analytics for anomaly detection -&gt; Kinesis data streams digest the output -&gt; Lambda function analyses the results and send an SMS notification using SNS.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Data is ingested and analyzed for detecting anomalies through Kinesis Data streams -&gt; Results are sent to a lambda function which will send SMS through SNS topics.\" is incorrect.</p><p>Kinesis Data Streams cannot be used for anomaly detection.</p><p><strong>INCORRECT:</strong> \" Data is ingested through Kinesis Data streams -&gt; Output is sent to Kinesis Data Firehose for anomaly detection -&gt; Results are sent to a lambda function which will send SMS through SNS topics.\" is incorrect.</p><p>Kinesis Firehose cannot be used for anomaly detection.</p><p><strong>INCORRECT:</strong> \" Data is ingested through Kinesis Data Firehose -&gt; Output is sent to Kinesis Data Analytics for anomaly detection -&gt; Results are sent to a lambda function which will send SMS through SNS topics.\" is incorrect.</p><p>We want a <strong>real-time</strong> data ingestion which is applicable by using kinesis data streams.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html\">https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>", "answers": ["<p>Data is ingested and analyzed for detecting anomalies through Kinesis Data streams -&gt; Results are sent to a lambda function which will send SMS through SNS topics.</p>", "<p>Data is ingested through Kinesis Data streams -&gt; Output is sent to Kinesis Data Analytics for anomaly detection -&gt; Kinesis data streams digest the output -&gt; Lambda function analyses the results and send an SMS notification using SNS.</p>", "<p>Data is ingested through Kinesis Data streams -&gt; Output is sent to Kinesis Data Firehose for anomaly detection -&gt; Results are sent to a lambda function which will send SMS through SNS topics.</p>", "<p>Data is ingested through Kinesis Data Firehose -&gt; Output is sent to Kinesis Data Analytics for anomaly detection -&gt; Results are sent to a lambda function which will send SMS through SNS topics.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company wants to use anomaly detection to detect whenever their servers\u2019 sensors produce spiked readings. The company is willing to ingest real-time structured data containing features of their servers including -but not limited to- \u201cTemperature\u201d and \u201cUsage\u201d. The data is to be sent for live anomaly detection which should send an SMS to every technician if a faulty server was found.Which combination of services should the company use?", "related_lectures": []}, {"_class": "assessment", "id": 68126090, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A weather company decided to collect daily temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entries, each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created four regression models in which they produced a residual plot for each of them.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_09-51-19-6e516f863cc326a92e388b45b133508d.JPG\"><p>Which of the following models should they deploy?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The model describes the data accurately when the predicted values are close to the actual ones as the residual equation = Actual values - predicted values. Also, the data should be distributed evenly on the vertical side.</p><p><strong>CORRECT: </strong>\"Model 4\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Model 1\" is incorrect.</p><p>Model 1 contains outliers which are not dealt with. Different transformations should be applied to the outliers, or we should simply remove them in some circumstances.</p><p><strong>INCORRECT:</strong> \"Model 2\" is incorrect.</p><p>Model 2 appears to have a non-linear relation which is not captured by the model.</p><p><strong>INCORRECT:</strong> \"Model 3\" is incorrect.</p><p>Model 3 suffers from heteroscedasticity, where the error gets larger as the prediction value increases.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html</a></p>", "answers": ["<p>Model 1</p>", "<p>Model 2</p>", "<p>Model 3</p>", "<p>Model 4</p>"]}, "correct_response": ["d"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A weather company decided to collect daily temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entries, each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created four regression models in which they produced a residual plot for each of them.Which of the following models should they deploy?", "related_lectures": []}, {"_class": "assessment", "id": 68126092, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A well-known car\u2019s manufacturer decided to initiate a research and development project to produce self-driving cars in the near future. A team consisting of machine learning engineers and data engineers are supposed to produce a model by the end of the year. They should first attach a camera above the company\u2019s cars to collect images of the environment around them. The cameras should take separate frames, and these frames are saved to train them in a model later. Then, they should implement a model where it could distinguish different object types such as \u201cTrash bins\u201d, \u201cpeople\u201d, \u201cpavement\u201d and many more on a pixel-level.</p><p>Which SageMaker\u2019s algorithm should they implement to successfully train their model on this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Now this could be confusing somehow as \u201cImage classification\u201d , \u201cObject detection\u201d and \u201cSemantic segmentation\u201d nearly point at the same task. However, these 3 are different types of algorithms used for different scenarios.</p><ul><li><p>Image classification: The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. Despite telling us what objects are in the image, it does not tell us where these objects are within the image\u2019s dimension.</p></li><li><p>Object detection: The Amazon SageMaker Object Detection algorithm detects and classifies objects in images using a single deep neural network. It is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene <strong>using boundary boxes to determine the object\u2019s position</strong>.</p></li><li><p>Semantic segmentation: The SageMaker semantic segmentation algorithm provides <strong>a fine-grained, pixel-level approach</strong> to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes.</p></li></ul><p>AWS states: \u201c For comparison, the SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\">Image Classification Algorithm</a> is a supervised learning algorithm that analyses only whole images, classifying them into one of multiple output categories. The <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\">Object Detection Algorithm</a> is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box.</p><p>Because the semantic segmentation algorithm classifies every pixel in an image, it also provides information about the shapes of the objects contained in the image. The segmentation output is represented as a grayscale image, called a <em>segmentation mask</em>. A segmentation mask is a grayscale image with the same shape as the input image. \u201d</p><p><strong>CORRECT: </strong>\"Semantic segmentation algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Image classification algorithm.\" is incorrect.</p><p>Image classification algorithm only classifies the objects in the image; however, it does not provide pixel-level classification.</p><p><strong>INCORRECT:</strong> \"Object detection algorithm.\" is incorrect.</p><p>Object detection algorithm classifies the objects in the image using pre-defined dimensions of boundary boxes, so it won\u2019t be accurate as a pixel-level classification. Also, the task requires a pixel-level classification.</p><p><strong>INCORRECT:</strong> \"Object2Vec algorithm.\" is incorrect.</p><p>This is not a computer vision algorithm; however, it is a somehow general form of \u201cword2vec\u201d generalized to handle more complex objects than just words</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html</a></p>", "answers": ["<p>Image classification algorithm.</p>", "<p>Object detection algorithm.</p>", "<p>Semantic segmentation algorithm.</p>", "<p>Object2Vec algorithm</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A well-known car\u2019s manufacturer decided to initiate a research and development project to produce self-driving cars in the near future. A team consisting of machine learning engineers and data engineers are supposed to produce a model by the end of the year. They should first attach a camera above the company\u2019s cars to collect images of the environment around them. The cameras should take separate frames, and these frames are saved to train them in a model later. Then, they should implement a model where it could distinguish different object types such as \u201cTrash bins\u201d, \u201cpeople\u201d, \u201cpavement\u201d and many more on a pixel-level.Which SageMaker\u2019s algorithm should they implement to successfully train their model on this task?", "related_lectures": []}, {"_class": "assessment", "id": 68126094, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to start building a machine learning model for predictive maintenance purposes. The plan is to detect if a machine will fail up to 2 hours before failing in order to repair it immediately without further damages for the machine or the production line.</p><p>They decided to collect their own dataset from various sensors. The data collected resembled measurements such as \u201cTemperature\u201d, \u201cPressure\u201d and \u201cFrequency\u201d and a target label stating whether the machine will fail. A machine learning model should be constructed where given sensor measurements at ten timestamps, the model should predict whether the machine will fail.</p><p>Which neural network architecture should be adopted?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>RNN (Recurrent neural networks) are a type of neural networks which can predict an output based on a sequence of data in a given time period. They can be used in a range of applications such as predictive maintenance, machine translation and text generation</p><p><strong>CORRECT: </strong>\"Recurrent neural network.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Convolutional neural network.\" is incorrect.</p><p>Convolutional neural network \u201cCNN\u201d is a type of neural network architecture used in analyzing images.</p><p><strong>INCORRECT:</strong> \"KNN.\" is incorrect.</p><p>K-Nearest Neighbor \u201cKNN\u201d is not a neural network architecture type, however, it\u2019s an algorithm used for classification/regression.</p><p><strong>INCORRECT:</strong> \"Feed forward network.\" is incorrect.</p><p>This is the most basic type of feed forward network; however, we did not use it as a time-based pattern should be extracted from the data which is achieved by the RNN architecture.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/forecasting-time-series-with-dynamic-deep-learning-on-aws/\">https://aws.amazon.com/blogs/machine-learning/forecasting-time-series-with-dynamic-deep-learning-on-aws/</a></p>", "answers": ["<p>Convolutional neural network.</p>", "<p>Recurrent neural network.</p>", "<p>KNN.</p>", "<p>Feed forward network.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company wants to start building a machine learning model for predictive maintenance purposes. The plan is to detect if a machine will fail up to 2 hours before failing in order to repair it immediately without further damages for the machine or the production line.They decided to collect their own dataset from various sensors. The data collected resembled measurements such as \u201cTemperature\u201d, \u201cPressure\u201d and \u201cFrequency\u201d and a target label stating whether the machine will fail. A machine learning model should be constructed where given sensor measurements at ten timestamps, the model should predict whether the machine will fail.Which neural network architecture should be adopted?", "related_lectures": []}, {"_class": "assessment", "id": 68126096, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A medical college decided to transfer its courses to its online platform for its current students to learn remotely. The platform contains all the medical courses for each college year. Also, the syllabus of a current college year is somehow connected to that of the previous year, so the college decided to initiate a service in their website in which students could search for a medical word and the website should respond with the top 10 words that have the strongest semantic relationships with this word. The college claims that this method should make it easier for students to remember the context of the searched words for easier lookups.</p><p>Which algorithms should be used to perform this operation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cThe Word2vec algorithm maps words to high-quality distributed vectors. The resulting vector representation of a word is called a <em>word embedding</em>. Words that are semantically similar correspond to vectors that are close together. That way, word embeddings capture the semantic relationships between words.\u201d</p><p>Now that we have the words embeddings, we could have a great number of features representing this word, so we should decrease the number of features in order to visualize them in a low dimension space (2D or 3D). The resulting graph from drawing each word in the feature space packs words that are similar beside each other.</p><p><strong>CORRECT: </strong>\"Apply Blazingtext Word2Vec algorithm to capture the semantic relationships between current words. This will transform current words into vectors which should be passed to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Apply Blazingtext text classification algorithm to capture the semantic relationships between current words. This will transform current words into vectors which should be passed to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.\" is incorrect.</p><p>Word2Vec should be used to capture semantic relationships between words.</p><p><strong>INCORRECT:</strong> \" One-hot encode the words present and pass this encoding to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.\" is incorrect.</p><p>Word2Vec should be used instead of one-hot encoding which is basically a sparse matrix containing \u201c0\u201d and \u201c1\u201d and not capturing any relationships between words.</p><p><strong>INCORRECT:</strong> \" One-hot encode the words present and pass this encoding to the LDA algorithm to reduce the number of features and visualize the top 10 similar words.\" is incorrect.</p><p>Word2Vec should be used instead of one-hot encoding which is basically a sparse matrix containing \u201c0\u201d and \u201c1\u201d and not capturing any relationships between words. Also, LDA is an NLP algorithm which is used for topic modelling, so it isn\u2019t a dimensionality reduction algorithm.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html</a></p>", "answers": ["<p>Apply Blazingtext text classification algorithm to capture the semantic relationships between current words. This will transform current words into vectors which should be passed to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.</p>", "<p>Apply Blazingtext Word2Vec algorithm to capture the semantic relationships between current words. This will transform current words into vectors which should be passed to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.</p>", "<p>One-hot encode the words present and pass this encoding to the PCA algorithm to reduce the number of features and visualize the top 10 similar words.</p>", "<p>One-hot encode the words present and pass this encoding to the LDA algorithm to reduce the number of features and visualize the top 10 similar words.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A medical college decided to transfer its courses to its online platform for its current students to learn remotely. The platform contains all the medical courses for each college year. Also, the syllabus of a current college year is somehow connected to that of the previous year, so the college decided to initiate a service in their website in which students could search for a medical word and the website should respond with the top 10 words that have the strongest semantic relationships with this word. The college claims that this method should make it easier for students to remember the context of the searched words for easier lookups.Which algorithms should be used to perform this operation?", "related_lectures": []}, {"_class": "assessment", "id": 68126098, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a history of access logs stored in an Amazon S3 bucket. The company\u2019s data engineer filtered out the dataset to only contain pairs of (user/IP) where it contains the name of the IAM user along with the IP address used to access a specific service. The machine learning engineer is tasked with creating a model which takes an input of (users/ID) pairs and outputs a score. A threshold is to be set such that a score higher than this threshold should be flagged as anomalous.</p><p>Which algorithm should be used to train such model and what is the type of this machine learning problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cAmazon SageMaker IP Insights is an unsupervised learning algorithm that learns the usage patterns for IPv4 addresses. It is designed to capture associations between IPv4 addresses and various entities, such as user IDs or account numbers. You can use it to identify a user attempting to log into a web service from an anomalous IP address, for example. Or you can use it to identify an account that is attempting to create computing resources from an unusual IP address. Trained IP Insight models can be hosted at an endpoint for making real-time predictions or used for processing batch transforms.\u201d</p><p><strong>CORRECT: </strong>\" IP insights, unsupervised learning.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" IP insights, supervised learning.\" is incorrect.</p><p>IP insights is correct, however it\u2019s an unsupervised algorithm.</p><p><strong>INCORRECT:</strong> \" Latent Dirichlet Allocation, unsupervised learning.\" is incorrect.</p><p>Latent Dirichlet Allocation \u201cLDA\u201d is a Natural Language Processing algorithm used to organize documents into topics.</p><p><strong>INCORRECT:</strong> \" Latent Dirichlet Allocation, supervised learning.\" is incorrect.</p><p>Latent Dirichlet Allocation \u201cLDA\u201d is a Natural Language Processing algorithm used to organize documents into topics. Also, it\u2019s an unsupervised algorithm.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ip-insights.html</a></p>", "answers": ["<p>Latent Dirichlet Allocation, unsupervised learning.</p>", "<p>Latent Dirichlet Allocation, supervised learning.</p>", "<p>IP insights, supervised learning.</p>", "<p>IP insights, unsupervised learning.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has a history of access logs stored in an Amazon S3 bucket. The company\u2019s data engineer filtered out the dataset to only contain pairs of (user/IP) where it contains the name of the IAM user along with the IP address used to access a specific service. The machine learning engineer is tasked with creating a model which takes an input of (users/ID) pairs and outputs a score. A threshold is to be set such that a score higher than this threshold should be flagged as anomalous.Which algorithm should be used to train such model and what is the type of this machine learning problem?", "related_lectures": []}, {"_class": "assessment", "id": 68126100, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist is training on a classification model that classifies different types of sea creatures from their physical and biological features. The specialist has a massive data which contains different features for each creature. The dataset is currently on the specialist\u2019s private S3 bucket. It has over 20,000 rows where each row contains 35 different features of the creature and a label containing the target \u201cName of creature\u201d.</p><p>There are 20 different creatures such that each one has exactly 1000 high-resolution images. The dataset is ordered such that row 1-1000 contains the first creature, row 1001-2000 contains the second creature and so on. The specialist noticed that after 20 epochs using stochastic gradient descent, the model cannot extract a pattern to differentiate those creatures.</p><p>What is the most probable solution to this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cIn Amazon ML, you must shuffle your data because the SGD algorithm is influenced by the order of the rows in the training data. Shuffling your training data results in better ML models because it helps the SGD algorithm avoid solutions that are optimal for the first type of data it sees, but not for the full range of data. Shuffling mixes up the order of your data so that the SGD algorithm doesn't encounter one type of data for too many observations in succession.</p><p>If it sees only one type of data for many successive weight updates, the algorithm might not be able to correct the model weights for a new data type because the update might be too large. Additionally, when the data isn't presented randomly, it's difficult for the algorithm to find the optimal solution for all the data types quickly; in some cases, the algorithm might never find the optimal solution. Shuffling the training data helps the algorithm to converge on the optimal solution sooner.\u201d</p><p><strong>CORRECT: </strong>\" Shuffle the data before training.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Learning rate should be increased.\" is incorrect.</p><p>Learning rate affects the rate in which we update our parameters. The problem here is that the model learns a specific pattern in the first type and fine tune itself for this type for the first 1000 rows, then it would be hard to update the weights for further types. This is solved by shuffling the data.</p><p><strong>INCORRECT:</strong> \" More data per creature should be collected.\" is incorrect.</p><p>We cannot decide whether we need more data or not as the main problem here is that the model learns a specific pattern in the first type and fine tune itself for this type for the first 1000 rows, then it would be hard to update the weights for further types. This is solved by shuffling the data.</p><p><strong>INCORRECT:</strong> \" Increase the number of epochs.\" is incorrect.</p><p>By increasing the number of epochs, we are not directly solving the root cause of the problem, we are just wasting a lot of time in finding the best weights.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html</a></p>", "answers": ["<p>Learning rate should be increased.</p>", "<p>More data per creature should be collected.</p>", "<p>Increase the number of epochs.</p>", "<p>Shuffle the data before training.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist is training on a classification model that classifies different types of sea creatures from their physical and biological features. The specialist has a massive data which contains different features for each creature. The dataset is currently on the specialist\u2019s private S3 bucket. It has over 20,000 rows where each row contains 35 different features of the creature and a label containing the target \u201cName of creature\u201d.There are 20 different creatures such that each one has exactly 1000 high-resolution images. The dataset is ordered such that row 1-1000 contains the first creature, row 1001-2000 contains the second creature and so on. The specialist noticed that after 20 epochs using stochastic gradient descent, the model cannot extract a pattern to differentiate those creatures.What is the most probable solution to this problem?", "related_lectures": []}, {"_class": "assessment", "id": 68126102, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company collected information from its customers, with their consent, including age, spending patterns, etc. The company would like to segment these customers into different groups using k-means clustering algorithm, however, they do not know how many groups would be a great fit for all of them.</p><p>Which of the following figures could help identify the best number of segmented groups?</p><p>Figure 1:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-24_10-54-02-8ff0770079fb18b9080c4ed19edea9e7.png\"><p><br></p><p>Figure 2:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-24_10-54-02-c067f52e8fb600663ac86fd8bf6c9e89.png\"><p><br></p><p>Figure 3:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-24_10-54-02-89b7a3e2e212c751b239fdca95c8adeb.png\"><p>Figure 4:</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-24_10-54-02-28bbc2f93cf6de0e17fc70942b037dce.png\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Figure 3 is a line chart which is used to figure out the suitable \u201ck\u201d which is the number of segments. To plot this graph, a different number of \u201ck\u201d is plotted against a distortion score. The method used to find the most suitable k in this chart is called the <strong>elbow method.</strong></p><p><strong>CORRECT: </strong>\"Figure 3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Figure 1\" is incorrect.</p><p>This is a histogram chart which is used to find the frequencies of a range of data.</p><p><strong>INCORRECT:</strong> \"Figure 2\" is incorrect.</p><p>This is a bubble chart which is used to represent 3-D information on a 2-D scale.</p><p><strong>INCORRECT:</strong> \"Figure 4\" is incorrect.</p><p>This is a Pie chart which is used to represent data into sections where each section\u2019s area represents the ratio of the actual data to the whole dataset.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/k-means-clustering-with-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/k-means-clustering-with-amazon-sagemaker/</a></p>", "answers": ["<p>Figure 1</p>", "<p>Figure 2</p>", "<p>Figure 3</p>", "<p>Figure 4</p>"]}, "correct_response": ["c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A company collected information from its customers, with their consent, including age, spending patterns, etc. The company would like to segment these customers into different groups using k-means clustering algorithm, however, they do not know how many groups would be a great fit for all of them.Which of the following figures could help identify the best number of segmented groups?Figure 1:Figure 2:Figure 3:Figure 4:", "related_lectures": []}]}
5824444
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 68126022, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Machine Learning Specialist built a machine learning model to classify a given image into ten separate categories. The model achieved the following accuracies:</p><p>Training accuracy = 95%</p><p>Validation accuracy = 76%</p><p>Test accuracy = 73%</p><p>Taking into the consideration a domain expert accuracy of 97%, which of the following statements is correct?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The model clearly overfits the training data as the validation and test accuracies are far below the training accuracy so it does not generalize well to new data points. Also, the training accuracy is close enough to the domain expert accuracy, so the model performed well in predicting the relation (in the training dataset only).</p><p>Remember:</p><p><strong>High bias -&gt; underfitting (too simple) (The model has not established the perfect relation between inputs and outputs)</strong></p><p><strong>High variance -&gt; overfitting (too complex) (model not generalizing well to new data points)</strong></p><p><strong>CORRECT: </strong>\"The model suffers from low bias, high variance.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The model suffers from high bias, low variance.\" is incorrect.</p><p>The model is overfitting the data, so it has high variance. Also, the difference between the training accuracy and the domain expert accuracy is small so it has low bias.</p><p><strong>INCORRECT:</strong> \" The model suffers from high bias, high variance.\" is incorrect.</p><p>The difference between the training accuracy and the domain expert accuracy is small so it has low bias.</p><p><strong>INCORRECT:</strong> \" The model suffers from low bias, low variance.\" is incorrect.</p><p>The model is overfitting the data, so it has high variance.</p>", "answers": ["<p>The model suffers from high bias, low variance.</p>", "<p>The model suffers from low bias, high variance.</p>", "<p>The model suffers from high bias, high variance.</p>", "<p>The model suffers from low bias, low variance.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A Machine Learning Specialist built a machine learning model to classify a given image into ten separate categories. The model achieved the following accuracies:Training accuracy = 95%Validation accuracy = 76%Test accuracy = 73%Taking into the consideration a domain expert accuracy of 97%, which of the following statements is correct?", "related_lectures": []}, {"_class": "assessment", "id": 68126024, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning model has been trained and tested using Amazon SageMaker. The model achieved the business\u2019s metric and is ready to be deployed on an edge device.</p><p>Which services should be used to successfully optimize and deploy the model on this device? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Amazon SageMaker Neo is used to optimize machine learning models to be used on the cloud or edge devices, while AWS IOT Greengrass is responsible for deploying the machine learning model on edge devices.</p><p>Remember:</p><ul><li><p>NeO for model\u2019s Optimization.</p></li></ul><p><strong>CORRECT: </strong>\"SageMaker Neo <strong>and</strong> AWS IOT Greengrass.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"SageMaker Autopilot.\" is incorrect.</p><p>SageMaker Autopilot is used to automate the processes of building, training, and tuning the best machine learning model not for optimizing and deploying the model on edge devices.</p><p><strong>INCORRECT:</strong> \"AWS Glue.\" is incorrect.</p><p>AWS Glue is a managed ETL (Extract, Transform, Load) service.</p><p><strong>INCORRECT:</strong> \" SageMaker Ground Truth.\" is incorrect.</p><p>SageMaker Ground Truth in the pre-processing phase where it provides 3 methods to label the data:</p><ul><li><p>Mechanical Turk</p></li><li><p>Private workforce</p></li><li><p>Using 3rd party vendors.</p></li></ul><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/11/aws-iot-greengrass-now-supports-amazon-sagemaker-neo/\"><strong>https://aws.amazon.com/about-aws/whats-new/2018/11/aws-iot-greengrass-now-supports-amazon-sagemaker-neo/</strong></a></p>", "answers": ["<p>SageMaker Neo</p>", "<p>SageMaker Autopilot</p>", "<p>AWS IOT Greengrass</p>", "<p>AWS Glue</p>", "<p>SageMaker Ground Truth</p>"]}, "correct_response": ["a", "c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A machine learning model has been trained and tested using Amazon SageMaker. The model achieved the business\u2019s metric and is ready to be deployed on an edge device.Which services should be used to successfully optimize and deploy the model on this device? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 68126026, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has three types of data: old, raw, and transformed data. The company wants a cost-effective solution to store these types of data under those restrictions:</p><ul><li><p>Old data should not be altered and should be retrieved in a maximum of 48 hours.</p></li><li><p>Raw data has a random-access pattern.</p></li><li><p>Transformed data should be highly available and accessed immediately when retrieved.</p></li></ul><p>Which combination of storage classes should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The key to answering this question is to understand the attributes of each storage class:</p><ul><li><p>S3 Standard: Low retrieval time, high availability &gt;=3 AZ.</p></li><li><p>S3 Standard-IA: Low retrieval time, Low availability 1 AZ.</p></li><li><p>S3 Intelligent tiering: for random access patterns, Low retrieval time, high availability &gt;=3 AZ</p></li><li><p>S3 Glacier deep archive: High retrieval time(hours-days), high availability &gt;=3 AZ</p></li></ul><p>Therefore, it would be best to use S3 Glacier deep archive with a vault lock policy for old data, S3 Intelligent tiering for raw data, and S3 standard for transformed data.</p><p><strong>CORRECT: </strong>\"Use S3 Glacier deep archive with a vault lock policy for old data, S3 Intelligent tiering for raw data and S3 standard for transformed data.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use S3 Glacier deep archive for old data, S3 Intelligent tiering for raw data and S3 standard for transformed data.\" is incorrect.</p><p>A vault lock policy should be applied to the S3 Glacier deep archive as per the company\u2019s restrictions.</p><p><strong>INCORRECT:</strong> \"Use S3 Glacier deep archive for old data, S3 Standard-IA for raw data and S3 standard for transformed data.\" is incorrect.</p><p>Raw data has a random-access pattern, so Intelligent tiering is best suited for its storage.</p><p><strong>INCORRECT:</strong> \"Use S3 Glacier deep archive with a vault lock policy for old data, S3 Standard-IA for raw data and S3 standard for transformed data.\" is incorrect.</p><p>Raw data has a random-access pattern, so Intelligent tiering is best suited for its storage.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>", "answers": ["<p>Use S3 Glacier deep archive for old data, S3 Intelligent tiering for raw data and S3 standard for transformed data.</p>", "<p>Use S3 Glacier deep archive for old data, S3 Standard-IA for raw data and S3 standard for transformed data.</p>", "<p>Use S3 Glacier deep archive with a vault lock policy for old data, S3 Intelligent tiering for raw data and S3 standard for transformed data.</p>", "<p>Use S3 Glacier deep archive with a vault lock policy for old data, S3 Standard-IA for raw data and S3 standard for transformed data.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company has three types of data: old, raw, and transformed data. The company wants a cost-effective solution to store these types of data under those restrictions:Old data should not be altered and should be retrieved in a maximum of 48 hours.Raw data has a random-access pattern.Transformed data should be highly available and accessed immediately when retrieved.Which combination of storage classes should be used?", "related_lectures": []}, {"_class": "assessment", "id": 68126028, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A governmental dataset contains the following columns: \u201cID\u201d, \u201cRace\u201d \u201cGender\u201d, \u201cAge group\u201d, \u201ceducational level\u201d, \u201cincome\u201d. The dataset contains 100,000 entries, each corresponding to information about a person in a specific city. 10,000 educational level data are found missing which resembles 10% of the total data.</p><p>The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias. Which action should be taken to overcome this problem?</p><p>What is the most accurate action to be taken?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Missing data are one of the main problems which are inevitable in most of the datasets. There are many ways to overcome this problem such as:</p><ol><li><p>Deleting the missing rows entirely. While this is the easiest solution, it simply deletes many valuable information.</p></li><li><p>Imputing missing data using mean. This is a bad technique to be used if the dataset contains outliers.</p></li><li><p>Imputing missing data using mode. This is suitable for categorical features.</p></li><li><p>Imputing missing data using KNN. This is suitable for numerical features.</p></li><li><p>Imputing missing data using deep learning. This is suitable and accurate for categorical features.</p></li></ol><p><strong>CORRECT: </strong>\" Impute missing data using deep learning.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Impute missing data using KNN.\" is incorrect.</p><p>Imputation using KNN is best suitable for numerical features not categorical.</p><p><strong>INCORRECT:</strong> \" Impute missing data using the mode of the column.\" is incorrect.</p><p>This is not the most accurate action as it can introduce bias.</p><p><strong>INCORRECT:</strong> \" Delete the rows corresponding to the 10,000 missing income data.\" is incorrect.</p><p>10% of the data is missing so this is not the most accurate action.</p>", "answers": ["<p>Delete the rows corresponding to the 10,000 missing income data.</p>", "<p>Impute missing data using the mode of the column.</p>", "<p>Impute missing data using KNN.</p>", "<p>Impute missing data using deep learning.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A governmental dataset contains the following columns: \u201cID\u201d, \u201cRace\u201d \u201cGender\u201d, \u201cAge group\u201d, \u201ceducational level\u201d, \u201cincome\u201d. The dataset contains 100,000 entries, each corresponding to information about a person in a specific city. 10,000 educational level data are found missing which resembles 10% of the total data.The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias. Which action should be taken to overcome this problem?What is the most accurate action to be taken?", "related_lectures": []}, {"_class": "assessment", "id": 68126030, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a massive dataset containing images of different animals. The company is building an XGBoost classification model. The data currently resides in an S3 bucket in a single folder called \u201cDataset\u201d. A data engineer should split the data into training and validation for future training on Amazon SageMaker.</p><p>What is the correct sequence of events to perform this action?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Training and validation datasets\u2019 locations should be provided separately to model.fit() method for training the data. XGBoost can take a csv file as an input provided that the headers had been removed.</p><p><strong>CORRECT: </strong>\" Download the data in Amazon SageMaker, split the data into train and validation folders and re-upload the data as csv files, while removing the headers, into Amazon S3. Use the model.fit() function and provide the datasets separately.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Download the data in Amazon SageMaker, split the data into train and validation folders and re-upload the data as csv files, while maintaining the headers, into Amazon S3. Use the model.fit() function and provide the datasets separately.\" is incorrect.</p><p>Headers should be removed from the training and validation csv files.</p><p><strong>INCORRECT:</strong> \" Use the model.fit() function and provide the number of training and validation examples to be extracted from the \u201cDataset\u201d folder.\" is incorrect.</p><p>model.fit() cannot extract training and validation data. Data should be provided separately in 2 channels.</p><p><strong>INCORRECT:</strong> \" Use the model.fit() function and provide the percentage of training and validation examples to be extracted from the \u201cDataset\u201d folder.\" is incorrect.</p><p>model.fit() cannot extract training and validation data. Data should be provided separately in 2 channels.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html</a></p>", "answers": ["<p>Download the data in Amazon SageMaker, split the data into train and validation folders and re-upload the data as csv files, while maintaining the headers, into Amazon S3. Use the model.fit() function and provide the datasets separately.</p>", "<p>Download the data in Amazon SageMaker, split the data into train and validation folders and re-upload the data as csv files, while removing the headers, into Amazon S3. Use the model.fit() function and provide the datasets separately.</p>", "<p>Use the model.fit() function and provide the number of training and validation examples to be extracted from the \u201cDataset\u201d folder.</p>", "<p>Use the model.fit() function and provide the percentage of training and validation examples to be extracted from the \u201cDataset\u201d folder.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company has a massive dataset containing images of different animals. The company is building an XGBoost classification model. The data currently resides in an S3 bucket in a single folder called \u201cDataset\u201d. A data engineer should split the data into training and validation for future training on Amazon SageMaker.What is the correct sequence of events to perform this action?", "related_lectures": []}, {"_class": "assessment", "id": 68126032, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer is training a multivariate regression model to predict prices of used cars. The model takes in 30 features containing the engine type, engine size, car\u2019s severe scratches and miles driven. The model is about to begin training which could take up to 72 hours. The engineer wants to notified by email if the machine learning model overfits during training to take the appropriate measures.</p><p>How should he set up this alarm?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. Alarms are set using CloudWatch and can be further sent to an SNS topic to notify users. On the other hand, CloudTrail is used to audit activity to track user activity through API calls.</p><p>SNS can push notifications to subscribers, and it could be integrated with Amazon CloudWatch to alarm them for example when a metric exceeds a pre-set threshold. SQS is a queuing service where a system must pull the message in queue in order to process it. In this example, a user wants to send an alarm to an email; therefore, an SNS is used.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_10-21-51-e3b265b860f2ee29a7eeb19dd157980a.jpg\"><p><strong>CORRECT: </strong>\"Create an alarm using Amazon CloudWatch with an SNS topic.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an alarm using Amazon CloudTrail with an SNS topic.\" is incorrect.</p><p>CloudTrail is used for tracking user\u2019s activity through API calls.</p><p><strong>INCORRECT:</strong> \"Create an alarm using Amazon CloudTrail with an SQS topic.\" is incorrect.</p><p>CloudTrail is used for tracking user\u2019s activity through API calls. SQS is a queue service not a publishing service.</p><p><strong>INCORRECT:</strong> \"Create an alarm using Amazon CloudWatch with an SQS topic.\" is incorrect.</p><p>While CloudWatch is correct, SNS should be used as a publishing service rather than SQS.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html</a></p>", "answers": ["<p>Create an alarm using Amazon CloudTrail with an SNS topic.</p>", "<p>Create an alarm using Amazon CloudTrail with an SQS topic.</p>", "<p>Create an alarm using Amazon CloudWatch with an SQS topic.</p>", "<p>Create an alarm using Amazon CloudWatch with an SNS topic.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A machine learning engineer is training a multivariate regression model to predict prices of used cars. The model takes in 30 features containing the engine type, engine size, car\u2019s severe scratches and miles driven. The model is about to begin training which could take up to 72 hours. The engineer wants to notified by email if the machine learning model overfits during training to take the appropriate measures.How should he set up this alarm?", "related_lectures": []}, {"_class": "assessment", "id": 68126034, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data scientist trained a binary classification model for spam detection. The provided training dataset is balanced, and the scientist wants to measure the overall performance of the model while emphasizing on both false positives and false negatives.</p><p>Which metric should the data scientist use for the model\u2019s evaluation?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>F1 is used to measure the overall performance of the model taking into consideration false positives and false negatives. Recall emphasizes on false negatives and precision emphasizes on false positives. Accuracy is better used when true positives and true negatives are more important.</p><p><strong>CORRECT: </strong>\"F1 score\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Accuracy\" is incorrect.</p><p>Accuracy is better used when true positives and true negatives are more important.</p><p><strong>INCORRECT:</strong> \"Precision\" is incorrect.</p><p>Precision indicates if the model mistakes negative examples a lot.</p><p><strong>INCORRECT:</strong> \"Recall\" is incorrect.</p><p>Recall indicates if the model mistakes positive examples a lot.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>Accuracy.</p>", "<p>Precision.</p>", "<p>Recall.</p>", "<p>F1 score.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A data scientist trained a binary classification model for spam detection. The provided training dataset is balanced, and the scientist wants to measure the overall performance of the model while emphasizing on both false positives and false negatives.Which metric should the data scientist use for the model\u2019s evaluation?", "related_lectures": []}, {"_class": "assessment", "id": 68126036, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An effective vaccine for a specific virus has just been discovered by a government. The government decided to vaccinate all its infected citizens in less than a month to halt the spread of the virus. The government ordered all its citizens to take the virus test and at the same time demanded that every local or private hospital should send the test data immediately (real-time) to a central governmental database. The test produces numerical values in which infected people could be represented as anomalies to the data.</p><p>The government wants to use the cloud to implement a fast and scalable solution to ingest the data from different hospitals in real-time, detect infected citizens, transform the data format into JSON and stream the data to a data lake source to further visualize the data.</p><p>Which services should the government use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Kinesis Data Analytics provides a function (RANDOM_CUT_FOREST) that can assign an anomaly score to each record based on values in the numeric columns. Amazon Kinesis Data Streams ingests data in <strong>real-time</strong>, while Amazon Kinesis Firehose ingests data in <strong>near</strong> <strong>real-time</strong>. Amazon Firehose has an option to transform the data using a lambda function.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_10-24-16-08bd83064e5b471396de23cd63807080.jpg\"><p><strong>CORRECT: </strong>\" Use Kinesis Data Streams to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Firehose which will transform the data and transfer it to Amazon S3.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use Kinesis Firehose to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Firehose which will transform the data and transfer it to Amazon S3.\" is incorrect.</p><p>The solution requires data to be ingested in real time, so Amazon Kinesis Data Streams should be used as the first streaming source in the problem.</p><p><strong>INCORRECT:</strong> \" Use Kinesis Data Firehose to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Data Streams which will transform the data and transfer it to Amazon S3.\" is incorrect.</p><p>The solution requires data to be ingested in real time, so Amazon Kinesis Data Streams should be used as the first streaming source in the problem. Also, Kinesis Data Streams cannot transform data.</p><p><strong>INCORRECT:</strong> \" Use Kinesis Data Streams to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Firehose to discover anomalies, transform the data and transfer it to Amazon S3.\" is incorrect.</p><p>Amazon Kinesis Firehose does not discover anomalies. Amazon Kinesis Data Analytics is the streaming service which has a \u201cRANDOM_CUT_FOREST\u201d built in it to discover anomalies.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html</a></p>", "answers": ["<p>Use Kinesis Data Streams to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Firehose which will transform the data and transfer it to Amazon S3.</p>", "<p>Use Kinesis Firehose to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Firehose which will transform the data and transfer it to Amazon S3.</p>", "<p>Use Kinesis Data Firehose to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Analytics to discover anomalies and stream the output to Kinesis Data Streams which will transform the data and transfer it to Amazon S3.</p>", "<p>Use Kinesis Data Streams to stream the data to the cloud. Use \u201cRANDOM_CUT_FOREST\u201d function in Kinesis Data Firehose to discover anomalies, transform the data and transfer it to Amazon S3.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "An effective vaccine for a specific virus has just been discovered by a government. The government decided to vaccinate all its infected citizens in less than a month to halt the spread of the virus. The government ordered all its citizens to take the virus test and at the same time demanded that every local or private hospital should send the test data immediately (real-time) to a central governmental database. The test produces numerical values in which infected people could be represented as anomalies to the data.The government wants to use the cloud to implement a fast and scalable solution to ingest the data from different hospitals in real-time, detect infected citizens, transform the data format into JSON and stream the data to a data lake source to further visualize the data.Which services should the government use?", "related_lectures": []}, {"_class": "assessment", "id": 68126038, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A well-known electronics company will release its flagship product this year along with other products. The company wants to estimate the number of people pre-ordering their products and decides to send a public document form for people to fill in their names, desired product, and some relative information. The document is sent to an S3 bucket for further analysis. The company wants to deduplicate the records as some people most probably entered their data twice.</p><p>What is the easiest method to perform their desired action?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Glue could simply perform ETL operations on the data.</p><p>AWS Glue Crawler can identify data\u2019s schema while remaining in the S3 bucket.</p><p>AWS Glue FindMatches ML can spot duplicates in the data.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_10-25-45-c9c00261c3c170998b29532c707ee726.jpg\"><p><strong>CORRECT: </strong>\"Use AWS Glue\u2019s FindMatches ML to identify duplicates after identifying the data\u2019s schema using Glue crawler.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Setup an RDS table with a suitable structure, copy the data and perform SQL queries to remove duplicates.\" is incorrect.</p><p>RDS requires provisioning servers in advance. Furthermore, the customer should write the SQL queries to remove duplicates.</p><p><strong>INCORRECT:</strong> \"Setup a DynamoDB table, copy the data and perform SQL queries to remove duplicates.\" is incorrect.</p><p>DynamoDB is an unstructured database, and the document contains structured data.</p><p><strong>INCORRECT:</strong> \" Use AWS Macie to identify duplicates after identifying the data\u2019s schema using Glue crawler.\" is incorrect.</p><p>AWS Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data. It is not used for identifying duplicates.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/\">https://aws.amazon.com/about-aws/whats-new/2019/08/aws-glue-provides-findmatches-ml-transform-to-deduplicate/</a></p>", "answers": ["<p>Setup an RDS table with a suitable structure, copy the data and perform SQL queries to remove duplicates.</p>", "<p>Setup a DynamoDB table, copy the data and perform SQL queries to remove duplicates.</p>", "<p>Use AWS Macie to identify duplicates after identifying the data\u2019s schema using Glue crawler.</p>", "<p>Use AWS Glue\u2019s FindMatches ML to identify duplicates after identifying the data\u2019s schema using Glue crawler.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A well-known electronics company will release its flagship product this year along with other products. The company wants to estimate the number of people pre-ordering their products and decides to send a public document form for people to fill in their names, desired product, and some relative information. The document is sent to an S3 bucket for further analysis. The company wants to deduplicate the records as some people most probably entered their data twice.What is the easiest method to perform their desired action?", "related_lectures": []}, {"_class": "assessment", "id": 68126040, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to build a model to predict whether a customer will buy a product from its online store based on some features. A data scientist was hired for this role in which he started collecting data from various sources to design a suitable model. He decided to use a complex model with all the features included and at the same time apply regularization. All features present in the model are important and he does not want to lose any feature while predicting the outcome.</p><p>Which type of regularization technique should he use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>L1 regularization is a regularization method which performs feature selection (some feature\u2019s coordinates can approach 0).</p><p>L2 regularization is a regularization method in which all features remain considered.</p><p>Xavier and Adam are not regularization methods.</p><p><strong>CORRECT: </strong>\"L2 regularization\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"L1 regularization\" is incorrect.</p><p><strong>INCORRECT:</strong> \"Xavier\" is incorrect.</p><p>Xavier is a weight initialization technique not a regularization method.</p><p><strong>INCORRECT:</strong> \"Adam\" is incorrect.</p><p>Adam is an optimizer not a regularization method.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html</a></p>", "answers": ["<p>L1 regularization.</p>", "<p>L2 regularization.</p>", "<p>Xavier.</p>", "<p>Adam.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company wants to build a model to predict whether a customer will buy a product from its online store based on some features. A data scientist was hired for this role in which he started collecting data from various sources to design a suitable model. He decided to use a complex model with all the features included and at the same time apply regularization. All features present in the model are important and he does not want to lose any feature while predicting the outcome.Which type of regularization technique should he use?", "related_lectures": []}, {"_class": "assessment", "id": 68126042, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer is training a multivariate regression model to predict house prices. The engineer uses Amazon SageMaker for training the model using TensorFlow custom code. The model takes in 30 features including, but not limited to, \u201clocation\u201d, \u201cHome size\u201d, \u201cCondition\u201d and \u201cAge\u201d.</p><p>The model is about to begin training and could train for a long time. The engineer wants to be alerted via a cell phone (SMS) if the machine learning model overfits during training to take the appropriate measures.</p><p>How should the alarm be set up?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The debugger rules could act when specific training issues occur such as overfitting,<strong> </strong>overtraining, class imbalance and many others. The action could be a message sent to a subscriber\u2019s email or an SMS or even an order to stop the training job immediately.</p><p><strong>CORRECT: </strong>\"Configure debugger rules with built in actions using the SageMaker SDK to act when overfitting occurs, set up an SNS topic to send a notification via SMS and subscribe to this topic. Use the appropriate IAM roles for the SMDebugRules.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Create an alarm using Amazon CloudTrail with an SNS topic which includes an email.\" is incorrect.</p><p>CloudTrail is used for tracking user\u2019s activity through API calls.</p><p><strong>INCORRECT:</strong> \"Create an alarm using Amazon CloudTrail with an SQS topic which includes an email.\" is incorrect.</p><p>CloudTrail is used for tracking user\u2019s activity through API calls. SQS is a queue service not a publish one.</p><p><strong>INCORRECT:</strong> \"Configure debugger rules with built in actions using the SageMaker SDK to act when overfitting occurs, set up an SNS topic to send a notification via SMS and subscribe to this topic. Use the appropriate IAM roles for the SMDebugRules.\" is incorrect.</p><p>SQS is a queue service used for pulling messages, not a publishing service that pushes messages.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-actions.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-actions.html</a></p>", "answers": ["<p>Create an alarm using Amazon CloudTrail with an SNS topic which includes an email.</p>", "<p>Create an alarm using Amazon CloudTrail with an SQS topic which includes an email.</p>", "<p>Configure debugger rules with built in actions using the SageMaker SDK to act when overfitting occurs, set up an SNS topic to send a notification via SMS and subscribe to the topic. Use the appropriate IAM roles for the SMDebugRules.</p>", "<p>Configure debugger rules with built in actions using the SageMaker SDK to act when overfitting occurs, set up an SQS topic to send a notification via SMS and subscribe to this topic. Use the appropriate IAM roles for the SMDebugRules.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A machine learning engineer is training a multivariate regression model to predict house prices. The engineer uses Amazon SageMaker for training the model using TensorFlow custom code. The model takes in 30 features including, but not limited to, \u201clocation\u201d, \u201cHome size\u201d, \u201cCondition\u201d and \u201cAge\u201d.The model is about to begin training and could train for a long time. The engineer wants to be alerted via a cell phone (SMS) if the machine learning model overfits during training to take the appropriate measures.How should the alarm be set up?", "related_lectures": []}, {"_class": "assessment", "id": 68126044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A medical company currently owns 10 hospitals across different states. Each hospital is required to send the patients records securely to the company\u2019s S3 bucket which is within a VPC. The company wants to apply some transformations along with extracting the schema of the data and the results should be uploaded to the same bucket. The company wants to use a managed architecture to reduce operational overhead.</p><p>Which service should be used to achieve the company\u2019s requirement?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data. It could be used for ETL purposes as well and it\u2019s a totally managed service in which we don\u2019t have access to the infrastructure. Therefore, Glue is a serverless service which will extract metadata and apply transformations</p><p><strong>CORRECT: </strong>\"AWS Glue\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"AWS EMR\" is incorrect.</p><p>This is not a managed service as we have access to the underlying clusters.</p><p><strong>INCORRECT:</strong> \"AWS Pipeline\" is incorrect.</p><p>The company requires a managed service for extracting metadata and applying transformation.</p><p><strong>INCORRECT:</strong> \"AWS Autopilot\" is incorrect.</p><p>SageMaker Autopilot is used to automate the processes of building, training, and tuning the best machine learning model not for optimizing it. It is not an ETL service.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html\">https://docs.aws.amazon.com/glue/latest/dg/schema-registry.html</a></p><p><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming.html\">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming.html</a></p>", "answers": ["<p>AWS EMR.</p>", "<p>AWS Glue.</p>", "<p>AWS Pipeline.</p>", "<p>AWS Autopilot.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A medical company currently owns 10 hospitals across different states. Each hospital is required to send the patients records securely to the company\u2019s S3 bucket which is within a VPC. The company wants to apply some transformations along with extracting the schema of the data and the results should be uploaded to the same bucket. The company wants to use a managed architecture to reduce operational overhead.Which service should be used to achieve the company\u2019s requirement?", "related_lectures": []}, {"_class": "assessment", "id": 68126046, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a ready-made model that is used to detect a specific product from the production lines using a camera. The model classifies an image taken whether it has this product or not and it uses a CNN architecture. They just released a new product and would also like to have a model which recognizes it.</p><p>What is the most time efficient method the company should try?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is a good example of transfer learning where we already have a pre-trained model with its artifacts. The new task should be similar to the previous one when using transfer learning. The key point is that the layers near the input have already learnt how to find vertical, horizontal edges and overall shapes, so we only need to train the last layers in order to fine tune it to classify the new product correctly.</p><p><strong>CORRECT: </strong>\" Freeze all layers\u2019 weights except for the last 2 layers including the output and re-train the model.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Change the model\u2019s architecture as well as re-initialize all the weights and train the new model.\" is incorrect.</p><p>This is not the most time efficient solution, as we are simply deleting all the previous knowledge.</p><p><strong>INCORRECT:</strong> \" Freeze the last 2 layers\u2019 weights including the output and re-train the model.\" is incorrect.</p><p>The layers near the output are responsible for distinguishing the whole shape, so we need to train them in order to classify the new product correctly.</p><p><strong>INCORRECT:</strong> \" Leave the architecture unaltered as well as re-initialize all the weights and train the new model.\" is incorrect.</p><p>Re-initializing the whole weights is not the most time efficient solution as it will not use the previous knowledge learned from the previous model.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/IC-HowItWorks.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/IC-HowItWorks.html</a></p>", "answers": ["<p>Freeze all layers\u2019 weights except for the last 2 layers including the output and re-train the model.</p>", "<p>Change the model\u2019s architecture as well as re-initialize all the weights and train the new model.</p>", "<p>Freeze the last 2 layers\u2019 weights including the output and re-train the model.</p>", "<p>Leave the architecture unaltered as well as re-initialize all the weights and train the new model.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company has a ready-made model that is used to detect a specific product from the production lines using a camera. The model classifies an image taken whether it has this product or not and it uses a CNN architecture. They just released a new product and would also like to have a model which recognizes it.What is the most time efficient method the company should try?", "related_lectures": []}, {"_class": "assessment", "id": 68126048, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A weather company decided to collect daily data temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entry each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created a linear regression model using SageMaker\u2019s linear learner algorithm in which they produced a residual plot to gain insights.</p><p>Which correct insight could be deducted from the graph below?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_10-30-39-ad3b4c5fdf45ff68c7d1279e55feb898.JPG\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The residual plot represents the error between the actual values and the predicted values, so a negative residual means that the model over-estimated the actual value for example: Temperature prediction = 20, Actual temperature = 50. On the other hand, positive residual means that the model under-estimated the actual value. Residual = Actual value \u2013 Predicted value. Here, we could clearly observe that the model under-estimates the actual values at the beginning, then over-estimates, then under-estimates the actual values again following a non-linear pattern which could be resolved using a non-linear algorithm.</p><p><strong>CORRECT: </strong>\" The model should use a non-linear algorithm instead.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" The model is suitable as it perfectly fits the values.\" is incorrect.</p><p>The model does not fit the values as sometimes it under-estimates and sometimes it over-estimates the actual value.</p><p><strong>INCORRECT:</strong> \" The model suffers from some outliers.\" is incorrect.</p><p>The model does not have outliers.</p><p><strong>INCORRECT:</strong> \" As the prediction value becomes higher, the error increase.\" is incorrect.</p><p>There is an upward then downward residual trend when the prediction values increase.</p>", "answers": ["<p>The model should use a non-linear algorithm instead.</p>", "<p>The model is suitable as it perfectly fits the values.</p>", "<p>The model suffers from some outliers.</p>", "<p>As the prediction value becomes higher, the error increase.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A weather company decided to collect daily data temperature data from various sources across a time span of 5 years. The data engineers managed to acquire 20 features for each temperature value. The dataset now contains almost 2,000 data entry each containing 20 features of the day and the average temperature recorded in that day. The machine learning team created a linear regression model using SageMaker\u2019s linear learner algorithm in which they produced a residual plot to gain insights.Which correct insight could be deducted from the graph below?", "related_lectures": []}, {"_class": "assessment", "id": 68126050, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A hypermarket wants to construct a computer vision model to classify different product images into 40 unique products to organize their warehouse using robots. An image dataset was already collected by the hypermarket and a machine learning specialist was hired for this role. The specialist trained the model on Amazon SageMaker using its ready-made Pytorch container.</p><p>The model should be deployed on a SageMaker endpoint in which the hypermarket could request it anytime from any of its branches. The model should be available throughout the day as it will be used by robots. The model consists of a pre-processing, predictions, and post-processing containers.</p><p>Which method should the specialist use for inference deployment?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The model needs to be available throughout the day, so we should choose real-time inference over batch inference. In batch inferencing, we get inferences on an entire dataset which is usually run on a schedule like every day or ever week. Both real-time inferencing and batch transform supports running an inference pipeline where it contains all the model\u2019s containers such as \u201cpre-processing\u201d, \u201cpredictions\u201d and \u201cpost-processing\u201d containers. All the containers of the same model should reside on the same EC2 instance.</p><p><strong>CORRECT: </strong>\" Use real-time inference with inference pipelines. He should deploy the model\u2019s containers where all the containers should be on the same EC2 instance.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use real-time inference with inference pipelines. He should deploy the model\u2019s containers where each container should be deployed on a separate EC2 instance.\" is incorrect.</p><p>Model\u2019s containers should run on the same instance.</p><p><strong>INCORRECT:</strong> \" Use Batch transform with inference pipelines to initiate an EC2 instance carrying the 3 containers.\" is incorrect.</p><p>Real-time inference should be used as the inferencing should be done throughout the day.</p><p><strong>INCORRECT:</strong> \" Use Batch transform with inference pipelines to initiate 3 instances each carrying a container.\" is incorrect.</p><p>Real-time inference should be used as the inferencing should be done throughout the day.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html</a></p>", "answers": ["<p>Use real-time inference with inference pipelines. He should deploy the model\u2019s containers where each container should be deployed on a separate EC2 instance.</p>", "<p>Use Batch transform with inference pipelines to initiate an EC2 instance carrying the 3 containers.</p>", "<p>Use real-time inference with inference pipelines. He should deploy the model\u2019s containers where all the containers should be on the same EC2 instance.</p>", "<p>Use Batch transform with inference pipelines to initiate 3 instances each carrying a container.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A hypermarket wants to construct a computer vision model to classify different product images into 40 unique products to organize their warehouse using robots. An image dataset was already collected by the hypermarket and a machine learning specialist was hired for this role. The specialist trained the model on Amazon SageMaker using its ready-made Pytorch container.The model should be deployed on a SageMaker endpoint in which the hypermarket could request it anytime from any of its branches. The model should be available throughout the day as it will be used by robots. The model consists of a pre-processing, predictions, and post-processing containers.Which method should the specialist use for inference deployment?", "related_lectures": []}, {"_class": "assessment", "id": 68126052, "assessment_type": "multi-select", "prompt": {"question": "<p>A telecommunication company has a gigantic database containing some features of their customers including but not limited to \u201ccontract_type\u201d, \u201cgender\u201d, \u201clocation\u201d, \u201cmonthly_charges\u201d. The company lately suffered from massive losses due to customers leaving the company. They want to build a model to predict if a customer will churn in the future so as to provide him/her with incentives. The company will use an algorithm from Amazon SageMaker to train their model.</p><p>Which algorithms are best suited for this use case? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>This is a supervised binary classification algorithm as the company wants to predict if the customer will churn or not. The reason why it\u2019s considered a supervised algorithm is that it has features to learn from when trying to predict the target label. XGBoost and Linear Learner algorithms are commonly used in classification problems.</p><p><strong>CORRECT: </strong>\"XGBoost algorithm <strong>and</strong> Linear Learner algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"DeepAR algorithm.\" is incorrect.</p><p>DeepAR is used for forecasting one dimensional time series data using a Recurrent Neural Network (RNN).</p><p><strong>INCORRECT:</strong> \"Latent Dirichlet Allocation.\" is incorrect.</p><p>Latent Dirichlet Allocation is used for NLP problems where it is used for topic modelling.</p><p><strong>INCORRECT:</strong> \"Semantic segmentation.\" is incorrect.</p><p>Semantic Segmentation is used for computer vision problems to classify objects by pixels.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html</a></p>", "answers": ["<p>DeepAR algorithm.</p>", "<p>XGBoost algorithm.</p>", "<p>Latent Dirichlet Allocation.</p>", "<p>Linear Learner algorithm.</p>", "<p>Semantic segmentation.</p>"]}, "correct_response": ["b", "d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A telecommunication company has a gigantic database containing some features of their customers including but not limited to \u201ccontract_type\u201d, \u201cgender\u201d, \u201clocation\u201d, \u201cmonthly_charges\u201d. The company lately suffered from massive losses due to customers leaving the company. They want to build a model to predict if a customer will churn in the future so as to provide him/her with incentives. The company will use an algorithm from Amazon SageMaker to train their model.Which algorithms are best suited for this use case? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 68126054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An electronics company has a dataset of its past sales for each of its products which includes \u201cSmart phones\u201d, \u201cSmart watches\u201d, \u201cTVs\u201d and much more. The dataset contains products which are accompanied with factors affecting their demand. For instance, \u201cSmart phones\u201d has factors affecting their demand such as \u201cseason\u201d, \u201ccompetitor\u2019s price\u201d, \u201cstore location\u201d and many more. The company wants to predict the future demand of each of its products. However, they do not currently have a specialist in this field.</p><p>Which service should the company use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The closest answers for forecasting using time series are SageMaker\u2019s DeepAR and AWS forecast. Since the company does not have a machine learning background, therefore AWS forecast is more suitable for this use case.</p><p>AWS states: \u201cForecast uses machine learning to combine time series data with additional variables to build forecasts. Forecast requires no machine learning experience to get started. You only need to provide historical data, plus any additional data that you believe may impact your forecasts.\u201d</p><p><strong>CORRECT: </strong>\" AWS forecast.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon SageMaker\u2019s DeepAR.\" is incorrect.</p><p>DeepAR is indeed used in forecasting using time series data, however, the company has no machine learning expertise and DeepAR requires some knowledge in this field.</p><p><strong>INCORRECT:</strong> \"AWS factorization machines.\" is incorrect.</p><p>Factorization machines are used for classification or regression problems where it is best suited for sparse data.</p><p><strong>INCORRECT:</strong> \"AWS PCA.\" is incorrect.</p><p>PCA is a dimensionality reduction algorithm.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/forecast/features/\">https://aws.amazon.com/forecast/features/</a></p>", "answers": ["<p>Amazon SageMaker\u2019s DeepAR.</p>", "<p>AWS factorization machines.</p>", "<p>AWS forecast.</p>", "<p>AWS PCA.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "An electronics company has a dataset of its past sales for each of its products which includes \u201cSmart phones\u201d, \u201cSmart watches\u201d, \u201cTVs\u201d and much more. The dataset contains products which are accompanied with factors affecting their demand. For instance, \u201cSmart phones\u201d has factors affecting their demand such as \u201cseason\u201d, \u201ccompetitor\u2019s price\u201d, \u201cstore location\u201d and many more. The company wants to predict the future demand of each of its products. However, they do not currently have a specialist in this field.Which service should the company use?", "related_lectures": []}, {"_class": "assessment", "id": 68126056, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to build a custom sentiment analysis model to train on the reviews of their clients as some clients still use old English language words which are not used anymore by the modern language. A machine learning specialist was hired for this role and was handed a dataset containing one paragraph consisting of thousands of lines and the sentiment of each sentence is given in another file. The specialist should train a custom model to recognize the sentiments of the future reviews.</p><p>Which services/methods should the specialist use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Blazingtext algorithm has 2 modes:</p><ul><li><p>Word2Vec: for producing word embeddings.</p></li><li><p>Text classification: for classifying text into labels.</p></li></ul><p>The training/validation file should contain a training sentence per line along with the labels. Labels are words that are prefixed by the string <em>__label__</em></p><p><em>Example: </em>__label0__ \u201cI had a bad experience.\u201d</p><p>Another format called the augmented manifest format could be used in which it enables you to do training in pipe mode without needing to create RecordIO files. While using the format, an S3 manifest file needs to be generated that contains the list of sentences and their corresponding labels. The manifest file format should be in <a href=\"http://jsonlines.org/\">JSON Lines</a> format in which each line represents one sample. The sentences are specified using the source tag and the label can be specified using the label tag. Both source and label tags should be provided under the AttributeNames parameter value as specified in the request.\u201d</p><p>Example: {\"source\":\"linux ready for prime time , intel says , despite all the linux hype\", \"label\":1}</p><p><br></p><p><strong>CORRECT: </strong>\" Tokenize the document into one sentence per line with its corresponding label, pass the inputs to Blazingtext text classification algorithm and train a custom model to detect the sentiment.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Tokenize the document into one sentence per line with its corresponding label, pass the inputs to Blazingtext Word2Vec algorithm and train a custom model to detect the sentiment.\" is incorrect.</p><p>Blazingtext with Text classification should be used to classify sentences by sentiment.</p><p><strong>INCORRECT:</strong> \" Pass the inputs to Blazingtext Word2Vec algorithm as 2 separate files (label file and file containing the paragraph) and train a custom model to detect the sentiment.\" is incorrect.</p><p>The training/validation file should contain a training sentence per line along with the labels. Labels are words that are prefixed by the string <em>__label__</em></p><p><strong>INCORRECT:</strong> \" Pass the inputs to Blazingtext Word2Vec algorithm as 2 separate files (label file and file containing the paragraph) and train a custom model to detect the sentiment.\" is incorrect.</p><p>The training/validation file should contain a training sentence per line along with the labels. Labels are words that are prefixed by the string <em>__label__</em></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html</a></p>", "answers": ["<p>Tokenize the document into one sentence per line with its corresponding label, pass the inputs to Blazingtext Word2Vec algorithm and train a custom model to detect the sentiment.</p>", "<p>Pass the inputs to Blazingtext Word2Vec algorithm as 2 separate files (label file and file containing the paragraph) and train a custom model to detect the sentiment.</p>", "<p>Tokenize the document into one sentence per line with its corresponding label, pass the inputs to Blazingtext text classification algorithm and train a custom model to detect the sentiment.</p>", "<p>Pass the inputs to Blazingtext Word2Vec algorithm as 2 separate files (label file and file containing the paragraph) and train a custom model to detect the sentiment.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company wants to build a custom sentiment analysis model to train on the reviews of their clients as some clients still use old English language words which are not used anymore by the modern language. A machine learning specialist was hired for this role and was handed a dataset containing one paragraph consisting of thousands of lines and the sentiment of each sentence is given in another file. The specialist should train a custom model to recognize the sentiments of the future reviews.Which services/methods should the specialist use?", "related_lectures": []}, {"_class": "assessment", "id": 68126058, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has just released a new machine learning model (variant1) and wanted to validate it against an existing model. A Sagemaker\u2019s endpoint is already in service, and a model (Variant2) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The company used a deployment strategy to deploy both models and the given figure is from Cloudwatch where it represents the total invocations for each variant along time.</p><p>Based on this graph, which deployment method is the company adopting?</p><p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_10-44-37-3d549e1a63df56181dc3a5dc328fb5ed.jpg\"></p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The A/B testing method uses the 2 model variants with half the traffic serving each of them. The goal is to test the new model fairly with the other one.</p><p><strong>CORRECT: </strong>\"A/B testing.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Canary deployment.\" is incorrect.</p><p>The canary deployment releases the new version to a small set of users, then gradually increases its traffic.</p><p><strong>INCORRECT:</strong> \" Blue/Green deployment.\" is incorrect.</p><p>A blue/green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version.</p><p><strong>INCORRECT:</strong> \" Rolling deployment.\" is incorrect.</p><p>AWS quotes: \u201cA rolling deployment is a deployment strategy that slowly replaces previous versions of an application with new versions of an application by completely replacing the infrastructure on which the application is running. For example, in a rolling deployment in Amazon ECS, containers running previous versions of the application will be replaced one-by-one with containers running new versions of the application.\u201d</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/rolling-deployments.html\">https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/rolling-deployments.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html\">https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/bluegreen-deployments.html</a></p><p>Graph image: <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/model-invocations-even-dist.png\">https://docs.aws.amazon.com/sagemaker/latest/dg/images/model-invocations-even-dist.png</a></p><p>Graph source: <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html</a></p>", "answers": ["<p>A/B testing.</p>", "<p>Canary deployment.</p>", "<p>Blue/Green deployment.</p>", "<p>Rolling deployment.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company has just released a new machine learning model (variant1) and wanted to validate it against an existing model. A Sagemaker\u2019s endpoint is already in service, and a model (Variant2) is already serving the traffic, however, the company believes that the newer model will serve better as it had outstanding results in the training set. The company used a deployment strategy to deploy both models and the given figure is from Cloudwatch where it represents the total invocations for each variant along time.Based on this graph, which deployment method is the company adopting?", "related_lectures": []}, {"_class": "assessment", "id": 68126060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>An electronics company will start to receive excel files containing unstructured data with sales, revenue, and expenses on a daily basis from all its global branches. The main purpose is to perform SQL queries on the data as well as using business intelligence tools to gain insights.</p><p>Also, a machine learning specialist will train on this data to produce a forecasting model which forecasts revenues with a time span of 1 day. The company wants to receive the unstructured data in an appropriate data lake and automate the process of extracting, loading, and transforming this data to a parquet format, using a fully managed service, to be loaded to a suitable data warehouse and visualized later using business intelligence tools.</p><p>Which combination of services should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon S3 is the storage service for amazon acting as a data lake. Most probably if a solution requires data lake, then S3 is the answer. The company aims for a fully managed service for the ETL process, so AWS Glue is the perfect solution. Amazon Redshift Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes</p><p><strong>CORRECT: </strong>\" S3 bucket for storing incoming data. AWS Glue for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Quicksight for data visualization.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" DynamoDB for storing incoming data. AWS Batch for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Quicksight for data visualization.\" is incorrect.</p><p>S3 should be used as the data lake for storing incoming data not DynamoDB. Also, AWS Batch is most suitable for batch jobs.</p><p><strong>INCORRECT:</strong> \" S3 bucket for storing incoming data. AWS EMR for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Athena for data visualization.\" is incorrect.</p><p>The company is looking for a fully managed ETL service. While EMR is an ETL service, however it requires management of clusters. Also, Athena is used to perform SQL queries on data, so it is not used as a data visualization tool.</p><p><strong>INCORRECT:</strong> \" S3 bucket for storing incoming data. AWS Glue for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Athena for data visualization.\" is incorrect.</p><p>AWS Athena is used to perform SQL queries on data, so it is not used as a data visualization tool.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>", "answers": ["<p>S3 bucket for storing incoming data. AWS Glue for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Quicksight for data visualization.</p>", "<p>DynamoDB for storing incoming data. AWS Batch for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Quicksight for data visualization.</p>", "<p>S3 bucket for storing incoming data. AWS EMR for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Athena for data visualization.</p>", "<p>S3 bucket for storing incoming data. AWS Glue for ETL processes and transforming the data to a parquet format. AWS Redshift as the data warehouse. AWS Athena for data visualization.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "An electronics company will start to receive excel files containing unstructured data with sales, revenue, and expenses on a daily basis from all its global branches. The main purpose is to perform SQL queries on the data as well as using business intelligence tools to gain insights.Also, a machine learning specialist will train on this data to produce a forecasting model which forecasts revenues with a time span of 1 day. The company wants to receive the unstructured data in an appropriate data lake and automate the process of extracting, loading, and transforming this data to a parquet format, using a fully managed service, to be loaded to a suitable data warehouse and visualized later using business intelligence tools.Which combination of services should be used?", "related_lectures": []}]}
5824446
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 57677938, "assessment_type": "multi-select", "prompt": {"question": "<p>A Machine Learning Specialist built a neural network model which produced the following accuracies:</p><p>Training accuracy = 95%</p><p>Validation accuracy = 45%</p><p>Test accuracy = 41%</p><p>Which of the following are possible explanations for the drop in accuracy between training-validation and training-test? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>The model is trained using the training data points, so when validating/testing the model, we should provide a similar distribution of data to that of the training. In this example, we could tell that there is a significant drop in the validation and test accuracies from that of the training accuracy, which is most probably due to validating/testing with different distributions. Another possible explanation could be that the model overfitted on the training data set as the training accuracy was higher than the validation and test accuracies by a significant level.</p><p>Remember: We cannot simply train the model on <strong>high resolution, clear images</strong> and expect it to perform well on <strong>blurry images</strong>.</p><p><strong>CORRECT: </strong>\" The model suffers from overfitting on the training dataset <strong>and</strong> The validation and test data sets come from another distribution than the training data set.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The neural network layers should be increased.\" is incorrect.</p><p>The model has low bias, so increasing layers won\u2019t have a significant effect.</p><p><strong>INCORRECT:</strong> \"Pipe mode should be used when training instead of file mode.\" is incorrect.</p><p>Pipe mode is used for streaming data, which speeds up the training process. It has nothing to do with enhancing models\u2019 accuracies.</p><p><strong>INCORRECT:</strong> \"The number of epochs was too low for the model to learn. Training with higher epochs should solve this problem.\" is incorrect.</p><p>Increasing the number of epochs will increase the training accuracy (95%) slightly. This has no effect on decreasing the gap between training-validation or training-test accuracies.</p>", "answers": ["<p>The number of epochs was too low for the model to learn. Training with higher epochs should solve this problem.</p>", "<p>The model suffers from overfitting on the training dataset.</p>", "<p>Pipe mode should be used when training instead of file mode.</p>", "<p>The neural network layers should be increased.</p>", "<p>The validation and test data sets come from another distribution than the training data set.</p>"]}, "correct_response": ["b", "e"], "section": "MLS Domain 3 - Modeling", "question_plain": "A Machine Learning Specialist built a neural network model which produced the following accuracies:Training accuracy = 95%Validation accuracy = 45%Test accuracy = 41%Which of the following are possible explanations for the drop in accuracy between training-validation and training-test? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57677944, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist wants to deploy a security system in his house to detect potential robberies. He already has a camera setup for each room and the entrance as well. The system should automatically recognize an intruder and send the specialist an SMS to take further action such as calling the police. The specialist wants to use an accurate computer vision system for detection.</p><p>Which architecture should the specialist deploy?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Kinesis Video Streams is a fully managed AWS service that enables video streaming directly to AWS. Amazon Rekognition can Determine the similarity of a face against another picture or from a private image repository. So, the specialist could upload pictures of his family members inside his private repository and the house\u2019s cameras could continuously send live video feed to compare the existing face with the faces in the repository. Amazon Kinesis can then take the analysis stream results and send it to a lambda function which can publish messages to a preferred SNS topic.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_10-49-23-072a6b31eac5a2a5be55288a7ead8b1a.jpg\"><p><strong>CORRECT: </strong>\"A Kinesis video stream to ingest the live video feed -&gt; Amazon Rekognition with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Lambda function to be the consumer of the data stream which publishes the message to the SNS topic if an intruder is present.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Kinesis Data Streams to ingest the live video feed -&gt; Amazon Rekognition with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Lambda function to be the consumer of the data stream which publishes the message to the SNS topic if an intruder is present.\" is incorrect.</p><p>Kinesis Data Streams cannot be used for video data.</p><p><strong>INCORRECT:</strong> \"Use Kinesis Data Streams to ingest the live video feed -&gt; Amazon Lex with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Send the output directly to an SNS topic if an intruder is present.\" is incorrect.</p><p>Amazon Lex is used to build conversational interfaces into any application using NLP.</p><p><strong>INCORRECT:</strong> \"Use Kinesis video Streams to ingest the live video feed -&gt; Amazon Lex with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Send the output directly to an SNS topic if an intruder is present.\" is incorrect.</p><p>Output cannot be sent directly from Kinesis Data Streams to the SNS topic. Lambda function should be placed in order to programmatically check the result and send the desired output to the SNS topic.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/how-it-works.html\">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/how-it-works.html</a></p>", "answers": ["<p>A Kinesis video stream to ingest the live video feed -&gt; Amazon Rekognition with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Lambda function to be the consumer of the data stream which publishes the message to the SNS topic if an intruder is present.</p>", "<p>Use Kinesis Data Streams to ingest the live video feed -&gt; Amazon Rekognition with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Lambda function to be the consumer of the data stream which publishes the message to the SNS topic if an intruder is present.</p>", "<p>Use Kinesis Data Streams to ingest the live video feed -&gt; Amazon Lex with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Send the output directly to an SNS topic if an intruder is present.</p>", "<p>Use Kinesis video Streams to ingest the live video feed -&gt; Amazon Lex with a pre-set face collection to identify the intruder -&gt; Analysis results are sent to Kinesis Data Streams -&gt; Send the output directly to an SNS topic if an intruder is present.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A machine learning specialist wants to deploy a security system in his house to detect potential robberies. He already has a camera setup for each room and the entrance as well. The system should automatically recognize an intruder and send the specialist an SMS to take further action such as calling the police. The specialist wants to use an accurate computer vision system for detection.Which architecture should the specialist deploy?", "related_lectures": []}, {"_class": "assessment", "id": 57677948, "assessment_type": "multi-select", "prompt": {"question": "<p>A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of positive and negative samples where positive samples indicate that the patient had cancer.</p><p>An XGBoost model was chosen and the \u201ceval_metric\u201d hyperparameter was set to \u201cvalidation:error\u201d. The model achieved a high accuracy of 98% in the training dataset and an acceptable 90% accuracy in the test set. On the other hand, the model achieved a low recall of 0.60. The company demanded that the data science team should come up with a solution to this problem as the business metric\u2019s recall was 0.85.</p><p>What should the Data Science team do to approach the business metric using the same model? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>XGBoost\u2019s hyperparameters:</p><ul><li><p>Gamma: Regularization parameter.</p></li><li><p>ETA: This parameter prevents overfitting.</p></li><li><p>Alpha: This is the L1 regularization term.</p></li><li><p>Scale_pos_weight: Adjusts the balance between positive and negative weights.</p></li><li><p>eval_metric: This is used for model evaluation and has many options, some of them are:</p></li></ul><p>&nbsp; &nbsp; &nbsp; &nbsp;o&nbsp; &nbsp; validation:auc: Area under the curve</p><p>&nbsp; &nbsp; &nbsp; &nbsp;o&nbsp; &nbsp; validation: error: Binary classification error rate.</p><p>When tuning on \u201cvalidation:error\u201d, it emphasizes on the accuracy, while \u201cvalidation:auc\u201d emphasizes on the ability of the model to distinguish between positive and negative samples. This is why in our case, the \u201cvalidaton:auc\u201d will enhance the model\u2019s recall.</p><p><strong>CORRECT: </strong>\"Change the \u201cscale_pos_weight\u201d hyperparameter <strong>AND</strong> Change the \u201ceval_metric\u201d to \u201cvalidation:auc\u201d.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increase the \u201cGamma\u201d hyperparameter.\" is incorrect.</p><p>Gamma is used for regularization purposes.</p><p><strong>INCORRECT:</strong> \"Decrease the \u201cETA\u201d hyperparameter.\" is incorrect.</p><p>ETA is used to prevent overfitting.</p><p><strong>INCORRECT:</strong> \" Increase the \u201cAlpha\u201d hyperparameter.\" is incorrect.</p><p>Alpha is used for regularization purposes.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html</a></p>", "answers": ["<p>Increase the \u201cGamma\u201d hyperparameter.</p>", "<p>Decrease the \u201cETA\u201d hyperparameter.</p>", "<p>Change the \u201cscale_pos_weight\u201d hyperparameter.</p>", "<p>Change the \u201ceval_metric\u201d to \u201cvalidation:auc\u201d.</p>", "<p>Increase the \u201cAlpha\u201d hyperparameter.</p>"]}, "correct_response": ["c", "d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A medical company has decided to automate the process of cancer screening using machine learning. The Data Science team collected the company\u2019s patients\u2019 records over five years which consisted of positive and negative samples where positive samples indicate that the patient had cancer.An XGBoost model was chosen and the \u201ceval_metric\u201d hyperparameter was set to \u201cvalidation:error\u201d. The model achieved a high accuracy of 98% in the training dataset and an acceptable 90% accuracy in the test set. On the other hand, the model achieved a low recall of 0.60. The company demanded that the data science team should come up with a solution to this problem as the business metric\u2019s recall was 0.85.What should the Data Science team do to approach the business metric using the same model? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57677956, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning engineer is in the process of building a decision trees algorithm. The algorithm should classify an activity, resembled as a collection of many features, as suspicious or not. He observes that most of the features have a weak correlation with each other. However, feature 1 and feature 5 have a correlation of 0.95.</p><p>What is the easiest solution to this problem, if any?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>While strong correlations have an impact on some algorithms, \u201cdecision trees\u201d has feature selection embedded in it. So, strongly correlated features won\u2019t have a strong impact on them.</p><p><strong>CORRECT: </strong>\"Leave both features as they will have minimum impact on the algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Remove either feature 1 or feature 5 as they will worsen the performance of the algorithm.\" is incorrect.</p><p>Strongly correlated features barely have an impact on decision trees algorithm.</p><p><strong>INCORRECT:</strong> \"Remove both features as they will worsen the performance of the algorithm.\" is incorrect.</p><p>Removing both features will make the model simpler and it would remove valuable information.</p><p><strong>INCORRECT:</strong> \"Apply PCA and substitute the resulting feature with feature 1 and feature 5\" is incorrect.</p><p>This is not the easiest solution to the problem. Also, both features are highly correlation.</p>", "answers": ["<p>Remove either feature 1 or feature 5 as they will worsen the performance of the algorithm.</p>", "<p>Remove both features as they will worsen the performance of the algorithm.</p>", "<p>Leave both features as they will have minimum impact on the algorithm.</p>", "<p>Apply PCA and substitute the resulting feature with feature 1 and feature 5.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A machine learning engineer is in the process of building a decision trees algorithm. The algorithm should classify an activity, resembled as a collection of many features, as suspicious or not. He observes that most of the features have a weak correlation with each other. However, feature 1 and feature 5 have a correlation of 0.95.What is the easiest solution to this problem, if any?", "related_lectures": []}, {"_class": "assessment", "id": 57677962, "assessment_type": "multi-select", "prompt": {"question": "<p>The figure below is a histogram containing the distribution of residuals for a linear regression model.</p><p>Which insights are correct for this model? (Select TWO.)</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_10-54-20-0923e3403b2237e739a5fb091aafed28.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>This histogram indicates that the model\u2019s error predictions are somewhat balanced. This is one of the best cases for a linear regressor. On the other hand, if the residual frequencies are biased to negative or positive values, then it is not represented well using this model.</p><p><strong>CORRECT: </strong>\"This model is suitable for deployment <strong>and</strong> This is a zero-centred bell shape histogram.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"The model should be changed to a non-linear regression model to capture the underlying non-linear relation.\" is incorrect.</p><p>The linear model captures the underlying relation between the inputs and the outputs.</p><p><strong>INCORRECT:</strong> \"This is a non-zero-centred bell shape histogram.\" is incorrect.</p><p>This is clearly a zero-centred bell shape histogram.</p><p><strong>INCORRECT:</strong> \"The model should be trained for more epochs.\" is incorrect.</p><p>We cannot build such an insight from the given graph; however, we could tell that the model is suitable.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression.html</a></p>", "answers": ["<p>This model is suitable for deployment.</p>", "<p>The model should be changed to a non-linear regression model to capture the underlying non-linear relation.</p>", "<p>This is a zero-centred bell shape histogram.</p>", "<p>This is a non-zero-centred bell shape histogram.</p>", "<p>The model should be trained for more epochs.</p>"]}, "correct_response": ["a", "c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "The figure below is a histogram containing the distribution of residuals for a linear regression model.Which insights are correct for this model? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57677968, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company suspects that their AWS account has been compromised and an unauthorized user had access to certain services. The company wants to view a history of the API calls made along with the IP addresses that made this call.</p><p>Which service should the company use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS CloudTrail enables you to spot:</p><ul><li><p>Which users and accounts called AWS APIs for services that support CloudTrail.</p></li><li><p>The source IP address the calls were made from.</p></li><li><p>When the calls occurred.</p></li></ul><p><strong>CORRECT: </strong>\"AWS CloudTrail.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Amazon Detective.\" is incorrect.</p><p>Amazon Detective analyses, investigates, and identify the root cause of potential security issues or suspicious activities. However, in order to spot the source IP address making the call, CloudTrail is used.</p><p><strong>INCORRECT:</strong> \"Amazon CloudWatch.\" is incorrect.</p><p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. It does not monitor IP addresses making calls to AWS services.</p><p><strong>INCORRECT:</strong> \"Amazon X-ray.\" is incorrect.</p><p>AWS X-Ray helps in identifying the root cause of performance issues and errors in a microservices architecture.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html</a></p>", "answers": ["<p>Amazon Detective.</p>", "<p>Amazon X-ray.</p>", "<p>Amazon CloudWatch.</p>", "<p>AWS CloudTrail.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company suspects that their AWS account has been compromised and an unauthorized user had access to certain services. The company wants to view a history of the API calls made along with the IP addresses that made this call.Which service should the company use?", "related_lectures": []}, {"_class": "assessment", "id": 57677982, "assessment_type": "multiple-choice", "prompt": {"question": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_10-57-26-58691da9f05fe2b4ce76ab832feb78c0.jpg\"><p>Based on the diagram above, what is the predicted class-frequency percent of the \u201cThriller\u201d genre?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The predicted class-frequency percentage of the \u201cThriller\u201d genre is 9.33%. 9.33% of the total observations were predicted as \u201cThriller\u201d.</p><p><strong>CORRECT: </strong>\"9.33%\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"77.56%\" is incorrect.</p><p>This indicates the percentage of the total observations that were predicted as \u201cRomance\u201d.</p><p><strong>INCORRECT:</strong> \"21.23%\" is incorrect.</p><p>This indicates the percentage of true \u201cThriller\u201d values present in the dataset.</p><p><strong>INCORRECT:</strong> \"20.85%\" is incorrect.</p><p>This indicates the percentage of true \u201cAdventure\u201d values present in the dataset.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html</a></p>", "answers": ["<p>77.56%</p>", "<p>9.33%</p>", "<p>21.23%</p>", "<p>20.85%</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "Based on the diagram above, what is the predicted class-frequency percent of the \u201cThriller\u201d genre?", "related_lectures": []}, {"_class": "assessment", "id": 57677988, "assessment_type": "multi-select", "prompt": {"question": "<p>A company aims to build a machine learning model using its customers\u2019 data. The data is unlabeled and currently sits in an S3 bucket. The company claims that the data is insensitive, however, the data should be checked before labelling and training. Labelling the data requires a massive workforce which the company currently lacks.</p><p>Which services should the company use to ensure all the data is insensitive and for labelling them? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>AWS quotes \u201cAmazon <strong>Macie</strong> is a fully managed data security and data privacy service that <strong>uses machine learning</strong> and pattern matching to <strong>discover </strong>and <strong>protect</strong> your <strong>sensitive data</strong> in AWS.\u201d</p><p>Amazon Ground Truth is a service with Amazon SageMaker that labels datasets for further use in building machine learning models. Three options are available when using this service:</p><ol><li><p>Mechanical Turk: A team of global, on-demand workers from Amazon.</p></li><li><p>Private labelling workforce: A team of workers from the company.</p></li><li><p>Vendor: A selection of experienced vendors who specialize in providing data labelling services.</p></li></ol><p>Here, the most suitable labelling method is Mechanical Turk as the company lacks the workforce and also the company claims that the data is insensitive.</p><p><strong>CORRECT: </strong>\" Amazon Macie <strong>and</strong> Amazon Ground Truth (Mechanical Turk).\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Amazon Quicksight.\" is incorrect.</p><p>Quicksight is a machine learning powered business intelligence tool.</p><p><strong>INCORRECT:</strong> \" Amazon Ground Truth (private labelling workforce).\" is incorrect.</p><p>a private labelling workforce cannot be used as the company does not have enough workforce<strong>.</strong></p><p><strong>INCORRECT:</strong> \" Amazon Autopilot.\" is incorrect.</p><p>Amazon Autopilot is a service which creates machine learning models automatically.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html\">https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html</a></p>", "answers": ["<p>Amazon Quicksight.</p>", "<p>Amazon Macie.</p>", "<p>Amazon Ground Truth (private labelling workforce).</p>", "<p>Amazon Ground Truth (Mechanical Turk).</p>", "<p>Amazon Autopilot.</p>"]}, "correct_response": ["b", "d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company aims to build a machine learning model using its customers\u2019 data. The data is unlabeled and currently sits in an S3 bucket. The company claims that the data is insensitive, however, the data should be checked before labelling and training. Labelling the data requires a massive workforce which the company currently lacks.Which services should the company use to ensure all the data is insensitive and for labelling them? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57678016, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A start-up company just completed its first year with great success in selling their products. A well-known company decided to invest in this start-up, however it asked them for their revenue and expenses graph for this yearly period along with a forecast for the upcoming 6 months. The company noticed that their data contains outliers.</p><p>Which service/method should they use to produce this forecasted graph in the least amount of time?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Quicksight ML insights can produce forecast on a given time-series data without machine learning prior knowledge. The tool is powerful and saves a lot of time as it also removes outliers and imputes missing data beforehand. This is definitely the fastest implementation of a forecasted graph.</p><p><strong>CORRECT: </strong>\" Use Amazon Quicksight to automatically forecast the data for the upcoming 6 months.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Train a Recurrent neural network on an EC2 instance for the past year and produce a forecast for the upcoming 6 months.\" is incorrect.</p><p>This will require a significant amount of time for writing the forecasting model\u2019s code using TensorFlow and training on it.</p><p><strong>INCORRECT:</strong> \" Train a Recurrent neural network on Amazon SageMaker by spinning up a training instance to train on the data for the past year and produce a forecast for the upcoming 6 months.\" is incorrect.</p><p>This will require a significant amount of time for writing the forecasting model\u2019s code using TensorFlow and training on it.</p><p><strong>INCORRECT:</strong> \"Use DeepAR algorithm in Amazon SageMaker to train a model for the past year and produce a forecast for the upcoming 6 months.\" is incorrect.</p><p>This will require a lot of time to prepare the inputs, train the model and predict the future data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/forecast-function.html\">https://docs.aws.amazon.com/quicksight/latest/user/forecast-function.html</a></p>", "answers": ["<p>Train a Recurrent neural network on an EC2 instance for the past year and produce a forecast for the upcoming 6 months.</p>", "<p>Train a Recurrent neural network on Amazon SageMaker by spinning up a training instance to train on the data for the past year and produce a forecast for the upcoming 6 months.</p>", "<p>Use Amazon Quicksight to automatically forecast the data for the upcoming 6 months.</p>", "<p>Use DeepAR algorithm in Amazon SageMaker to train a model for the past year and produce a forecast for the upcoming 6 months.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A start-up company just completed its first year with great success in selling their products. A well-known company decided to invest in this start-up, however it asked them for their revenue and expenses graph for this yearly period along with a forecast for the upcoming 6 months. The company noticed that their data contains outliers.Which service/method should they use to produce this forecasted graph in the least amount of time?", "related_lectures": []}, {"_class": "assessment", "id": 57678022, "assessment_type": "multi-select", "prompt": {"question": "<p>A company wants to accurately predict the salary of its upcoming employees, to better negotiate the price, based on some features such as \u201cage\u201d, \u201cprevious salary\u201d, \u201carea\u201d and \u201cskill level\u201d. They hired a machine learning specialist to build the required model. The specialist finished training 2 parallel models using XGBoost and Linear learner algorithms, however, he wants to deploy the better model.</p><p>Which metrics could be used to compare both models? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>RMSE (Root Mean Squared Error) and MSE (Mean Squared Error) metrics are both used to evaluate linear regression models.</p><p>Other mentioned metrics are for classification tasks.</p><p>The smaller the value of the RMSE, the better. If a model perfectly predicts all the values, then it would have an RMSE of zero (ideal case).</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-07-42-9c0e3924b4e59a1144798a143b0280d9.jpg\"><p>The equation of MSE is the same as that of RMSE but without the square root.</p><p>For classification tasks:</p><p>F1 is used to measure the overall performance of the model taking into consideration false positives and false negatives. Recall emphasizes on false negatives and precision emphasizes on false positives.</p><p><strong>CORRECT: </strong>\"MSE <strong>and</strong> RMSE.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"F1 score\" is incorrect.</p><p>F1 score is used for classification model\u2019s evaluation.</p><p><strong>INCORRECT:</strong> \"Recall\" is incorrect.</p><p>Recall is used for classification model\u2019s evaluation.</p><p><strong>INCORRECT:</strong> \"Precision\" is incorrect.</p><p>Precision is used for classification model\u2019s evaluation.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/regression-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/regression-model-insights.html</a></p>", "answers": ["<p>F1 score.</p>", "<p>Recall.</p>", "<p>MSE.</p>", "<p>Precision.</p>", "<p>RMSE.</p>"]}, "correct_response": ["c", "e"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company wants to accurately predict the salary of its upcoming employees, to better negotiate the price, based on some features such as \u201cage\u201d, \u201cprevious salary\u201d, \u201carea\u201d and \u201cskill level\u201d. They hired a machine learning specialist to build the required model. The specialist finished training 2 parallel models using XGBoost and Linear learner algorithms, however, he wants to deploy the better model.Which metrics could be used to compare both models? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57678038, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck. The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method. She already made a python script to train the mode using TensorFlow, however she wants to use her custom dependencies and environment.</p><p>Which is the best option for the specialist to choose from?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The specialist wants to use her own custom dependencies and environment, so the appropriate option to choose is to first dockerize the training script with all the dependencies, upload it to ECR then use the image uri for training the model.</p><p><strong>CORRECT: </strong>\" Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d to sagemaker.estimator.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s built in algorithm for image classification and supply the training code.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just used as they are to train on our datasets. Also, they are constantly upgraded with enhanced versions.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s script mode in which she will supply the python code to a pre-built TensorFlow container. Provide entry_point=\"tf-train.py\" to the TensorFlow estimator.\" is incorrect.</p><p>The specialist wants to use her own custom dependencies and environment, so we cannot use SageMaker\u2019s TensorFlow pre-built container.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s built in algorithm for image classification, dockerize the training code first then supply the training code to the image classification algorithm.\" is incorrect.</p><p>We cannot use SageMaker\u2019s built in algorithm as they are built using specific frameworks and we do not have access to their underlying frameworks/codes or anything. They are just used as they are to train on our datasets. We cannot supply any codes to those algorithms.</p><p><strong>References:</strong></p><p><a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/tensorflow_bring_your_own\">https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/tensorflow_bring_your_own</a></p>", "answers": ["<p>Use SageMaker\u2019s built in algorithm for image classification and supply the training code.</p>", "<p>Use SageMaker\u2019s script mode in which she will supply the python code to the pre-built TensorFlow container. Provide the script in the \u201centry_point\u201d argument in the TensorFlow estimator.</p>", "<p>Dockerize the code with all the necessary dependencies, upload the docker image to Amazon ECR. Pull the image at training time by providing the image uri to \u201cimage_uri\u201d argument in sagemaker.estimator.</p>", "<p>Use SageMaker\u2019s built in algorithm for image classification, dockerize the training code first then supply the training code to the image classification algorithm.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A recycling company wants to automate the process of identification of different waste products in order to transport them into different conveyer belts for different recycling techniques. A camera is placed above the main conveyer belt which holds all the waste products delivered from the garbage truck. The camera constantly takes pictures of the products, and 10 robotic arms are supposed to capture different types of waste at the same time. A machine learning engineer was hired to build a model for this classification method. She already made a python script to train the mode using TensorFlow, however she wants to use her custom dependencies and environment.Which is the best option for the specialist to choose from?", "related_lectures": []}, {"_class": "assessment", "id": 57678044, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a massive database containing their customers\u2019 information including \u201cTotal purchases\u201d, \u201cpurchase response time\u201d, \u201cGeographic location\u201d, \u201cIncome\u201d and \u201cGender\u201d. The company wants to launch a new product; however, it wants to adopt a new marketing strategy where it will target each group of people sharing similar traits differently.</p><p>For instance, if a group of people share similar low income status and are from a developing country, they will be able to purchase the product at a lower cost than those with a higher income and from a developed country.</p><p>The company hired a machine learning specialist to create a model to identify different groups of people.</p><p>Which algorithm should the specialist adopt?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A segmentation algorithm should be used to segment customers into some groups where each group carries somehow similar customers in the feature space. The segmentation algorithm is the <strong>K-means clustering</strong> algorithm.</p><p>Remember:</p><ul><li><p><strong>K-means </strong>for <strong>segmentation</strong>, <strong>KNN</strong> for <strong>classification/regression</strong>.</p></li></ul><p><strong>CORRECT: </strong>\"K-means clustering\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"KNN\" is incorrect.</p><p>KNN algorithm is used for classification/regression problems.</p><p><strong>INCORRECT:</strong> \"Collaborative filtering\" is incorrect.</p><p>Collaborative filtering is used in recommender systems for calculating ratings based on ratings of similar users.</p><p><strong>INCORRECT:</strong> \"Logistic regression\" is incorrect.</p><p>Logistic regression is used for classification problems not for segmentation.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/machine-learning/building-a-customized-recommender-system-in-amazon-sagemaker/\">https://aws.amazon.com/blogs/machine-learning/building-a-customized-recommender-system-in-amazon-sagemaker/</a></p>", "answers": ["<p>Collaborative filtering.</p>", "<p>Logistic regression.</p>", "<p>K-means clustering.</p>", "<p>KNN.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company has a massive database containing their customers\u2019 information including \u201cTotal purchases\u201d, \u201cpurchase response time\u201d, \u201cGeographic location\u201d, \u201cIncome\u201d and \u201cGender\u201d. The company wants to launch a new product; however, it wants to adopt a new marketing strategy where it will target each group of people sharing similar traits differently.For instance, if a group of people share similar low income status and are from a developing country, they will be able to purchase the product at a lower cost than those with a higher income and from a developed country.The company hired a machine learning specialist to create a model to identify different groups of people.Which algorithm should the specialist adopt?", "related_lectures": []}, {"_class": "assessment", "id": 57678050, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist is handed a dataset of customers\u2019 audio calls with the goal of transcribing those calls. The dataset has pairs of audio recordings and texts where each recording is less than five minutes and a text corresponding to the conversation in that call. A machine learning specialist is given the task of creating a model and training it on this dataset where it could transcribe calls in the future for storing them as text. The specialist decided to use the SageMaker\u2019s Seq2Seq model.</p><p>Which training instance type would be most suitable for this job?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cCurrently Amazon SageMaker seq2seq is only supported on GPU instance types and is only set up to train on a single machine.\u201d</p><p><strong>CORRECT: </strong>\"p2.xlarge instance.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"c5.xlarge instance.\" is incorrect.</p><p>SageMaker\u2019s Seq2Seq algorithm can only be trained on a GPU power processors (P family).\u201d</p><p><strong>INCORRECT:</strong> \"r5.xlarge instance.\" is incorrect.</p><p>SageMaker\u2019s Seq2Seq algorithm can only be trained on a GPU power processors (P family).\u201d</p><p><strong>INCORRECT:</strong> \"t3.xlarge instance.\" is incorrect.</p><p>SageMaker\u2019s Seq2Seq algorithm can only be trained on a GPU power processors (P family).\u201d</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html</a></p>", "answers": ["<p>p2.xlarge instance.</p>", "<p>c5.xlarge instance.</p>", "<p>r5.xlarge instance.</p>", "<p>t3.xlarge instance.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist is handed a dataset of customers\u2019 audio calls with the goal of transcribing those calls. The dataset has pairs of audio recordings and texts where each recording is less than five minutes and a text corresponding to the conversation in that call. A machine learning specialist is given the task of creating a model and training it on this dataset where it could transcribe calls in the future for storing them as text. The specialist decided to use the SageMaker\u2019s Seq2Seq model.Which training instance type would be most suitable for this job?", "related_lectures": []}, {"_class": "assessment", "id": 57678054, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A large library decided to shift its activity to be fully internet-based as they will no longer have a physical location. The books will be presented as a soft copy on their newly established website. The library also aims to build a machine learning model to help their users search for books faster. They want to recommend books to a user that are similar to books they have read.</p><p>Which type of algorithm should the library use to implement such service?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is a recommendation system machine learning problem. There are 2 types of recommendation systems mentioned in the above question:</p><ul><li><p>Collaborative filtering is used in recommender systems for calculating ratings based on ratings of similar users. Here, we have user A reading book X, if we got the books that other users recommend who read the same book X, then user A will most likely prefer those books. In fact, we do not know any features about other books, just that similar users liked them.</p></li><li><p>Content-based filtering is used when user A read book X and we recommend other books that have similar features as book X. Here, we know the features of other books, so we know that they could both be similar.</p></li></ul><p><strong>CORRECT: </strong>\" Content based filtering.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Collaborative filtering.\" is incorrect.</p><p>The library wants to recommend books to a certain user based on similar books to what he/she just read. This is Content based filtering.</p><p><strong>INCORRECT:</strong> \" K-means clustering.\" is incorrect.</p><p>K-means clustering is used to segment objects into groups in the feature space.</p><p><strong>INCORRECT:</strong> \" KNN.\" is incorrect.</p><p>K-nearest neighbor \u201cKNN\u201d algorithm is used in classification and regression problems.</p>", "answers": ["<p>Content based filtering.</p>", "<p>Collaborative filtering.</p>", "<p>K-means clustering.</p>", "<p>KNN.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A large library decided to shift its activity to be fully internet-based as they will no longer have a physical location. The books will be presented as a soft copy on their newly established website. The library also aims to build a machine learning model to help their users search for books faster. They want to recommend books to a user that are similar to books they have read.Which type of algorithm should the library use to implement such service?", "related_lectures": []}, {"_class": "assessment", "id": 57678060, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A telecommunication company has a gigantic database containing some features of their customers including but not limited to \u201ccontract_type\u201d, \u201cgender\u201d, \u201clocation\u201d, \u201cmonthly_charges\u201d. The company lately suffered from massive losses due to customers leaving the company.</p><p>They want to build a model to predict if a customer will churn in the future so as to provide him/her with incentives. The company hired a machine learning specialist to train a model on the existing data. The specialist initiated a hyperparameter tuning job with Bayesian search which lasted 3 hours, however, someone has accidently shut down the tuning job process.</p><p>What should the specialist do in order not to lose progress?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cUse warm start to start a hyperparameter tuning job using one or more previous tuning jobs as a starting point. The results of previous tuning jobs are used to inform which combinations of hyperparameters to search over in the new tuning job.\u201d</p><p>This is mainly used in scenarios such as:</p><ul><li><p>You want to gradually increase the number of training jobs over several tuning jobs based on the results you see after each iteration.</p></li><li><p>You get new data and want to tune a model using the new data.</p></li><li><p>You want to change the ranges of hyperparameters that you used in a previous tuning job, change static hyperparameters to tuneable, or change tuneable hyperparameters to static values.</p></li><li><p>You stopped a previous hyperparameter job early or it stopped unexpectedly.</p></li></ul><p><strong>CORRECT: </strong>\" Run a warm start hyperparameter tuning job.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use incremental training to continue training from where the training job ended.\" is incorrect.</p><p>Incremental training is used when we notice that the inferences aren\u2019t as good as before when we first created the model, so we use the artifacts from an existing model and use an expanded dataset to train a new model.</p><p><strong>INCORRECT:</strong> \" The progress is already lost, and he should re-train the model again.\" is incorrect.</p><p>The progress is not lost, and we could catch up from where we left by using a \u201cwarm start hyperparameter tuning job.\u201d</p><p><strong>INCORRECT:</strong> \" Run a hyperparameter tuning job with Bayesian search instead.\" is incorrect.</p><p>The search method used is not the problem as it didn\u2019t cause any troubles. The problem was that the tuning job was stopped.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html</a></p>", "answers": ["<p>Use incremental training to continue training from where the training job ended.</p>", "<p>Run a warm start hyperparameter tuning job.</p>", "<p>The progress is already lost, and he should re-train the model again.</p>", "<p>Run a hyperparameter tuning job with Bayesian search instead.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A telecommunication company has a gigantic database containing some features of their customers including but not limited to \u201ccontract_type\u201d, \u201cgender\u201d, \u201clocation\u201d, \u201cmonthly_charges\u201d. The company lately suffered from massive losses due to customers leaving the company.They want to build a model to predict if a customer will churn in the future so as to provide him/her with incentives. The company hired a machine learning specialist to train a model on the existing data. The specialist initiated a hyperparameter tuning job with Bayesian search which lasted 3 hours, however, someone has accidently shut down the tuning job process.What should the specialist do in order not to lose progress?", "related_lectures": []}, {"_class": "assessment", "id": 57678070, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company launched a new product two weeks ago and they hired a machine learning specialist to design a new system to detect the sentiment of their customers\u2019 reviews automatically. The company does not have a labelled dataset in order for the specialist to train on. The company insists that the specialist should come up with a solution as soon as possible and that they have no intent to buy labelled data. The specialist should devise a solution that:</p><ul><li><p>Translate the sentiment to English language.</p></li><li><p>Extract the sentiment of the reviews and dump the output in the S3 bucket.</p></li></ul><p>Which services should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Amazon Comprehend: Uses Natural language processing to gain insights and relationships in text data. It could be used to detect the sentiment of a given sentence, that\u2019s why it\u2019s applicable in this scenario. Amazon Translate is a neural machine translation service that adopts language translation.</p><p><strong>CORRECT: </strong>\" Amazon Translate for translating the sentiment. Amazon Comprehend for extracting the sentiment of the reviews.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Amazon Textract for translating the sentiment. Amazon Comprehend for extracting the sentiment of the reviews.\" is incorrect.</p><p>Amazon Textract is an AI service which extracts printed text, handwriting and data from any document. It cannot be used for translation.</p><p><strong>INCORRECT:</strong> \" Amazon Comprehend for translating the sentiment. Amazon Textract for extracting the sentiment of the reviews.\" is incorrect.</p><p>Amazon Textract is an AI service which extracts printed text, handwriting and data from any document. It cannot be used for sentiment analysis. Amazon comprehend is not used for translation.</p><p><strong>INCORRECT:</strong> \" Amazon Translate for translating the sentiment. Amazon Textract for extracting the sentiment of the reviews.\" is incorrect.</p><p>Amazon Textract is an AI service which extracts printed text, handwriting and data from any document. It cannot be used for extracting sentiments.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html\">https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html</a></p><p><a href=\"https://aws.amazon.com/glue\">https://aws.amazon.com/glue</a></p><p><a href=\"https://docs.aws.amazon.com/athena/latest/ug/what-is.html\">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><p><a href=\"https://aws.amazon.com/quicksight/\">https://aws.amazon.com/quicksight/</a></p>", "answers": ["<p>Amazon Textract for translating the sentiment. Amazon Comprehend for extracting the sentiment of the reviews.</p>", "<p>Amazon Comprehend for translating the sentiment. Amazon Textract for extracting the sentiment of the reviews.</p>", "<p>Amazon Translate for translating the sentiment. Amazon Textract for extracting the sentiment of the reviews.</p>", "<p>Amazon Translate for translating the sentiment. Amazon Comprehend for extracting the sentiment of the reviews.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company launched a new product two weeks ago and they hired a machine learning specialist to design a new system to detect the sentiment of their customers\u2019 reviews automatically. The company does not have a labelled dataset in order for the specialist to train on. The company insists that the specialist should come up with a solution as soon as possible and that they have no intent to buy labelled data. The specialist should devise a solution that:Translate the sentiment to English language.Extract the sentiment of the reviews and dump the output in the S3 bucket.Which services should be used?", "related_lectures": []}, {"_class": "assessment", "id": 57678074, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A governmental dataset includes, but not limited to, the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome_month\u201d, \u201cdebt\u201d, \u201cIncome_year\u201d. The target label is whether this specific citizen would be a criminal or not. A machine learning model should be built for this prediction using SageMaker\u2019s XGBoost algorithm. The \u201cIncome_month\u201d and \u201cIncome_year\u201d are found to have a correlation of 0.97.</p><p>What is the recommended action to be taken for such columns?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>When 2 features are found to have a strong correlation (positive or negative), one of them should be removed as they will affect the learning of the model whether it was a classification problem or a regression one.</p><p><strong>CORRECT: </strong>\" Remove one of those features.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Delete both columns.\" is incorrect.</p><p>If we deleted both columns, we are deleting information among them.</p><p><strong>INCORRECT:</strong> \" Create a new feature from both columns and leave them.\" is incorrect.</p><p>AWS states: \u201cIt is generally a best practice to include as many variables in your training data as possible. However, the noise introduced by including many variables with little predictive power might negatively affect the quality and accuracy of your ML model.\u201d</p><p><strong>INCORRECT:</strong> \" Create a new feature from both columns and delete them.\" is incorrect.</p><p>Only one of the columns should remain.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/data-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/data-insights.html</a></p>", "answers": ["<p>Delete both columns.</p>", "<p>Remove one of those features.</p>", "<p>Create a new feature from both columns and leave them.</p>", "<p>Create a new feature from both columns and delete them.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A governmental dataset includes, but not limited to, the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome_month\u201d, \u201cdebt\u201d, \u201cIncome_year\u201d. The target label is whether this specific citizen would be a criminal or not. A machine learning model should be built for this prediction using SageMaker\u2019s XGBoost algorithm. The \u201cIncome_month\u201d and \u201cIncome_year\u201d are found to have a correlation of 0.97.What is the recommended action to be taken for such columns?", "related_lectures": []}, {"_class": "assessment", "id": 57678078, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning engineer noticed that a farm\u2019s weeds are harmful to the actual healthy crops and decided to create a computer vision model to help distinguish weeds from crops.</p><p>A convolutional neural network was used for this use case and the engineer trained the model for 3 consecutive days with data augmentation during training. At the end of the third day, the engineer noticed that the training accuracy is 93% while the validation and test accuracies were capped at 76%.</p><p>Which techniques were to be applied in order to overcome this situation? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>This is a good example of overfitting problems where the training accuracy seems to be higher than the validation/test accuracies by a significant amount. The model is now fine-tuned on the training data and cannot generalize well to have a high accuracy in either of the validation or test sets. Overfitting problems could be reduced by using dropout or early stopping.</p><p><strong>CORRECT: </strong>\"Early stopping <strong>and</strong> Dropout\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increasing the number of convolutional layers.\" is incorrect.</p><p>Increasing the number of convolutional layers will simply make the model more complex which may help overfitting on the training set.</p><p><strong>INCORRECT:</strong> \"Increasing the number of epochs.\" is incorrect.</p><p>The model already scored a high accuracy of 93% so increasing the epochs won\u2019t help the overfitting problem.</p><p><strong>INCORRECT:</strong> \"Do not use data augmentation.\" is incorrect.</p><p>Data augmentation is a method to increase the amount of training data by flipping, cropping, and applying other transformations on the dataset. Data augmentation helps the model not to overfit so it should be used.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html</a></p>", "answers": ["<p>Increasing the number of convolutional layers.</p>", "<p>Early stopping.</p>", "<p>Increasing the number of epochs.</p>", "<p>Dropout.</p>", "<p>Do not use data augmentation.</p>"]}, "correct_response": ["b", "d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning engineer noticed that a farm\u2019s weeds are harmful to the actual healthy crops and decided to create a computer vision model to help distinguish weeds from crops.A convolutional neural network was used for this use case and the engineer trained the model for 3 consecutive days with data augmentation during training. At the end of the third day, the engineer noticed that the training accuracy is 93% while the validation and test accuracies were capped at 76%.Which techniques were to be applied in order to overcome this situation? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 57678082, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a history of their access logs stored in their S3 bucket. The company\u2019s data engineer filtered out the dataset to only contain pairs of (user/IP) where it contains the name of the IAM user along with the IP address he used to access a specific service. The company recently suffered from a data breach coming from specific IP addresses which were recorded by the security team.</p><p>What approach should the company take to stop further breaches from these specific IP addresses in the least amount of time?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The company wants to stop the breaches from specific IP addresses <strong>only</strong>. So, using IP insights for training a model is not clearly the best solution.</p><p><strong>CORRECT: </strong>\" Use a rule-based solution.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use IP insights with (users/ID) pairs as input in order to output a score in the future that infers how anomalous the pattern of the event is.\" is incorrect.</p><p>While IP insights is used to detect anomalous IP addresses based on an anomaly score it outputs, however we only want to restrict access to specific IP addresses, so this is not the most suitable solution as just restricting those IPs.</p><p><strong>INCORRECT:</strong> \" Use Amazon Rekognition to spot anomalous IP addresses automatically.\" is incorrect.</p><p>Amazon Rekognition is a computer vision algorithm which is used to detect, classify objects within an image.</p><p><strong>INCORRECT:</strong> \" Use SageMaker\u2019s Object2Vec algorithm with (users/ID) pairs as input in order to output a score in the future that infers how anomalous the pattern of the event is IP insights, unsupervised learning.\" is incorrect.</p><p>Object2Vec will just create embeddings, however it won\u2019t find the anomalous IP addresses based on a score.</p>", "answers": ["<p>Use IP insights with (users/ID) pairs as input in order to output a score in the future that infers how anomalous the pattern of the event is.</p>", "<p>Use Amazon Rekognition to spot anomalous IP addresses automatically.</p>", "<p>Use SageMaker\u2019s Object2Vec algorithm with (users/ID) pairs as input in order to output a score in the future that infers how anomalous the pattern of the event is IP insights, unsupervised learning.</p>", "<p>Use a rule-based solution.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company has a history of their access logs stored in their S3 bucket. The company\u2019s data engineer filtered out the dataset to only contain pairs of (user/IP) where it contains the name of the IAM user along with the IP address he used to access a specific service. The company recently suffered from a data breach coming from specific IP addresses which were recorded by the security team.What approach should the company take to stop further breaches from these specific IP addresses in the least amount of time?", "related_lectures": []}, {"_class": "assessment", "id": 57678088, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to make a custom translation machine learning model. They have access to articles written in English, Italian, French and Arabic languages. Every article has its corresponding translation in these languages. They hired a machine learning specialist to build the model, however they noted that sentences sometimes are long which might be difficult to train.</p><p>What is the best model that the specialist should train the articles on?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>LSTM is a type of recurrent neural networks in which it has a strong memory which carries information from previous nodes in the neural network. It is most suitable for this type of machine translation as it has a memory to store information from long sentences.</p><p><strong>CORRECT: </strong>\"LSTM.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"GRU.\" is incorrect.</p><p>GRU is also an example of Recurrent Neural Networks \u201cRNN\u201d, however it does not have an internal memory to remember information for long sentences.</p><p><strong>INCORRECT:</strong> \"CNN.\" is incorrect.</p><p>Convolutional Neural Networks are used for computer vision problems not for sequence-to-sequence models.</p><p><strong>INCORRECT:</strong> \"Feedforward neural network.\" is incorrect.</p><p>Feedforward neural network are not suitable for sequence-to-sequence problems.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-howitworks.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-howitworks.html</a></p>", "answers": ["<p>LSTM.</p>", "<p>GRU.</p>", "<p>CNN.</p>", "<p>Feedforward neural network.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company wants to make a custom translation machine learning model. They have access to articles written in English, Italian, French and Arabic languages. Every article has its corresponding translation in these languages. They hired a machine learning specialist to build the model, however they noted that sentences sometimes are long which might be difficult to train.What is the best model that the specialist should train the articles on?", "related_lectures": []}]}
5824448
~~~
{"count": 20, "next": null, "previous": null, "results": [{"_class": "assessment", "id": 64762180, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company needs to initiate a streaming ingestion solution which ingests real-time structured data from an airline ticket booking system. The data should be analysed using business intelligence tools on a weekly basis and it should also detect anomalies from the same stream.</p><p>What is the cheapest and most efficient architectural solution that the company should adopt?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The keyword for choosing data streams over kinesis firehose is that the solution needs to be <strong>real-time. </strong>One tricky part is that kinesis firehose does not have the ability to send stream to multiple destinations while data streams can. Also, Redshift is a data warehouse which easily connects to business intelligence tools.</p><p>The figure below further illustrates the services used beyond kinesis analytics to trigger actions when anomalies are detected.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-07-13_20-33-14-ff64de0678641950f6d5d35a0ea7f6c1.png\"><p><strong>CORRECT: </strong>\"Use Kinesis data streams to collect the data, send the output to Amazon kinesis data analytics for anomaly detection. Also connect the output stream of the kinesis data streams to Amazon kinesis firehose which subsequently dumps the output to Amazon Redshift for weekly analysis.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Kinesis data streams to collect the data, send the output to Amazon kinesis data analytics for anomaly detection. Also connect the output stream of the kinesis data streams to Amazon kinesis firehose which subsequently dumps the output to an S3 bucket for weekly analysis.\" is incorrect.</p><p>The company needs to store data somewhere where it can use business intelligence tools where (Redshift) plays an important role. S3 is a data lake storing structured and unstructured data.</p><p><strong>INCORRECT:</strong> \" Use Kinesis firehose to collect the data, send the output to both Amazon kinesis data analytics for anomaly detection and S3 bucket for weekly analysis. \" is incorrect.</p><p>The solution needs <strong>real-time</strong> ingestion stream; however, kinesis firehose is for near real-time. Kinesis can only send data to one destination at the moment. Also, the destination should be Redshift not S3.</p><p><strong>INCORRECT:</strong> \"Use Kinesis firehose to collect the data, send the output to both Amazon kinesis data analytics for anomaly detection and Amazon Redshift for weekly analysis.\" is incorrect.</p><p>The solution needs <strong>real-time</strong> ingestion stream; however, kinesis firehose is for near real-time. Kinesis can only send data to one destination at the moment.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/redshift/\">https://aws.amazon.com/redshift/</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-analytics/\">https://aws.amazon.com/kinesis/data-analytics/</a></p>", "answers": ["<p>Use Kinesis data streams to collect the data, send the output to Amazon kinesis data analytics for anomaly detection. Also connect the output stream of the kinesis data streams to Amazon kinesis firehose which subsequently dumps the output to an S3 bucket for weekly analysis.</p>", "<p>Use Kinesis data streams to collect the data, send the output to Amazon kinesis data analytics for anomaly detection. Also connect the output stream of the kinesis data streams to Amazon kinesis firehose which subsequently dumps the output to Amazon Redshift for weekly analysis.</p>", "<p>Use Kinesis firehose to collect the data, send the output to both Amazon kinesis data analytics for anomaly detection and Amazon Redshift for weekly analysis.</p>", "<p>Use Kinesis firehose to collect the data, send the output to both Amazon kinesis data analytics for anomaly detection and S3 bucket for weekly analysis.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company needs to initiate a streaming ingestion solution which ingests real-time structured data from an airline ticket booking system. The data should be analysed using business intelligence tools on a weekly basis and it should also detect anomalies from the same stream.What is the cheapest and most efficient architectural solution that the company should adopt?", "related_lectures": []}, {"_class": "assessment", "id": 64762186, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A government has a GPS tracking system for all its citizens. Every single cell phone used by a citizen sends its GPS coordinates via the government\u2019s app to the cloud. The main goal is to identify traffic jams along the country. The app should stream the data and the streaming service should scale seamlessly without management beforehand. The streamed data should be checked against hotspots to identify if many coordinates were present near each other in the feature space. The output should trigger a function to send an SMS to the nearest traffic enforcers to act immediately.</p><p>What is the most appropriate and cost-effective combination of services that the government should use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The main difference between Amazon Kinesis Data Streams and Kinesis Firehose is that Data Streams requires scaling manually, while Kinesis Firehose is fully managed and can scale automatically. Spotting \u201chotspots\u201d is a feature in Amazon Kinesis Data Analytics using simple SQL commands. AWS lambda is, then, triggered and a notification is sent to the subscriber to take further action.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-22-18-9894372e7b616e6ad382e2dc0ed6d8e4.jpg\"><p><strong>CORRECT: </strong>\"Use Amazon Kinesis Data Firehose for data streaming from the application. The output should be sent to Amazon Kinesis Data Analytics to indicate the hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Analytics directly to ingest data and indicate the hotspots. The output stream is sent to lambda which takes an action to use the SNS notification service.\" is incorrect.</p><p>While Kinesis Data Analytics spots \u201chotspots\u201d within data, it supports only the following streaming sources: Kinesis Data Stream, Kinesis Firehose.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for data streaming from the application. The output should be sent to Amazon Kinesis Data Analytics to indicate the hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.\" is incorrect.</p><p>Kinesis Data Streams cannot be automatically scaled.</p><p><strong>INCORRECT:</strong> \"Use Amazon Kinesis Data Streams for data streaming from the application. The output should be sent to Amazon Kinesis Data Firehose to the indicate hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.\" is incorrect.</p><p>Kinesis Data Streams cannot be automatically scaled. Also, Firehose do not indicate hotspots present in data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-hotspots-detection.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-hotspots-detection.html</a></p>", "answers": ["<p>Use Amazon Kinesis Data Firehose for data streaming from the application. The output should be sent to Amazon Kinesis Data Analytics to indicate the hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.</p>", "<p>Use Amazon Kinesis Data Analytics directly to ingest data and indicate the hotspots. The output stream is sent to lambda which takes an action to use the SNS notification service.</p>", "<p>Use Amazon Kinesis Data Streams for data streaming from the application. The output should be sent to Amazon Kinesis Data Analytics to indicate the hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.</p>", "<p>Use Amazon Kinesis Data Streams for data streaming from the application. The output should be sent to Amazon Kinesis Data Firehose to indicate the hotspots. Finally, the output stream is sent to lambda which takes an action to use the SNS notification service.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A government has a GPS tracking system for all its citizens. Every single cell phone used by a citizen sends its GPS coordinates via the government\u2019s app to the cloud. The main goal is to identify traffic jams along the country. The app should stream the data and the streaming service should scale seamlessly without management beforehand. The streamed data should be checked against hotspots to identify if many coordinates were present near each other in the feature space. The output should trigger a function to send an SMS to the nearest traffic enforcers to act immediately.What is the most appropriate and cost-effective combination of services that the government should use?", "related_lectures": []}, {"_class": "assessment", "id": 64762190, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning engineer wants to build a linear regression model. The training data includes 10 features including income, incentives, health insurance, taxes. The engineer wants to check if the dataset contains outliers. He wants to visualize the data and at the same time check for these outliers.</p><p>Which of the following graphs could be used to fulfill his needs? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>A Box and whisker plot shows the distribution of the data along with outliers. The definition of an outlier could differ from a problem to another, but it is usually a data point outside the whiskers of the box plot. Also, outliers could be easily spot from a histogram.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-25-01-797ed9cbf0068a6db0bb0df67929a297.jpg\"><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-25-01-0f60f472f637b395ec26ffdb8f1b92c3.jpg\"><p><strong>CORRECT: </strong>\"Box and whisker plot <strong>AND</strong> Histogram\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Heatmap\" is incorrect.</p><p>Heatmap is a visualization of the magnitude of a metric in colors.</p><p><strong>INCORRECT:</strong> \"Tree map\" is incorrect.</p><p>Tree map is used to show a hierarchical view of items.</p><p><strong>INCORRECT:</strong> \"Bubble chart\" is incorrect.</p><p>Bubble chart is a 3D chart for visualizing 3-dimension data on a 2D axis where the third dimension is the size of the point on the plot.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/quicksight/latest/user/working-with-visual-types.html\">https://docs.aws.amazon.com/quicksight/latest/user/working-with-visual-types.html</a></p><p><br></p>", "answers": ["<p>Box and whisker plot</p>", "<p>Heatmap</p>", "<p>Histogram</p>", "<p>Tree map</p>", "<p>Bubble chart</p>"]}, "correct_response": ["a", "c"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A machine learning engineer wants to build a linear regression model. The training data includes 10 features including income, incentives, health insurance, taxes. The engineer wants to check if the dataset contains outliers. He wants to visualize the data and at the same time check for these outliers.Which of the following graphs could be used to fulfill his needs? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 64762192, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company has a massive dataset of petabytes-scale containing high quality images of DNA shapes. The company aims to generate DNA using a Generative Adversarial Network. This data should enter the process of data preparation, feature engineering, model training, model evaluation, and model deployment.</p><p>Given the scale of this data, which services should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cAmazon EMR (previously called Amazon Elastic MapReduce) is a managed cluster platform that simplifies running big data frameworks, such as <a href=\"https://aws.amazon.com/elasticmapreduce/details/hadoop\">Apache Hadoop</a> and <a href=\"https://aws.amazon.com/elasticmapreduce/details/spark\">Apache Spark</a>, on AWS to process and analyze vast amounts of data.\u201d. On the other hand, Amazon SageMaker is used for the actual training and model deployment. If EMR and SageMaker should be used as in the above use case, then the data could be pre-processed in Amazon EMR, then SageMaker could be called using the SageMaker\u2019s SDK for training and deployment.</p><p><strong>CORRECT: </strong>\" Use Amazon EMR in a zeppelin notebook for data preparation, feature engineering and Amazon SageMaker SDK for the other processes.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Use Amazon SageMaker for all data processes.\" is incorrect.</p><p>The scale of the data is massive, so Amazon SageMaker would be slow in data pre-processing.</p><p><strong>INCORRECT:</strong> \"Use Amazon EMR for all data processes.\" is incorrect.</p><p>Amazon EMR is great at pre-processing massive datasets, however SageMaker is better in training and deploying models.</p><p><strong>INCORRECT:</strong> \"Use Amazon SageMaker in a zeppelin notebook for model training, model evaluation, and model deployment and Amazon EMR SDK in the same notebook for the other processes.\" is incorrect.</p><p>SageMaker uses Jupyter notebooks not zeppelin.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-sagemaker.html\">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-sagemaker.html</a></p>", "answers": ["<p>Use Amazon SageMaker for all data processes.</p>", "<p>Use Amazon EMR for all data processes.</p>", "<p>Use Amazon EMR in a zeppelin notebook for data preparation, feature engineering and Amazon SageMaker SDK for the other processes.</p>", "<p>Use Amazon SageMaker in a zeppelin notebook for model training, model evaluation, and model deployment and Amazon EMR SDK in the same notebook for the other processes.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company has a massive dataset of petabytes-scale containing high quality images of DNA shapes. The company aims to generate DNA using a Generative Adversarial Network. This data should enter the process of data preparation, feature engineering, model training, model evaluation, and model deployment.Given the scale of this data, which services should be used?", "related_lectures": []}, {"_class": "assessment", "id": 64762194, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A hyperparameter tuning job is about to start in Amazon SageMaker. The data engineer wants to run several parallel training jobs at a time, then make use of the knowledge gained about the best hyperparameters range to run another parallel training jobs until the recommended accuracy is reached.</p><p>Which is the best search technique for this scenario?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS quotes \u201cBayesian search treats hyperparameter tuning like a <a href=\"https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#[regression]\">[regression]</a> problem. Given a set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the metric that you choose. To solve a regression problem, hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results and runs training jobs to test these values. After testing the first set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test.\u201d</p><p>In random search, hyperparameter tuning chooses a random combination from a given range and kicks off multiple training jobs at the same time. These jobs are not dependent on each other, and knowledge is not passed from a job to another as they are run in parallel.</p><p>AWS does not support grid search method as it is arguably inefficient compared to random and Bayesian search.</p><p><strong>CORRECT: </strong>\"Bayesian search\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Grid search\" is incorrect.</p><p>Grid search is not supported by AWS as it is arguably inefficient.</p><p><strong>INCORRECT:</strong> \"Random search\" is incorrect.</p><p>Random search does not pass knowledge about the best upcoming hyperparameter ranges from a job to another.</p><p><strong>INCORRECT:</strong> \"Random and grid search combined\" is incorrect.</p><p>This method is not possible.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html</a></p>", "answers": ["<p>Grid Search.</p>", "<p>Bayesian Search.</p>", "<p>Random Search.</p>", "<p>Random and Grid Search combined.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A hyperparameter tuning job is about to start in Amazon SageMaker. The data engineer wants to run several parallel training jobs at a time, then make use of the knowledge gained about the best hyperparameters range to run another parallel training jobs until the recommended accuracy is reached.Which is the best search technique for this scenario?", "related_lectures": []}, {"_class": "assessment", "id": 64762196, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning engineer has a dataset containing images of ten different types of plants and wants to create a computer vision model to distinguish between those types. The engineer trained a feedforward neural network using raw image pixels. The engineer noticed that the neural network is taking a long time to train.</p><p>What could be a solution to this problem? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Pixel values range from 0 to 255. We should normalize the pixels to much smaller range such as a 0-1 range. This ensures that the computation becomes easier and faster and for the model to converge faster. A convolutional neural network is better than feedforward neural networks in the computer vision problems as it uses filter layers to spot specific features such as horizontal edges, vertical edges, etc. in an image.</p><p><strong>CORRECT: </strong>\"Normalizing the input pixels <strong>and</strong> Using convolutional neural network instead.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Increasing the number of layers.\" is incorrect.</p><p>Increasing the number of layers makes the neural network more complex. The problem is caused by not applying normalization on the pixels.</p><p><strong>INCORRECT:</strong> \"Decreasing the learning rate.\" is incorrect.</p><p>Decreasing the learning rate makes the training process slower.</p><p><strong>INCORRECT:</strong> \"Increasing the number of neurons per hidden layer.\" is incorrect.</p><p>Increasing the number of neurons per hidden layer makes the neural network more complex. The problem is caused by not applying normalization on the pixels.</p>", "answers": ["<p>Increasing the number of layers.</p>", "<p>Normalizing the input pixels.</p>", "<p>Decreasing the learning rate.</p>", "<p>Using convolutional neural network instead.</p>", "<p>Increasing the number of neurons per hidden layer.</p>"]}, "correct_response": ["b", "d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning engineer has a dataset containing images of ten different types of plants and wants to create a computer vision model to distinguish between those types. The engineer trained a feedforward neural network using raw image pixels. The engineer noticed that the neural network is taking a long time to train.What could be a solution to this problem? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 64762198, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A clothing dataset contains features such as \u201cSize\u201d, \u201cType\u201d, \u201cOrigin\u201d. The \u201cType\u201d column contains the values: \u201cSweater\u201d, \u201cDress\u201d and \u201cSkirt\u201d. A machine learning specialist wants to build a model to predict the prices of clothes given those features. The specialist wants to first apply a feature engineering technique on the \u201cType\u201d column.</p><p>What is the most appropriate feature engineering technique to be used on this column?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This is an example of nominal data. The most appropriate technique for this data type is to one-hot encode the labels. One-hot encoding is simply providing extra columns with their number being equal to the number of labels such as \u201cType_Sweater\u201d, \u201cType_Dress\u201d, \u201cType_Skirt\u201d. A \u201c1\u201d value is placed in the label\u2019s column corresponding to a given row and the other labels take a \u201c0\u201d value. This will make the model\u2019s training much easier.</p><p><strong>CORRECT: </strong>\"One-hot encoding.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Mapping each label to a corresponding number.\" is incorrect.</p><p>This would be a suitable technique for ordinal data not for nominal ones as this would give a biased order to the data.</p><p><strong>INCORRECT:</strong> \"Normalization.\" is incorrect.</p><p>Normalization is applied on numerical data not categorical ones.</p><p><strong>INCORRECT:</strong> \"Log transformation.\" is incorrect.</p><p>Log transformation is applied on numerical data not categorical ones.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html</a></p>", "answers": ["<p>One-hot encoding.</p>", "<p>Mapping each label to a corresponding number.</p>", "<p>Normalization.</p>", "<p>Log transformation.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A clothing dataset contains features such as \u201cSize\u201d, \u201cType\u201d, \u201cOrigin\u201d. The \u201cType\u201d column contains the values: \u201cSweater\u201d, \u201cDress\u201d and \u201cSkirt\u201d. A machine learning specialist wants to build a model to predict the prices of clothes given those features. The specialist wants to first apply a feature engineering technique on the \u201cType\u201d column.What is the most appropriate feature engineering technique to be used on this column?", "related_lectures": []}, {"_class": "assessment", "id": 64762200, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A data scientist trained a binary classification model to predict if it will rain based on some features. Using the confusion matrix below, determine the model\u2019s F1 score and recall.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_11-30-04-242d0f36e92df8b9285d3b78eb9901e5.jpg\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-24_10-29-17-db1f7621aea72f1fc561de8def19104b.png\"><p><strong>CORRECT: </strong>\"F1 score = 3/8, recall = 3/4 \" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"F1 score = 3/8, recall = 1/4\" is incorrect.</p><p>Recall value is incorrect.</p><p><strong>INCORRECT:</strong> \"F1 score = 1/4, recall = 3/8\" is incorrect.</p><p>Both values are incorrect.</p><p><strong>INCORRECT:</strong> \"F1 score = 3/16, recall = 1/4 \" is incorrect.</p><p>This is a common mistake as people tend to calculate precision and recall and F1 score but they forget the \u201c2*\u201d in the F1 score\u2019s equation. F1 score is 2 * (3/16).</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>F1 score = 3/8, recall = 1/4</p>", "<p>F1 score = 1/4, recall = 3/8</p>", "<p>F1 score = 3/16, recall = 1/4</p>", "<p>F1 score = 3/8, recall = 3/4</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A data scientist trained a binary classification model to predict if it will rain based on some features. Using the confusion matrix below, determine the model\u2019s F1 score and recall.", "related_lectures": []}, {"_class": "assessment", "id": 64762202, "assessment_type": "multi-select", "prompt": {"question": "<p>A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. 10,000 income\u2019s data are found missing which resemble 10% of the total income data. Income\u2019s data are in USD and range from $2,000 to $5,000,000. The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias.</p><p>What are the suitable techniques for imputing missing data in this case? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>MICE technique is by far the most modern technique used for imputing missing data.</p><p>Missing data are one of the main problems which are inevitable in most of the datasets. There are many ways to overcome this problem such as:</p><ul><li><p>Deleting the missing rows entirely. While this is the easiest solution, it simply deletes many valuable information.</p></li><li><p>Imputing missing data using mean. This is a bad technique to be used if the dataset contains outliers.</p></li><li><p>Imputing missing data using mode. This is suitable for categorical features.</p></li><li><p>Imputing missing data using KNN. This is suitable for numerical features.</p></li><li><p>Imputing missing data using deep learning. This is suitable and accurate for categorical features.</p></li><li><p>Imputing missing data using zeros: This will alter the data\u2019s distribution and could introduce bias.</p></li></ul><p><strong>CORRECT: </strong>\"Impute the missing data using MICE technique <strong>and</strong> Impute the missing data using KNN.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Add the value \u201c0\u201d to the missing data.\" is incorrect.</p><p>This will alter the data\u2019s distribution and could introduce bias.</p><p><strong>INCORRECT:</strong> \"Impute the missing data using the mean of the income\u2019s column.\" is incorrect.</p><p>This is a bad technique to be used if the dataset contains outliers. Also, it could introduce bias.</p><p><strong>INCORRECT:</strong> \"Impute the missing data using deep learning.\" is incorrect.</p><p>This is suitable and accurate for categorical features.</p>", "answers": ["<p>Impute the missing data using MICE technique.</p>", "<p>Add the value \u201c0\u201d to the missing data.</p>", "<p>Impute the missing data using the mean of the income\u2019s column.</p>", "<p>Impute the missing data using KNN.</p>", "<p>Impute the missing data using deep learning.</p>"]}, "correct_response": ["a", "d"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A governmental dataset contains the following columns: \u201cID\u201d, \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d. The dataset contains 100,000 entry each corresponding to information about a person in a specific city. 10,000 income\u2019s data are found missing which resemble 10% of the total income data. Income\u2019s data are in USD and range from $2,000 to $5,000,000. The data was collected a few months ago and the government has no intention to spend time or budget on requesting the missing data from citizens. The machine learning engineer should search for a solution to complete those missing data without introducing bias.What are the suitable techniques for imputing missing data in this case? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 64762204, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A government\u2019s dataset contains 50 features including: \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d, \u201csocial status\u201d, \u201ceducation level\u201d and the target column \u201cis_criminal\u201d. The governorate believes it could predict whether a citizen will commit a crime in the future using those features. The dataset contains 100,000 entry each corresponding to information about a citizen. The government hired the top four machine learning specialists and each one of them produced a model along with its \u201cROC\u201d curve. The figure below shows the ROC curves of the 4 models.</p><p>Which model should the government adopt?</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_11-34-34-8fff9af9d0356804f6aed1a3dbbdacb9.JPG\">", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The AUC (Area under curve) of the ROC curve is an evaluation of how well the model is performing. The higher the AUC, the better the model in distinguishing between classes. Model 3 clearly has the highest area under the curve which is clearly shown in the following figure which omits other models to illustrate the area.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-34-35-be17b3e256df8ed7d2c5bc3728a94cd8.JPG\"><p><strong>CORRECT: </strong>\"Model 3\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Model 1\" is incorrect.</p><p>Model 3 has higher AUC.</p><p><strong>INCORRECT:</strong> \"Model 2\" is incorrect.</p><p>Model 3 has higher AUC.</p><p><strong>INCORRECT:</strong> \"Model 4\" is incorrect.</p><p>Model 3 has higher AUC.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>Model 1.</p>", "<p>Model 2.</p>", "<p>Model 3.</p>", "<p>Model 4.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A government\u2019s dataset contains 50 features including: \u201cGender\u201d, \u201cAge\u201d, \u201cIncome\u201d, \u201cdebt\u201d, \u201csocial status\u201d, \u201ceducation level\u201d and the target column \u201cis_criminal\u201d. The governorate believes it could predict whether a citizen will commit a crime in the future using those features. The dataset contains 100,000 entry each corresponding to information about a citizen. The government hired the top four machine learning specialists and each one of them produced a model along with its \u201cROC\u201d curve. The figure below shows the ROC curves of the 4 models.Which model should the government adopt?", "related_lectures": []}, {"_class": "assessment", "id": 64762206, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company uses anomaly detection to determine faulty servers within their fleet of servers. The company ingests real-time structured data containing features of their servers including -but not limited to- \u201cTemperature\u201d and \u201cUsage\u201d using Kinesis data streams. The data is sent to Kinesis Data Analytics for live anomaly detection which sends an alarm using kinesis data streams, lambda and SNS if a faulty server was found and at the same time to Kinesis Firehose to be dumped in S3 for storage.</p><p>The company found that the system has been reliable enough to dispense employees looking after the servers on a daily basis and decided to only hire a small team to take care of a faulty server when it fails. The company lately installed a data center and connected the new servers\u2019 sensors to the same architecture.</p><p>One month later, they found out that 25 servers had failed without even noticing. A team was responsible for this investigation and the first thing was to check the data residing in S3 which held the server\u2019s features. The data stored in the S3 bucket showed that those faulty servers sent information about their health, but no action was taken then.</p><p>What was the most probable solution to this problem?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The company stated that they checked the data residing in S3, which held the faulty servers\u2019 data, and ensured that they sent the information back then when they went down, so there is no problem with the kinesis firehose. Also, if there was a problem with the kinesis firehose, we cannot change shards as we basically do not have access to them. Also, sensors already sent the information to the S3 bucket, so they were working back then. The only possible explanation is that the Kinesis Data Streams was overwhelmed by data which exceeded its limit, and the data was rejected with a \u201cProvisionedThroughputExceeded\u201d exception. The reason is that the company installed other services without increasing the capacity of the kinesis data streams. The solution to this was to increase the number of shards back then to have extra space.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_11-35-58-926be98cab48e27a2e955fed408173ac.jpg\"><p><strong>CORRECT: </strong>\" Increasing the number of shards of the Kinesis Data Streams using the \u201cUpdateShardCount\u201d API.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Increasing the number of shards of the Kinesis Firehose using the \u201c<a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html\">UpdateShardCount\u201d API</a>.\" is incorrect.</p><p>You cannot control the number of shards in a Kinesis Firehose.</p><p><strong>INCORRECT:</strong> \" Substituting the \u201cTemperature\u201d and other sensors with new ones as they were probably faulty.\" is incorrect.</p><p>The sensors were healthy as they sent data to Firehose -&gt; S3.</p><p><strong>INCORRECT:</strong> \" Decreasing the number of shards of the Kinesis Firehose using the \u201cUpdateShardCount\u201d API.\" is incorrect.</p><p>You cannot control number of shards in a Kinesis Firehose.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html\">https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html</a></p>", "answers": ["<p>Increasing the number of shards of the Kinesis Firehose using the \u201cUpdateShardCount\u201d API.</p>", "<p>Substituting the \u201cTemperature\u201d and other sensors with new ones as they were probably faulty.</p>", "<p>Increasing the number of shards of the Kinesis Data Streams using the \u201cUpdateShardCount\u201d API.</p>", "<p>Decreasing the number of shards of the Kinesis Firehose using the \u201cUpdateShardCount\u201d API.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A company uses anomaly detection to determine faulty servers within their fleet of servers. The company ingests real-time structured data containing features of their servers including -but not limited to- \u201cTemperature\u201d and \u201cUsage\u201d using Kinesis data streams. The data is sent to Kinesis Data Analytics for live anomaly detection which sends an alarm using kinesis data streams, lambda and SNS if a faulty server was found and at the same time to Kinesis Firehose to be dumped in S3 for storage.The company found that the system has been reliable enough to dispense employees looking after the servers on a daily basis and decided to only hire a small team to take care of a faulty server when it fails. The company lately installed a data center and connected the new servers\u2019 sensors to the same architecture.One month later, they found out that 25 servers had failed without even noticing. A team was responsible for this investigation and the first thing was to check the data residing in S3 which held the server\u2019s features. The data stored in the S3 bucket showed that those faulty servers sent information about their health, but no action was taken then.What was the most probable solution to this problem?", "related_lectures": []}, {"_class": "assessment", "id": 64762208, "assessment_type": "multi-select", "prompt": {"question": "<p>A machine learning specialist is handed a dataset containing articles written in different languages with the goal of producing a model which can translate a given input to a specified language. The specialist built two machine learning models using Amazon\u2019s Seq2seq algorithm and would like to compare between them to deploy the most effective model.</p><p>Which metrics/scores should be used to compare the models? (Select TWO.)</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", "", ""], "explanation": "<p>Perplexity and BLEU score are used specifically for evaluating the performance of the machine-translated text. BLEU score represents a number between \u201c0\u201d and \u201c1\u201d or could be represented as a percentage where the higher the BLEU score, the better the translation quality.</p><p><strong>CORRECT: </strong>\"BLEU score <strong>and</strong> Perplexity.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Precision.\" is incorrect.</p><p>Precision is used in evaluating classification problems.</p><p><strong>INCORRECT:</strong> \"Recall.\" is incorrect.</p><p>Recall is used in evaluating classification problems.</p><p><strong>INCORRECT:</strong> \"RMSE.\" is incorrect.</p><p>RMSE is used in evaluating regression problems.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-tuning.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-tuning.html</a></p>", "answers": ["<p>Precision.</p>", "<p>Recall.</p>", "<p>Perplexity.</p>", "<p>RMSE.</p>", "<p>BLEU score.</p>"]}, "correct_response": ["c", "e"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist is handed a dataset containing articles written in different languages with the goal of producing a model which can translate a given input to a specified language. The specialist built two machine learning models using Amazon\u2019s Seq2seq algorithm and would like to compare between them to deploy the most effective model.Which metrics/scores should be used to compare the models? (Select TWO.)", "related_lectures": []}, {"_class": "assessment", "id": 64762210, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A massive ecommerce website has hundreds of products in electronics, clothes, and machine parts. The company owning the website wants to enhance their user experience, so they hired a machine learning engineer to create a model. The model\u2019s purpose is to predict whether a customer will buy a product from their website.</p><p>They handed the engineer a database containing features which influences the customers\u2019 purchasing intention and the target label \u201cPurchase\u201d which indicates whether the past customer purchased a product or not. The engineer wants to train a model to predict, given those features, whether a customer would purchase any product on the website or not.</p><p>Which algorithm should be used?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Logistic regression is a supervised learning technique which uses a logistic function where given specific features, it can predict the outcome as a probability. Note, that the features are already present that\u2019s why it\u2019s a supervised learning technique.</p><p><strong>CORRECT: </strong>\"Logistic regression.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Linear regression.\" is incorrect.</p><p>Linear regression is used for predicting numerical values. In this problem we want to predict a categorical value which decides whether a customer will purchase a product or not.</p><p><strong>INCORRECT:</strong> \"K-means clustering.\" is incorrect.</p><p>K-means clustering is used when we want to segment different groups in the feature space carrying similar traits.</p><p><strong>INCORRECT:</strong> \"Collaborative filtering.\" is incorrect.</p><p>Collaborative filtering is used in recommender systems for calculating ratings based on ratings of similar users.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html</a></p>", "answers": ["<p>Linear Regression.</p>", "<p>Logistic Regression.</p>", "<p>K-means clustering.</p>", "<p>Collaborative filtering.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A massive ecommerce website has hundreds of products in electronics, clothes, and machine parts. The company owning the website wants to enhance their user experience, so they hired a machine learning engineer to create a model. The model\u2019s purpose is to predict whether a customer will buy a product from their website.They handed the engineer a database containing features which influences the customers\u2019 purchasing intention and the target label \u201cPurchase\u201d which indicates whether the past customer purchased a product or not. The engineer wants to train a model to predict, given those features, whether a customer would purchase any product on the website or not.Which algorithm should be used?", "related_lectures": []}, {"_class": "assessment", "id": 64762212, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist was hired by a hospital to produce a model to predict whether a patient has diabetes. The specialist collected information from previous patients such as \u201cblood pressure\u201d, \u201cpregnancies\u201d, \u201cBMI\u201d and age. She created a model using logistic regression algorithm with a threshold of 0.7. She also produced the following ROC graph. Unfortunately, she did not achieve the business metric which was set by the hospital for the model to score at least 80% sensitivity. The hospital demands that the specialist should find a solution to achieve this metric.</p><p>The graph below shows the ROC curve with the threshold value at each point.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question/2023-03-20_12-03-12-4c83ae9d23ef0e8390a0702e92838c03.jpg\"><p>What is the most suitable action that the specialist should take?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>The ROC curve illustrates the effect of changing the threshold on the false positive rate and the true positive rate. The problem is clearly with the threshold value as it should be set to a value near 0.4 which will surpass the business\u2019s metric (80%). Also, you should know that the true positive rate is the same as the sensitivity of the mode.</p><p>AWS states:</p><p>\u201cAn ML model that has good predictive accuracy will predict higher scores to the actual 1s and lower scores to the actual 0s. A perfect model will have the two histograms at two different ends of the x-axis showing that actual positives all got high scores and actual negatives all got low scores. However, ML models make mistakes, and a typical graph will show that the two histograms overlap at certain scores. An extremely poor performing model will be unable to distinguish between the positive and negative classes, and both classes will have mostly overlapping histograms.\u201d</p><p>This graph is another visualization to the effect of altering the threshold.</p><img src=\"https://img-b.udemycdn.com/redactor/raw/practice_test_question_explanation/2023-03-20_12-03-13-d03e143fa47421916bfad9eef2674faf.jpg\"><p><strong>CORRECT: </strong>\" The threshold should be decreased.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" She should use XGBoost algorithm instead.\" is incorrect.</p><p>By checking the ROC curve, it is obvious that changing the threshold only will result in achieving the business\u2019s metric. Changing the algorithm is not the most suitable solution.</p><p><strong>INCORRECT:</strong> \"The threshold should be increased.\" is incorrect.</p><p>Increasing the threshold will worsen the sensitivity (True positive rate).</p><p><strong>INCORRECT:</strong> \"The model should be trained for more epochs.\" is incorrect.</p><p>We cannot deduce that the model should be further trained from the ROC curve. Also, this is not the most suitable solution to this scenario as the ROC curve clearly identifies that the problem is in choosing the wrong threshold value.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html</a></p>", "answers": ["<p>She should use XGBoost algorithm instead.</p>", "<p>The threshold should be increased.</p>", "<p>The threshold should be decreased.</p>", "<p>The model should be trained for more epochs.</p>"]}, "correct_response": ["c"], "section": "MLS Domain 3 - Modeling", "question_plain": "A machine learning specialist was hired by a hospital to produce a model to predict whether a patient has diabetes. The specialist collected information from previous patients such as \u201cblood pressure\u201d, \u201cpregnancies\u201d, \u201cBMI\u201d and age. She created a model using logistic regression algorithm with a threshold of 0.7. She also produced the following ROC graph. Unfortunately, she did not achieve the business metric which was set by the hospital for the model to score at least 80% sensitivity. The hospital demands that the specialist should find a solution to achieve this metric.The graph below shows the ROC curve with the threshold value at each point.What is the most suitable action that the specialist should take?", "related_lectures": []}, {"_class": "assessment", "id": 64762214, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning specialist is handed a dataset of customers\u2019 audio calls with the goal of transcribing those calls. The company has a budget for labelling those calls so as for the machine learning specialist to train on. The company has a strict policy stating that any customer related data including audio calls should not be exposed to the public. The specialist therefore needs a workforce from inside the company to work on the task of transcribing those calls.</p><p>Which task labelling type should the specialist use, and which algorithm should be adopted to train on the data given that the inputs should be the audio and the outputs should be the text in this audio?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>A private labelling workforce should label the data so Amazon Ground truth (private labelling workforce) should be chosen to privately label the data. Also, Seq2Seq is a supervised learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.</p><p><strong>CORRECT: </strong>\" Use Amazon Ground Truth (private labelling workforce) to label the data and Seq2Seq algorithm to train the model.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Use Amazon Ground Truth (private labelling workforce) to label the data and XGBoost\u2019s algorithm to train the model.\" is incorrect.</p><p>XGBoost cannot handle the task of predicting text given an audio.</p><p><strong>INCORRECT:</strong> \" Use Amazon Ground Truth (mechanical turk) to label the data and Seq2Seq algorithm to train the model.\" is incorrect.</p><p>A private workforce should be chosen as mechanical turk is Amazon\u2019s public workforce.</p><p><strong>INCORRECT:</strong> \" Use Amazon Ground Truth (mechanical turk) to label the data and DeepAR algorithm to train the model.\" is incorrect.</p><p>A private workforce should be chosen as mechanical turk is Amazon\u2019s public workforce. Also, DeepAR algorithm is used for forecasting one dimensional time series data so it cannot handle predicting sequences of text given a sequence of audio.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-console.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-console.html</a></p>", "answers": ["<p>Use Amazon Ground Truth (private labelling workforce) to label the data and XGBoost\u2019s algorithm to train the model.</p>", "<p>Use Amazon Ground Truth (mechanical turk) to label the data and Seq2Seq algorithm to train the model.</p>", "<p>Use Amazon Ground Truth (mechanical turk) to label the data and DeepAR algorithm to train the model.</p>", "<p>Use Amazon Ground Truth (private labelling workforce) to label the data and Seq2Seq algorithm to train the model.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 2 - Exploratory Data Analysis", "question_plain": "A machine learning specialist is handed a dataset of customers\u2019 audio calls with the goal of transcribing those calls. The company has a budget for labelling those calls so as for the machine learning specialist to train on. The company has a strict policy stating that any customer related data including audio calls should not be exposed to the public. The specialist therefore needs a workforce from inside the company to work on the task of transcribing those calls.Which task labelling type should the specialist use, and which algorithm should be adopted to train on the data given that the inputs should be the audio and the outputs should be the text in this audio?", "related_lectures": []}, {"_class": "assessment", "id": 64762216, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company noticed that there was a moderate difference between the training data they used to train their machine learning model on and the new incoming data. This change happens on a daily or weekly basis, so they decided to create a workflow in which they could orchestrate the process of extracting their data from their S3 bucket, apply simple transformations on this data, train the data using Amazon SageMaker, save the new model\u2019s artifact and finally bring this model to production.</p><p>Which AWS services could be used for both triggering the task daily/weekly and orchestrating the whole workflow?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>Here, we want to use a high-level orchestration service which orchestrates the workflows on AWS services level, so AWS step functions is used. Also, CloudWatch events have cron jobs to trigger AWS services at certain time intervals.</p><p><strong>CORRECT: </strong>\" Amazon CloudWatch events to trigger the job and AWS step functions to orchestrate the workflow.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Amazon CloudWatch alarms to trigger the job and AWS Batch to orchestrate the workflow \" is incorrect.</p><p>AWS Batch is used to orchestrate jobs on an EC2 level, while Step Functions is used for orchestrating workflows on AWS services\u2019 level. Also, CloudWatch events are used to trigger services not alarms.</p><p><strong>INCORRECT:</strong> \" Amazon CloudWatch alarms to trigger the job and AWS Lambda to orchestrate the workflow.\" is incorrect.</p><p>Lambda could be used to orchestrate pieces of the workflow such as training the model, waiting for training, deploying the model, etc. However, we are looking for a service to orchestrate the whole workflow. Also, CloudWatch events are used to trigger services not alarms.</p><p><strong>INCORRECT:</strong> \" Amazon CloudWatch events to trigger the job and AWS Lambda to orchestrate the workflow.\" is incorrect.</p><p>Lambda could be used to orchestrate pieces of the workflow such as training the model, waiting for training, deploying the model, etc. However, we are looking for a service to orchestrate the whole workflow.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-target.html\">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-target.html</a></p><p><a href=\"https://www.sagemakerworkshop.com/introduction/\">https://www.sagemakerworkshop.com/introduction/</a></p>", "answers": ["<p>Amazon CloudWatch alarms to trigger the job and AWS Batch to orchestrate the workflow.</p>", "<p>Amazon CloudWatch alarms to trigger the job and AWS Lambda to orchestrate the workflow.</p>", "<p>Amazon CloudWatch events to trigger the job and AWS Lambda to orchestrate the workflow.</p>", "<p>Amazon CloudWatch events to trigger the job and AWS step functions to orchestrate the workflow.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A company noticed that there was a moderate difference between the training data they used to train their machine learning model on and the new incoming data. This change happens on a daily or weekly basis, so they decided to create a workflow in which they could orchestrate the process of extracting their data from their S3 bucket, apply simple transformations on this data, train the data using Amazon SageMaker, save the new model\u2019s artifact and finally bring this model to production.Which AWS services could be used for both triggering the task daily/weekly and orchestrating the whole workflow?", "related_lectures": []}, {"_class": "assessment", "id": 64762218, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A Machine Learning Specialist built a computer vision model to classify a given image into ten separate categories. The model achieved the following accuracies:</p><p>Training accuracy = 95%</p><p>Validation accuracy = 76%</p><p>Test accuracy = 73%</p><p>The business\u2019s metric is to score an accuracy of 80% on the test set and an 85% accuracy on the training set. Which action should the specialist take to improve the metrics?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>We cannot combine all the datasets together. The model should be trained on the training dataset <strong>ONLY </strong>and validated using the validation dataset and tested using test dataset. If we combined all the datasets together, we could reach a higher accuracy in the validation and test datasets, however, we cannot do this as this may lead to the model fitting on all this data and we couldn\u2019t have an insight on the behavior of the model in future datasets where the model hasn\u2019t seen them in training. They should be used only for validation and testing purposes.</p><p><strong>CORRECT: </strong>\" Apply regularization techniques while training such as data augmentation. Train on the training dataset only.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" Combine all the datasets (train+validation+test) into a massive dataset and train the model on this new dataset.\" is incorrect.</p><p>If we did so, we won\u2019t have an insight on the behavior of the model when inferencing on new data.</p><p><strong>INCORRECT:</strong> \" Combine training and validation datasets into a massive dataset and train the model on this new dataset.\" is incorrect.</p><p>If we did so, we won\u2019t have an insight on the behavior of the model when inferencing on new data.</p><p><strong>INCORRECT:</strong> \" Combine all the datasets (train+validation+test) into a massive dataset and train the model on this new dataset. Also, apply regularization techniques while training such as data augmentation.\" is incorrect.</p><p>While applying regularization techniques is correct to prevent the model from overfitting and increase the accuracy of the validation and test sets, we cannot combine datasets and train on them.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html</a></p>", "answers": ["<p>Combine all the datasets (train+validation+test) into a massive dataset and train the model on this new dataset.</p>", "<p>Combine training and validation datasets into a massive dataset and train the model on this new dataset.</p>", "<p>Combine all the datasets (train+validation+test) into a massive dataset and train the model on this new dataset. Also, apply regularization techniques while training such as data augmentation.</p>", "<p>Apply regularization techniques while training such as data augmentation. Train on the training dataset only.</p>"]}, "correct_response": ["d"], "section": "MLS Domain 3 - Modeling", "question_plain": "A Machine Learning Specialist built a computer vision model to classify a given image into ten separate categories. The model achieved the following accuracies:Training accuracy = 95%Validation accuracy = 76%Test accuracy = 73%The business\u2019s metric is to score an accuracy of 80% on the test set and an 85% accuracy on the training set. Which action should the specialist take to improve the metrics?", "related_lectures": []}, {"_class": "assessment", "id": 64762220, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A company wants to predict the most suitable type of house for given requirements using machine learning. A machine learning engineered was given a dataset containing, but not limited to, \u201cSize\u201d, \u201clocation\u201d, \u201cparks\u201d and \u201cnumber_bedrooms\u201d. The target feature is the type of the house which contains either \u201cmodern\u201d or \u201cold\u201d and the labels are balanced through the entire dataset. He produced 2 different models using SageMaker\u2019s XGBoost and linear learner. The company wants the models to be evaluated without bias and an estimated average error should be presented for each model.</p><p>Which is the most suitable method the engineer should use to achieve these results?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cIn k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You train an ML model on all but one (k-1) of the subsets, and then evaluate the model on the subset that was not used for training. This process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time.</p><p><strong>CORRECT: </strong>\"K-fold cross-validation\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Holdout cross-validation\" is incorrect.</p><p>While holdout could be used, however it is not the most suitable validation technique as the model trains on a specific set and is tested on another specific set. In k-fold cross validation, the whole dataset is split into k folds and every time this fold is changed both for validation and test.</p><p><strong>INCORRECT:</strong> \"Stratified K-fold cross validation\" is incorrect.</p><p>This is suitable for unbalanced dataset.</p><p><strong>INCORRECT:</strong> \"RMSE\" is incorrect.</p><p>Root Mean Squared Error \u201cRMSE\u201d is a method of evaluating regression models.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/cross-validation.html\">https://docs.aws.amazon.com/machine-learning/latest/dg/cross-validation.html</a></p>", "answers": ["<p>Holdout cross-validation.</p>", "<p>K-fold cross-validation.</p>", "<p>Stratified K-fold cross validation.</p>", "<p>RMSE.</p>"]}, "correct_response": ["b"], "section": "MLS Domain 3 - Modeling", "question_plain": "A company wants to predict the most suitable type of house for given requirements using machine learning. A machine learning engineered was given a dataset containing, but not limited to, \u201cSize\u201d, \u201clocation\u201d, \u201cparks\u201d and \u201cnumber_bedrooms\u201d. The target feature is the type of the house which contains either \u201cmodern\u201d or \u201cold\u201d and the labels are balanced through the entire dataset. He produced 2 different models using SageMaker\u2019s XGBoost and linear learner. The company wants the models to be evaluated without bias and an estimated average error should be presented for each model.Which is the most suitable method the engineer should use to achieve these results?", "related_lectures": []}, {"_class": "assessment", "id": 64762222, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A machine learning company uses a hybrid architecture where it has some servers on-premises and others on the AWS cloud. The company receives data on a daily basis collected from other companies to train and deploy custom machine learning models.</p><p>The company wants to use a manageable, automated service which could move data over the network, while handling encryption, between their on-premises storage and AWS storage services, and also between their AWS storage services.</p><p>Which service should the company use?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>AWS states: \u201cAWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between on-premises storage systems and AWS storage services, and also between AWS storage services. DataSync can copy data between Network File System (NFS), Server Message Block (SMB) file servers, Hadoop Distributed File System (HDFS), self-managed object storage, <a href=\"https://docs.aws.amazon.com/snowball/latest/snowcone-guide/snowcone-what-is-snowcone.html\">AWS Snowcone</a>, Amazon Simple Storage Service (<a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\">Amazon S3</a>) buckets, <a href=\"https://docs.aws.amazon.com/efs/latest/ug/\">Amazon EFS</a> file systems, and <a href=\"https://docs.aws.amazon.com/fsx/latest/WindowsGuide/\">Amazon FSx for Windows File Server</a> file systems.\u201d</p><p><strong>CORRECT: </strong>\" AWS DataSync.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \" AWS Storage Gateway.\" is incorrect.</p><p>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage. It gives access to the data on cloud; however, it doesn\u2019t migrate data using encryption methods as DataSync.</p><p><strong>INCORRECT:</strong> \" AWS Direct Connect.\" is incorrect.</p><p>AWS Direct Connect is a cloud service that links your network directly to AWS, bypassing the internet to deliver more consistent, lower-latency performance. This is a direct connection between on-premises network to the cloud network. However, it is not a managed service that automatically transfers data using encryption methods such as DataSync.</p><p><strong>INCORRECT:</strong> \" AWS Snowmobile.\" is incorrect.</p><p>AWS Snowmobile is an exabyte-scale data transport solution that uses a secure semi-40-foot shipping container to transfer large amounts of data into and out of AWS. The company should book this truck in advance to transfer data.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/datasync/latest/userguide/other-use-cases.html\">https://docs.aws.amazon.com/datasync/latest/userguide/other-use-cases.html</a></p>", "answers": ["<p>AWS DataSync.</p>", "<p>AWS Storage Gateway.</p>", "<p>AWS Direct Connect.</p>", "<p>AWS Snowmobile.</p>"]}, "correct_response": ["a"], "section": "MLS Domain 1 - Data Engineering", "question_plain": "A machine learning company uses a hybrid architecture where it has some servers on-premises and others on the AWS cloud. The company receives data on a daily basis collected from other companies to train and deploy custom machine learning models.The company wants to use a manageable, automated service which could move data over the network, while handling encryption, between their on-premises storage and AWS storage services, and also between their AWS storage services.Which service should the company use?", "related_lectures": []}, {"_class": "assessment", "id": 64762224, "assessment_type": "multiple-choice", "prompt": {"question": "<p>A well-known car\u2019s manufacturer decided to initiate a research and development project to produce self-driving cars in the near future. A team consisting of machine learning engineers and data engineers are supposed to produce a model by the end of the year.</p><p>They should first plant a camera above the company\u2019s cars to collect images of the environment around them. The cameras should take separate frames, and these frames are saved to train them in a model later. Then, they should implement a model where it could detect different object types such as \u201cTrash bins\u201d, \u201cpeople\u201d, \u201cpavement\u201d and many more using bounding boxes where the dimensions of the boxes are known beforehand.</p><p>Which SageMaker algorithm should they implement to successfully train their model on this task?</p>", "relatedLectureIds": "", "feedbacks": ["", "", "", ""], "explanation": "<p>This could be confusing as \u201cImage classification\u201d, \u201cObject detection\u201d and \u201cSemantic segmentation\u201d nearly point to the same task. However, these 3 are different types of algorithms used for different scenarios.</p><ul><li><p>Image classification: The Amazon SageMaker image classification algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. Despite telling us what objects are in the image, it does not tell us where these objects are within the image\u2019s dimension.</p></li><li><p>Object detection: The Amazon SageMaker Object Detection algorithm detects and classifies objects in images using a single deep neural network. It is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene <strong>using boundary boxes to determine the object\u2019s position</strong>.</p></li><li><p>Semantic segmentation: The SageMaker semantic segmentation algorithm provides <strong>a fine-grained, pixel-level approach</strong> to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes.</p></li></ul><p>AWS states: \u201cFor comparison, the SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\">Image Classification Algorithm</a> is a supervised learning algorithm that analyses only whole images, classifying them into one of multiple output categories. The <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\">Object Detection Algorithm</a> is a supervised learning algorithm that detects and classifies all instances of an object in an image. It indicates the location and scale of each object in the image with a rectangular bounding box.\u201d</p><p><strong>CORRECT: </strong>\" Object detection algorithm.\" is the correct answer (as explained above.)</p><p><strong>INCORRECT:</strong> \"Image classification algorithm.\" is incorrect.</p><p>Image classification algorithm only classifies the objects in the image; however, it does not provide pixel-level classification.</p><p><strong>INCORRECT:</strong> \"Semantic Segmentation algorithm.\" is incorrect.</p><p>The SageMaker Semantic segmentation algorithm provides <strong>a fine-grained, pixel-level approach</strong> to developing computer vision applications. It tags every pixel in an image with a class label from a predefined set of classes</p><p><strong>INCORRECT:</strong> \"Object2Vec algorithm.\" is incorrect.</p><p>This is not a computer vision algorithm; however, it is a somehow general form of \u201cword2vec\u201d generalized to handle more complex objects than just words. AWS states: \u201cIt can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings are learned in a way that preserves the semantics of the relationship between pairs of objects in the original space in the embedding space.\u201d</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html</a></p>", "answers": ["<p>Image classification algorithm.</p>", "<p>Object detection algorithm.</p>", "<p>Semantic segmentation algorithm.</p>", "<p>Object2Vec algorithm</p>"]}, "correct_response": ["b"], "section": "MLS Domain 4 - Machine Learning Implementation and Operation", "question_plain": "A well-known car\u2019s manufacturer decided to initiate a research and development project to produce self-driving cars in the near future. A team consisting of machine learning engineers and data engineers are supposed to produce a model by the end of the year.They should first plant a camera above the company\u2019s cars to collect images of the environment around them. The cameras should take separate frames, and these frames are saved to train them in a model later. Then, they should implement a model where it could detect different object types such as \u201cTrash bins\u201d, \u201cpeople\u201d, \u201cpavement\u201d and many more using bounding boxes where the dimensions of the boxes are known beforehand.Which SageMaker algorithm should they implement to successfully train their model on this task?", "related_lectures": []}]}
